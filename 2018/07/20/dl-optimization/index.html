<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>[dl] optimization algorithm in deep learning | LucasX</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="Data ScienceMachine LearningDeep LearningOptimization" />
  
  
  
  
  <meta name="description" content="介绍与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，">
<meta name="keywords" content="Data Science,Machine Learning,Deep Learning,Optimization">
<meta property="og:type" content="article">
<meta property="og:title" content="[DL] Optimization Algorithm in Deep Learning">
<meta property="og:url" content="https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/index.html">
<meta property="og:site_name" content="LucasX">
<meta property="og:description" content="介绍与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg">
<meta property="og:updated_time" content="2019-11-01T15:48:27.854Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[DL] Optimization Algorithm in Deep Learning">
<meta name="twitter:description" content="介绍与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg">
  
    <link rel="alternate" href="/atom.xml" title="LucasX" type="application/atom+xml">
  

  

  <link rel="icon" href="/blog/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/blog/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/blog/css/style.css">

  <script src="/blog/js/jquery-3.1.1.min.js"></script>
  <script src="/blog/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/blog/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/blog/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/blog/css/header-post.css" >
  

  
  
  
    <link rel="stylesheet" href="/blog/css/vdonate.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/blog/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/blog/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/about">About</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/papers">Papers</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/projects">Projects</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/archives">Archives</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/blog/',
        CONTENT_URL: '/blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/blog/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-dl-optimization" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      [DL] Optimization Algorithm in Deep Learning
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/blog/2018/07/20/dl-optimization/" class="article-date">
	  <time datetime="2018-07-20T03:46:37.000Z" itemprop="datePublished">2018-07-20</time>
	</a>

      
      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，然后SGD/Adam Optimizer一波带走，但笔者还是觉得有必要把这些优化算法来一个自己的整理与总结的。</p>
<blockquote>
<p>注：本文大多数内容来自花书《<a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning</a>》，详情请阅读原著！</p>
</blockquote>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>Gradient Descent旨在朝”下坡”移动，而非明确寻求临界点。而牛顿法的目标是寻求梯度为0的点。</li>
<li>Gradient Clipping基本思想来源于梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统Gradient Descent算法提议更新很大一步时，启发式Gradient Clipping会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。</li>
<li>假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么$t$步之后，相当于乘以$W^t$，假设有特征值分解$W=V diag(\lambda)V^{-1}$，在这种情况下，很容易看出：<br>$$<br>W^t=(V diag(\lambda)V^{-1})^{t}=V diag(\lambda)^tV^{-1}<br>$$<br>因此，当特征值$\lambda_i$ 不在$1$附近时，若在量级上大于1则会出现Gradient Exploding；若小于$1$时，则会出现Gradient Vanishing。</li>
</ul>
<h2 id="Basic-Algorithm"><a href="#Basic-Algorithm" class="headerlink" title="Basic Algorithm"></a>Basic Algorithm</h2><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>SGD是如今深度学习领域应用非常广泛的一种优化算法，它按照数据生产分布抽取$m$ 个mini-batch (独立同分布)样本，通过计算这些mini-batch的梯度均值，我们可以得到梯度的无偏估计。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg" alt="SGD"></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>为了加速训练，Momentum积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。Momentum主要目的为了解决Hessian矩阵的病态条件和随机梯度的方差。<br>$$<br>v\leftarrow \alpha v-\epsilon \bigtriangledown_{\theta}(\frac{1}{m}\sum_{i=1}^m L(f(x^{(i)};\theta),y^{(i)})\\<br>\theta\leftarrow \theta + v<br>$$</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg" alt="Momentum"><br>SGD中步长只是梯度范数乘以学习率，现在步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果Momentum总是观测到梯度 $g$,那么它会在方向$-g$ 上不停加速，直到达到最终速度，其中步长大小为：<br>$$<br>\frac{\epsilon||g||}{1-\alpha}<br>$$<br>因此将Momentum超参数视为$\frac{1}{1-\alpha}$有助于理解。例如$\alpha=0.9$ 对应着最大速度10倍于Gradient Descent。</p>
<h3 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h3><p>更新规则如下：<br>$$<br>v\leftarrow \alpha v-\epsilon \bigtriangledown_{\theta}(\frac{1}{m}\sum_{i=1}^m L(f(x^{(i)};\theta+\alpha v),y^{(i)})\\<br>\theta\leftarrow \theta+v<br>$$<br>Nesterov和标准Momentum之间的区别在于梯度计算上，Nesterov中，梯度计算在施加Momentum之后。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg" alt="Nesterov"></p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>Learning rate是一个非常难以调整的超参数之一，如果我们相信方向敏感度在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动使用这些学习率是合理的。</p>
<p>AdaGrad是自适应学习率算法的一种。它独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg" alt="AdaGrad"></p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>RMSProp修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均，AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。<strong>RMSProp使用指数衰减平均以丢弃遥远过去的历史</strong>，使其能够在找到凸碗结构后快速收敛。</p>
<ul>
<li><p>Standard RMSProp<br><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg" alt="RMSProp"></p>
</li>
<li><p>RMSProp with Nesterov<br><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg" alt="RMSProp with Nesterov"></p>
</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>在Adam中，动量直接并入了梯度一阶矩(指数加权)的估计。将动量加入RMSProp最直接的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次，Adam包括偏置修正，修正从原点初始化的一阶矩(动量项)，和非中心的二阶矩的估计。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg" alt="Adam"></p>
<h2 id="二阶近似方法"><a href="#二阶近似方法" class="headerlink" title="二阶近似方法"></a>二阶近似方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>牛顿法是基于二阶泰勒级数展开在某点$\theta_0$附近来近似$J(\theta)$的优化方法，其忽略了高阶导数：<br>$$<br>J(\theta)\approx J(\theta_0)+(\theta-\theta_0)^T\bigtriangledown_{\theta} J(\theta_0) + \frac{1}{2}(\theta-\theta_0)^T H(\theta-\theta_0)<br>$$</p>
<p>更新规则：<br>$$<br>\theta^{\star}=\theta_0-H^{-1}\bigtriangledown _{\theta}J(\theta_0)<br>$$<br>因此，<strong>对于局部的二次函数(具有正定的$H$)，用$H^{-1}$重新调整梯度，牛顿法会直接跳到极小值</strong>。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg" alt="Newton&#39;s Method"></p>
<p>Deep Learning中，Loss Function表明通常是非凸的(有很多特征)，如鞍点。因此使用Newton’s Method是有问题的，若Hessian Matrix的特征值并不都是正的，Newton’s Method实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian Matrix来避免。常用的正则化策略包括在Hessian Matrix对角线上增加常数$\alpha$。正则化更新变为：<br>$$<br>\theta^{\star}=\theta_0-[H(f(\theta_0))+\alpha I]^{-1}\bigtriangledown _{\theta}J(\theta_0)<br>$$</p>
<h3 id="共轭梯度"><a href="#共轭梯度" class="headerlink" title="共轭梯度"></a>共轭梯度</h3><p>通过迭代下降的共轭方向以有效避免Hessian Matrix求逆计算的方法。</p>
<h3 id="BFGS"><a href="#BFGS" class="headerlink" title="BFGS"></a>BFGS</h3><p>BFGS是使用矩阵$M_t$近似逆，迭代地低秩更新精度以更好地近似$H^{-1}$。当Hessian逆近似$M_t$更新时，下降方向$\rho_t$为$\rho_t=M_tg_t$。该方向上的线性搜索用于决定该方向上的步长$\epsilon^{\star}$。参数的最后更新为：<br>$$<br>\theta_{t+1}=\theta_t + \epsilon^{\star}\rho_t<br>$$<br>相比于共轭梯度，BFGS的优点在于其花费较少的时间改进每个线搜索。另一方面，BFGS算法必须存储必须存储Hessian 逆矩阵$M$，需要$O(n^2)$的存储空间，使BFGS不适用于大多数参数巨大的Deep Model。</p>
<h2 id="优化策略和元算法"><a href="#优化策略和元算法" class="headerlink" title="优化策略和元算法"></a>优化策略和元算法</h2><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>设$H$是需要标准化的某层mini batch激活函数，每个样本的激活出现在矩阵的每一行中。为了标准化$H$，我们将其替换为<br>$$<br>H^{‘}=\frac{H-\mu}{\sigma}<br>$$</p>
<p>$$<br>\mu=\frac{1}{m}\sum_i H_{i,:}<br>$$</p>
<p>$$<br>\sigma = \sqrt{\delta+\frac{1}{m}\sum_i (H-\mu)_i^2}<br>$$<br>$\delta$是个很小的正值，以避免遇到$\sqrt{z}$的梯度在$z=0$处未定义的问题。</p>
<p>至关重要的是，我们反向传播这些操作，来计算$\mu$和$\sigma$，并应用它们于标准化$H$。这意味着，梯度不会再简单地增加$h_i$的标准差或均值；BatchNorm会消除这一操作的影响，归零其在梯度中的元素。</p>
<p>在测试阶段，$\mu$和$\sigma$可以被替换为训练阶段收集的运行均值。这使得模型可以对单一样本评估，而无需使用定义于整个mini-batch的$\mu$和$\sigma$。</p>

      
    </div>
    <footer class="article-footer">
      
        <div id="donation_div"></div>

<script src="/blog/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: 'Donate', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/WeChatPay.png',
  alipayImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/Alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>LucasX</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/blog/2018/07/20/dl-optimization/" target="_blank" title="[DL] Optimization Algorithm in Deep Learning">https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2018/07/22/ml-svm/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [ML] SVM
        
      </div>
    </a>
  
  
    <a href="/blog/2018/07/19/ml-nb/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[ML] Naive Bayes</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">2.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Algorithm"><span class="nav-number">3.</span> <span class="nav-text">Basic Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SGD"><span class="nav-number">3.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Momentum"><span class="nav-number">3.2.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Nesterov"><span class="nav-number">3.3.</span> <span class="nav-text">Nesterov</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaGrad"><span class="nav-number">3.4.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSProp"><span class="nav-number">3.5.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">3.6.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二阶近似方法"><span class="nav-number">4.</span> <span class="nav-text">二阶近似方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#牛顿法"><span class="nav-number">4.1.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共轭梯度"><span class="nav-number">4.2.</span> <span class="nav-text">共轭梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BFGS"><span class="nav-number">4.3.</span> <span class="nav-text">BFGS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化策略和元算法"><span class="nav-number">5.</span> <span class="nav-text">优化策略和元算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">5.1.</span> <span class="nav-text">Batch Normalization</span></a></li></ol></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2018 - 2020 LucasX All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/about" class="mobile-nav-link">About</a>
  
    <a href="/blog/papers" class="mobile-nav-link">Papers</a>
  
    <a href="/blog/projects" class="mobile-nav-link">Projects</a>
  
    <a href="/blog/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/scripts.js"></script>




  <script src="/blog/js/dialog.js"></script>








	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            LucasX
          </div>
          <div class="panel-body">
            Copyright © 2020 LucasX All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</body>
</html>