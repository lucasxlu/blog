<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>[llm] parameter efficient fine-tuning | LucasXU</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="Machine LearningDeep LearningLarge Language ModelFoundation ModelFine-tuning" />
  
  
  
  
  <meta name="description" content="Introduction大模型的训练通常包含两个阶段：self-supervised pre-training 来学习 general representation（近些年的趋势是堆大模型 + 自监督学习作为 Foundation Model），典型的代表性算法有 BERT、MAE、MoCo 系列以及 CLIP 为代表的多模态预训练大模型等supervised fine-tuning 在下游任务上">
<meta name="keywords" content="Machine Learning,Deep Learning,Large Language Model,Foundation Model,Fine-tuning">
<meta property="og:type" content="article">
<meta property="og:title" content="[LLM] Parameter Efficient Fine-tuning">
<meta property="og:url" content="https://lucasxlu.github.io/blog/2023/12/02/llm-ft/index.html">
<meta property="og:site_name" content="LucasXU">
<meta property="og:description" content="Introduction大模型的训练通常包含两个阶段：self-supervised pre-training 来学习 general representation（近些年的趋势是堆大模型 + 自监督学习作为 Foundation Model），典型的代表性算法有 BERT、MAE、MoCo 系列以及 CLIP 为代表的多模态预训练大模型等supervised fine-tuning 在下游任务上">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/dmt.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/deep_incubation_pipeline.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/deep_incubation_algo.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/spot_tune.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/spot_tune_exp.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/peft.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/bitfit_exp.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/scaled_pa.png">
<meta property="og:updated_time" content="2023-12-03T02:55:36.643Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[LLM] Parameter Efficient Fine-tuning">
<meta name="twitter:description" content="Introduction大模型的训练通常包含两个阶段：self-supervised pre-training 来学习 general representation（近些年的趋势是堆大模型 + 自监督学习作为 Foundation Model），典型的代表性算法有 BERT、MAE、MoCo 系列以及 CLIP 为代表的多模态预训练大模型等supervised fine-tuning 在下游任务上">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/dmt.png">
  
    <link rel="alternate" href="/atom.xml" title="LucasXU" type="application/atom+xml">
  

  

  <link rel="icon" href="/blog/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/blog/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/blog/css/style.css">

  <script src="/blog/js/jquery-3.1.1.min.js"></script>
  <script src="/blog/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/blog/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/blog/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/blog/css/header-post.css" >
  

  
  
  
    <link rel="stylesheet" href="/blog/css/vdonate.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/blog/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/blog/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/projects">Projects</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/archives">Archives</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/blog/',
        CONTENT_URL: '/blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/blog/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-llm-ft" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      [LLM] Parameter Efficient Fine-tuning
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/blog/2023/12/02/llm-ft/" class="article-date">
	  <time datetime="2023-12-02T14:04:06.000Z" itemprop="datePublished">2023-12-02</time>
	</a>

      
      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>大模型的训练通常包含两个阶段：<br><code>self-supervised pre-training</code> 来学习 <code>general representation</code>（近些年的趋势是堆大模型 + 自监督学习作为 <code>Foundation Model</code>），典型的代表性算法有 <code>BERT</code>、<code>MAE</code>、<code>MoCo</code> 系列以及 <code>CLIP</code> 为代表的多模态预训练大模型等<br><code>supervised fine-tuning</code> 在下游任务上进行适配<br><code>SFT (Supervised Fine-tuning)</code> 主要指的是第二部分，在模型参数量/数据量越来越大的背景下，<code>full fine-tuning</code> 成本太高，因此学术界&amp;工业界开始探索 <code>PEFT (Parameter Efficient Fine-tuning)</code>。<code>PEFT</code> 不仅能极大地提升 <code>fine-tuning</code> 效率，也能够很好地处理 <code>catastrophic forgetting</code> 以及 <code>OOD (Out of Distribution)</code> 问题。</p>
<h3 id="FT-方式"><a href="#FT-方式" class="headerlink" title="FT 方式"></a>FT 方式</h3><h4 id="DMT"><a href="#DMT" class="headerlink" title="DMT"></a>DMT</h4><p>大模型混合多种能力项数据进行微调时，会呈现高资源冲突，低资源增益的现象。《<a href="https://arxiv.org/pdf/2310.05492.pdf" target="_blank" rel="noopener">HOW ABILITIES IN LARGE LANGUAGE MODELS ARE AFFECTED BY SUPERVISED FINE-TUNING DATA COMPOSITION</a>》提出的 <code>DMT(Dual-stage Mixed Fine-tuning)</code> 策略通过在第一阶段微调特定能力数据，在第二阶段微调通用数据+少量的特定能力数据，可以在保留通用能力的同时，极大程度地挽救大模型对特定能力的灾难性遗忘，这为 SFT 的数据组成问题提供了一个简单易行的训练策略。值得注意的是，第二阶段微调时混合的特定能力数据量需要根据需求而定。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/dmt.png" alt="DMT"></p>
<h4 id="Deep-Incubation"><a href="#Deep-Incubation" class="headerlink" title="Deep Incubation"></a>Deep Incubation</h4><p>针对大模型训练 independency &amp; compatibility 冲突的问题，设计了一种基于 Meta Model 的训练框架。且这种训练方式要比 E2E 效率 &amp; 效果更优。<br>先训练一个 Meta Model 来 link 所有的 sub-module<br>再单独训练 sub-module，然后 replace Meta Model 中对应的部分</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/deep_incubation_pipeline.png" alt="Deep Incubation Pipeline"></p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/deep_incubation_algo.png" alt="Deep Incubation Algorithm"></p>
<h4 id="SpotTune"><a href="#SpotTune" class="headerlink" title="SpotTune"></a>SpotTune</h4><p>在最常见的 SFT 训练方式中，「freeze shared layer、fine-tune task layer」以及「full fine-tuning」是最常见的做法。但是前者在 target task 上的效果往往不如后者，而后者又容易产生 <code>catastrophic forgetting</code> 问题，且在 target dataset 不足的时候会存在严重的过拟合现象。针对这个问题，作者采用了一个 <code>policy network</code> 来在 instance-level 判断 input image 应该传到 fine-tuned layers 还是 pre-trained layers。在实践中，以一个 residual/transformer block 为单位，<code>policy network</code> 判断该 block 是否该被 freeze or fine-tune。大致思路如下：</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/spot_tune.png" alt="SpotTune"></p>
<p>值得一提的是，<code>SpotTune</code> 不仅效率高，而且在下游的许多任务上，效果甚至超过了 full fine-tuning：</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/spot_tune_exp.png" alt="SpotTune Experiment"></p>
<h3 id="PEFT"><a href="#PEFT" class="headerlink" title="PEFT"></a>PEFT</h3><p>PEFT (Parameter Efficient Fine-tuning) freeze 大部分参数，只更新少量参数。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/peft.png" alt="SpotTune Experiment"></p>
<h4 id="Infused-Fine-tuning"><a href="#Infused-Fine-tuning" class="headerlink" title="Infused Fine-tuning"></a>Infused Fine-tuning</h4><h5 id="Adapter-based"><a href="#Adapter-based" class="headerlink" title="Adapter-based"></a>Adapter-based</h5><ul>
<li>P-tuning：仅针对 LLM embedding 加入新的参数</li>
<li>P-tuning V2：将 LLM 的 embedding 和每一层前都加上新的参数</li>
</ul>
<h5 id="Sparse-Fine-tuning"><a href="#Sparse-Fine-tuning" class="headerlink" title="Sparse Fine-tuning"></a>Sparse Fine-tuning</h5><h6 id="BitFit"><a href="#BitFit" class="headerlink" title="BitFit"></a>BitFit</h6><p>仅仅 fine-tune bias 参数，weight 参数保持不变。居然能够和 fine-tune 所有参数达到相近的 performance。这也引出了大模型的另一个话题：finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/bitfit_exp.png" alt="SpotTune Experiment"></p>
<h6 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h6><h6 id="Soft-Prompt"><a href="#Soft-Prompt" class="headerlink" title="Soft Prompt"></a>Soft Prompt</h6><blockquote>
<p>【附录】Hard Prompt：因为大模型具备较强的 zero-shot/few-shot 能力，所以 Hard Prompt 主要用 prompt 来提升 LLM 的效果。</p>
</blockquote>
<p>因为 prompt text 属于 discrete input，当 training examples 很多的时候，优化起来比较困难。所以就有了 soft prompt，soft prompt 指的是在 input embedding 层面添加一层 trainable layer，使其能够参与 BP 更新网络参数，伪代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soft_prompted_model</span><span class="params">(input_ids)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    PyTorch implementation of soft prompt</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_ids: input token id list</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = Embed(input_ids)</span><br><span class="line">    x = torch.cat([soft_prompt, x], dim=seq)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model(x)</span><br></pre></td></tr></table></figure>
<h5 id="Hybrid"><a href="#Hybrid" class="headerlink" title="Hybrid"></a>Hybrid</h5><p>鉴于 <code>LoRA</code>、<code>prompt-tuning</code>、<code>adapter</code> 的有效性，<code>scaled PA</code> 提出了一种 unified framework 来同时融合这几种 fine-tune 方案。</p>
<p><img src="https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/llm-ft/scaled_pa.png" alt="SpotTune Experiment"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://huggingface.co/docs/peft/index" target="_blank" rel="noopener">Parameter Efficient Fine-tuning</a></li>
<li>Dong G, Yuan H, Lu K, et al. <a href="https://arxiv.org/pdf/2310.05492.pdf" target="_blank" rel="noopener">How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition</a>[J]. arXiv preprint arXiv:2310.05492, 2023.</li>
<li>Liu X, Zheng Y, Du Z, et al. <a href="https://arxiv.org/pdf/2310.05492.pdf" target="_blank" rel="noopener">GPT understands, too</a>[J]. AI Open, 2023.</li>
<li>Houlsby N, Giurgiu A, Jastrzebski S, et al. <a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf" target="_blank" rel="noopener">Parameter-efficient transfer learning for NLP</a>[C]//International Conference on Machine Learning. PMLR, 2019: 2790-2799.</li>
<li>Zaken E B, Ravfogel S, Goldberg Y. <a href="https://arxiv.org/pdf/2106.10199" target="_blank" rel="noopener">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a>[J]. arXiv preprint arXiv:2106.10199, 2021.</li>
<li>He J, Zhou C, Ma X, et al. <a href="https://arxiv.org/pdf/2110.04366" target="_blank" rel="noopener">Towards a unified view of parameter-efficient transfer learning</a>[J]. arXiv preprint arXiv:2110.04366, 2021.</li>
<li>Ding N, Qin Y, Yang G, et al. <a href="https://www.nature.com/articles/s42256-023-00626-4" target="_blank" rel="noopener">Parameter-efficient fine-tuning of large-scale pre-trained language models</a>[J]. Nature Machine Intelligence, 2023, 5(3): 220-235.</li>
<li>Lialin V, Deshpande V, Rumshisky A. <a href="https://arxiv.org/pdf/2303.15647" target="_blank" rel="noopener">Scaling down to scale up: A guide to parameter-efficient fine-tuning</a>[J]. arXiv preprint arXiv:2303.15647, 2023.</li>
<li>Ni Z, Wang Y, Yu J, et al. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Deep_Incubation_Training_Large_Models_by_Divide-and-Conquering_ICCV_2023_paper.pdf" target="_blank" rel="noopener">Deep incubation: Training large models by divide-and-conquering</a>[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 17335-17345.</li>
<li>Guo Y, Shi H, Kumar A, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_SpotTune_Transfer_Learning_Through_Adaptive_Fine-Tuning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Spottune: transfer learning through adaptive fine-tuning</a>[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 4805-4814.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <div id="donation_div"></div>

<script src="/blog/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: 'Donate', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/WeChatPay.png',
  alipayImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/Alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>LucasXU</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/blog/2023/12/02/llm-ft/" target="_blank" title="[LLM] Parameter Efficient Fine-tuning">https://lucasxlu.github.io/blog/2023/12/02/llm-ft/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2024/05/22/dl-robustness/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [DL] Improving Robustness of Deep Learning Models
        
      </div>
    </a>
  
  
    <a href="/blog/2023/04/15/dl-faiss/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[DL] A Deeper Look At Faiss</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FT-方式"><span class="nav-number">1.1.</span> <span class="nav-text">FT 方式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DMT"><span class="nav-number">1.1.1.</span> <span class="nav-text">DMT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Incubation"><span class="nav-number">1.1.2.</span> <span class="nav-text">Deep Incubation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SpotTune"><span class="nav-number">1.1.3.</span> <span class="nav-text">SpotTune</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PEFT"><span class="nav-number">1.2.</span> <span class="nav-text">PEFT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Infused-Fine-tuning"><span class="nav-number">1.2.1.</span> <span class="nav-text">Infused Fine-tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Adapter-based"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Adapter-based</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sparse-Fine-tuning"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Sparse Fine-tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#BitFit"><span class="nav-number">1.2.1.2.1.</span> <span class="nav-text">BitFit</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#LoRA"><span class="nav-number">1.2.1.2.2.</span> <span class="nav-text">LoRA</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Soft-Prompt"><span class="nav-number">1.2.1.2.3.</span> <span class="nav-text">Soft Prompt</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hybrid"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Hybrid</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2018 - 2024 LucasXU All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/projects" class="mobile-nav-link">Projects</a>
  
    <a href="/blog/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/scripts.js"></script>




  <script src="/blog/js/dialog.js"></script>








	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            LucasXU
          </div>
          <div class="panel-body">
            Copyright © 2024 LucasXU All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</body>
</html>