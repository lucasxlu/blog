{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/about/LICENSE","path":"about/LICENSE","modified":1,"renderable":0},{"_id":"source/about/CV_LuXu.pdf","path":"about/CV_LuXu.pdf","modified":1,"renderable":0},{"_id":"source/about/LucasX.jpg","path":"about/LucasX.jpg","modified":1,"renderable":0},{"_id":"source/images/WeChatPay.png","path":"images/WeChatPay.png","modified":1,"renderable":0},{"_id":"themes/hiker/source/css/archive.css","path":"css/archive.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/dialog.css","path":"css/dialog.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/header-post.css","path":"css/header-post.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/home.css","path":"css/home.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/vdonate.css","path":"css/vdonate.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/dialog.js","path":"js/dialog.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/home.js","path":"js/home.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/scripts.js","path":"js/scripts.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/vdonate.js","path":"js/vdonate.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/browser-support.png","path":"preview/browser-support.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/donation-btn.png","path":"preview/donation-btn.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/preview-abstract.png","path":"preview/preview-abstract.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/theme-color.png","path":"preview/theme-color.png","modified":1,"renderable":1},{"_id":"source/images/Alipay.jpg","path":"images/Alipay.jpg","modified":1,"renderable":0},{"_id":"themes/hiker/source/js/bootstrap.js","path":"js/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/jquery-3.1.1.min.js","path":"js/jquery-3.1.1.min.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/bootstrap.css","path":"css/bootstrap.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/avatar.jpg","path":"css/images/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/homelogo.jpg","path":"css/images/homelogo.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/mylogo.jpg","path":"css/images/mylogo.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/rocket.png","path":"css/images/rocket.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/preview-mobile.png","path":"preview/preview-mobile.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/home-bg.jpg","path":"css/images/home-bg.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/sample.jpg","path":"css/images/sample.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/pose.jpg","path":"css/images/pose.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/code-theme.jpg","path":"preview/code-theme.jpg","modified":1,"renderable":1},{"_id":"source/projects/LagouJob.pdf","path":"projects/LagouJob.pdf","modified":1,"renderable":0},{"_id":"source/about/Presentation_TransFBP.pdf","path":"about/Presentation_TransFBP.pdf","modified":1,"renderable":0},{"_id":"themes/hiker/source/preview/preview-pc.png","path":"preview/preview-pc.png","modified":1,"renderable":1},{"_id":"source/projects/MLJob.pdf","path":"projects/MLJob.pdf","modified":1,"renderable":0},{"_id":"source/about/Research_Overview.pdf","path":"about/Research_Overview.pdf","modified":1,"renderable":0},{"_id":"source/projects/JiaYuan.pdf","path":"projects/JiaYuan.pdf","modified":1,"renderable":0},{"_id":"source/about/Presentation.pdf","path":"about/Presentation.pdf","modified":1,"renderable":0},{"_id":"source/about/DL_for_Face_Analysis.pdf","path":"about/DL_for_Face_Analysis.pdf","modified":1,"renderable":0},{"_id":"source/projects/GovReport.pdf","path":"projects/GovReport.pdf","modified":1,"renderable":0},{"_id":"source/projects/IndustryReport.pdf","path":"projects/IndustryReport.pdf","modified":1,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"c874a9c65eaf1b3e4f81bfaf1f09876474f56c50","modified":1541216985174},{"_id":"themes/hiker/.gitignore","hash":"ea2b285a29690f1eabbad0f3a158e34e9ccd1d86","modified":1538368810463},{"_id":"themes/hiker/.travis.yml","hash":"7ed5eb33c899eb49ec323f7ed7ee431bea52bf4f","modified":1538368810464},{"_id":"themes/hiker/Gruntfile.js","hash":"412e30530784993c8997aa8b1319c669b83b91c2","modified":1538368810464},{"_id":"themes/hiker/LICENSE","hash":"df913ae888823f1551d5f1837902f7ccb2634459","modified":1538368810465},{"_id":"themes/hiker/README.md","hash":"2bc2899d048e6df9dff5ad01bdf550bb6a158e24","modified":1538368810466},{"_id":"themes/hiker/_config.yml","hash":"9f5d5f747eb104f520d645b350457dfd6d4fa6d9","modified":1538368810467},{"_id":"themes/hiker/package.json","hash":"5800ffd530e6ec7fb9e3f261b5e3a1286c2b54f6","modified":1538368812127},{"_id":"source/_posts/algo-sort.md","hash":"bcc5c47d727484142d8c72356a4347910f26df09","modified":1538368808403},{"_id":"source/_posts/book-storytelling-with-data.md","hash":"d630523771033fabcfa2b7f753db3bf3fa890e44","modified":1538368808404},{"_id":"source/_posts/cv-antispoofing.md","hash":"0c3107be0dcc936ef961c349d6b50541b743be20","modified":1540830325350},{"_id":"source/_posts/cv-detection.md","hash":"306792e8ba035a6d3db84ff294e87dfbcf89b554","modified":1542367993192},{"_id":"source/_posts/cv-face-rec.md","hash":"b13cb50b6cdb39522cdc5e15d43fa12854f328a8","modified":1538453982047},{"_id":"source/_posts/cv-iqa.md","hash":"71763dccf1cfc783a873739937c1dc73990d4f59","modified":1541340827400},{"_id":"source/_posts/dip-image-feature.md","hash":"e85e7d437fa64e75c64e0b28c32316d249bf41e3","modified":1538368808551},{"_id":"source/_posts/dl-ae.md","hash":"806a6bb62806f53e9096c6432bb7932759f1f054","modified":1538368808655},{"_id":"source/_posts/dl-architecture.md","hash":"dbb386deb2134506295702b265d7ecc703530e92","modified":1542551356777},{"_id":"source/_posts/dl-bn.md","hash":"4189407c07a8ef7e8a0faf44fa5134ddec35b5f2","modified":1542730625441},{"_id":"source/_posts/dl-bp.md","hash":"694363e13949ed24cde363d87334a5c3bacf6317","modified":1538368808673},{"_id":"source/_posts/dl-cnn.md","hash":"6852d9ab047b7759dfad7fbb8a224e39ecc7e00e","modified":1538368808836},{"_id":"source/_posts/dl-data-augmentation.md","hash":"749cc06d1c62ba1543f7810cebd2b2bd8952f409","modified":1541862191194},{"_id":"source/_posts/dl-optimization.md","hash":"389df4cf7981593c24187bd077da962fcebb1dac","modified":1538368808837},{"_id":"source/_posts/dl-regularization.md","hash":"a855b01508c19acc1ed9cfd4a0a397aa10a0707b","modified":1538368808901},{"_id":"source/_posts/dl-rnn.md","hash":"0b59ced6bfcafed68ebba04698ea0d8ecb23555d","modified":1538368808921},{"_id":"source/_posts/ml-clustering.md","hash":"d77a99e767eefe7ed92e35a7e7ee85bb6bee0aba","modified":1538368808983},{"_id":"source/_posts/ml-dimen-red-metric-learning.md","hash":"8c456922432af72fe5ea7f28f173f480314f7b62","modified":1538368808986},{"_id":"source/_posts/ml-dt.md","hash":"dbf7935c4401b854ce0176ed307694b271db9889","modified":1538368808993},{"_id":"source/_posts/ml-ensemble.md","hash":"8f727f4e650947015c526f79078cce3ca1951454","modified":1538368808997},{"_id":"source/_posts/ml-feml.md","hash":"6602263e2b8165101f04cffba46af1b898441cc7","modified":1538368809000},{"_id":"source/_posts/ml-knn.md","hash":"dd539203177f55483b9acd2ef0e5004809390ae8","modified":1538368809017},{"_id":"source/_posts/ml-lm.md","hash":"d0b73db8fbf476ce95442cd8b8a0d9265b046045","modified":1538368809047},{"_id":"source/_posts/ml-loss.md","hash":"947bd90a92c907004f33082df3da0206d781a1b7","modified":1538368809048},{"_id":"source/_posts/ml-lr-me.md","hash":"140672851319d2f17fc0dab9bab351f70cd3e0b3","modified":1538368809052},{"_id":"source/_posts/ml-model-selection-metric.md","hash":"0887be5123918022d8b98b5e15d7a77ae5d7203f","modified":1538368809054},{"_id":"source/_posts/ml-nb.md","hash":"6c426058095498e2ded9386f764a623ffca2cc22","modified":1538368809058},{"_id":"source/_posts/ml-svm.md","hash":"35aa11adce9ed6048af3db16dc711b2d18a3032a","modified":1538368809601},{"_id":"source/about/LICENSE","hash":"6cfbca72152ed7d78b31fb88f23b5c6087fff34d","modified":1538368809612},{"_id":"source/about/index.md","hash":"a72099cb184ff6b3755437f4496546dc0cda7c3c","modified":1542537756880},{"_id":"source/archive/index.md","hash":"fe084b4f62929343d70032c307570be2c6cb108c","modified":1538368809635},{"_id":"source/papers/index.md","hash":"250c4e3af4d7bc8c14bdaa65dfdb46e99433b3d6","modified":1542537795437},{"_id":"source/projects/index.md","hash":"a8b840da198db5173709e9dda103f921c4ed2db6","modified":1542505627514},{"_id":"source/tags/index.md","hash":"54f6b44c51d546a9c2fef80d5628f79744ecefc9","modified":1538368810462},{"_id":"themes/hiker/languages/de.yml","hash":"2801ddc0807ce707905203a9f2eb9ccced959991","modified":1538368810940},{"_id":"themes/hiker/languages/default.yml","hash":"6bd574a83c1445fdba9f451cf2cc9b2738e4b5cf","modified":1538368810941},{"_id":"themes/hiker/languages/en.yml","hash":"570f53de74ae233b0fe07abb61d2db7bf2a3f755","modified":1538368810941},{"_id":"themes/hiker/languages/es.yml","hash":"e3b4937da4cd2d0393b8a0ba310e70fc605cc431","modified":1538368810942},{"_id":"themes/hiker/languages/fr.yml","hash":"d67ddc5a00060e67de561b4be7bd14ac4e2d7186","modified":1538368810943},{"_id":"themes/hiker/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1538368810943},{"_id":"themes/hiker/languages/no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1538368811202},{"_id":"themes/hiker/languages/pt.yml","hash":"0ec64b7e134e802846125770782fab9590495bcd","modified":1538368811205},{"_id":"themes/hiker/languages/ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1538368811210},{"_id":"themes/hiker/languages/zh-CN.yml","hash":"17d040c8a0b2964b9d5cce305057cb5264169a0d","modified":1538368811212},{"_id":"themes/hiker/languages/zh-TW.yml","hash":"34d3b2fb9cc61a3f6df7936868fc8877fe08c842","modified":1538368811214},{"_id":"themes/hiker/layout/archive.ejs","hash":"fff20dc39a59641b35bbc921866921375ef584a5","modified":1538368811765},{"_id":"themes/hiker/layout/categories.ejs","hash":"5e8ec5304c76f80bcecb710d83c3dead4cab8a2a","modified":1538368811766},{"_id":"themes/hiker/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1538368811766},{"_id":"themes/hiker/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1538368811767},{"_id":"themes/hiker/layout/layout.ejs","hash":"f4e7d8cdf47f654f8c011ae21bc77f2c71219861","modified":1538368812111},{"_id":"themes/hiker/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1538368812112},{"_id":"themes/hiker/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1538368812113},{"_id":"themes/hiker/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1538368812125},{"_id":"themes/hiker/layout/tags.ejs","hash":"f1f8530e856cb7e7bfed044d2117a4d8b374709a","modified":1538368812126},{"_id":"themes/hiker/scripts/fancybox.js","hash":"4c130fc242cf9b59b5df6ca5eae3b14302311e8c","modified":1538368812131},{"_id":"source/about/CV_LuXu.pdf","hash":"a82c924ef6afad3e065bf180e328af495df75b68","modified":1540621004844},{"_id":"source/about/LucasX.jpg","hash":"b50fed3c6eed4cd39ada918654f11a3b086d02f2","modified":1538368809615},{"_id":"source/images/WeChatPay.png","hash":"b5d525b4ea3741bdd8edc8c5679a23aa2ec86c53","modified":1538368809643},{"_id":"source/_posts/book-storytelling-with-data/bar.png","hash":"29bcf6273e43b3229a3319cb6d1a057936972690","modified":1538368808407},{"_id":"source/_posts/book-storytelling-with-data/double-y-before.png","hash":"a89b89a3650914b6e0b3809c1d85c901dccc311d","modified":1538368808413},{"_id":"source/_posts/book-storytelling-with-data/double-y-revised.png","hash":"01619ff15d1726106b1718dc03b5fcaa7b7ba585","modified":1538368808460},{"_id":"source/_posts/book-storytelling-with-data/less-is-more.png","hash":"17aaf8978cde23669ec012e3efb6f3d03d5c6449","modified":1538368808464},{"_id":"source/_posts/book-storytelling-with-data/numbers.png","hash":"9d86f9507997d383ac1f8ae170db7a835e55cd8f","modified":1538368808466},{"_id":"source/_posts/book-storytelling-with-data/one-line-highlighted.png","hash":"1502949aba98a3ead7d016cb322f6c1e03269e65","modified":1538368808468},{"_id":"source/_posts/book-storytelling-with-data/pie-chart.png","hash":"e2e068280da0937a7866355615b122c737c643bc","modified":1538368808469},{"_id":"source/_posts/book-storytelling-with-data/report_revised.png","hash":"b2f8aa914801bbc5c2a336dad8ec728d13d452b6","modified":1538368808474},{"_id":"source/_posts/book-storytelling-with-data/spaghetti-graph.png","hash":"89f2210659ece1bf9a33124e93350f5eeff18388","modified":1538368808477},{"_id":"source/_posts/book-storytelling-with-data/tables.png","hash":"865d334c5466cde940316943cf2e1855fdc2b429","modified":1538368808479},{"_id":"source/_posts/cv-detection/cw_vs_spp.jpg","hash":"a852caba3d307a51a278ea707a6b16b83472db8b","modified":1538368808489},{"_id":"source/_posts/dl-architecture/dense_block.jpg","hash":"3d8de90b4d0467bcac4ebbf2eac4f71c694e482e","modified":1541434807540},{"_id":"source/_posts/dl-architecture/inception_module.jpg","hash":"f1f60f3ec596923f2ed2800cd053db0db3fed18a","modified":1542549776309},{"_id":"source/_posts/dl-architecture/proposed_residual_unit.jpg","hash":"6018312a7c6d3cc0e77012b9eb7a462d6ab86c3c","modified":1541939890019},{"_id":"source/_posts/dl-bn/BN_update.jpg","hash":"202106fa90dba5144d85b00bb5711bebb3dd146b","modified":1542704411749},{"_id":"source/_posts/dl-bp/fig2.png","hash":"b59eb2fcd6c25e3e4b6e7f92db93c48ed257341a","modified":1538368808801},{"_id":"source/_posts/dl-bp/fig3.png","hash":"8e61dc1b02e406fe50e45afea07b415d04604781","modified":1538368808806},{"_id":"source/_posts/dl-optimization/newtons_method.jpg","hash":"ec21c305192f2bafb9194fb56b4046d5c5b2db46","modified":1538368808886},{"_id":"source/_posts/dl-regularization/es.jpg","hash":"7c9598650d06f0baee27c569e945cf4e249da104","modified":1538368808916},{"_id":"source/_posts/dl-rnn/cg-unfold.jpg","hash":"7054e032d3cbb4107e375fe1d6cd92dbfd27624e","modified":1538368808934},{"_id":"source/_posts/dl-rnn/rnn-bp.jpg","hash":"4cfd440ae9b358a8b694427c84846591e1543e46","modified":1538368808944},{"_id":"source/_posts/dl-rnn/rnn3.jpg","hash":"d7ca30133caa000822317ec7bc3772e2c72b4fcc","modified":1538368808971},{"_id":"source/_posts/dl-rnn/teacher-forcing.jpg","hash":"037418943fffc91804709ea357f088b4570cb14f","modified":1538368808979},{"_id":"source/_posts/ml-feml/blackoff_bin.png","hash":"e6df155725db92f6745df1bb77d1bc2128b083bb","modified":1538368809010},{"_id":"source/_posts/ml-loss/log-cosh.png","hash":"7efea2aaa1339485ef55a2237ba9c33f41400a70","modified":1538368809050},{"_id":"themes/hiker/layout/_partial/after-footer.ejs","hash":"cb3e2715acd7fdde1d4c7d439dec6cae71179887","modified":1538368811219},{"_id":"themes/hiker/layout/_partial/archive-post.ejs","hash":"7f17ae361df4071091b37fe1420247523d20ed90","modified":1538368811222},{"_id":"themes/hiker/layout/_partial/archive.ejs","hash":"11290e1529e50fe7c8953e26e61477635bb4d501","modified":1538368811224},{"_id":"themes/hiker/layout/_partial/article.ejs","hash":"759a140d8f1d522516f652fda348099768e71d65","modified":1538368811225},{"_id":"themes/hiker/layout/_partial/baidu-analytics.ejs","hash":"1edc20ced3c255c09405f0750b6c7bdf328af554","modified":1538368811226},{"_id":"themes/hiker/layout/_partial/busuanzi-analytics.ejs","hash":"02cf1fde1915b20fea7f4d81c34ffa55dbe56e10","modified":1538368811649},{"_id":"themes/hiker/layout/_partial/cnzz-analytics.ejs","hash":"1d6f7c86f5b0f2a7636caace94bef3ed12309dce","modified":1538368811649},{"_id":"themes/hiker/layout/_partial/comment.ejs","hash":"5102c373b39985087854091b7b9f056fe0ba9a1b","modified":1538368811650},{"_id":"themes/hiker/layout/_partial/copyright.ejs","hash":"73d1b18b1f572f10121b85795b69b947bfc56419","modified":1538368811651},{"_id":"themes/hiker/layout/_partial/dialog.ejs","hash":"1b38de12939c21906c66f2fde16b2bc86b23c673","modified":1538368811652},{"_id":"themes/hiker/layout/_partial/donate.ejs","hash":"ce2dc1ca23cc12a6b91651aaf7845508ab95832f","modified":1538368811653},{"_id":"themes/hiker/layout/_partial/facebook-sdk.ejs","hash":"8fc5cf7abbfd587057fb86ee028c7f216d30d68c","modified":1538368811655},{"_id":"themes/hiker/layout/_partial/footer.ejs","hash":"d2949d40f7aac65d2ac8426f5099eded12a5ec94","modified":1538368811656},{"_id":"themes/hiker/layout/_partial/gauges-analytics.ejs","hash":"ace3000bd3e01d03041d5be24f7640b6c003a5b5","modified":1538368811657},{"_id":"themes/hiker/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1538368811696},{"_id":"themes/hiker/layout/_partial/head.ejs","hash":"4eb047dcc24ffd60d8e3c39b9e821d5c97fcc6e0","modified":1538368811697},{"_id":"themes/hiker/layout/_partial/header-post.ejs","hash":"dd0c9881065f8da929390d7c717d63b957c6cd76","modified":1538368811698},{"_id":"themes/hiker/layout/_partial/header.ejs","hash":"7d894fc61023289609006a2b03ba255edd722efa","modified":1538368811698},{"_id":"themes/hiker/layout/_partial/mobile-nav.ejs","hash":"347cf1befd2ea637c24bd5901929d8e36e359e75","modified":1538368811699},{"_id":"themes/hiker/layout/_partial/sidebar.ejs","hash":"c70869569749a8f48cce202fa57926c06b55fdab","modified":1538368811724},{"_id":"themes/hiker/layout/_partial/tencent-analytics.ejs","hash":"6a50e6fe7701ff131b2fc0c066a4615dd2a37da7","modified":1538368811725},{"_id":"themes/hiker/layout/_widget/archive.ejs","hash":"856a6352a0f8d55f3d2965eea8ad4ec517f6af96","modified":1538368811726},{"_id":"themes/hiker/layout/_widget/category.ejs","hash":"866790acc13fed44b7ef74c3e19c300a3d6180d8","modified":1538368811727},{"_id":"themes/hiker/layout/_widget/curtains.ejs","hash":"d96be843847211664b83b786bfee43fa2f897616","modified":1538368811727},{"_id":"themes/hiker/layout/_widget/recent_posts.ejs","hash":"16800f85ffb036d2644a26e02facd61acb3706e9","modified":1538368811728},{"_id":"themes/hiker/layout/_widget/social.ejs","hash":"5719baf6cee0ddd97a0a61351d5d107257531504","modified":1538368811729},{"_id":"themes/hiker/layout/_widget/tag.ejs","hash":"6017c54a8c3c8ff8db491cfbea3100c139da75d6","modified":1538368811730},{"_id":"themes/hiker/layout/_widget/tagcloud.ejs","hash":"7259c179aa0c41c02e467ad892292e90430aaabc","modified":1538368811764},{"_id":"themes/hiker/layout/search/baidu.ejs","hash":"8cc6f6e601b14d310f20eaf29dc55d6c60ab4ee4","modified":1538368812115},{"_id":"themes/hiker/layout/search/index-mobile.ejs","hash":"8e2e28b37a908f60e4953bf9175a7af329d15d40","modified":1538368812116},{"_id":"themes/hiker/layout/search/index.ejs","hash":"48e0d133a808000a60a4ce0d673737c37be2410d","modified":1538368812117},{"_id":"themes/hiker/layout/search/insight.ejs","hash":"5205e75f0ceedc38e2bd9904464324ce179b8e25","modified":1538368812118},{"_id":"themes/hiker/layout/search/swiftype.ejs","hash":"cce9c44180d9490f45b30b8f052ac82675a9d66a","modified":1538368812124},{"_id":"themes/hiker/source/css/_extend.styl","hash":"3c4dd93884eb25385d837fb975bb311a639ecc1d","modified":1538368812132},{"_id":"themes/hiker/source/css/_variables.styl","hash":"cbb703c722fb98b2f08cc803e8b954acdf7b5522","modified":1538368812253},{"_id":"themes/hiker/source/css/archive.css","hash":"6e7e24bc6c356fe83833ceb73986036248d2892c","modified":1538368812253},{"_id":"themes/hiker/source/css/dialog.css","hash":"c248717aecf61b42e2bbfece61140a5c44911b4b","modified":1538368812763},{"_id":"themes/hiker/source/css/header-post.css","hash":"18e6e212ae21df9dfa9a7f20b5eba798b151230c","modified":1538368812774},{"_id":"themes/hiker/source/css/home.css","hash":"5c80e0c163533afa5a6605818510ed2d95939613","modified":1538368812775},{"_id":"themes/hiker/source/css/style.styl","hash":"4704ff34504b467252f767ce23477d801bb47cd6","modified":1538368813316},{"_id":"themes/hiker/source/css/vdonate.css","hash":"b43acd89a3d87725d6496a4f780dcf5eb576f866","modified":1538368813316},{"_id":"themes/hiker/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1538368813317},{"_id":"themes/hiker/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1538368813318},{"_id":"themes/hiker/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1538368813319},{"_id":"themes/hiker/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1538368813319},{"_id":"themes/hiker/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1538368813320},{"_id":"themes/hiker/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1538368813331},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.css","hash":"2e54d51d21e68ebc4bb870f6e57d3bfb660d4f9c","modified":1538368813871},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.js","hash":"58193c802f307ec9bc9e586c0e8a13ebef45d2f8","modified":1538368813872},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.pack.js","hash":"2da892a02778236b64076e5e8802ef0566e1d9e8","modified":1538368813873},{"_id":"themes/hiker/source/js/dialog.js","hash":"f982934062fd6b2b385d1b6a6f7fbe4510087210","modified":1538368813875},{"_id":"themes/hiker/source/js/home.js","hash":"6534b583d310c57c49cb16aa75f6f33e9af41a92","modified":1538368813876},{"_id":"themes/hiker/source/js/insight.js","hash":"4b21b86ea9554d9f5a5da675f72632d9310b26d1","modified":1538368813877},{"_id":"themes/hiker/source/js/scripts.js","hash":"7b8531f6fdfeb1590cf096fbf913a63c66e5bbef","modified":1538368813878},{"_id":"themes/hiker/source/js/totop.js","hash":"aefa54321fdacd48537be444215bcb8df9190c3b","modified":1538368813891},{"_id":"themes/hiker/source/js/vdonate.js","hash":"a3482fae4f782028e7faa090f2f001ad9bca05e5","modified":1538368813892},{"_id":"themes/hiker/source/preview/browser-support.png","hash":"a6d8498553550c6b18a8f22bcd2f53c993c7d677","modified":1538368813893},{"_id":"themes/hiker/source/preview/donation-btn.png","hash":"ad78b1605b162e2399a1cdc5232f6a44298dba6c","modified":1538368813901},{"_id":"themes/hiker/source/preview/preview-abstract.png","hash":"3d18ad9ba38fe24770ba6758d8f1bb242f669ce3","modified":1538368813902},{"_id":"themes/hiker/source/preview/theme-color.png","hash":"725130ceea5e41bb2cc60b31e45275b4b0cc77b3","modified":1538368814205},{"_id":"source/_posts/book-storytelling-with-data/color.png","hash":"5a1ce74b4183937f2e29c59238579bd4f918f547","modified":1538368808409},{"_id":"source/_posts/book-storytelling-with-data/combined-approach-h.png","hash":"35e8e856207eb7edf0ac239b7c7923cbf83c89ec","modified":1538368808410},{"_id":"source/_posts/book-storytelling-with-data/combined-approach-v.png","hash":"04fa8fffc91eae0392946004514bd34297c602f3","modified":1538368808412},{"_id":"source/_posts/book-storytelling-with-data/h-bar.png","hash":"f276c5fed12fe9de8f4c46cbd81f44d4885cb1d2","modified":1538368808461},{"_id":"source/_posts/book-storytelling-with-data/heatmap.png","hash":"1fb509a1953858d2f2c4e74c163f05cdd48b24cd","modified":1538368808463},{"_id":"source/_posts/book-storytelling-with-data/no-attr.png","hash":"182fb755d50ee8337da97bde62b663a6186fe8b9","modified":1538368808465},{"_id":"source/_posts/book-storytelling-with-data/preatten-attr.png","hash":"ce99de1444d6007d917364b35f6c1724f22ccbab","modified":1538368808471},{"_id":"source/_posts/book-storytelling-with-data/report_original.png","hash":"e00799e3a98f098836ae894207e4b93bbd1ce472","modified":1538368808473},{"_id":"source/_posts/book-storytelling-with-data/slope.png","hash":"8fc7fb91dff0150bd15ae53a6f2704cee3c3640a","modified":1538368808476},{"_id":"source/_posts/book-storytelling-with-data/v-apart.png","hash":"20867199dc16d73f75508f56d96cc4f8fb1b6f40","modified":1538368808481},{"_id":"source/_posts/book-storytelling-with-data/with-attr.png","hash":"1f2b6ac983f833d16ed9d2bbfe93f9aab319adc0","modified":1538368808484},{"_id":"source/_posts/cv-detection/fastrcnn.jpg","hash":"05a2fb87e6a7ef2618a297ee502f6ab1b13138cd","modified":1538368808494},{"_id":"source/_posts/cv-detection/light_head_rcnn.jpg","hash":"8ffc357126b9026fc0078569a84e772e66400910","modified":1541940215601},{"_id":"source/_posts/cv-detection/pooling.jpg","hash":"74894b1fb4a0b636f3bbfa707279d43cfe9eea55","modified":1538368808511},{"_id":"source/_posts/cv-detection/spp_layer.jpg","hash":"0eee7ec1dc1fa32f5d2af5f9eda3febe265ee0ec","modified":1538368808529},{"_id":"source/_posts/cv-face-rec/centerloss_nn.jpg","hash":"2ce7356a98e3464de4af2ed44d480224df9cedce","modified":1538368808544},{"_id":"source/_posts/cv-face-rec/deepid.jpg","hash":"fb73c15181f5450b529db346bcfcc2f542f5a2c6","modified":1538368808548},{"_id":"source/_posts/cv-face-rec/facenet.jpg","hash":"ceb05829a1e56471a5b66566c8e39fd62d6b6da8","modified":1538368808550},{"_id":"source/_posts/dl-architecture/channel_shuffle.jpg","hash":"a2d3da16544ed4565d5c7fb60e8d22622ee269b1","modified":1538368808665},{"_id":"source/_posts/dl-architecture/dw-sep-conv.png","hash":"95a5cd9c35ae075a90ffbc16dc31527e63a0242a","modified":1538713506501},{"_id":"source/_posts/dl-architecture/resnext_block.jpg","hash":"5feb89892d4355f9983e7ab9985a592ccad5ab45","modified":1540183161260},{"_id":"source/_posts/dl-architecture/shufflenet_unit.jpg","hash":"b8ca7d6b4c4b016ca85fa2bac3102d51d00e0847","modified":1538368808672},{"_id":"source/_posts/dl-bn/BN_training.jpg","hash":"b832ccb3f8aa9f221117522d06d0aa2a53bb5ef5","modified":1542706110398},{"_id":"source/_posts/dl-bn/BN_transform.jpg","hash":"aaf729e340e62edd6a9ac8a195f2edb89c93efd5","modified":1542703570321},{"_id":"source/_posts/dl-bp/fig1.png","hash":"73b699a23997702d9991119d3025ab7adadd7475","modified":1538368808744},{"_id":"source/_posts/dl-bp/fig10.png","hash":"010d33e29baf56939eeb9a277ff435db5a64ee26","modified":1538368808763},{"_id":"source/_posts/dl-bp/fig13.png","hash":"96d0fbfcb7073c64556c1217c6b5160150057636","modified":1538368808791},{"_id":"source/_posts/dl-bp/fig4-1.png","hash":"9f2b40babb7d8b8a70feefbd153bae10dad4cf2e","modified":1538368808812},{"_id":"source/_posts/dl-bp/fig4-2.png","hash":"a8ccd9624d96756dbf1b0fe4257820e53607a529","modified":1538368808813},{"_id":"source/_posts/dl-bp/fig5.png","hash":"0b91ef06706cdba959ca68b7e8d137f5d40e4f49","modified":1538368808816},{"_id":"source/_posts/dl-bp/fig8.png","hash":"4dc393ded207bd643490e5b85581700527f3c73d","modified":1538368808833},{"_id":"source/_posts/dl-optimization/adagrad.jpg","hash":"56161cdc47bc8e5092c9ef4e9c59b880539682d6","modified":1538368808852},{"_id":"source/_posts/dl-optimization/adam.jpg","hash":"980b6d37b7dffe60122670c1120e2fb8f0a9846c","modified":1538368808856},{"_id":"source/_posts/dl-optimization/momentum.jpg","hash":"2e117a48a019bb5551e17f51ec235f037ca49fd9","modified":1538368808875},{"_id":"source/_posts/dl-optimization/nesterov.jpg","hash":"de21073a1690fe3f59080776ddaad52a4361e7d6","modified":1538368808882},{"_id":"source/_posts/dl-optimization/rmsprop.jpg","hash":"cea37d95e782d5def3453620abec49ae1651d1cd","modified":1538368808888},{"_id":"source/_posts/dl-optimization/rmsprop_with_nesterov.jpg","hash":"dd795b39039619772a7ceb986de5a1cc298a36c8","modified":1538368808895},{"_id":"source/_posts/dl-optimization/sgd.jpg","hash":"50eaee0a935720185f8e5fe08cbd70a5d6e1ce30","modified":1538368808898},{"_id":"source/_posts/dl-regularization/early_stopping.jpg","hash":"dd7ebb6c9c89abfa2cbd513244a09bb51577ac24","modified":1538368808910},{"_id":"source/_posts/dl-rnn/rnn1.jpg","hash":"dd64113bb658848050330391b2dff84c366aab60","modified":1538368808950},{"_id":"source/_posts/dl-rnn/rnn2.jpg","hash":"e8bf1cc6c955d81950b8ee3fac75f89feb15d144","modified":1538368808964},{"_id":"source/images/Alipay.jpg","hash":"8474d7edead93f8f283efd1c5d728fede56bea1e","modified":1538368809641},{"_id":"themes/hiker/source/js/bootstrap.js","hash":"474b25cebd06d57a38090c6716d5dfaa5591baad","modified":1538368813874},{"_id":"themes/hiker/source/js/jquery-3.1.1.min.js","hash":"042dd055cd289215835a58507c9531f808e1648a","modified":1538368813878},{"_id":"source/_posts/cv-detection/rcnn.png","hash":"4629ce79e338fa3fe8a6fbe90fe22ad9c7c1dc5b","modified":1538368808519},{"_id":"source/_posts/cv-face-rec/centerloss_update.jpg","hash":"4284af764a471431ae26d16d5bf5d6cb1852455a","modified":1538368808547},{"_id":"source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg","hash":"c105b82e5dde9cce7836cb4d1622b1d7a49bd261","modified":1540183672089},{"_id":"source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png","hash":"92f94c4c0e69016e86cbae2cf9e7b67af54fd7b1","modified":1538731370906},{"_id":"source/_posts/dl-bp/fig11.png","hash":"552a615d425cbdfc613bd4a70973584ca6555bd2","modified":1538368808773},{"_id":"source/_posts/dl-bp/fig12.png","hash":"e8ea9cf22191ce7eedaac6f7c5bae706e8b5d339","modified":1538368808789},{"_id":"source/_posts/dl-bp/fig14.png","hash":"a5423a0d44870f7f7dd482c9277bafb56f14fe8c","modified":1538368808794},{"_id":"source/_posts/dl-bp/fig6.png","hash":"faabe1d06f8419ce9ed187bafa27087e878efdb2","modified":1538368808818},{"_id":"source/_posts/dl-bp/fig7.png","hash":"59bf734c9c01c35c337908d6fd5edfffa786dbbb","modified":1538368808820},{"_id":"source/_posts/dl-bp/fig9.png","hash":"be4ed1dacabedf01f7af809eff84d8318d1d2859","modified":1538368808835},{"_id":"themes/hiker/layout/_partial/post/busuanzi-analytics.ejs","hash":"b282fb7e646dfe02dc89e7e786f233a6700dc808","modified":1538368811700},{"_id":"themes/hiker/layout/_partial/post/category.ejs","hash":"692c4fab11b31adce8f724f51203bec6ea759b9a","modified":1538368811703},{"_id":"themes/hiker/layout/_partial/post/date.ejs","hash":"cc160cd537a966c6ecb3c73c4a44207f254b0461","modified":1538368811704},{"_id":"themes/hiker/layout/_partial/post/gallery.ejs","hash":"bfde040b4c4a8ce43645e0783cdd2b944269ec80","modified":1538368811707},{"_id":"themes/hiker/layout/_partial/post/mathjax.ejs","hash":"ae998e014666cb354ded5a11a8a6bc7eae3e3c34","modified":1538368811708},{"_id":"themes/hiker/layout/_partial/post/nav.ejs","hash":"cbb3819ce512bd24db8bad41b8617d46eba82fdc","modified":1538368811710},{"_id":"themes/hiker/layout/_partial/post/tag.ejs","hash":"694b5101bcc44c9f9c1cc62e5ad2fdfb4b7c7a07","modified":1538368811721},{"_id":"themes/hiker/layout/_partial/post/title.ejs","hash":"9ee31f67ad337d5dcaaa10aa8ba55c7c22074b1c","modified":1538368811722},{"_id":"themes/hiker/layout/_partial/post/urlconvert.ejs","hash":"2133f1029632417f9043b9d4749d580ed0c75db0","modified":1538368811723},{"_id":"themes/hiker/source/css/_partial/archive.styl","hash":"b16abc653626645f570d48aca094e6d0ca1f231d","modified":1538368812134},{"_id":"themes/hiker/source/css/_partial/article.styl","hash":"720aff632a75f785751e7d17d5ff23ff4bbe96cf","modified":1538368812135},{"_id":"themes/hiker/source/css/_partial/comment.styl","hash":"43279aaaa00cc07b4a65b13ef01c391355e82717","modified":1538368812136},{"_id":"themes/hiker/source/css/_partial/footer.styl","hash":"203c03d670a6fd434efdb1d9047166db7d6d874b","modified":1538368812142},{"_id":"themes/hiker/source/css/_partial/header-post.styl","hash":"81539c15e42c40e1ffbc36952e4617e17fdbba9c","modified":1538368812143},{"_id":"themes/hiker/source/css/_partial/header.styl","hash":"5686f062d84928ceec78efbd01dba59609437efb","modified":1538368812144},{"_id":"themes/hiker/source/css/_partial/highlight.styl","hash":"a12d13407903d4847c4207e2cd62190367acae64","modified":1538368812246},{"_id":"themes/hiker/source/css/_partial/insight.styl","hash":"3b4042ebff85ded51eccd6183bad812d9aeef844","modified":1538368812247},{"_id":"themes/hiker/source/css/_partial/mobile.styl","hash":"3920e4c3cd11f294d3e7835ed628f169cbea6b21","modified":1538368812248},{"_id":"themes/hiker/source/css/_partial/sidebar-aside.styl","hash":"4f323b13a7594a46b5b21a9052bbdb7fb5144411","modified":1538368812248},{"_id":"themes/hiker/source/css/_partial/sidebar-bottom.styl","hash":"73909106254b7ec312367079687c0de37740bb31","modified":1538368812249},{"_id":"themes/hiker/source/css/_partial/sidebar.styl","hash":"a0b01d34317f4b17ce0f8c14cd978798ad2fbecc","modified":1538368812250},{"_id":"themes/hiker/source/css/_util/grid.styl","hash":"1aa883ab432d9e4139c89dcbd40ae2bd1528d029","modified":1538368812251},{"_id":"themes/hiker/source/css/_util/mixin.styl","hash":"429bad87fc156eacf226c5e35b0eafc277f2504b","modified":1538368812252},{"_id":"themes/hiker/source/css/bootstrap.css","hash":"41abf4a39af0a19b6e08b7e2d6ddacbd3892daaf","modified":1538368812257},{"_id":"themes/hiker/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1538368812765},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1538368812766},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1538368812773},{"_id":"themes/hiker/source/css/images/avatar.jpg","hash":"d685c61842df6ff3f44a96ba4ac15117502168ee","modified":1538368812776},{"_id":"themes/hiker/source/css/images/homelogo.jpg","hash":"4bfc9650c4fd6e60b09ae29f888a819cdf88e9fe","modified":1538368812781},{"_id":"themes/hiker/source/css/images/mylogo.jpg","hash":"c6bd830b95d59ad28489f949135640f401ee53e8","modified":1542367956368},{"_id":"themes/hiker/source/css/images/rocket.png","hash":"6dee0406955aa9b7a261161d30f2538a671e806b","modified":1538368812790},{"_id":"themes/hiker/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1538368813332},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1538368813333},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"4c9c395d705d22af7da06870d18f434e2a2eeaf9","modified":1538368813333},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-media.js","hash":"e14c32cc6823b81b2f758512f13ed8eb9ef2b454","modified":1538368813334},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1538368813335},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"83cdfea43632b613771691a11f56f99d85fb6dbd","modified":1538368813870},{"_id":"source/_posts/cv-detection/SSD_YOLO.png","hash":"494ee85e902567768c8105c75c10ba581d5f482b","modified":1540712145230},{"_id":"source/_posts/dl-architecture/evolution-of-separable-conv.png","hash":"9bcb851cc454591a3fedbbdd951b67bb6ecf6d12","modified":1538728571022},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1538368812772},{"_id":"source/_posts/cv-detection/SSD.png","hash":"9e96555172b3a1bd7aed7a514b301fa669f86e3b","modified":1540710106536},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.svg","hash":"a275426daefd3716c53561fad121d258a7f05b47","modified":1538368812769},{"_id":"source/_posts/dl-optimization/deep-learning.png","hash":"715462fb653522db028a15b762c0805613348aa1","modified":1538368808868},{"_id":"themes/hiker/source/preview/preview-mobile.png","hash":"7679a50aef93fc364cbcee4a52cd84604bf741b1","modified":1538368813905},{"_id":"themes/hiker/source/css/images/home-bg.jpg","hash":"cd9b73e3aeefd17f4a16528f783ac66093b66f5e","modified":1538728781274},{"_id":"themes/hiker/source/css/images/sample.jpg","hash":"02f5c2de79d093b94f72f5a361528dbbc7645d9e","modified":1538728732551},{"_id":"themes/hiker/source/css/images/pose.jpg","hash":"4d394a662e7d6d2de6a4598688c7cd3dba5c2fa3","modified":1538368812788},{"_id":"source/_posts/dl-bp/Derivation-of-CNN.pdf","hash":"5d7911c93ddcb34cac088d99bd0cae9124e5dcd1","modified":1538368808734},{"_id":"source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf","hash":"616cca53572c50c8e4911be9ccdaebfc1cfbca41","modified":1538368808709},{"_id":"themes/hiker/source/preview/code-theme.jpg","hash":"8c8512fd04e6106033656d10e92d51de76cca6d8","modified":1538368813900},{"_id":"source/projects/LagouJob.pdf","hash":"29dfa5252ec3448a86636005696b353f312fdba5","modified":1538368810298},{"_id":"source/about/Presentation_TransFBP.pdf","hash":"df22e0411b115a8dc5386d05cd264e0a90c85922","modified":1542537559268},{"_id":"themes/hiker/source/preview/preview-pc.png","hash":"2471e0697938721e4c5d7c66940e165c59a31a0f","modified":1538368814203},{"_id":"source/projects/MLJob.pdf","hash":"bb695129aca0adc5ac3b53408bee869170c912ad","modified":1538368810318},{"_id":"source/about/Research_Overview.pdf","hash":"2afc75056ddff10d4a03769c5040f2dc22057aa4","modified":1539788622175},{"_id":"source/projects/JiaYuan.pdf","hash":"cba0c80dd81fc63b8bea31e5c80a026eecc189de","modified":1538368810285},{"_id":"source/about/Presentation.pdf","hash":"a1bc00bb925f8fed059aac4f76ccf01c3d2f44f7","modified":1538368809631},{"_id":"source/_posts/ml-knn/Demo.png","hash":"d9fa12f1e40924a0db57761d09d52dce450b4f7b","modified":1538368809042},{"_id":"source/about/DL_for_Face_Analysis.pdf","hash":"4b4ffd9019ad773656ec509e2f84011e51b3e361","modified":1542505372193},{"_id":"source/_posts/dip-image-feature/TPAMI-A performance evaluation of local descriptors.pdf","hash":"934f349ee27cddc019ffd0db86bb28f980fad4e2","modified":1538368808647},{"_id":"source/projects/GovReport.pdf","hash":"c4fc0d750fb44b19a1c5c228a0b5757ac74bf2a7","modified":1538368810174},{"_id":"source/projects/IndustryReport.pdf","hash":"53d00824c8253eef8933bcc2d9dd1915565562cd","modified":1538368810269},{"_id":"public/archive/index.html","hash":"1176dfddf7a592a92f7526c838aabee82e6ef704","modified":1542730662460},{"_id":"public/papers/index.html","hash":"0eb265d8293156b61ed32e434151126cc1514d6d","modified":1542730662460},{"_id":"public/tags/Algorithm/index.html","hash":"16fa72d893be9d1abc1eb4770088ee7d58f3ac1d","modified":1542730662461},{"_id":"public/tags/Data-Structure/index.html","hash":"1dc9ba3438c2c03f579bc222260cc3d0518577c2","modified":1542730662461},{"_id":"public/tags/Graph/index.html","hash":"55ae50412497b2f5e9708f26056b35b4d93ad6e7","modified":1542730662462},{"_id":"public/tags/Data-Visualization/index.html","hash":"ee4b563554cda6e43a2ac058baa85639c66e060c","modified":1542730662462},{"_id":"public/tags/Machine-Learning/page/3/index.html","hash":"d1e24cef552682119f571baa3d7adae4b61a801d","modified":1542730662462},{"_id":"public/tags/Deep-Learning/page/2/index.html","hash":"2d7222f765db790816ca7f07cdcb26fc78f84816","modified":1542730662463},{"_id":"public/tags/Computer-Vision/index.html","hash":"1ccf1a747297d628b95b3e22db67f7f65a10c14b","modified":1542730662463},{"_id":"public/tags/Face-Anti-Spoofing/index.html","hash":"898d2286934f616c754d549dad05de56dee8403b","modified":1542730662463},{"_id":"public/tags/Object-Detection/index.html","hash":"13a7a6f943e108972965814a3fb01a84f65c662f","modified":1542730662463},{"_id":"public/tags/Face-Recognition/index.html","hash":"fa12f0babf701dcb744e71cbdef148a09bb61971","modified":1542730662463},{"_id":"public/tags/Digital-Image-Processing/index.html","hash":"777af8edbc020fabe7f4ecf6a0e87312e6b0517b","modified":1542730662463},{"_id":"public/tags/Auto-Encoder/index.html","hash":"9b2be6275768e4d157ca67f04efbcf455042b25c","modified":1542730662463},{"_id":"public/tags/Image-Classification/index.html","hash":"db47f6a54d9b3f1bdd1c314f551f2b3e86e70732","modified":1542730662463},{"_id":"public/tags/Network-Architecture/index.html","hash":"711228566448262c7bf1922a9307d24bbc3db5e0","modified":1542730662463},{"_id":"public/tags/Optimization/index.html","hash":"ed3aa8aad5d9a16ff49c83ac33d688233f4e90bd","modified":1542730662464},{"_id":"public/tags/CNN/index.html","hash":"b8a716b3823e358ebb6b9ed1bb90f7a8b3b551f5","modified":1542730662464},{"_id":"public/tags/Data-Augmentation/index.html","hash":"6cb5f969e85b7b9a223e96ef1da24c6488d6c104","modified":1542730662464},{"_id":"public/tags/Regularization/index.html","hash":"42382534dd308cbbc798586b1732c1d80aea7d8f","modified":1542730662464},{"_id":"public/tags/RNN/index.html","hash":"107f27949d53e13c58057b30038fa74a3043c7d8","modified":1542730662464},{"_id":"public/tags/NLP/index.html","hash":"070d7410a33709904a67a5bb099f3f6b7f9cab19","modified":1542730662464},{"_id":"public/tags/Feature-Engineering/index.html","hash":"dce8fb15ebdb40b4e50312428f0aa9a5c481b7f9","modified":1542730662464},{"_id":"public/about/index.html","hash":"dd24381953cbb09a7556a0a1799235dee905943f","modified":1542730662464},{"_id":"public/projects/index.html","hash":"9e9fdb37c0df3d270f37a35324e7f08d3be96616","modified":1542730662464},{"_id":"public/tags/index.html","hash":"6e1683baf8388c892b415a6a9a73945731fd324f","modified":1542730662464},{"_id":"public/2018/11/20/dl-bn/index.html","hash":"edf6f6c5384b917d7749a19f3016032ca721f107","modified":1542730662464},{"_id":"public/2018/11/18/dl-architecture/index.html","hash":"03cf4e82c9ffbd9b7dff3e3faa80f6f7bf0e28bf","modified":1542730662465},{"_id":"public/2018/11/11/cv-detection/index.html","hash":"762eb11a959022e077c37a42617059b78afc1819","modified":1542730662465},{"_id":"public/2018/11/10/dl-data-augmentation/index.html","hash":"4092b4d61d0e14828cc578dd91819f76c4ef0176","modified":1542730662465},{"_id":"public/2018/11/04/cv-iqa/index.html","hash":"2373ebc31864774fb910e8aeebcd62a274ffab0e","modified":1542730662465},{"_id":"public/2018/10/30/cv-antispoofing/index.html","hash":"83edd8805c7d0b0b41df44c566406bb6b9113b65","modified":1542730662465},{"_id":"public/2018/09/03/cv-face-rec/index.html","hash":"9822db074a90c381e55d4beecdbf2e4bcea3b451","modified":1542730662465},{"_id":"public/2018/08/22/dl-ae/index.html","hash":"7024bf23b610ba3b405c71bc0f0035d046a31fe5","modified":1542730662465},{"_id":"public/2018/08/22/dip-image-feature/index.html","hash":"56b1adee737bc03fa087ad8bd48255735691fbd3","modified":1542730662465},{"_id":"public/2018/08/20/ml-dimen-red-metric-learning/index.html","hash":"8199bbc270f132109ad116b8d65ff3c8bca27784","modified":1542730662465},{"_id":"public/2018/08/20/ml-feml/index.html","hash":"7f5780dc9d436091d67708a07bcce69472dea104","modified":1542730662466},{"_id":"public/2018/08/11/book-storytelling-with-data/index.html","hash":"9e00e407e6296a34c6128b34df4e018aed2cc9f1","modified":1542730662466},{"_id":"public/2018/08/10/dl-rnn/index.html","hash":"2d67e53251e0d7dc215901256d0f30fe740d4e40","modified":1542730662466},{"_id":"public/2018/08/06/dl-regularization/index.html","hash":"3e1865a8c7b687170c7251b3bf0e1c2b53dc37df","modified":1542730662466},{"_id":"public/2018/08/02/algo-sort/index.html","hash":"704a011e27aa88db3f90dc4a026ec5de3500ef5d","modified":1542730662466},{"_id":"public/2018/07/30/ml-clustering/index.html","hash":"4e89efeca36a69c2741d4cbb49db3c1414d97a92","modified":1542730662466},{"_id":"public/2018/07/27/dl-cnn/index.html","hash":"77ad7783d3eb9cba7e5b1de0cf5efb3d94989159","modified":1542730662466},{"_id":"public/2018/07/25/dl-bp/index.html","hash":"be343bdf299a8e7c24bf0951d756b7280d04790f","modified":1542730662466},{"_id":"public/2018/07/25/ml-ensemble/index.html","hash":"41f908812fbad7b4fff9a0d1a9969f54025b4b0b","modified":1542730662466},{"_id":"public/2018/07/24/ml-loss/index.html","hash":"eff199dfd431abf0464fa4ae34be4a5ebea1e8e1","modified":1542730662466},{"_id":"public/2018/07/24/ml-dt/index.html","hash":"1de70be3ab88a5a822a17693196e40cc856a548f","modified":1542730662467},{"_id":"public/2018/07/23/ml-lr-me/index.html","hash":"f24f6c185a060abafffb9164acce89489e124a0b","modified":1542730662467},{"_id":"public/2018/07/22/ml-svm/index.html","hash":"e0b4a3362e3a6e4a8dee2836c8b72c70c6cdf352","modified":1542730662467},{"_id":"public/2018/07/20/dl-optimization/index.html","hash":"66226b0c80b43ba819b8135328fd91a5a3f92545","modified":1542730662467},{"_id":"public/2018/07/19/ml-nb/index.html","hash":"fc080ef8f36370a1e6b963412ba0a63e11b274ef","modified":1542730662467},{"_id":"public/2018/07/19/ml-knn/index.html","hash":"de1ababe7dd7bfa712830d251c0b65ba4b2602f1","modified":1542730662467},{"_id":"public/2018/07/19/ml-model-selection-metric/index.html","hash":"cfc8c19021440b019f605097da8e83f917c7a896","modified":1542730662468},{"_id":"public/2018/07/18/ml-lm/index.html","hash":"7f9d057110320d5706404739aa0eeb2f89536e7c","modified":1542730662468},{"_id":"public/archives/index.html","hash":"6bf55548a8371da3b54130d542a850d904ab7abe","modified":1542730662468},{"_id":"public/archives/page/2/index.html","hash":"2c7fa693e412a837c73c7fa0ccf23cbb45b0bd62","modified":1542730662468},{"_id":"public/archives/page/3/index.html","hash":"80edcb4cd062230514296972ba1e1f7acc797055","modified":1542730662468},{"_id":"public/archives/2018/index.html","hash":"ce17ca219c4665ced3200baec794b47f59ac578c","modified":1542730662468},{"_id":"public/archives/2018/page/2/index.html","hash":"04160895de4a53b5683de12c6178dda89d62ecfd","modified":1542730662469},{"_id":"public/archives/2018/page/3/index.html","hash":"12dbd709e839ce164368a386c020aa8a2dc81bc4","modified":1542730662469},{"_id":"public/archives/2018/07/index.html","hash":"8932b9f0a2ad5e3aab8308e3d7ae444d16022830","modified":1542730662469},{"_id":"public/archives/2018/07/page/2/index.html","hash":"ada29ebd093cda06173511da5d824e2c6dfdc790","modified":1542730662469},{"_id":"public/archives/2018/08/index.html","hash":"9396f0a0050fd549dd4db87351bbdb7972cb59e2","modified":1542730662469},{"_id":"public/archives/2018/09/index.html","hash":"fb39708edc24cb61ff6197bc8b788e346899fcff","modified":1542730662469},{"_id":"public/archives/2018/10/index.html","hash":"87cb35dbb1093ca157ddcc880b761e2ff1bcda1d","modified":1542730662469},{"_id":"public/archives/2018/11/index.html","hash":"ea8f54f69619f68c13760fc1f53be619c1ccaffe","modified":1542730662470},{"_id":"public/page/2/index.html","hash":"250d584e62fec513e27ba0a4a2622826096da6a2","modified":1542730662470},{"_id":"public/index.html","hash":"7336cfe50e797f5dcfb87219586da605e62c3afa","modified":1542730662470},{"_id":"public/page/3/index.html","hash":"15ec187c7712ce0069d4a2dd6f577e9e7a7ff895","modified":1542730662470},{"_id":"public/tags/Data-Science/index.html","hash":"5a99f5552f16b42c369dddec4f8d7f0ce336254f","modified":1542730662470},{"_id":"public/tags/Data-Science/page/2/index.html","hash":"e1f44b3d647ccbab556a331d0508ba185077d25a","modified":1542730662470},{"_id":"public/tags/Machine-Learning/index.html","hash":"4262f11bc1168ca88c08e05150a488ab69710593","modified":1542730662470},{"_id":"public/tags/Machine-Learning/page/2/index.html","hash":"6de93c82dcbc7954679c38af99e671d3ec14bb4c","modified":1542730662470},{"_id":"public/tags/Deep-Learning/index.html","hash":"95b0091172748bb19c06ad338cd9ddedb406c913","modified":1542730662470},{"_id":"public/CNAME","hash":"c874a9c65eaf1b3e4f81bfaf1f09876474f56c50","modified":1542730662495},{"_id":"public/about/LICENSE","hash":"6cfbca72152ed7d78b31fb88f23b5c6087fff34d","modified":1542730662495},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1542730662495},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1542730662495},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1542730662495},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1542730662495},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1542730662495},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1542730662496},{"_id":"public/preview/browser-support.png","hash":"a6d8498553550c6b18a8f22bcd2f53c993c7d677","modified":1542730662496},{"_id":"public/preview/donation-btn.png","hash":"ad78b1605b162e2399a1cdc5232f6a44298dba6c","modified":1542730662497},{"_id":"public/preview/preview-abstract.png","hash":"3d18ad9ba38fe24770ba6758d8f1bb242f669ce3","modified":1542730662497},{"_id":"public/preview/theme-color.png","hash":"725130ceea5e41bb2cc60b31e45275b4b0cc77b3","modified":1542730662498},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1542730662498},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1542730662498},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1542730662498},{"_id":"public/css/images/avatar.jpg","hash":"d685c61842df6ff3f44a96ba4ac15117502168ee","modified":1542730662498},{"_id":"public/css/images/homelogo.jpg","hash":"4bfc9650c4fd6e60b09ae29f888a819cdf88e9fe","modified":1542730662498},{"_id":"public/css/images/mylogo.jpg","hash":"c6bd830b95d59ad28489f949135640f401ee53e8","modified":1542730662498},{"_id":"public/css/images/rocket.png","hash":"6dee0406955aa9b7a261161d30f2538a671e806b","modified":1542730662498},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1542730662499},{"_id":"public/about/CV_LuXu.pdf","hash":"a82c924ef6afad3e065bf180e328af495df75b68","modified":1542730663364},{"_id":"public/about/LucasX.jpg","hash":"b50fed3c6eed4cd39ada918654f11a3b086d02f2","modified":1542730663365},{"_id":"public/images/WeChatPay.png","hash":"b5d525b4ea3741bdd8edc8c5679a23aa2ec86c53","modified":1542730663371},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1542730663371},{"_id":"public/css/archive.css","hash":"17cc72203cad1b0be66008d662c7494507aaee8b","modified":1542730663422},{"_id":"public/css/dialog.css","hash":"5e0333adf3f496e0d443767fe228a1d4b1a2bafc","modified":1542730663422},{"_id":"public/css/header-post.css","hash":"3f6d1f5593a353b4b05a67ee14e04ec9c986db21","modified":1542730663422},{"_id":"public/css/home.css","hash":"2a7bfef438468b5b8be3f84ea6818156d371077d","modified":1542730663423},{"_id":"public/css/vdonate.css","hash":"bca2d291a71e7358654c51f23e8bfb467b2bc8b2","modified":1542730663423},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1542730663423},{"_id":"public/js/dialog.js","hash":"01e8b337c1721e0486fd5044f98b233e84ba1985","modified":1542730663423},{"_id":"public/js/home.js","hash":"e403c3290d76c5f58571cbfe4414236e41a7ac94","modified":1542730663423},{"_id":"public/js/scripts.js","hash":"e06a8948375df71cbf77abf8617db438ece811b3","modified":1542730663423},{"_id":"public/js/insight.js","hash":"f79ab175d1c8c4fb59328ee4fd9eb95808eb0be5","modified":1542730663423},{"_id":"public/js/totop.js","hash":"29bb40144ac238d22b25d59df465aff8dc38bfd0","modified":1542730663424},{"_id":"public/js/vdonate.js","hash":"5738414c642d30e43943a69287b3d25a0b6be135","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1542730663425},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1542730663425},{"_id":"public/css/style.css","hash":"924f37d406a58b29c37b06a10e8783438c0fbc71","modified":1542730663425},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1542730663425},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1542730663425},{"_id":"public/js/bootstrap.js","hash":"3b965a36a6b08854ad6eddedf85c5319fd392b4a","modified":1542730663426},{"_id":"public/js/jquery-3.1.1.min.js","hash":"f647a6d37dc4ca055ced3cf64bbc1f490070acba","modified":1542730663426},{"_id":"public/css/bootstrap.css","hash":"64fdc2e7c3f8a164d21c5632b5adbbb9990ea802","modified":1542730663427},{"_id":"public/images/Alipay.jpg","hash":"8474d7edead93f8f283efd1c5d728fede56bea1e","modified":1542730663491},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"a275426daefd3716c53561fad121d258a7f05b47","modified":1542730663491},{"_id":"public/css/images/home-bg.jpg","hash":"cd9b73e3aeefd17f4a16528f783ac66093b66f5e","modified":1542730663543},{"_id":"public/preview/preview-mobile.png","hash":"7679a50aef93fc364cbcee4a52cd84604bf741b1","modified":1542730663567},{"_id":"public/css/images/sample.jpg","hash":"02f5c2de79d093b94f72f5a361528dbbc7645d9e","modified":1542730663568},{"_id":"public/css/images/pose.jpg","hash":"4d394a662e7d6d2de6a4598688c7cd3dba5c2fa3","modified":1542730663570},{"_id":"public/preview/code-theme.jpg","hash":"8c8512fd04e6106033656d10e92d51de76cca6d8","modified":1542730663696},{"_id":"public/projects/LagouJob.pdf","hash":"29dfa5252ec3448a86636005696b353f312fdba5","modified":1542730663721},{"_id":"public/preview/preview-pc.png","hash":"2471e0697938721e4c5d7c66940e165c59a31a0f","modified":1542730663736},{"_id":"public/about/Presentation_TransFBP.pdf","hash":"df22e0411b115a8dc5386d05cd264e0a90c85922","modified":1542730663745},{"_id":"public/projects/MLJob.pdf","hash":"bb695129aca0adc5ac3b53408bee869170c912ad","modified":1542730663769},{"_id":"public/about/Research_Overview.pdf","hash":"2afc75056ddff10d4a03769c5040f2dc22057aa4","modified":1542730663774},{"_id":"public/projects/JiaYuan.pdf","hash":"cba0c80dd81fc63b8bea31e5c80a026eecc189de","modified":1542730663774},{"_id":"public/about/Presentation.pdf","hash":"a1bc00bb925f8fed059aac4f76ccf01c3d2f44f7","modified":1542730663794},{"_id":"public/about/DL_for_Face_Analysis.pdf","hash":"4b4ffd9019ad773656ec509e2f84011e51b3e361","modified":1542730663824},{"_id":"public/projects/GovReport.pdf","hash":"c4fc0d750fb44b19a1c5c228a0b5757ac74bf2a7","modified":1542730663892},{"_id":"public/projects/IndustryReport.pdf","hash":"53d00824c8253eef8933bcc2d9dd1915565562cd","modified":1542730663914}],"Category":[],"Data":[],"Page":[{"layout":"archives","title":"Archives","header-img":null,"comments":0,"date":"2018-07-18T04:28:56.000Z","description":"All posted archives.","_content":"","source":"archive/index.md","raw":"---\nlayout: \"archives\"\ntitle: \"Archives\"\nheader-img: \ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"All posted archives.\"\n---\n","updated":"2018-10-01T04:40:09.635Z","path":"archive/index.html","_id":"cjopy03bh0001608wjqgww8ux","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"About Me","description":"Self introduction and personal information.","header-img":"img/header_img/archive-bg.png","_content":"## Self Introduction\nI am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. \n\n![LucasX](https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg)\n\n__Curriculum Vitae__: [[PDF](./CV_LuXu.pdf)] \n\n## Academic Papers\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](./Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]  \n\n## Projects\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]  \n  An AI cloud platform based on Java & Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.\n* [Lagou Job](https://github.com/lucasxlu/LagouJob.git)  \n  A repository for job data crawling and data analysis based on Python3. This repo has received over **200 stars** and **120+ forks**. More details can be read at [here](https://www.zhihu.com/question/36132174/answer/94392659).\n\n## Competition\n* My algorithm named **TreeCNN** ranks the 1st place on **Facial Expression Recognition Challenge** on [Real-word Affective Face Database (RAF-DB)](http://www.whdeng.cn/raf/model1.html).\n\n\n## Experience\n* Image Quality Assessment & Automatic License Examination & Face Analysis\n  During my intern at [DiDi](https://www.didiglobal.com/), we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.\n  * Building deep models for IQA with 93.15% precision and 98.35% recall\n  * Developing and training deep models for license type classification with 99.72% precision and 99.73% recall \n  * Developing deep models for Facial Beauty Prediction with 0.89 PC\n  * Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall\n  * Writing 16 patents about security products about DiDi\n\n\n* [Pig Face Recognition](http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html)  \n  Our team developed a **Pig Face Recognition System** with [Guangzhou Yingzi Technology Co.,Ltd](http://www.yingzi.com/), to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   \n  * Building deep model and training on Amazon S3 GPU server.\n  * Encapsulate RESTful API and web backend development.\n  \n  \n* Mal-Image Recognition System  \n  I developed a Mal-Image Recognition System with [Wuhan ZhiLiFeng Information .,Ltd.](http://zlfinfo.com.cn/), the model achieves a mAP with 92.17% on detecting mal-images.  \n  * Image data crawling and preprocessing.\n  * Building machine learning algorithms.\n  * Model training and deployment.\n\n\n* Web Crawling and Java Developer  \n  During working at [Beijing Jiewen Technology Co.,Ltd.](http://www.jiewen.com.cn/), Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.\n  * Developing and maintaining web information system based on Java SSM framework and MySQL database.\n  * Developing web crawlers to collect data from O2O websites (e.g. [Ctrip](http://www.ctrip.com/), [Dazhong Dianping](http://www.dianping.com/), [AMap](https://www.amap.com/)), and storing them in MySQL and MongoDB.\n  * Data statistics, analysis and visualization.\n\n## Skills\n* English: (CET-6: 575/710); Proficiency in Reading and Writing    \n* Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    \n* Programming: Java == Python > R > C++  \n* Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  \n* Database: MongoDB/MySQL/Oracle  \n* Others: Web Crawler/Web Development/PPT\n\n## Award\n* 1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [[Slides](./Presentation.pdf)]\n* 1st Scholar Prize [[Slides](./Research_Overview.pdf)]\n* 2nd Scholar Prize  \n* 3rd Prize in NECCS (national level)  \n\n## Report & Tutorial\n* Deep Learning for Face Analysis [[Slides](./DL_for_Face_Analysis.pdf)]\n\n## Interests\n* Coding  \n* Reading and Writing  \n* Design \n\n\n## Contact\n* [Zhihu](https://www.zhihu.com/people/xulu-0620/activities)\n* [Github](https://github.com/lucasxlu)  \n* Email: xulu0620@gmail.com","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About Me\"\ndescription: \"Self introduction and personal information.\"\nheader-img: \"img/header_img/archive-bg.png\"\n---\n## Self Introduction\nI am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. \n\n![LucasX](https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg)\n\n__Curriculum Vitae__: [[PDF](./CV_LuXu.pdf)] \n\n## Academic Papers\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](./Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]  \n\n## Projects\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]  \n  An AI cloud platform based on Java & Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.\n* [Lagou Job](https://github.com/lucasxlu/LagouJob.git)  \n  A repository for job data crawling and data analysis based on Python3. This repo has received over **200 stars** and **120+ forks**. More details can be read at [here](https://www.zhihu.com/question/36132174/answer/94392659).\n\n## Competition\n* My algorithm named **TreeCNN** ranks the 1st place on **Facial Expression Recognition Challenge** on [Real-word Affective Face Database (RAF-DB)](http://www.whdeng.cn/raf/model1.html).\n\n\n## Experience\n* Image Quality Assessment & Automatic License Examination & Face Analysis\n  During my intern at [DiDi](https://www.didiglobal.com/), we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.\n  * Building deep models for IQA with 93.15% precision and 98.35% recall\n  * Developing and training deep models for license type classification with 99.72% precision and 99.73% recall \n  * Developing deep models for Facial Beauty Prediction with 0.89 PC\n  * Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall\n  * Writing 16 patents about security products about DiDi\n\n\n* [Pig Face Recognition](http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html)  \n  Our team developed a **Pig Face Recognition System** with [Guangzhou Yingzi Technology Co.,Ltd](http://www.yingzi.com/), to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   \n  * Building deep model and training on Amazon S3 GPU server.\n  * Encapsulate RESTful API and web backend development.\n  \n  \n* Mal-Image Recognition System  \n  I developed a Mal-Image Recognition System with [Wuhan ZhiLiFeng Information .,Ltd.](http://zlfinfo.com.cn/), the model achieves a mAP with 92.17% on detecting mal-images.  \n  * Image data crawling and preprocessing.\n  * Building machine learning algorithms.\n  * Model training and deployment.\n\n\n* Web Crawling and Java Developer  \n  During working at [Beijing Jiewen Technology Co.,Ltd.](http://www.jiewen.com.cn/), Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.\n  * Developing and maintaining web information system based on Java SSM framework and MySQL database.\n  * Developing web crawlers to collect data from O2O websites (e.g. [Ctrip](http://www.ctrip.com/), [Dazhong Dianping](http://www.dianping.com/), [AMap](https://www.amap.com/)), and storing them in MySQL and MongoDB.\n  * Data statistics, analysis and visualization.\n\n## Skills\n* English: (CET-6: 575/710); Proficiency in Reading and Writing    \n* Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    \n* Programming: Java == Python > R > C++  \n* Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  \n* Database: MongoDB/MySQL/Oracle  \n* Others: Web Crawler/Web Development/PPT\n\n## Award\n* 1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [[Slides](./Presentation.pdf)]\n* 1st Scholar Prize [[Slides](./Research_Overview.pdf)]\n* 2nd Scholar Prize  \n* 3rd Prize in NECCS (national level)  \n\n## Report & Tutorial\n* Deep Learning for Face Analysis [[Slides](./DL_for_Face_Analysis.pdf)]\n\n## Interests\n* Coding  \n* Reading and Writing  \n* Design \n\n\n## Contact\n* [Zhihu](https://www.zhihu.com/people/xulu-0620/activities)\n* [Github](https://github.com/lucasxlu)  \n* Email: xulu0620@gmail.com","date":"2018-11-18T10:42:36.880Z","updated":"2018-11-18T10:42:36.880Z","path":"about/index.html","comments":1,"_id":"cjopy03bk0003608w96m0ditw","content":"<h2 id=\"Self-Introduction\"><a href=\"#Self-Introduction\" class=\"headerlink\" title=\"Self Introduction\"></a>Self Introduction</h2><p>I am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg\" alt=\"LucasX\"></p>\n<p><strong>Curriculum Vitae</strong>: [<a href=\"./CV_LuXu.pdf\">PDF</a>] </p>\n<h2 id=\"Academic-Papers\"><a href=\"#Academic-Papers\" class=\"headerlink\" title=\"Academic Papers\"></a>Academic Papers</h2><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]  </li>\n</ul>\n<h2 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h2><ul>\n<li>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]<br>An AI cloud platform based on Java &amp; Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.</li>\n<li><a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Lagou Job</a><br>A repository for job data crawling and data analysis based on Python3. This repo has received over <strong>200 stars</strong> and <strong>120+ forks</strong>. More details can be read at <a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">here</a>.</li>\n</ul>\n<h2 id=\"Competition\"><a href=\"#Competition\" class=\"headerlink\" title=\"Competition\"></a>Competition</h2><ul>\n<li>My algorithm named <strong>TreeCNN</strong> ranks the 1st place on <strong>Facial Expression Recognition Challenge</strong> on <a href=\"http://www.whdeng.cn/raf/model1.html\" target=\"_blank\" rel=\"noopener\">Real-word Affective Face Database (RAF-DB)</a>.</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Image Quality Assessment &amp; Automatic License Examination &amp; Face Analysis<br>During my intern at <a href=\"https://www.didiglobal.com/\" target=\"_blank\" rel=\"noopener\">DiDi</a>, we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.<ul>\n<li>Building deep models for IQA with 93.15% precision and 98.35% recall</li>\n<li>Developing and training deep models for license type classification with 99.72% precision and 99.73% recall </li>\n<li>Developing deep models for Facial Beauty Prediction with 0.89 PC</li>\n<li>Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall</li>\n<li>Writing 16 patents about security products about DiDi</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><a href=\"http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html\" target=\"_blank\" rel=\"noopener\">Pig Face Recognition</a><br>Our team developed a <strong>Pig Face Recognition System</strong> with <a href=\"http://www.yingzi.com/\" target=\"_blank\" rel=\"noopener\">Guangzhou Yingzi Technology Co.,Ltd</a>, to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   <ul>\n<li>Building deep model and training on Amazon S3 GPU server.</li>\n<li>Encapsulate RESTful API and web backend development.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Mal-Image Recognition System<br>I developed a Mal-Image Recognition System with <a href=\"http://zlfinfo.com.cn/\" target=\"_blank\" rel=\"noopener\">Wuhan ZhiLiFeng Information .,Ltd.</a>, the model achieves a mAP with 92.17% on detecting mal-images.  <ul>\n<li>Image data crawling and preprocessing.</li>\n<li>Building machine learning algorithms.</li>\n<li>Model training and deployment.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Web Crawling and Java Developer<br>During working at <a href=\"http://www.jiewen.com.cn/\" target=\"_blank\" rel=\"noopener\">Beijing Jiewen Technology Co.,Ltd.</a>, Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.<ul>\n<li>Developing and maintaining web information system based on Java SSM framework and MySQL database.</li>\n<li>Developing web crawlers to collect data from O2O websites (e.g. <a href=\"http://www.ctrip.com/\" target=\"_blank\" rel=\"noopener\">Ctrip</a>, <a href=\"http://www.dianping.com/\" target=\"_blank\" rel=\"noopener\">Dazhong Dianping</a>, <a href=\"https://www.amap.com/\" target=\"_blank\" rel=\"noopener\">AMap</a>), and storing them in MySQL and MongoDB.</li>\n<li>Data statistics, analysis and visualization.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Skills\"><a href=\"#Skills\" class=\"headerlink\" title=\"Skills\"></a>Skills</h2><ul>\n<li>English: (CET-6: 575/710); Proficiency in Reading and Writing    </li>\n<li>Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    </li>\n<li>Programming: Java == Python &gt; R &gt; C++  </li>\n<li>Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  </li>\n<li>Database: MongoDB/MySQL/Oracle  </li>\n<li>Others: Web Crawler/Web Development/PPT</li>\n</ul>\n<h2 id=\"Award\"><a href=\"#Award\" class=\"headerlink\" title=\"Award\"></a>Award</h2><ul>\n<li>1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [<a href=\"./Presentation.pdf\">Slides</a>]</li>\n<li>1st Scholar Prize [<a href=\"./Research_Overview.pdf\">Slides</a>]</li>\n<li>2nd Scholar Prize  </li>\n<li>3rd Prize in NECCS (national level)  </li>\n</ul>\n<h2 id=\"Report-amp-Tutorial\"><a href=\"#Report-amp-Tutorial\" class=\"headerlink\" title=\"Report &amp; Tutorial\"></a>Report &amp; Tutorial</h2><ul>\n<li>Deep Learning for Face Analysis [<a href=\"./DL_for_Face_Analysis.pdf\">Slides</a>]</li>\n</ul>\n<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h2><ul>\n<li>Coding  </li>\n<li>Reading and Writing  </li>\n<li>Design </li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><ul>\n<li><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">Zhihu</a></li>\n<li><a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">Github</a>  </li>\n<li>Email: <a href=\"mailto:xulu0620@gmail.com\" target=\"_blank\" rel=\"noopener\">xulu0620@gmail.com</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Self-Introduction\"><a href=\"#Self-Introduction\" class=\"headerlink\" title=\"Self Introduction\"></a>Self Introduction</h2><p>I am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg\" alt=\"LucasX\"></p>\n<p><strong>Curriculum Vitae</strong>: [<a href=\"./CV_LuXu.pdf\">PDF</a>] </p>\n<h2 id=\"Academic-Papers\"><a href=\"#Academic-Papers\" class=\"headerlink\" title=\"Academic Papers\"></a>Academic Papers</h2><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]  </li>\n</ul>\n<h2 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h2><ul>\n<li>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]<br>An AI cloud platform based on Java &amp; Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.</li>\n<li><a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Lagou Job</a><br>A repository for job data crawling and data analysis based on Python3. This repo has received over <strong>200 stars</strong> and <strong>120+ forks</strong>. More details can be read at <a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">here</a>.</li>\n</ul>\n<h2 id=\"Competition\"><a href=\"#Competition\" class=\"headerlink\" title=\"Competition\"></a>Competition</h2><ul>\n<li>My algorithm named <strong>TreeCNN</strong> ranks the 1st place on <strong>Facial Expression Recognition Challenge</strong> on <a href=\"http://www.whdeng.cn/raf/model1.html\" target=\"_blank\" rel=\"noopener\">Real-word Affective Face Database (RAF-DB)</a>.</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Image Quality Assessment &amp; Automatic License Examination &amp; Face Analysis<br>During my intern at <a href=\"https://www.didiglobal.com/\" target=\"_blank\" rel=\"noopener\">DiDi</a>, we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.<ul>\n<li>Building deep models for IQA with 93.15% precision and 98.35% recall</li>\n<li>Developing and training deep models for license type classification with 99.72% precision and 99.73% recall </li>\n<li>Developing deep models for Facial Beauty Prediction with 0.89 PC</li>\n<li>Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall</li>\n<li>Writing 16 patents about security products about DiDi</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><a href=\"http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html\" target=\"_blank\" rel=\"noopener\">Pig Face Recognition</a><br>Our team developed a <strong>Pig Face Recognition System</strong> with <a href=\"http://www.yingzi.com/\" target=\"_blank\" rel=\"noopener\">Guangzhou Yingzi Technology Co.,Ltd</a>, to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   <ul>\n<li>Building deep model and training on Amazon S3 GPU server.</li>\n<li>Encapsulate RESTful API and web backend development.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Mal-Image Recognition System<br>I developed a Mal-Image Recognition System with <a href=\"http://zlfinfo.com.cn/\" target=\"_blank\" rel=\"noopener\">Wuhan ZhiLiFeng Information .,Ltd.</a>, the model achieves a mAP with 92.17% on detecting mal-images.  <ul>\n<li>Image data crawling and preprocessing.</li>\n<li>Building machine learning algorithms.</li>\n<li>Model training and deployment.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Web Crawling and Java Developer<br>During working at <a href=\"http://www.jiewen.com.cn/\" target=\"_blank\" rel=\"noopener\">Beijing Jiewen Technology Co.,Ltd.</a>, Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.<ul>\n<li>Developing and maintaining web information system based on Java SSM framework and MySQL database.</li>\n<li>Developing web crawlers to collect data from O2O websites (e.g. <a href=\"http://www.ctrip.com/\" target=\"_blank\" rel=\"noopener\">Ctrip</a>, <a href=\"http://www.dianping.com/\" target=\"_blank\" rel=\"noopener\">Dazhong Dianping</a>, <a href=\"https://www.amap.com/\" target=\"_blank\" rel=\"noopener\">AMap</a>), and storing them in MySQL and MongoDB.</li>\n<li>Data statistics, analysis and visualization.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Skills\"><a href=\"#Skills\" class=\"headerlink\" title=\"Skills\"></a>Skills</h2><ul>\n<li>English: (CET-6: 575/710); Proficiency in Reading and Writing    </li>\n<li>Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    </li>\n<li>Programming: Java == Python &gt; R &gt; C++  </li>\n<li>Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  </li>\n<li>Database: MongoDB/MySQL/Oracle  </li>\n<li>Others: Web Crawler/Web Development/PPT</li>\n</ul>\n<h2 id=\"Award\"><a href=\"#Award\" class=\"headerlink\" title=\"Award\"></a>Award</h2><ul>\n<li>1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [<a href=\"./Presentation.pdf\">Slides</a>]</li>\n<li>1st Scholar Prize [<a href=\"./Research_Overview.pdf\">Slides</a>]</li>\n<li>2nd Scholar Prize  </li>\n<li>3rd Prize in NECCS (national level)  </li>\n</ul>\n<h2 id=\"Report-amp-Tutorial\"><a href=\"#Report-amp-Tutorial\" class=\"headerlink\" title=\"Report &amp; Tutorial\"></a>Report &amp; Tutorial</h2><ul>\n<li>Deep Learning for Face Analysis [<a href=\"./DL_for_Face_Analysis.pdf\">Slides</a>]</li>\n</ul>\n<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h2><ul>\n<li>Coding  </li>\n<li>Reading and Writing  </li>\n<li>Design </li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><ul>\n<li><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">Zhihu</a></li>\n<li><a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">Github</a>  </li>\n<li>Email: <a href=\"mailto:xulu0620@gmail.com\" target=\"_blank\" rel=\"noopener\">xulu0620@gmail.com</a></li>\n</ul>\n"},{"layout":"papers","title":"Papers","header-img":"img/header_img/archive-bg.png","comments":0,"date":"2018-07-18T04:28:56.000Z","description":"Academic papers list.","_content":"# Academic Papers List\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](../about/Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]   \n","source":"papers/index.md","raw":"---\nlayout: \"papers\"\ntitle: \"Papers\"\nheader-img: \"img/header_img/archive-bg.png\"\ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"Academic papers list.\"\n---\n# Academic Papers List\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](../about/Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]   \n","updated":"2018-11-18T10:43:15.437Z","path":"papers/index.html","_id":"cjopy03fp004r608wvt8gmo5n","content":"<h1 id=\"Academic-Papers-List\"><a href=\"#Academic-Papers-List\" class=\"headerlink\" title=\"Academic Papers List\"></a>Academic Papers List</h1><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"../about/Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]   </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Academic-Papers-List\"><a href=\"#Academic-Papers-List\" class=\"headerlink\" title=\"Academic Papers List\"></a>Academic Papers List</h1><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"../about/Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]   </li>\n</ul>\n"},{"layout":"projects","title":"Projects","header-img":"img/header_img/archive-bg.png","comments":0,"date":"2018-07-18T04:28:56.000Z","description":"Projects for ML/CV/AI/Data Science.","_content":"# Projects List\n**For more information about my projects, please follow my [github](https://github.com/lucasxlu).**\n\n* Lagou Job Data Analysis [[Article](https://www.zhihu.com/question/36132174/answer/94392659)] [[Code](https://github.com/lucasxlu/LagouJob.git)] [[Slides](LagouJob.pdf)]\n    > A repository for web data crawling, data storage, and data analysis of [lagou.com](https://www.lagou.com) based on Python3.\n\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]\n    > An AI Cloud Platform with RESTful APIs.\n\n* JiaYuan User Profile [[Article](https://zhuanlan.zhihu.com/p/24515034)] [[Code](https://github.com/lucasxlu/JiaYuan.git)] [[Slides](JiaYuan.pdf)]\n    > A repository for web data crawling, user profile of [jiayuan.com](http://www.jiayuan.com/).\n\n* XiaoLu AI [[Article-Face Beauty Recognition](https://zhuanlan.zhihu.com/p/29399781)] [[Article-Mate Face Recognition](https://zhuanlan.zhihu.com/p/35135539)] [[Code](https://github.com/lucasxlu/XiaoLuAI.git)]\n    > A repository in computer vision and NLP.\n\n* Zhihu Live Analysis [[Article-1](https://zhuanlan.zhihu.com/p/30514792)] [[Article-2](https://zhuanlan.zhihu.com/p/31651544)] [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]\n    > A repository for data crawling, data analysis, data mining and data visualization.\n\n* Image Guard [[Article](https://zhuanlan.zhihu.com/p/29016317)] [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder)]\n    > A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.\n\n* Douban Sentiment Analysis [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)]\n    > A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.\n\n* Web Data Mining [[Article](https://zhuanlan.zhihu.com/p/28954770)] [[Code](https://github.com/lucasxlu/DataHouse.git)]\n    > A repository for web data crawling, analysis and mining.\n\n* Ctrip Web Crawler [[Code](https://github.com/lucasxlu/CtripPro.git)]\n    > A repository for multi-thread web crawler of Ctrip based on Java.\n\n* MateFace (WeChat Mini Program) [[Code](https://github.com/lucasxlu/mateface.git)]\n    > A WeChat Mini Program for mate face comparision.\n\n* Industry Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./IndustryReport.pdf)]\n    > A repository for industry analysis and report.\n\n* AI Career Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./MLJob.pdf)]\n    > A repository and report for career analysis in AI/ML.\n\n* Government Report [[Slides](./GovReport.pdf)]\n    > A repository for government report analysis and report.","source":"projects/index.md","raw":"---\nlayout: \"projects\"\ntitle: \"Projects\"\nheader-img: \"img/header_img/archive-bg.png\"\ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"Projects for ML/CV/AI/Data Science.\"\n---\n# Projects List\n**For more information about my projects, please follow my [github](https://github.com/lucasxlu).**\n\n* Lagou Job Data Analysis [[Article](https://www.zhihu.com/question/36132174/answer/94392659)] [[Code](https://github.com/lucasxlu/LagouJob.git)] [[Slides](LagouJob.pdf)]\n    > A repository for web data crawling, data storage, and data analysis of [lagou.com](https://www.lagou.com) based on Python3.\n\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]\n    > An AI Cloud Platform with RESTful APIs.\n\n* JiaYuan User Profile [[Article](https://zhuanlan.zhihu.com/p/24515034)] [[Code](https://github.com/lucasxlu/JiaYuan.git)] [[Slides](JiaYuan.pdf)]\n    > A repository for web data crawling, user profile of [jiayuan.com](http://www.jiayuan.com/).\n\n* XiaoLu AI [[Article-Face Beauty Recognition](https://zhuanlan.zhihu.com/p/29399781)] [[Article-Mate Face Recognition](https://zhuanlan.zhihu.com/p/35135539)] [[Code](https://github.com/lucasxlu/XiaoLuAI.git)]\n    > A repository in computer vision and NLP.\n\n* Zhihu Live Analysis [[Article-1](https://zhuanlan.zhihu.com/p/30514792)] [[Article-2](https://zhuanlan.zhihu.com/p/31651544)] [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]\n    > A repository for data crawling, data analysis, data mining and data visualization.\n\n* Image Guard [[Article](https://zhuanlan.zhihu.com/p/29016317)] [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder)]\n    > A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.\n\n* Douban Sentiment Analysis [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)]\n    > A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.\n\n* Web Data Mining [[Article](https://zhuanlan.zhihu.com/p/28954770)] [[Code](https://github.com/lucasxlu/DataHouse.git)]\n    > A repository for web data crawling, analysis and mining.\n\n* Ctrip Web Crawler [[Code](https://github.com/lucasxlu/CtripPro.git)]\n    > A repository for multi-thread web crawler of Ctrip based on Java.\n\n* MateFace (WeChat Mini Program) [[Code](https://github.com/lucasxlu/mateface.git)]\n    > A WeChat Mini Program for mate face comparision.\n\n* Industry Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./IndustryReport.pdf)]\n    > A repository for industry analysis and report.\n\n* AI Career Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./MLJob.pdf)]\n    > A repository and report for career analysis in AI/ML.\n\n* Government Report [[Slides](./GovReport.pdf)]\n    > A repository for government report analysis and report.","updated":"2018-11-18T01:47:07.514Z","path":"projects/index.html","_id":"cjopy03fr004s608wnng9al6e","content":"<h1 id=\"Projects-List\"><a href=\"#Projects-List\" class=\"headerlink\" title=\"Projects List\"></a>Projects List</h1><p><strong>For more information about my projects, please follow my <a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">github</a>.</strong></p>\n<ul>\n<li><p>Lagou Job Data Analysis [<a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"LagouJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, data storage, and data analysis of <a href=\"https://www.lagou.com\" target=\"_blank\" rel=\"noopener\">lagou.com</a> based on Python3.</p>\n</blockquote>\n</li>\n<li><p>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]</p>\n<blockquote>\n<p>An AI Cloud Platform with RESTful APIs.</p>\n</blockquote>\n</li>\n<li><p>JiaYuan User Profile [<a href=\"https://zhuanlan.zhihu.com/p/24515034\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/JiaYuan.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"JiaYuan.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, user profile of <a href=\"http://www.jiayuan.com/\" target=\"_blank\" rel=\"noopener\">jiayuan.com</a>.</p>\n</blockquote>\n</li>\n<li><p>XiaoLu AI [<a href=\"https://zhuanlan.zhihu.com/p/29399781\" target=\"_blank\" rel=\"noopener\">Article-Face Beauty Recognition</a>] [<a href=\"https://zhuanlan.zhihu.com/p/35135539\" target=\"_blank\" rel=\"noopener\">Article-Mate Face Recognition</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository in computer vision and NLP.</p>\n</blockquote>\n</li>\n<li><p>Zhihu Live Analysis [<a href=\"https://zhuanlan.zhihu.com/p/30514792\" target=\"_blank\" rel=\"noopener\">Article-1</a>] [<a href=\"https://zhuanlan.zhihu.com/p/31651544\" target=\"_blank\" rel=\"noopener\">Article-2</a>] [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for data crawling, data analysis, data mining and data visualization.</p>\n</blockquote>\n</li>\n<li><p>Image Guard [<a href=\"https://zhuanlan.zhihu.com/p/29016317\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.</p>\n</blockquote>\n</li>\n<li><p>Douban Sentiment Analysis [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.</p>\n</blockquote>\n</li>\n<li><p>Web Data Mining [<a href=\"https://zhuanlan.zhihu.com/p/28954770\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, analysis and mining.</p>\n</blockquote>\n</li>\n<li><p>Ctrip Web Crawler [<a href=\"https://github.com/lucasxlu/CtripPro.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for multi-thread web crawler of Ctrip based on Java.</p>\n</blockquote>\n</li>\n<li><p>MateFace (WeChat Mini Program) [<a href=\"https://github.com/lucasxlu/mateface.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A WeChat Mini Program for mate face comparision.</p>\n</blockquote>\n</li>\n<li><p>Industry Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./IndustryReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for industry analysis and report.</p>\n</blockquote>\n</li>\n<li><p>AI Career Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./MLJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository and report for career analysis in AI/ML.</p>\n</blockquote>\n</li>\n<li><p>Government Report [<a href=\"./GovReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for government report analysis and report.</p>\n</blockquote>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Projects-List\"><a href=\"#Projects-List\" class=\"headerlink\" title=\"Projects List\"></a>Projects List</h1><p><strong>For more information about my projects, please follow my <a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">github</a>.</strong></p>\n<ul>\n<li><p>Lagou Job Data Analysis [<a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"LagouJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, data storage, and data analysis of <a href=\"https://www.lagou.com\" target=\"_blank\" rel=\"noopener\">lagou.com</a> based on Python3.</p>\n</blockquote>\n</li>\n<li><p>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]</p>\n<blockquote>\n<p>An AI Cloud Platform with RESTful APIs.</p>\n</blockquote>\n</li>\n<li><p>JiaYuan User Profile [<a href=\"https://zhuanlan.zhihu.com/p/24515034\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/JiaYuan.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"JiaYuan.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, user profile of <a href=\"http://www.jiayuan.com/\" target=\"_blank\" rel=\"noopener\">jiayuan.com</a>.</p>\n</blockquote>\n</li>\n<li><p>XiaoLu AI [<a href=\"https://zhuanlan.zhihu.com/p/29399781\" target=\"_blank\" rel=\"noopener\">Article-Face Beauty Recognition</a>] [<a href=\"https://zhuanlan.zhihu.com/p/35135539\" target=\"_blank\" rel=\"noopener\">Article-Mate Face Recognition</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository in computer vision and NLP.</p>\n</blockquote>\n</li>\n<li><p>Zhihu Live Analysis [<a href=\"https://zhuanlan.zhihu.com/p/30514792\" target=\"_blank\" rel=\"noopener\">Article-1</a>] [<a href=\"https://zhuanlan.zhihu.com/p/31651544\" target=\"_blank\" rel=\"noopener\">Article-2</a>] [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for data crawling, data analysis, data mining and data visualization.</p>\n</blockquote>\n</li>\n<li><p>Image Guard [<a href=\"https://zhuanlan.zhihu.com/p/29016317\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.</p>\n</blockquote>\n</li>\n<li><p>Douban Sentiment Analysis [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.</p>\n</blockquote>\n</li>\n<li><p>Web Data Mining [<a href=\"https://zhuanlan.zhihu.com/p/28954770\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, analysis and mining.</p>\n</blockquote>\n</li>\n<li><p>Ctrip Web Crawler [<a href=\"https://github.com/lucasxlu/CtripPro.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for multi-thread web crawler of Ctrip based on Java.</p>\n</blockquote>\n</li>\n<li><p>MateFace (WeChat Mini Program) [<a href=\"https://github.com/lucasxlu/mateface.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A WeChat Mini Program for mate face comparision.</p>\n</blockquote>\n</li>\n<li><p>Industry Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./IndustryReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for industry analysis and report.</p>\n</blockquote>\n</li>\n<li><p>AI Career Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./MLJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository and report for career analysis in AI/ML.</p>\n</blockquote>\n</li>\n<li><p>Government Report [<a href=\"./GovReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for government report analysis and report.</p>\n</blockquote>\n</li>\n</ul>\n"},{"layout":"tags","title":"Tags","description":"You can find articles by tags.","header-img":null,"_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\ndescription: \"You can find articles by tags.\"\nheader-img: \n---\n","date":"2018-10-01T04:40:10.462Z","updated":"2018-10-01T04:40:10.462Z","path":"tags/index.html","comments":1,"_id":"cjopy03fu004t608wopdyyyl8","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"[Algorithm] Sort","date":"2018-08-02T03:37:29.000Z","mathjax":true,"catagories":["Algorithm","Data Structure","Graph"],"_content":"## \n### \n ____\n\n> $N$$N^2/2$$N$\n\n```java\npublic int[] selectSort (int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        int min = i;\n        for (int j = i + 1; j < input.length; j++) {\n            if (input[j] < input[min]) {\n                min = j;\n            }\n        }\n        if (input[min] != input[i]) {\n            int tmp = input[min];\n            input[min] = input[i];\n            input[i] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n* \n* $N$\n\n### \n____\n\n> $N$$\\sim N^2/4$$\\sim N^2/4$$\\sim N^2/2$$\\sim N^2/2$$N-1$0\n\n```java\npublic int[] insertSort(int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        for (int j = i; j > 0 && input[j] < input[j - 1]; j--) {\n            int tmp = input[j];\n            input[j] = input[j - 1];\n            input[j - 1] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n> \n\n> $O(N^2)$\n\n### \n____\n\n$h$$h$$h$$h$$h-$1$h$\n\n\n\n```java\npublic int[] shellSort(int[] input) {\n    int h = 1;\n    while (h < input.length / 3)\n        h = 3 * h + 1;\n\n    while (h >= 1) {\n        for (int i = h; i < input.length; i++) {\n            for (int j = i; j >= h && input[j] < input[j - h]; j -= h) {\n                int tmp = input[j];\n                input[j] = input[j - h];\n                input[j - h] = tmp;\n            }\n\n            h /= 3;\n        }\n    }\n\n    return input;\n}\n```\n\n> $N^{3/2}$\n\n### \n$N$$NlogN$$N$\n\n* \n```java\nprivate static void merge(Comparable[] input, int low, int mid, int high) {\n    int i = low, j = mid + 1;\n    for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n        aux[k] = input[k];\n    }\n\n    for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n        if (i > mid) input[k] = aux[j++];\n        else if (j > high) input[k] = aux[i++];\n        else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n        else input[k] = aux[i++];\n    }\n}\n```\n\n* \n```java\npublic class Merge {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        sort(a, 0, a.length - 1);\n    }\n\n    private static void merge(Comparable[] input, int low, int mid, int high) {\n        int i = low, j = mid + 1;\n        for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n            aux[k] = input[k];\n        }\n\n        for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n            if (i > mid) input[k] = aux[j++];\n            else if (j > high) input[k] = aux[i++];\n            else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n            else input[k] = aux[i++];\n        }\n    }\n\n    private static void sort(Comparable[] a, int lo, int hi) {\n        // sort array a[lo...hi]\n        if (hi <= lo) return;\n        int mid = lo + (hi - lo) / 2;\n        sort(a, lo, mid);\n        sort(a, mid + 1, hi);\n        merge(a, lo, mid, hi);\n    }\n}\n```\n\n> $N$$\\frac{1}{2}NlgN$$NlgN$\n\n*   \n\n```java\npublic class MergeBU {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        for (int sz = 1; sz < a.length; sz=sz+sz) {\n            for (int lo = 0; lo < a.length - sz; lo += sz + sz) {\n                merge(a, lo, lo + sz - 1, Math.min(lo + 2 * sz - 1, a.length - 1));\n            }\n\n        }\n    }\n}\n```\n\n### \n$N$$NlgN$\n\n\n\n a[lo]  i  j  a[lo]a[j] j \n\n> $N$$\\sim 2NlgN$$\\frac{N^2}{2}$\n\n","source":"_posts/algo-sort.md","raw":"---\ntitle: \"[Algorithm] Sort\"\ndate: 2018-08-02 11:37:29\nmathjax: true\ntags:\n- Algorithm\n- Data Structure\n- Graph\ncatagories:\n- Algorithm\n- Data Structure\n- Graph\n---\n## \n### \n ____\n\n> $N$$N^2/2$$N$\n\n```java\npublic int[] selectSort (int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        int min = i;\n        for (int j = i + 1; j < input.length; j++) {\n            if (input[j] < input[min]) {\n                min = j;\n            }\n        }\n        if (input[min] != input[i]) {\n            int tmp = input[min];\n            input[min] = input[i];\n            input[i] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n* \n* $N$\n\n### \n____\n\n> $N$$\\sim N^2/4$$\\sim N^2/4$$\\sim N^2/2$$\\sim N^2/2$$N-1$0\n\n```java\npublic int[] insertSort(int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        for (int j = i; j > 0 && input[j] < input[j - 1]; j--) {\n            int tmp = input[j];\n            input[j] = input[j - 1];\n            input[j - 1] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n> \n\n> $O(N^2)$\n\n### \n____\n\n$h$$h$$h$$h$$h-$1$h$\n\n\n\n```java\npublic int[] shellSort(int[] input) {\n    int h = 1;\n    while (h < input.length / 3)\n        h = 3 * h + 1;\n\n    while (h >= 1) {\n        for (int i = h; i < input.length; i++) {\n            for (int j = i; j >= h && input[j] < input[j - h]; j -= h) {\n                int tmp = input[j];\n                input[j] = input[j - h];\n                input[j - h] = tmp;\n            }\n\n            h /= 3;\n        }\n    }\n\n    return input;\n}\n```\n\n> $N^{3/2}$\n\n### \n$N$$NlogN$$N$\n\n* \n```java\nprivate static void merge(Comparable[] input, int low, int mid, int high) {\n    int i = low, j = mid + 1;\n    for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n        aux[k] = input[k];\n    }\n\n    for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n        if (i > mid) input[k] = aux[j++];\n        else if (j > high) input[k] = aux[i++];\n        else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n        else input[k] = aux[i++];\n    }\n}\n```\n\n* \n```java\npublic class Merge {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        sort(a, 0, a.length - 1);\n    }\n\n    private static void merge(Comparable[] input, int low, int mid, int high) {\n        int i = low, j = mid + 1;\n        for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n            aux[k] = input[k];\n        }\n\n        for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n            if (i > mid) input[k] = aux[j++];\n            else if (j > high) input[k] = aux[i++];\n            else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n            else input[k] = aux[i++];\n        }\n    }\n\n    private static void sort(Comparable[] a, int lo, int hi) {\n        // sort array a[lo...hi]\n        if (hi <= lo) return;\n        int mid = lo + (hi - lo) / 2;\n        sort(a, lo, mid);\n        sort(a, mid + 1, hi);\n        merge(a, lo, mid, hi);\n    }\n}\n```\n\n> $N$$\\frac{1}{2}NlgN$$NlgN$\n\n*   \n\n```java\npublic class MergeBU {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        for (int sz = 1; sz < a.length; sz=sz+sz) {\n            for (int lo = 0; lo < a.length - sz; lo += sz + sz) {\n                merge(a, lo, lo + sz - 1, Math.min(lo + 2 * sz - 1, a.length - 1));\n            }\n\n        }\n    }\n}\n```\n\n### \n$N$$NlgN$\n\n\n\n a[lo]  i  j  a[lo]a[j] j \n\n> $N$$\\sim 2NlgN$$\\frac{N^2}{2}$\n\n","slug":"algo-sort","published":1,"updated":"2018-10-01T04:40:08.403Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bc0000608wv8lsgxk0","content":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> <strong></strong></p>\n<blockquote>\n<p>$N$$N^2/2$$N$</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] selectSort (<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> min = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i + <span class=\"number\">1</span>; j &lt; input.length; j++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (input[j] &lt; input[min]) &#123;</span><br><span class=\"line\">                min = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (input[min] != input[i]) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[min];</span><br><span class=\"line\">            input[min] = input[i];</span><br><span class=\"line\">            input[i] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li></li>\n<li>$N$</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong></p>\n<blockquote>\n<p>$N$$\\sim N^2/4$$\\sim N^2/4$$\\sim N^2/2$$\\sim N^2/2$$N-1$0</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] insertSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt; <span class=\"number\">0</span> &amp;&amp; input[j] &lt; input[j - <span class=\"number\">1</span>]; j--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">            input[j] = input[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            input[j - <span class=\"number\">1</span>] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p></p>\n</blockquote>\n<blockquote>\n<p>$O(N^2)$</p>\n</blockquote>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong></p>\n<p>$h$$h$$h$$h$$h-$1$h$</p>\n<p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] shellSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> h = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &lt; input.length / <span class=\"number\">3</span>)</span><br><span class=\"line\">        h = <span class=\"number\">3</span> * h + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &gt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = h; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt;= h &amp;&amp; input[j] &lt; input[j - h]; j -= h) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">                input[j] = input[j - h];</span><br><span class=\"line\">                input[j - h] = tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            h /= <span class=\"number\">3</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>$N^{3/2}$</p>\n</blockquote>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$N$$NlogN$$N$</p>\n<ul>\n<li><p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">        aux[k] = input[k];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Merge</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        sort(a, <span class=\"number\">0</span>, a.length - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">            aux[k] = input[k];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a, <span class=\"keyword\">int</span> lo, <span class=\"keyword\">int</span> hi)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// sort array a[lo...hi]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hi &lt;= lo) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mid = lo + (hi - lo) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        sort(a, lo, mid);</span><br><span class=\"line\">        sort(a, mid + <span class=\"number\">1</span>, hi);</span><br><span class=\"line\">        merge(a, lo, mid, hi);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<blockquote>\n<p>$N$$\\frac{1}{2}NlgN$$NlgN$</p>\n</blockquote>\n<ul>\n<li><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MergeBU</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = <span class=\"number\">1</span>; sz &lt; a.length; sz=sz+sz) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> lo = <span class=\"number\">0</span>; lo &lt; a.length - sz; lo += sz + sz) &#123;</span><br><span class=\"line\">                merge(a, lo, lo + sz - <span class=\"number\">1</span>, Math.min(lo + <span class=\"number\">2</span> * sz - <span class=\"number\">1</span>, a.length - <span class=\"number\">1</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$N$$NlgN$</p>\n<p></p>\n<p> a[lo]  i  j  a[lo]a[j] j </p>\n<blockquote>\n<p>$N$$\\sim 2NlgN$$\\frac{N^2}{2}$</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> <strong></strong></p>\n<blockquote>\n<p>$N$$N^2/2$$N$</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] selectSort (<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> min = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i + <span class=\"number\">1</span>; j &lt; input.length; j++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (input[j] &lt; input[min]) &#123;</span><br><span class=\"line\">                min = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (input[min] != input[i]) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[min];</span><br><span class=\"line\">            input[min] = input[i];</span><br><span class=\"line\">            input[i] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li></li>\n<li>$N$</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong></p>\n<blockquote>\n<p>$N$$\\sim N^2/4$$\\sim N^2/4$$\\sim N^2/2$$\\sim N^2/2$$N-1$0</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] insertSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt; <span class=\"number\">0</span> &amp;&amp; input[j] &lt; input[j - <span class=\"number\">1</span>]; j--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">            input[j] = input[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            input[j - <span class=\"number\">1</span>] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p></p>\n</blockquote>\n<blockquote>\n<p>$O(N^2)$</p>\n</blockquote>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong></p>\n<p>$h$$h$$h$$h$$h-$1$h$</p>\n<p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] shellSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> h = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &lt; input.length / <span class=\"number\">3</span>)</span><br><span class=\"line\">        h = <span class=\"number\">3</span> * h + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &gt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = h; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt;= h &amp;&amp; input[j] &lt; input[j - h]; j -= h) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">                input[j] = input[j - h];</span><br><span class=\"line\">                input[j - h] = tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            h /= <span class=\"number\">3</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>$N^{3/2}$</p>\n</blockquote>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$N$$NlogN$$N$</p>\n<ul>\n<li><p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">        aux[k] = input[k];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Merge</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        sort(a, <span class=\"number\">0</span>, a.length - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">            aux[k] = input[k];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a, <span class=\"keyword\">int</span> lo, <span class=\"keyword\">int</span> hi)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// sort array a[lo...hi]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hi &lt;= lo) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mid = lo + (hi - lo) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        sort(a, lo, mid);</span><br><span class=\"line\">        sort(a, mid + <span class=\"number\">1</span>, hi);</span><br><span class=\"line\">        merge(a, lo, mid, hi);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<blockquote>\n<p>$N$$\\frac{1}{2}NlgN$$NlgN$</p>\n</blockquote>\n<ul>\n<li><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MergeBU</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = <span class=\"number\">1</span>; sz &lt; a.length; sz=sz+sz) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> lo = <span class=\"number\">0</span>; lo &lt; a.length - sz; lo += sz + sz) &#123;</span><br><span class=\"line\">                merge(a, lo, lo + sz - <span class=\"number\">1</span>, Math.min(lo + <span class=\"number\">2</span> * sz - <span class=\"number\">1</span>, a.length - <span class=\"number\">1</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$N$$NlgN$</p>\n<p></p>\n<p> a[lo]  i  j  a[lo]a[j] j </p>\n<blockquote>\n<p>$N$$\\sim 2NlgN$$\\frac{N^2}{2}$</p>\n</blockquote>\n"},{"title":"[Book] Storytelling With Data","date":"2018-08-11T14:54:59.000Z","catagories":["Data Science","Data Visualization","PPT","Presentation"],"_content":"## Introduction\n(Data Visualization)Data Science ____ Google[Storytelling With Data](http://www.storytellingwithdata.com/book/)\n\n## \n### \n()\n\n![Original Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png)\n> \n\n  \n![Revised Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png)\n\n### \n____\n![Tables](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png)\n\n ____   \n![Heatmap](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png)\n\n### \n* \n* 3D\n* y  \n![Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png)  \n![Revised Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png)\n\n## \n* \n* ()\n* \n* Less is More:  \n![Less is More](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png)\n\n## \n### \n![Preattentive Attributes](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png)\n\n### \n![Without Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png)\n\n  \n![With Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png)\n\n### \n\n\n![Color](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png)\n\n### \"\"\n\n![Spaghetti Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png)\n\n ____\n####  ( + )\n![One Line Highlighted](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png)\n\n#### \n![Vertically Apart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png)\n\n####  ( + )\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png)\n\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png)\n\n### \n  \n![Pie Chart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png)\n\n#### 1: \n![Show Numbers Directly](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png)\n\n#### 2: \n![Simple Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png)\n\n#### 3: \n![Stacked Horizontal Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png)\n\n#### 4: \n![Slope Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png)\n>  ____ ____\n","source":"_posts/book-storytelling-with-data.md","raw":"---\ntitle: \"[Book] Storytelling With Data\"\ndate: 2018-08-11 22:54:59\ntags:\n- Data Visualization\n- Data Science\ncatagories:\n- Data Science\n- Data Visualization\n- PPT\n- Presentation\n---\n## Introduction\n(Data Visualization)Data Science ____ Google[Storytelling With Data](http://www.storytellingwithdata.com/book/)\n\n## \n### \n()\n\n![Original Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png)\n> \n\n  \n![Revised Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png)\n\n### \n____\n![Tables](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png)\n\n ____   \n![Heatmap](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png)\n\n### \n* \n* 3D\n* y  \n![Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png)  \n![Revised Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png)\n\n## \n* \n* ()\n* \n* Less is More:  \n![Less is More](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png)\n\n## \n### \n![Preattentive Attributes](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png)\n\n### \n![Without Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png)\n\n  \n![With Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png)\n\n### \n\n\n![Color](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png)\n\n### \"\"\n\n![Spaghetti Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png)\n\n ____\n####  ( + )\n![One Line Highlighted](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png)\n\n#### \n![Vertically Apart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png)\n\n####  ( + )\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png)\n\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png)\n\n### \n  \n![Pie Chart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png)\n\n#### 1: \n![Show Numbers Directly](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png)\n\n#### 2: \n![Simple Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png)\n\n#### 3: \n![Stacked Horizontal Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png)\n\n#### 4: \n![Slope Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png)\n>  ____ ____\n","slug":"book-storytelling-with-data","published":1,"updated":"2018-10-01T04:40:08.404Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bi0002608wgrt7lxch","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>(Data Visualization)Data Science <strong></strong> Google<a href=\"http://www.storytellingwithdata.com/book/\" target=\"_blank\" rel=\"noopener\">Storytelling With Data</a></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>()</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png\" alt=\"Original Report\"></p>\n<blockquote>\n<p></p>\n</blockquote>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png\" alt=\"Revised Report\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png\" alt=\"Tables\"></p>\n<p> <strong></strong> <br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png\" alt=\"Heatmap\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ul>\n<li></li>\n<li>3D</li>\n<li>y<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png\" alt=\"Double Y-axis\"><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png\" alt=\"Revised Double Y-axis\"></li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ul>\n<li></li>\n<li>()</li>\n<li></li>\n<li>Less is More:<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png\" alt=\"Less is More\"></li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png\" alt=\"Preattentive Attributes\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png\" alt=\"Without Preattentive Attributes in Graph\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png\" alt=\"With Preattentive Attributes in Graph\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png\" alt=\"Color\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png\" alt=\"Spaghetti Graph\"></p>\n<p> <strong></strong></p>\n<h4 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\" ( + )\"></a> ( + )</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png\" alt=\"One Line Highlighted\"></p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png\" alt=\"Vertically Apart\"></p>\n<h4 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\" ( + )\"></a> ( + )</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png\" alt=\"Combined Approach\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png\" alt=\"Combined Approach\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png\" alt=\"Pie Chart\"></p>\n<h4 id=\"1-\"><a href=\"#1-\" class=\"headerlink\" title=\"1: \"></a>1: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png\" alt=\"Show Numbers Directly\"></p>\n<h4 id=\"2-\"><a href=\"#2-\" class=\"headerlink\" title=\"2: \"></a>2: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png\" alt=\"Simple Bar Graph\"></p>\n<h4 id=\"3-\"><a href=\"#3-\" class=\"headerlink\" title=\"3: \"></a>3: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png\" alt=\"Stacked Horizontal Bar Graph\"></p>\n<h4 id=\"4-\"><a href=\"#4-\" class=\"headerlink\" title=\"4: \"></a>4: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png\" alt=\"Slope Graph\"></p>\n<blockquote>\n<p> <strong></strong> <strong></strong></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>(Data Visualization)Data Science <strong></strong> Google<a href=\"http://www.storytellingwithdata.com/book/\" target=\"_blank\" rel=\"noopener\">Storytelling With Data</a></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>()</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png\" alt=\"Original Report\"></p>\n<blockquote>\n<p></p>\n</blockquote>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png\" alt=\"Revised Report\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><strong></strong><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png\" alt=\"Tables\"></p>\n<p> <strong></strong> <br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png\" alt=\"Heatmap\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ul>\n<li></li>\n<li>3D</li>\n<li>y<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png\" alt=\"Double Y-axis\"><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png\" alt=\"Revised Double Y-axis\"></li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ul>\n<li></li>\n<li>()</li>\n<li></li>\n<li>Less is More:<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png\" alt=\"Less is More\"></li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png\" alt=\"Preattentive Attributes\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png\" alt=\"Without Preattentive Attributes in Graph\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png\" alt=\"With Preattentive Attributes in Graph\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png\" alt=\"Color\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png\" alt=\"Spaghetti Graph\"></p>\n<p> <strong></strong></p>\n<h4 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\" ( + )\"></a> ( + )</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png\" alt=\"One Line Highlighted\"></p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png\" alt=\"Vertically Apart\"></p>\n<h4 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\" ( + )\"></a> ( + )</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png\" alt=\"Combined Approach\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png\" alt=\"Combined Approach\"></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png\" alt=\"Pie Chart\"></p>\n<h4 id=\"1-\"><a href=\"#1-\" class=\"headerlink\" title=\"1: \"></a>1: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png\" alt=\"Show Numbers Directly\"></p>\n<h4 id=\"2-\"><a href=\"#2-\" class=\"headerlink\" title=\"2: \"></a>2: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png\" alt=\"Simple Bar Graph\"></p>\n<h4 id=\"3-\"><a href=\"#3-\" class=\"headerlink\" title=\"3: \"></a>3: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png\" alt=\"Stacked Horizontal Bar Graph\"></p>\n<h4 id=\"4-\"><a href=\"#4-\" class=\"headerlink\" title=\"4: \"></a>4: </h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png\" alt=\"Slope Graph\"></p>\n<blockquote>\n<p> <strong></strong> <strong></strong></p>\n</blockquote>\n"},{"title":"[CV] Face Anti-Spoofing","date":"2018-10-29T16:23:26.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Face Anti-Spoofing"],"_content":"## Introduction\nFace Anti-spoofingiPhoneX FaceIDanti-spoofingFace Anti-spoofingresearch benchmarkdataset capacitymodeloverfittingBinary ClassificationPrecisionRecall99.9%+Loss\n\nAttack3\n* print attack: \n* replay attack: \n* 3D mask attack: 3D\n\nAnti-spoofing\n* Image Quality Analysis: (recapture)moire pattern (LBP descriptor)patterndeep modelsPaper[1]R ChannelGBGrayScale\n* Command Motion: \n* 3D Depth Information: print/replay attack2D3DReject Option\n\n\nCVPR/ECCV/TIP/TIFS/Paperidea\n\n## Reference\n1. Patel K, Han H, Jain A K. [Secure face unlock: Spoof detection on smartphones](http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf)[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.\n2. ","source":"_posts/cv-antispoofing.md","raw":"---\ntitle: \"[CV] Face Anti-Spoofing\"\ndate: 2018-10-30 00:23:26\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Anti-Spoofing\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Anti-Spoofing\n---\n## Introduction\nFace Anti-spoofingiPhoneX FaceIDanti-spoofingFace Anti-spoofingresearch benchmarkdataset capacitymodeloverfittingBinary ClassificationPrecisionRecall99.9%+Loss\n\nAttack3\n* print attack: \n* replay attack: \n* 3D mask attack: 3D\n\nAnti-spoofing\n* Image Quality Analysis: (recapture)moire pattern (LBP descriptor)patterndeep modelsPaper[1]R ChannelGBGrayScale\n* Command Motion: \n* 3D Depth Information: print/replay attack2D3DReject Option\n\n\nCVPR/ECCV/TIP/TIFS/Paperidea\n\n## Reference\n1. Patel K, Han H, Jain A K. [Secure face unlock: Spoof detection on smartphones](http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf)[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.\n2. ","slug":"cv-antispoofing","published":1,"updated":"2018-10-29T16:25:25.350Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bo0005608wvuyyafyt","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Face Anti-spoofingiPhoneX FaceIDanti-spoofingFace Anti-spoofingresearch benchmarkdataset capacitymodeloverfittingBinary ClassificationPrecisionRecall99.9%+Loss</p>\n<p>Attack3</p>\n<ul>\n<li>print attack: </li>\n<li>replay attack: </li>\n<li>3D mask attack: 3D</li>\n</ul>\n<p>Anti-spoofing</p>\n<ul>\n<li>Image Quality Analysis: (recapture)moire pattern (LBP descriptor)patterndeep modelsPaper[1]R ChannelGBGrayScale</li>\n<li>Command Motion: </li>\n<li>3D Depth Information: print/replay attack2D3DReject Option</li>\n</ul>\n<p>CVPR/ECCV/TIP/TIFS/Paperidea</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Patel K, Han H, Jain A K. <a href=\"http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf\" target=\"_blank\" rel=\"noopener\">Secure face unlock: Spoof detection on smartphones</a>[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.</li>\n<li></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Face Anti-spoofingiPhoneX FaceIDanti-spoofingFace Anti-spoofingresearch benchmarkdataset capacitymodeloverfittingBinary ClassificationPrecisionRecall99.9%+Loss</p>\n<p>Attack3</p>\n<ul>\n<li>print attack: </li>\n<li>replay attack: </li>\n<li>3D mask attack: 3D</li>\n</ul>\n<p>Anti-spoofing</p>\n<ul>\n<li>Image Quality Analysis: (recapture)moire pattern (LBP descriptor)patterndeep modelsPaper[1]R ChannelGBGrayScale</li>\n<li>Command Motion: </li>\n<li>3D Depth Information: print/replay attack2D3DReject Option</li>\n</ul>\n<p>CVPR/ECCV/TIP/TIFS/Paperidea</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Patel K, Han H, Jain A K. <a href=\"http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf\" target=\"_blank\" rel=\"noopener\">Secure face unlock: Spoof detection on smartphones</a>[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.</li>\n<li></li>\n</ol>\n"},{"title":"[CV] Object Detection","date":"2018-11-11T13:07:05.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Object Detection"],"_content":"## Introduction\nObject DetectionComputer Vision(/)RCNN--SPPNet--Fast RCNN--Faster RCNN--FCN--Mask RCNNYOLO v1/2/3, SSDObject Detection\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n## RCNN (Region-based CNN)\n> Paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)\n\n### What is RCNN?\nPaperDeep LearningObject DetectionMachine Learning/Pattern Recognition**Feature Matters in approximately every task!** RCNNCNNFeaturedetectorfeature(SIFT/LBP/HOG)\n\nRCNNRegions with CNN features(1)[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)2000Region Proposal(2)Pretrained CNNRegion Proposaldeep feature(from pool5)(3)linear SVMone-VS-restObject DetectionClassificationSelective Searchbbox<font color=\"orange\">Bounding Box Regression</font>()RCNNidea\n\n![RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png)\n\n### Details of RCNN\n#### Pretraining and Fine-tuning\nRCNNBase NetworkImageNettrain1000 classclassifieroutput neuron21(20Foreground + 1Background)target datasetsfine-tuneSelective SearchRegion Proposalgroundtruth Bbox$IOU\\geq 0.5$positive samplenegative sample\n\npool5 features learned from ImageNetgeneraldomain-specific datasetsnon-linear classifier\n\n#### Bounding Box Regression\n$N$training pairs$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$$P^i$$(x,y)$widthheight$G=(G_x,G_y,G_w,G_h)$groundtruth bboxBBox Regressionmappingproposed box $P$  groundtruth box $G$\n\n$x,y$transformation$d_x(P),d_y(P)$<font color=\"red\">scale-invariant translation$w,h$log-space translation</font>input proposalpredicted groundtruth box $\\hat{G}$:\n$$\n\\hat{G}_x=P_w d_x(P)+P_x\n$$\n\n$$\n\\hat{G}_y=P_h d_x(P)+P_y\n$$\n\n$$\n\\hat{G}_w=P_w exp(d_w(P))\n$$\n\n$$\n\\hat{G}_h=P_h exp(d_h(P))\n$$\n\n$d_{\\star}(P)$$pool_5$ featureOLS\n$$\nw_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2\n$$\n\nThe regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:\n$$\nt_x=\\frac{G_x-P_x}{P_w}\n$$\n\n$$\nt_y=\\frac{G_y-P_y}{P_h}\n$$\n\n$$\nt_w=log(\\frac{G_w}{P_w})\n$$\n\n$$\nt_h=log(\\frac{G_h}{P_h})\n$$\n\nProposed BboxGroundtruth Bbox($IOU\\geq 0.6$)Bounding Box Regression\n\nDeep LearningObject Detection--RCNN\n\n\n## SPPNet\n> Paper: [Spatial pyramid pooling in deep convolutional networks for visual recognition.](https://arxiv.org/pdf/1406.4729v4.pdf)\n\n### What is SPPNet?\nSPPNet(Spatial Pyramid Pooling)RCNNObject DetectionSPPNetRCNNRCNNRegion-based CNNCNNbase networkCNNsquared inputFC Layersfeature mapSSPlayerfeature mapconcatenatedetectionregion proposalfeature mapRCNN2000(2000 Region Proposal)\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)Deep ArchitectureFully Convolutional Architecture()Fully Connected LayersGlobal Average Pooling\n\n![Crop/Warp VS SPP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg)\n\n\n### Why SPPNet?\nSPPNetKaiming He\n1. SPPinput sizefixed size outputsliding window\n2. SPP uses multi-level spatial binssliding windowsingle window size<font color=\"red\">multi-level poolingobject deformationrobust</font>\n3. SPP can pool features extracted at variable scales thanks to the flexibility of input scales.\n4. Training with variable-size images increases scale-invariance and reduces over-fitting.\n\n### Details of SPPNet\n#### SPP Layer\nSPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.\n\n![SPP Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg)\n\nspatial bin poolingk M-dimensional feature concatenationfixed lengthfeature vectorFC Layers/SVMMLtrain\n\n#### SPP for Detection\nRCNN2KRegion Proposalfeedforwad Pretrained CNNSPPNetimage(possible multi-scale))feedforwadCNN<font color=\"red\">feature map</font>candidate windowSPP Layer poolfixed-length feature representation of the window\n\n![Pooling](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg)\n\nRegion ProposalRCNN[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)2000bbox candidateimage resize$min(w, h)=s$ 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the featureswindowPooling12800-Dimensional (25650) FC LayersRCNNlinear SVM\n\nSPP Detectorgroundtruth bbox$IOU\\geq 0.3$positive samplenegative sample\n\n\n## Fast RCNN\n> Paper: [Fast RCNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)\n\nFast RCNNObject Detectionnoveltybranchmulti-task learning(category classificationbbox regression)\n\n### Why Fast RCNN?\n(RCNN/SPPNet)\n1. (RCNN/SPP)multi-stage pipelineSelective Search2KRegion Proposallog lossfine-tunedeep CNNDeep CNNfeaturelinear SVMBounding Box Regression\n2. CNNRegion Proposaldeep featurelinear SVM\n3. testingDeep CNNfeaturefeaturelinear SVM\n\nFast RCNN\n1. multi-task lossobject classificationbounding box regression\n2. Training is single stage.\n3. Higher performance than RCNN and SPPNet.\n\n### Details of Fast RCNN\n![Fast RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg)\n\nFast RCNN pipelinewhole image with several object proposalsCNNfeatureobject proposal<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>RoI Pooling Layerfeature vectormulti-branchclassificationbbox regression\n\nFast RCNN<font color=\"red\">RoI Pooling</font>RoI Pooling\n\n#### The RoI pooling layer\nFast RCNN\n> RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.\n> \n> In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.\n>\n> RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].\n\nvalid region proposalfeature mapmax poolingfeature map\n\n#### Fine-tuning for detection\n##### Multi-Task Loss\nFast RCNN$K+1$ (Kobject class + 1background) classification($p=(p_0,p_1,\\cdots,p_K)$)bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)\n\nWe use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(linear regressionwidthheightlog). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:\n$$\nL(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)\n$$\n$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.\n\nLoss Function(regression loss)$[u\\geq 1]$$u\\geq 1$10settingbackground$[u\\geq 1]$0regression lossregressionMSE LossMSE LossCost-sensitive LossoutliersRoss$Smooth L_1 Loss$\n$$\nL_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)\n$$\n\nSmooth L1 Loss\n$$\nsmooth_{L_1}(x)=\n\\begin{cases}\n0.5x^2 & if |x|<1\\\\\n|x|-0.5 & otherwise\n\\end{cases}\n$$\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)Machine LearningLoss[](https://lucasxlu.github.io/blog/2018/07/24/ml-loss/)\n\n##### Mini-batch sampling\nFine-tuningmini-batch$N=2$image64samplegroundtruth bbox $IOU\\geq 0.5$foreground samples[$u=1$]background samples[$u=0$]\n\n### Fast R-CNN detection\nFully Connected LayersFC Layerstruncated SVD\n\nIn this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:\n$$\nW\\approx U\\Sigma_t V^T\n$$\n$U$$W$$t$left-singular vectors$u\\times t$$\\Sigma_t$$W$$t$singular value$t\\times t$$V$$W$$t$right-singular vectors$v\\times t$Truncated SVD$uv$$t(u+v)$\n\n\n1. $conv_1$ layers feature mapgeneraltask-independentfeature\n2. Region ProposalDetection (featuresampling mini-batchkaiming He[Focal Loss](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf))\n\n\n## Faster RCNN\n> Paper: [Faster r-cnn: Towards real-time object detection with region proposal networks](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n\nFaster RCNNObject DetectionRegion Proposal Network(RPN)RPNRCNN--SPP--Fast RCNNtwo-stage detector<font color=\"red\">Selective Search</font>2000Region Proposal<font color=\"red\">Selective Search</font>RPNNetwork ArchitectureRegion ProposalRPNobject boundsobjectness scoreRPNFeatureDetection NetworkRegion Proposalcost-freeFaster RCNN**Faster**\n\n\n### What is Faster RCNN?\n$$\nFaster RCNN = Fast RCNN + RPN\n$$\n\ndetectorshared features between proposals<font color=\"red\">Region Proposal</font>RPN\n\nconv feature maps used by region-based detectorsregion proposalsconv features<font color=\"red\">RPNconv layerencodeconv feature map position(256-d)conv layerconv feature map positionkregion proposal with various scales and aspect ratiosobjectness scoreregression bounds</font>RPN\n\n### Region Proposal Network\nRPNimageobject proposalsobjectness scoreRPNregion proposals\n\nshared conv feature mapslideinput conv feature map$n\\times n$spatial windowsliding window256-dfeature vectorfeature vectorfully connected layers--box regressionbox classificationsliding windowfully-connected layersspatial locations$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers\n\n#### Translation-Invariant Anchors\nsliding window location$k$region proposalsregression layer$4k$encoding$k$bboxoutputsclassification layer$2k$scores(region proposalobject/non-object)\n\n> The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n\n3scales3aspect ratiossliding position$k=9$anchors$W\\times H$conv feature map$WHk$anchor**translation invariant**\n\n#### A Loss Function for Learning Region Proposals\nRPNanchorbinary class(object)anchorspositive label:\n1. groundtruth bboxIoUanchor\n2. groundtruth bbox $IoU\\geq 0.7$anchor\n\ngroundtruth bboxanchorgroundtruth bbox $IoU\\leq 0.3$anchornegative anchorpositivenegativeanchortrainingFaster RCNNLoss Function\n$$\nL(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})\n$$\n$p_i$anchor $i$ objectanchorpositivegroundtruth label $p_i^{{\\star}}$1anchornegative0$t_i$4bbox$t_i^{\\star}$groundtruth positive anchor$L_{cls}$Log Loss(object VS non-object)regression loss$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$$R$Smooth L1 Loss(Fast RCNN)$p_i^{\\star} L_{reg}$<font color=\"red\">positive anchor ($p_i^{\\star}=1$)($p_i^{\\star}=0$)</font>\n\nBounding Box Regressionpipeline\n$$\nt_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})\n$$\n\n$$\nt_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})\n$$\n\n> In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. **To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights**. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.\n\n#### Sharing Convolutional Features for Region Proposal and Object Detection\n1. Fine-tuneImageNetPretrainRPNregion proposal task\n2. RPNregion proposaltrain Fast RCNNRPNFaster RCNN\n3. detector networkRPNfix shared conv layersfine-tuneRPNRPNFast RCNN\n4. Fixshared conv layersfine-tune Fast RCNNfc layersRPNFast RCNNunified network\n\n\n## SSD\n> Paper: [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325v5.pdf)\n\nSSDone-stage detectorone-stagetwo-stageDL DetectorRCNN/SSP/Fast RCNN/Faster RCNNtwo-stage detectors**region proposalsdetectionregion proposalsclassification**one-stage detectionregion proposalsbboxFaster RCNNtwo-stage detectionregion proposal generation(Selective Search)RPNregion proposals\n\n### What is SSD?\nSSD**Convolution Filterobject categorybbox offset**SSDconv filterfeature mapScale Invariant\n\n![SSD Framework](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png)\n\n### Details of SSD\nSSDDCNNfeaturefixed-sizebboxbboxpresence scoreNMSFeature ExtractionDCNN\n1. **Multi-scale feature maps for detection**: base feature extraction networkconv layers(multi-scalefeature maps)multi-scaledetection\n2. **Convolutional predictors for detection**: feature layer```small conv filters```fixed-size detection predictions\n3. **Default boxes and aspect ratios**: feature map cellcelldefault boxrelative offsetclass-score(boxclass instance)given location$k$box4bbox offset$c$class score```feature map location```$(c+4)k$filters$m\\times n$```feature map```$(c+4)kmn$outputFaster RCNNanchor box**SSDresolutionfeature mapfeature mapdefault box shapeoutput box shape**\n\n![SSD and YOLO](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png)\n\n#### Training of SSD\nSSDdefault boxvaried location/ratio/scaleboxesboxgroundtruthMatching Strategy: groundtruth boxdefault boxeslocation/ratio/scaleboxesgroundtruth boxJaccard OverlapboxesSSDLoss\n$$\nL(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))\n$$\n$N$matched default boxesLocalisation LossFaster RCNNSmooth L1$x_{ij}^p=\\{0,1\\}$$i$default box$j$groundtruth boxindicator\n$$\nL_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)\n$$\n\n$$\n\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w\n$$\n\n$$\n\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h\n$$\n\n$$\n\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})\n$$\n\n$$\n\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})\n$$\n\nConfidence LossSoftmax Loss:\n$$\nL_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)\n$$\n$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$\n\nSSDlarge objectsmall objecthigher layersfeature mapsmall objectinput size$300\\times 300$$512\\times 512$**Zoom Data Augmentation**(zoom inlarge objects, zoom outsmall objects)\n\n\n## Light-head RCNN\n> Paper: [Light-Head R-CNN: In Defense of Two-Stage Object Detector](https://arxiv.org/pdf/1711.07264v2.pdf)\n\n### Introduction\nLight-head RCNNtwo-stage detectorheavy-headtwo-stage detectortwo-stage detectorRoI Warp/ Faster RCNN2fully connected layersnRoI RecognitionRFCNscore mapsbackbone network```light-head``` RCNN```light-head``````thin feature map + cheap RCNN subnet (poolingfully connected layer)```\n\ntwo-stage detectordetectionclassificationstageregion proposal (```body```)stageregion proposal (```head```)two-stage detectoraccuracyone-stage detectoraccuracyheadheavyLight-head RCNN\n> In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce \"thin\" feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.\n\n### Delve Into Light-Head RCNN\n#### RCNN Subnet\n> Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, **R-FCN first produces a set of score maps for each region, whose channel number will be $classes\\_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation**.\n\nFaster RCNNRoI Classificationglobal average poolingfully connected layer```GAPspatial localization```Faster RCNNRoIfeedforwardRCNN subnetproposal\n\n#### Thin Feature Maps for RoI Warping\nfeed region proposalRCNN subnetRoI warpingfixed shapefeature mapslight-head```thin feature maps```RoI Pooling```RoI warping on thin feature maps```traininginferenceRoI poolingthin feature mapsGAPspatial information\n\n![Light Head RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg)\n\n### Experiments\nregression lossclassification loss```regression lossdoublebalance multi-task training```\n\n\n## YOLO v1\n> Paper: [You Only Look Once: Unified, Real-Time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n\n## Reference\n1. Girshick, Ross, et al. [\"Rich feature hierarchies for accurate object detection and semantic segmentation.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. He, Kaiming, et al. [\"Spatial pyramid pooling in deep convolutional networks for visual recognition.\"](https://arxiv.org/pdf/1406.4729v4.pdf) European conference on computer vision. Springer, Cham, 2014.\n3. Girshick, Ross. [\"Fast r-cnn.\"](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) Proceedings of the IEEE international conference on computer vision. 2015.\n4. Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollr. [\"Focal Loss for Dense Object Detection.\"](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)\n5. Ren, Shaoqing, et al. [\"Faster r-cnn: Towards real-time object detection with region proposal networks.\"](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) Advances in neural information processing systems. 2015.\n6. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016, October). [Ssd: Single shot multibox detector](https://arxiv.org/pdf/1512.02325v5.pdf). In European conference on computer vision (pp. 21-37). Springer, Cham.\n7. Redmon, Joseph, et al. [\"You only look once: Unified, real-time object detection.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n8. Li Z, Peng C, Yu G, et al. [Light-head r-cnn: In defense of two-stage object detector](https://arxiv.org/pdf/1711.07264v2.pdf)[J]. arXiv preprint arXiv:1711.07264, 2017.","source":"_posts/cv-detection.md","raw":"---\ntitle: \"[CV] Object Detection\"\ndate: 2018-11-11 21:07:05\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Object Detection\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Object Detection\n---\n## Introduction\nObject DetectionComputer Vision(/)RCNN--SPPNet--Fast RCNN--Faster RCNN--FCN--Mask RCNNYOLO v1/2/3, SSDObject Detection\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n## RCNN (Region-based CNN)\n> Paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)\n\n### What is RCNN?\nPaperDeep LearningObject DetectionMachine Learning/Pattern Recognition**Feature Matters in approximately every task!** RCNNCNNFeaturedetectorfeature(SIFT/LBP/HOG)\n\nRCNNRegions with CNN features(1)[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)2000Region Proposal(2)Pretrained CNNRegion Proposaldeep feature(from pool5)(3)linear SVMone-VS-restObject DetectionClassificationSelective Searchbbox<font color=\"orange\">Bounding Box Regression</font>()RCNNidea\n\n![RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png)\n\n### Details of RCNN\n#### Pretraining and Fine-tuning\nRCNNBase NetworkImageNettrain1000 classclassifieroutput neuron21(20Foreground + 1Background)target datasetsfine-tuneSelective SearchRegion Proposalgroundtruth Bbox$IOU\\geq 0.5$positive samplenegative sample\n\npool5 features learned from ImageNetgeneraldomain-specific datasetsnon-linear classifier\n\n#### Bounding Box Regression\n$N$training pairs$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$$P^i$$(x,y)$widthheight$G=(G_x,G_y,G_w,G_h)$groundtruth bboxBBox Regressionmappingproposed box $P$  groundtruth box $G$\n\n$x,y$transformation$d_x(P),d_y(P)$<font color=\"red\">scale-invariant translation$w,h$log-space translation</font>input proposalpredicted groundtruth box $\\hat{G}$:\n$$\n\\hat{G}_x=P_w d_x(P)+P_x\n$$\n\n$$\n\\hat{G}_y=P_h d_x(P)+P_y\n$$\n\n$$\n\\hat{G}_w=P_w exp(d_w(P))\n$$\n\n$$\n\\hat{G}_h=P_h exp(d_h(P))\n$$\n\n$d_{\\star}(P)$$pool_5$ featureOLS\n$$\nw_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2\n$$\n\nThe regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:\n$$\nt_x=\\frac{G_x-P_x}{P_w}\n$$\n\n$$\nt_y=\\frac{G_y-P_y}{P_h}\n$$\n\n$$\nt_w=log(\\frac{G_w}{P_w})\n$$\n\n$$\nt_h=log(\\frac{G_h}{P_h})\n$$\n\nProposed BboxGroundtruth Bbox($IOU\\geq 0.6$)Bounding Box Regression\n\nDeep LearningObject Detection--RCNN\n\n\n## SPPNet\n> Paper: [Spatial pyramid pooling in deep convolutional networks for visual recognition.](https://arxiv.org/pdf/1406.4729v4.pdf)\n\n### What is SPPNet?\nSPPNet(Spatial Pyramid Pooling)RCNNObject DetectionSPPNetRCNNRCNNRegion-based CNNCNNbase networkCNNsquared inputFC Layersfeature mapSSPlayerfeature mapconcatenatedetectionregion proposalfeature mapRCNN2000(2000 Region Proposal)\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)Deep ArchitectureFully Convolutional Architecture()Fully Connected LayersGlobal Average Pooling\n\n![Crop/Warp VS SPP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg)\n\n\n### Why SPPNet?\nSPPNetKaiming He\n1. SPPinput sizefixed size outputsliding window\n2. SPP uses multi-level spatial binssliding windowsingle window size<font color=\"red\">multi-level poolingobject deformationrobust</font>\n3. SPP can pool features extracted at variable scales thanks to the flexibility of input scales.\n4. Training with variable-size images increases scale-invariance and reduces over-fitting.\n\n### Details of SPPNet\n#### SPP Layer\nSPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.\n\n![SPP Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg)\n\nspatial bin poolingk M-dimensional feature concatenationfixed lengthfeature vectorFC Layers/SVMMLtrain\n\n#### SPP for Detection\nRCNN2KRegion Proposalfeedforwad Pretrained CNNSPPNetimage(possible multi-scale))feedforwadCNN<font color=\"red\">feature map</font>candidate windowSPP Layer poolfixed-length feature representation of the window\n\n![Pooling](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg)\n\nRegion ProposalRCNN[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)2000bbox candidateimage resize$min(w, h)=s$ 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the featureswindowPooling12800-Dimensional (25650) FC LayersRCNNlinear SVM\n\nSPP Detectorgroundtruth bbox$IOU\\geq 0.3$positive samplenegative sample\n\n\n## Fast RCNN\n> Paper: [Fast RCNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)\n\nFast RCNNObject Detectionnoveltybranchmulti-task learning(category classificationbbox regression)\n\n### Why Fast RCNN?\n(RCNN/SPPNet)\n1. (RCNN/SPP)multi-stage pipelineSelective Search2KRegion Proposallog lossfine-tunedeep CNNDeep CNNfeaturelinear SVMBounding Box Regression\n2. CNNRegion Proposaldeep featurelinear SVM\n3. testingDeep CNNfeaturefeaturelinear SVM\n\nFast RCNN\n1. multi-task lossobject classificationbounding box regression\n2. Training is single stage.\n3. Higher performance than RCNN and SPPNet.\n\n### Details of Fast RCNN\n![Fast RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg)\n\nFast RCNN pipelinewhole image with several object proposalsCNNfeatureobject proposal<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>RoI Pooling Layerfeature vectormulti-branchclassificationbbox regression\n\nFast RCNN<font color=\"red\">RoI Pooling</font>RoI Pooling\n\n#### The RoI pooling layer\nFast RCNN\n> RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.\n> \n> In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.\n>\n> RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].\n\nvalid region proposalfeature mapmax poolingfeature map\n\n#### Fine-tuning for detection\n##### Multi-Task Loss\nFast RCNN$K+1$ (Kobject class + 1background) classification($p=(p_0,p_1,\\cdots,p_K)$)bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)\n\nWe use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(linear regressionwidthheightlog). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:\n$$\nL(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)\n$$\n$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.\n\nLoss Function(regression loss)$[u\\geq 1]$$u\\geq 1$10settingbackground$[u\\geq 1]$0regression lossregressionMSE LossMSE LossCost-sensitive LossoutliersRoss$Smooth L_1 Loss$\n$$\nL_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)\n$$\n\nSmooth L1 Loss\n$$\nsmooth_{L_1}(x)=\n\\begin{cases}\n0.5x^2 & if |x|<1\\\\\n|x|-0.5 & otherwise\n\\end{cases}\n$$\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)Machine LearningLoss[](https://lucasxlu.github.io/blog/2018/07/24/ml-loss/)\n\n##### Mini-batch sampling\nFine-tuningmini-batch$N=2$image64samplegroundtruth bbox $IOU\\geq 0.5$foreground samples[$u=1$]background samples[$u=0$]\n\n### Fast R-CNN detection\nFully Connected LayersFC Layerstruncated SVD\n\nIn this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:\n$$\nW\\approx U\\Sigma_t V^T\n$$\n$U$$W$$t$left-singular vectors$u\\times t$$\\Sigma_t$$W$$t$singular value$t\\times t$$V$$W$$t$right-singular vectors$v\\times t$Truncated SVD$uv$$t(u+v)$\n\n\n1. $conv_1$ layers feature mapgeneraltask-independentfeature\n2. Region ProposalDetection (featuresampling mini-batchkaiming He[Focal Loss](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf))\n\n\n## Faster RCNN\n> Paper: [Faster r-cnn: Towards real-time object detection with region proposal networks](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n\nFaster RCNNObject DetectionRegion Proposal Network(RPN)RPNRCNN--SPP--Fast RCNNtwo-stage detector<font color=\"red\">Selective Search</font>2000Region Proposal<font color=\"red\">Selective Search</font>RPNNetwork ArchitectureRegion ProposalRPNobject boundsobjectness scoreRPNFeatureDetection NetworkRegion Proposalcost-freeFaster RCNN**Faster**\n\n\n### What is Faster RCNN?\n$$\nFaster RCNN = Fast RCNN + RPN\n$$\n\ndetectorshared features between proposals<font color=\"red\">Region Proposal</font>RPN\n\nconv feature maps used by region-based detectorsregion proposalsconv features<font color=\"red\">RPNconv layerencodeconv feature map position(256-d)conv layerconv feature map positionkregion proposal with various scales and aspect ratiosobjectness scoreregression bounds</font>RPN\n\n### Region Proposal Network\nRPNimageobject proposalsobjectness scoreRPNregion proposals\n\nshared conv feature mapslideinput conv feature map$n\\times n$spatial windowsliding window256-dfeature vectorfeature vectorfully connected layers--box regressionbox classificationsliding windowfully-connected layersspatial locations$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers\n\n#### Translation-Invariant Anchors\nsliding window location$k$region proposalsregression layer$4k$encoding$k$bboxoutputsclassification layer$2k$scores(region proposalobject/non-object)\n\n> The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n\n3scales3aspect ratiossliding position$k=9$anchors$W\\times H$conv feature map$WHk$anchor**translation invariant**\n\n#### A Loss Function for Learning Region Proposals\nRPNanchorbinary class(object)anchorspositive label:\n1. groundtruth bboxIoUanchor\n2. groundtruth bbox $IoU\\geq 0.7$anchor\n\ngroundtruth bboxanchorgroundtruth bbox $IoU\\leq 0.3$anchornegative anchorpositivenegativeanchortrainingFaster RCNNLoss Function\n$$\nL(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})\n$$\n$p_i$anchor $i$ objectanchorpositivegroundtruth label $p_i^{{\\star}}$1anchornegative0$t_i$4bbox$t_i^{\\star}$groundtruth positive anchor$L_{cls}$Log Loss(object VS non-object)regression loss$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$$R$Smooth L1 Loss(Fast RCNN)$p_i^{\\star} L_{reg}$<font color=\"red\">positive anchor ($p_i^{\\star}=1$)($p_i^{\\star}=0$)</font>\n\nBounding Box Regressionpipeline\n$$\nt_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})\n$$\n\n$$\nt_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})\n$$\n\n> In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. **To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights**. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.\n\n#### Sharing Convolutional Features for Region Proposal and Object Detection\n1. Fine-tuneImageNetPretrainRPNregion proposal task\n2. RPNregion proposaltrain Fast RCNNRPNFaster RCNN\n3. detector networkRPNfix shared conv layersfine-tuneRPNRPNFast RCNN\n4. Fixshared conv layersfine-tune Fast RCNNfc layersRPNFast RCNNunified network\n\n\n## SSD\n> Paper: [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325v5.pdf)\n\nSSDone-stage detectorone-stagetwo-stageDL DetectorRCNN/SSP/Fast RCNN/Faster RCNNtwo-stage detectors**region proposalsdetectionregion proposalsclassification**one-stage detectionregion proposalsbboxFaster RCNNtwo-stage detectionregion proposal generation(Selective Search)RPNregion proposals\n\n### What is SSD?\nSSD**Convolution Filterobject categorybbox offset**SSDconv filterfeature mapScale Invariant\n\n![SSD Framework](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png)\n\n### Details of SSD\nSSDDCNNfeaturefixed-sizebboxbboxpresence scoreNMSFeature ExtractionDCNN\n1. **Multi-scale feature maps for detection**: base feature extraction networkconv layers(multi-scalefeature maps)multi-scaledetection\n2. **Convolutional predictors for detection**: feature layer```small conv filters```fixed-size detection predictions\n3. **Default boxes and aspect ratios**: feature map cellcelldefault boxrelative offsetclass-score(boxclass instance)given location$k$box4bbox offset$c$class score```feature map location```$(c+4)k$filters$m\\times n$```feature map```$(c+4)kmn$outputFaster RCNNanchor box**SSDresolutionfeature mapfeature mapdefault box shapeoutput box shape**\n\n![SSD and YOLO](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png)\n\n#### Training of SSD\nSSDdefault boxvaried location/ratio/scaleboxesboxgroundtruthMatching Strategy: groundtruth boxdefault boxeslocation/ratio/scaleboxesgroundtruth boxJaccard OverlapboxesSSDLoss\n$$\nL(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))\n$$\n$N$matched default boxesLocalisation LossFaster RCNNSmooth L1$x_{ij}^p=\\{0,1\\}$$i$default box$j$groundtruth boxindicator\n$$\nL_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)\n$$\n\n$$\n\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w\n$$\n\n$$\n\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h\n$$\n\n$$\n\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})\n$$\n\n$$\n\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})\n$$\n\nConfidence LossSoftmax Loss:\n$$\nL_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)\n$$\n$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$\n\nSSDlarge objectsmall objecthigher layersfeature mapsmall objectinput size$300\\times 300$$512\\times 512$**Zoom Data Augmentation**(zoom inlarge objects, zoom outsmall objects)\n\n\n## Light-head RCNN\n> Paper: [Light-Head R-CNN: In Defense of Two-Stage Object Detector](https://arxiv.org/pdf/1711.07264v2.pdf)\n\n### Introduction\nLight-head RCNNtwo-stage detectorheavy-headtwo-stage detectortwo-stage detectorRoI Warp/ Faster RCNN2fully connected layersnRoI RecognitionRFCNscore mapsbackbone network```light-head``` RCNN```light-head``````thin feature map + cheap RCNN subnet (poolingfully connected layer)```\n\ntwo-stage detectordetectionclassificationstageregion proposal (```body```)stageregion proposal (```head```)two-stage detectoraccuracyone-stage detectoraccuracyheadheavyLight-head RCNN\n> In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce \"thin\" feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.\n\n### Delve Into Light-Head RCNN\n#### RCNN Subnet\n> Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, **R-FCN first produces a set of score maps for each region, whose channel number will be $classes\\_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation**.\n\nFaster RCNNRoI Classificationglobal average poolingfully connected layer```GAPspatial localization```Faster RCNNRoIfeedforwardRCNN subnetproposal\n\n#### Thin Feature Maps for RoI Warping\nfeed region proposalRCNN subnetRoI warpingfixed shapefeature mapslight-head```thin feature maps```RoI Pooling```RoI warping on thin feature maps```traininginferenceRoI poolingthin feature mapsGAPspatial information\n\n![Light Head RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg)\n\n### Experiments\nregression lossclassification loss```regression lossdoublebalance multi-task training```\n\n\n## YOLO v1\n> Paper: [You Only Look Once: Unified, Real-Time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n\n## Reference\n1. Girshick, Ross, et al. [\"Rich feature hierarchies for accurate object detection and semantic segmentation.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. He, Kaiming, et al. [\"Spatial pyramid pooling in deep convolutional networks for visual recognition.\"](https://arxiv.org/pdf/1406.4729v4.pdf) European conference on computer vision. Springer, Cham, 2014.\n3. Girshick, Ross. [\"Fast r-cnn.\"](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) Proceedings of the IEEE international conference on computer vision. 2015.\n4. Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollr. [\"Focal Loss for Dense Object Detection.\"](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)\n5. Ren, Shaoqing, et al. [\"Faster r-cnn: Towards real-time object detection with region proposal networks.\"](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) Advances in neural information processing systems. 2015.\n6. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016, October). [Ssd: Single shot multibox detector](https://arxiv.org/pdf/1512.02325v5.pdf). In European conference on computer vision (pp. 21-37). Springer, Cham.\n7. Redmon, Joseph, et al. [\"You only look once: Unified, real-time object detection.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n8. Li Z, Peng C, Yu G, et al. [Light-head r-cnn: In defense of two-stage object detector](https://arxiv.org/pdf/1711.07264v2.pdf)[J]. arXiv preprint arXiv:1711.07264, 2017.","slug":"cv-detection","published":1,"updated":"2018-11-16T11:33:13.192Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bp0006608wtrrk0xw5","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Object DetectionComputer Vision(/)RCNNSPPNetFast RCNNFaster RCNNFCNMask RCNNYOLO v1/2/3, SSDObject Detection</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"RCNN-Region-based-CNN\"><a href=\"#RCNN-Region-based-CNN\" class=\"headerlink\" title=\"RCNN (Region-based CNN)\"></a>RCNN (Region-based CNN)</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n</blockquote>\n<h3 id=\"What-is-RCNN\"><a href=\"#What-is-RCNN\" class=\"headerlink\" title=\"What is RCNN?\"></a>What is RCNN?</h3><p>PaperDeep LearningObject DetectionMachine Learning/Pattern Recognition<strong>Feature Matters in approximately every task!</strong> RCNNCNNFeaturedetectorfeature(SIFT/LBP/HOG)</p>\n<p>RCNNRegions with CNN features(1)<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>2000Region Proposal(2)Pretrained CNNRegion Proposaldeep feature(from pool5)(3)linear SVMone-VS-restObject DetectionClassificationSelective Searchbbox<font color=\"orange\">Bounding Box Regression</font>()RCNNidea</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png\" alt=\"RCNN\"></p>\n<h3 id=\"Details-of-RCNN\"><a href=\"#Details-of-RCNN\" class=\"headerlink\" title=\"Details of RCNN\"></a>Details of RCNN</h3><h4 id=\"Pretraining-and-Fine-tuning\"><a href=\"#Pretraining-and-Fine-tuning\" class=\"headerlink\" title=\"Pretraining and Fine-tuning\"></a>Pretraining and Fine-tuning</h4><p>RCNNBase NetworkImageNettrain1000 classclassifieroutput neuron21(20Foreground + 1Background)target datasetsfine-tuneSelective SearchRegion Proposalgroundtruth Bbox$IOU\\geq 0.5$positive samplenegative sample</p>\n<p>pool5 features learned from ImageNetgeneraldomain-specific datasetsnon-linear classifier</p>\n<h4 id=\"Bounding-Box-Regression\"><a href=\"#Bounding-Box-Regression\" class=\"headerlink\" title=\"Bounding Box Regression\"></a>Bounding Box Regression</h4><p>$N$training pairs$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$$P^i$$(x,y)$widthheight$G=(G_x,G_y,G_w,G_h)$groundtruth bboxBBox Regressionmappingproposed box $P$  groundtruth box $G$</p>\n<p>$x,y$transformation$d_x(P),d_y(P)$<font color=\"red\">scale-invariant translation$w,h$log-space translation</font>input proposalpredicted groundtruth box $\\hat{G}$:<br>$$<br>\\hat{G}_x=P_w d_x(P)+P_x<br>$$</p>\n<p>$$<br>\\hat{G}_y=P_h d_x(P)+P_y<br>$$</p>\n<p>$$<br>\\hat{G}_w=P_w exp(d_w(P))<br>$$</p>\n<p>$$<br>\\hat{G}_h=P_h exp(d_h(P))<br>$$</p>\n<p>$d_{\\star}(P)$$pool_5$ featureOLS<br>$$<br>w_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2<br>$$</p>\n<p>The regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:<br>$$<br>t_x=\\frac{G_x-P_x}{P_w}<br>$$</p>\n<p>$$<br>t_y=\\frac{G_y-P_y}{P_h}<br>$$</p>\n<p>$$<br>t_w=log(\\frac{G_w}{P_w})<br>$$</p>\n<p>$$<br>t_h=log(\\frac{G_h}{P_h})<br>$$</p>\n<p>Proposed BboxGroundtruth Bbox($IOU\\geq 0.6$)Bounding Box Regression</p>\n<p>Deep LearningObject DetectionRCNN</p>\n<h2 id=\"SPPNet\"><a href=\"#SPPNet\" class=\"headerlink\" title=\"SPPNet\"></a>SPPNet</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a></p>\n</blockquote>\n<h3 id=\"What-is-SPPNet\"><a href=\"#What-is-SPPNet\" class=\"headerlink\" title=\"What is SPPNet?\"></a>What is SPPNet?</h3><p>SPPNet(Spatial Pyramid Pooling)RCNNObject DetectionSPPNetRCNNRCNNRegion-based CNNCNNbase networkCNNsquared inputFC Layersfeature mapSSPlayerfeature mapconcatenatedetectionregion proposalfeature mapRCNN2000(2000 Region Proposal)</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>Deep ArchitectureFully Convolutional Architecture()Fully Connected LayersGlobal Average Pooling</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg\" alt=\"Crop/Warp VS SPP\"></p>\n<h3 id=\"Why-SPPNet\"><a href=\"#Why-SPPNet\" class=\"headerlink\" title=\"Why SPPNet?\"></a>Why SPPNet?</h3><p>SPPNetKaiming He</p>\n<ol>\n<li>SPPinput sizefixed size outputsliding window</li>\n<li>SPP uses multi-level spatial binssliding windowsingle window size<font color=\"red\">multi-level poolingobject deformationrobust</font></li>\n<li>SPP can pool features extracted at variable scales thanks to the flexibility of input scales.</li>\n<li>Training with variable-size images increases scale-invariance and reduces over-fitting.</li>\n</ol>\n<h3 id=\"Details-of-SPPNet\"><a href=\"#Details-of-SPPNet\" class=\"headerlink\" title=\"Details of SPPNet\"></a>Details of SPPNet</h3><h4 id=\"SPP-Layer\"><a href=\"#SPP-Layer\" class=\"headerlink\" title=\"SPP Layer\"></a>SPP Layer</h4><p>SPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg\" alt=\"SPP Layer\"></p>\n<p>spatial bin poolingk M-dimensional feature concatenationfixed lengthfeature vectorFC Layers/SVMMLtrain</p>\n<h4 id=\"SPP-for-Detection\"><a href=\"#SPP-for-Detection\" class=\"headerlink\" title=\"SPP for Detection\"></a>SPP for Detection</h4><p>RCNN2KRegion Proposalfeedforwad Pretrained CNNSPPNetimage(possible multi-scale))feedforwadCNN<font color=\"red\">feature map</font>candidate windowSPP Layer poolfixed-length feature representation of the window</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg\" alt=\"Pooling\"></p>\n<p>Region ProposalRCNN<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>2000bbox candidateimage resize$min(w, h)=s$ 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the featureswindowPooling12800-Dimensional (25650) FC LayersRCNNlinear SVM</p>\n<p>SPP Detectorgroundtruth bbox$IOU\\geq 0.3$positive samplenegative sample</p>\n<h2 id=\"Fast-RCNN\"><a href=\"#Fast-RCNN\" class=\"headerlink\" title=\"Fast RCNN\"></a>Fast RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast RCNN</a></p>\n</blockquote>\n<p>Fast RCNNObject Detectionnoveltybranchmulti-task learning(category classificationbbox regression)</p>\n<h3 id=\"Why-Fast-RCNN\"><a href=\"#Why-Fast-RCNN\" class=\"headerlink\" title=\"Why Fast RCNN?\"></a>Why Fast RCNN?</h3><p>(RCNN/SPPNet)</p>\n<ol>\n<li>(RCNN/SPP)multi-stage pipelineSelective Search2KRegion Proposallog lossfine-tunedeep CNNDeep CNNfeaturelinear SVMBounding Box Regression</li>\n<li>CNNRegion Proposaldeep featurelinear SVM</li>\n<li>testingDeep CNNfeaturefeaturelinear SVM</li>\n</ol>\n<p>Fast RCNN</p>\n<ol>\n<li>multi-task lossobject classificationbounding box regression</li>\n<li>Training is single stage.</li>\n<li>Higher performance than RCNN and SPPNet.</li>\n</ol>\n<h3 id=\"Details-of-Fast-RCNN\"><a href=\"#Details-of-Fast-RCNN\" class=\"headerlink\" title=\"Details of Fast RCNN\"></a>Details of Fast RCNN</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg\" alt=\"Fast RCNN\"></p>\n<p>Fast RCNN pipelinewhole image with several object proposalsCNNfeatureobject proposal<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>RoI Pooling Layerfeature vectormulti-branchclassificationbbox regression</p>\n<p>Fast RCNN<font color=\"red\">RoI Pooling</font>RoI Pooling</p>\n<h4 id=\"The-RoI-pooling-layer\"><a href=\"#The-RoI-pooling-layer\" class=\"headerlink\" title=\"The RoI pooling layer\"></a>The RoI pooling layer</h4><p>Fast RCNN</p>\n<blockquote>\n<p>RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.</p>\n<p>In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.</p>\n<p>RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].</p>\n</blockquote>\n<p>valid region proposalfeature mapmax poolingfeature map</p>\n<h4 id=\"Fine-tuning-for-detection\"><a href=\"#Fine-tuning-for-detection\" class=\"headerlink\" title=\"Fine-tuning for detection\"></a>Fine-tuning for detection</h4><h5 id=\"Multi-Task-Loss\"><a href=\"#Multi-Task-Loss\" class=\"headerlink\" title=\"Multi-Task Loss\"></a>Multi-Task Loss</h5><p>Fast RCNN$K+1$ (Kobject class + 1background) classification($p=(p_0,p_1,\\cdots,p_K)$)bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)</p>\n<p>We use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(linear regressionwidthheightlog). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:<br>$$<br>L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)<br>$$<br>$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.</p>\n<p>Loss Function(regression loss)$[u\\geq 1]$$u\\geq 1$10settingbackground$[u\\geq 1]$0regression lossregressionMSE LossMSE LossCost-sensitive LossoutliersRoss$Smooth L_1 Loss$<br>$$<br>L_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)<br>$$</p>\n<p>Smooth L1 Loss<br>$$<br>smooth_{L_1}(x)=<br>\\begin{cases}<br>0.5x^2 &amp; if |x|&lt;1\\\\<br>|x|-0.5 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>Machine LearningLoss<a href=\"https://lucasxlu.github.io/blog/2018/07/24/ml-loss/\"></a></p>\n</blockquote>\n<h5 id=\"Mini-batch-sampling\"><a href=\"#Mini-batch-sampling\" class=\"headerlink\" title=\"Mini-batch sampling\"></a>Mini-batch sampling</h5><p>Fine-tuningmini-batch$N=2$image64samplegroundtruth bbox $IOU\\geq 0.5$foreground samples[$u=1$]background samples[$u=0$]</p>\n<h3 id=\"Fast-R-CNN-detection\"><a href=\"#Fast-R-CNN-detection\" class=\"headerlink\" title=\"Fast R-CNN detection\"></a>Fast R-CNN detection</h3><p>Fully Connected LayersFC Layerstruncated SVD</p>\n<p>In this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:<br>$$<br>W\\approx U\\Sigma_t V^T<br>$$<br>$U$$W$$t$left-singular vectors$u\\times t$$\\Sigma_t$$W$$t$singular value$t\\times t$$V$$W$$t$right-singular vectors$v\\times t$Truncated SVD$uv$$t(u+v)$</p>\n<p></p>\n<ol>\n<li>$conv_1$ layers feature mapgeneraltask-independentfeature</li>\n<li>Region ProposalDetection (featuresampling mini-batchkaiming He<a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss</a>)</li>\n</ol>\n<h2 id=\"Faster-RCNN\"><a href=\"#Faster-RCNN\" class=\"headerlink\" title=\"Faster RCNN\"></a>Faster RCNN</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks</a></p>\n</blockquote>\n<p>Faster RCNNObject DetectionRegion Proposal Network(RPN)RPNRCNNSPPFast RCNNtwo-stage detector<font color=\"red\">Selective Search</font>2000Region Proposal<font color=\"red\">Selective Search</font>RPNNetwork ArchitectureRegion ProposalRPNobject boundsobjectness scoreRPNFeatureDetection NetworkRegion Proposalcost-freeFaster RCNN<strong>Faster</strong></p>\n<h3 id=\"What-is-Faster-RCNN\"><a href=\"#What-is-Faster-RCNN\" class=\"headerlink\" title=\"What is Faster RCNN?\"></a>What is Faster RCNN?</h3><p>$$<br>Faster RCNN = Fast RCNN + RPN<br>$$<br><br>detectorshared features between proposals<font color=\"red\">Region Proposal</font>RPN</p>\n<p>conv feature maps used by region-based detectorsregion proposalsconv features<font color=\"red\">RPNconv layerencodeconv feature map position(256-d)conv layerconv feature map positionkregion proposal with various scales and aspect ratiosobjectness scoreregression bounds</font>RPN</p>\n<h3 id=\"Region-Proposal-Network\"><a href=\"#Region-Proposal-Network\" class=\"headerlink\" title=\"Region Proposal Network\"></a>Region Proposal Network</h3><p>RPNimageobject proposalsobjectness scoreRPNregion proposals</p>\n<p>shared conv feature mapslideinput conv feature map$n\\times n$spatial windowsliding window256-dfeature vectorfeature vectorfully connected layersbox regressionbox classificationsliding windowfully-connected layersspatial locations$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers</p>\n<h4 id=\"Translation-Invariant-Anchors\"><a href=\"#Translation-Invariant-Anchors\" class=\"headerlink\" title=\"Translation-Invariant Anchors\"></a>Translation-Invariant Anchors</h4><p>sliding window location$k$region proposalsregression layer$4k$encoding$k$bboxoutputsclassification layer$2k$scores(region proposalobject/non-object)</p>\n<blockquote>\n<p>The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.</p>\n</blockquote>\n<p>3scales3aspect ratiossliding position$k=9$anchors$W\\times H$conv feature map$WHk$anchor<strong>translation invariant</strong></p>\n<h4 id=\"A-Loss-Function-for-Learning-Region-Proposals\"><a href=\"#A-Loss-Function-for-Learning-Region-Proposals\" class=\"headerlink\" title=\"A Loss Function for Learning Region Proposals\"></a>A Loss Function for Learning Region Proposals</h4><p>RPNanchorbinary class(object)anchorspositive label:</p>\n<ol>\n<li>groundtruth bboxIoUanchor</li>\n<li>groundtruth bbox $IoU\\geq 0.7$anchor</li>\n</ol>\n<p>groundtruth bboxanchorgroundtruth bbox $IoU\\leq 0.3$anchornegative anchorpositivenegativeanchortrainingFaster RCNNLoss Function<br>$$<br>L(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})<br>$$<br>$p_i$anchor $i$ objectanchorpositivegroundtruth label $p_i^$1anchornegative0$t_i$4bbox$t_i^{\\star}$groundtruth positive anchor$L_{cls}$Log Loss(object VS non-object)regression loss$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$$R$Smooth L1 Loss(Fast RCNN)$p_i^{\\star} L_{reg}$<font color=\"red\">positive anchor ($p_i^{\\star}=1$)($p_i^{\\star}=0$)</font></p>\n<p>Bounding Box Regressionpipeline<br>$$<br>t_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})<br>$$</p>\n<p>$$<br>t_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})<br>$$</p>\n<blockquote>\n<p>In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. <strong>To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights</strong>. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.</p>\n</blockquote>\n<h4 id=\"Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\"><a href=\"#Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\" class=\"headerlink\" title=\"Sharing Convolutional Features for Region Proposal and Object Detection\"></a>Sharing Convolutional Features for Region Proposal and Object Detection</h4><ol>\n<li>Fine-tuneImageNetPretrainRPNregion proposal task</li>\n<li>RPNregion proposaltrain Fast RCNNRPNFaster RCNN</li>\n<li>detector networkRPNfix shared conv layersfine-tuneRPNRPNFast RCNN</li>\n<li>Fixshared conv layersfine-tune Fast RCNNfc layersRPNFast RCNNunified network</li>\n</ol>\n<h2 id=\"SSD\"><a href=\"#SSD\" class=\"headerlink\" title=\"SSD\"></a>SSD</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">SSD: Single Shot MultiBox Detector</a></p>\n</blockquote>\n<p>SSDone-stage detectorone-stagetwo-stageDL DetectorRCNN/SSP/Fast RCNN/Faster RCNNtwo-stage detectors<strong>region proposalsdetectionregion proposalsclassification</strong>one-stage detectionregion proposalsbboxFaster RCNNtwo-stage detectionregion proposal generation(Selective Search)RPNregion proposals</p>\n<h3 id=\"What-is-SSD\"><a href=\"#What-is-SSD\" class=\"headerlink\" title=\"What is SSD?\"></a>What is SSD?</h3><p>SSD<strong>Convolution Filterobject categorybbox offset</strong>SSDconv filterfeature mapScale Invariant</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png\" alt=\"SSD Framework\"></p>\n<h3 id=\"Details-of-SSD\"><a href=\"#Details-of-SSD\" class=\"headerlink\" title=\"Details of SSD\"></a>Details of SSD</h3><p>SSDDCNNfeaturefixed-sizebboxbboxpresence scoreNMSFeature ExtractionDCNN</p>\n<ol>\n<li><strong>Multi-scale feature maps for detection</strong>: base feature extraction networkconv layers(multi-scalefeature maps)multi-scaledetection</li>\n<li><strong>Convolutional predictors for detection</strong>: feature layer<code>small conv filters</code>fixed-size detection predictions</li>\n<li><strong>Default boxes and aspect ratios</strong>: feature map cellcelldefault boxrelative offsetclass-score(boxclass instance)given location$k$box4bbox offset$c$class score<code>feature map location</code>$(c+4)k$filters$m\\times n$<code>feature map</code>$(c+4)kmn$outputFaster RCNNanchor box<strong>SSDresolutionfeature mapfeature mapdefault box shapeoutput box shape</strong></li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png\" alt=\"SSD and YOLO\"></p>\n<h4 id=\"Training-of-SSD\"><a href=\"#Training-of-SSD\" class=\"headerlink\" title=\"Training of SSD\"></a>Training of SSD</h4><p>SSDdefault boxvaried location/ratio/scaleboxesboxgroundtruthMatching Strategy: groundtruth boxdefault boxeslocation/ratio/scaleboxesgroundtruth boxJaccard OverlapboxesSSDLoss<br>$$<br>L(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))<br>$$<br>$N$matched default boxesLocalisation LossFaster RCNNSmooth L1$x_{ij}^p=\\{0,1\\}$$i$default box$j$groundtruth boxindicator<br>$$<br>L_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h<br>$$</p>\n<p>$$<br>\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})<br>$$</p>\n<p>$$<br>\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})<br>$$</p>\n<p>Confidence LossSoftmax Loss:<br>$$<br>L_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)<br>$$<br>$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$</p>\n<p>SSDlarge objectsmall objecthigher layersfeature mapsmall objectinput size$300\\times 300$$512\\times 512$<strong>Zoom Data Augmentation</strong>(zoom inlarge objects, zoom outsmall objects)</p>\n<h2 id=\"Light-head-RCNN\"><a href=\"#Light-head-RCNN\" class=\"headerlink\" title=\"Light-head RCNN\"></a>Light-head RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>Light-head RCNNtwo-stage detectorheavy-headtwo-stage detectortwo-stage detectorRoI Warp/ Faster RCNN2fully connected layersnRoI RecognitionRFCNscore mapsbackbone network<code>light-head</code> RCNN<code>light-head</code><code>thin feature map + cheap RCNN subnet (poolingfully connected layer)</code></p>\n<p>two-stage detectordetectionclassificationstageregion proposal (<code>body</code>)stageregion proposal (<code>head</code>)two-stage detectoraccuracyone-stage detectoraccuracyheadheavyLight-head RCNN</p>\n<blockquote>\n<p>In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce thin feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.</p>\n</blockquote>\n<h3 id=\"Delve-Into-Light-Head-RCNN\"><a href=\"#Delve-Into-Light-Head-RCNN\" class=\"headerlink\" title=\"Delve Into Light-Head RCNN\"></a>Delve Into Light-Head RCNN</h3><h4 id=\"RCNN-Subnet\"><a href=\"#RCNN-Subnet\" class=\"headerlink\" title=\"RCNN Subnet\"></a>RCNN Subnet</h4><blockquote>\n<p>Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, <strong>R-FCN first produces a set of score maps for each region, whose channel number will be $classes_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation</strong>.</p>\n</blockquote>\n<p>Faster RCNNRoI Classificationglobal average poolingfully connected layer<code>GAPspatial localization</code>Faster RCNNRoIfeedforwardRCNN subnetproposal</p>\n<h4 id=\"Thin-Feature-Maps-for-RoI-Warping\"><a href=\"#Thin-Feature-Maps-for-RoI-Warping\" class=\"headerlink\" title=\"Thin Feature Maps for RoI Warping\"></a>Thin Feature Maps for RoI Warping</h4><p>feed region proposalRCNN subnetRoI warpingfixed shapefeature mapslight-head<code>thin feature maps</code>RoI Pooling<code>RoI warping on thin feature maps</code>traininginferenceRoI poolingthin feature mapsGAPspatial information</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg\" alt=\"Light Head RCNN\"></p>\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>regression lossclassification loss<code>regression lossdoublebalance multi-task training</code></p>\n<h2 id=\"YOLO-v1\"><a href=\"#YOLO-v1\" class=\"headerlink\" title=\"YOLO v1\"></a>YOLO v1</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You Only Look Once: Unified, Real-Time Object Detection</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Girshick, Ross, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>He, Kaiming, et al. <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a> European conference on computer vision. Springer, Cham, 2014.</li>\n<li>Girshick, Ross. <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast r-cnn.</a> Proceedings of the IEEE international conference on computer vision. 2015.</li>\n<li>Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollr. <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss for Dense Object Detection.</a></li>\n<li>Ren, Shaoqing, et al. <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks.</a> Advances in neural information processing systems. 2015.</li>\n<li>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., &amp; Berg, A. C. (2016, October). <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">Ssd: Single shot multibox detector</a>. In European conference on computer vision (pp. 21-37). Springer, Cham.</li>\n<li>Redmon, Joseph, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You only look once: Unified, real-time object detection.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</li>\n<li>Li Z, Peng C, Yu G, et al. <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-head r-cnn: In defense of two-stage object detector</a>[J]. arXiv preprint arXiv:1711.07264, 2017.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Object DetectionComputer Vision(/)RCNNSPPNetFast RCNNFaster RCNNFCNMask RCNNYOLO v1/2/3, SSDObject Detection</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"RCNN-Region-based-CNN\"><a href=\"#RCNN-Region-based-CNN\" class=\"headerlink\" title=\"RCNN (Region-based CNN)\"></a>RCNN (Region-based CNN)</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n</blockquote>\n<h3 id=\"What-is-RCNN\"><a href=\"#What-is-RCNN\" class=\"headerlink\" title=\"What is RCNN?\"></a>What is RCNN?</h3><p>PaperDeep LearningObject DetectionMachine Learning/Pattern Recognition<strong>Feature Matters in approximately every task!</strong> RCNNCNNFeaturedetectorfeature(SIFT/LBP/HOG)</p>\n<p>RCNNRegions with CNN features(1)<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>2000Region Proposal(2)Pretrained CNNRegion Proposaldeep feature(from pool5)(3)linear SVMone-VS-restObject DetectionClassificationSelective Searchbbox<font color=\"orange\">Bounding Box Regression</font>()RCNNidea</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png\" alt=\"RCNN\"></p>\n<h3 id=\"Details-of-RCNN\"><a href=\"#Details-of-RCNN\" class=\"headerlink\" title=\"Details of RCNN\"></a>Details of RCNN</h3><h4 id=\"Pretraining-and-Fine-tuning\"><a href=\"#Pretraining-and-Fine-tuning\" class=\"headerlink\" title=\"Pretraining and Fine-tuning\"></a>Pretraining and Fine-tuning</h4><p>RCNNBase NetworkImageNettrain1000 classclassifieroutput neuron21(20Foreground + 1Background)target datasetsfine-tuneSelective SearchRegion Proposalgroundtruth Bbox$IOU\\geq 0.5$positive samplenegative sample</p>\n<p>pool5 features learned from ImageNetgeneraldomain-specific datasetsnon-linear classifier</p>\n<h4 id=\"Bounding-Box-Regression\"><a href=\"#Bounding-Box-Regression\" class=\"headerlink\" title=\"Bounding Box Regression\"></a>Bounding Box Regression</h4><p>$N$training pairs$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$$P^i$$(x,y)$widthheight$G=(G_x,G_y,G_w,G_h)$groundtruth bboxBBox Regressionmappingproposed box $P$  groundtruth box $G$</p>\n<p>$x,y$transformation$d_x(P),d_y(P)$<font color=\"red\">scale-invariant translation$w,h$log-space translation</font>input proposalpredicted groundtruth box $\\hat{G}$:<br>$$<br>\\hat{G}_x=P_w d_x(P)+P_x<br>$$</p>\n<p>$$<br>\\hat{G}_y=P_h d_x(P)+P_y<br>$$</p>\n<p>$$<br>\\hat{G}_w=P_w exp(d_w(P))<br>$$</p>\n<p>$$<br>\\hat{G}_h=P_h exp(d_h(P))<br>$$</p>\n<p>$d_{\\star}(P)$$pool_5$ featureOLS<br>$$<br>w_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2<br>$$</p>\n<p>The regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:<br>$$<br>t_x=\\frac{G_x-P_x}{P_w}<br>$$</p>\n<p>$$<br>t_y=\\frac{G_y-P_y}{P_h}<br>$$</p>\n<p>$$<br>t_w=log(\\frac{G_w}{P_w})<br>$$</p>\n<p>$$<br>t_h=log(\\frac{G_h}{P_h})<br>$$</p>\n<p>Proposed BboxGroundtruth Bbox($IOU\\geq 0.6$)Bounding Box Regression</p>\n<p>Deep LearningObject DetectionRCNN</p>\n<h2 id=\"SPPNet\"><a href=\"#SPPNet\" class=\"headerlink\" title=\"SPPNet\"></a>SPPNet</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a></p>\n</blockquote>\n<h3 id=\"What-is-SPPNet\"><a href=\"#What-is-SPPNet\" class=\"headerlink\" title=\"What is SPPNet?\"></a>What is SPPNet?</h3><p>SPPNet(Spatial Pyramid Pooling)RCNNObject DetectionSPPNetRCNNRCNNRegion-based CNNCNNbase networkCNNsquared inputFC Layersfeature mapSSPlayerfeature mapconcatenatedetectionregion proposalfeature mapRCNN2000(2000 Region Proposal)</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>Deep ArchitectureFully Convolutional Architecture()Fully Connected LayersGlobal Average Pooling</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg\" alt=\"Crop/Warp VS SPP\"></p>\n<h3 id=\"Why-SPPNet\"><a href=\"#Why-SPPNet\" class=\"headerlink\" title=\"Why SPPNet?\"></a>Why SPPNet?</h3><p>SPPNetKaiming He</p>\n<ol>\n<li>SPPinput sizefixed size outputsliding window</li>\n<li>SPP uses multi-level spatial binssliding windowsingle window size<font color=\"red\">multi-level poolingobject deformationrobust</font></li>\n<li>SPP can pool features extracted at variable scales thanks to the flexibility of input scales.</li>\n<li>Training with variable-size images increases scale-invariance and reduces over-fitting.</li>\n</ol>\n<h3 id=\"Details-of-SPPNet\"><a href=\"#Details-of-SPPNet\" class=\"headerlink\" title=\"Details of SPPNet\"></a>Details of SPPNet</h3><h4 id=\"SPP-Layer\"><a href=\"#SPP-Layer\" class=\"headerlink\" title=\"SPP Layer\"></a>SPP Layer</h4><p>SPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg\" alt=\"SPP Layer\"></p>\n<p>spatial bin poolingk M-dimensional feature concatenationfixed lengthfeature vectorFC Layers/SVMMLtrain</p>\n<h4 id=\"SPP-for-Detection\"><a href=\"#SPP-for-Detection\" class=\"headerlink\" title=\"SPP for Detection\"></a>SPP for Detection</h4><p>RCNN2KRegion Proposalfeedforwad Pretrained CNNSPPNetimage(possible multi-scale))feedforwadCNN<font color=\"red\">feature map</font>candidate windowSPP Layer poolfixed-length feature representation of the window</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg\" alt=\"Pooling\"></p>\n<p>Region ProposalRCNN<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>2000bbox candidateimage resize$min(w, h)=s$ 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the featureswindowPooling12800-Dimensional (25650) FC LayersRCNNlinear SVM</p>\n<p>SPP Detectorgroundtruth bbox$IOU\\geq 0.3$positive samplenegative sample</p>\n<h2 id=\"Fast-RCNN\"><a href=\"#Fast-RCNN\" class=\"headerlink\" title=\"Fast RCNN\"></a>Fast RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast RCNN</a></p>\n</blockquote>\n<p>Fast RCNNObject Detectionnoveltybranchmulti-task learning(category classificationbbox regression)</p>\n<h3 id=\"Why-Fast-RCNN\"><a href=\"#Why-Fast-RCNN\" class=\"headerlink\" title=\"Why Fast RCNN?\"></a>Why Fast RCNN?</h3><p>(RCNN/SPPNet)</p>\n<ol>\n<li>(RCNN/SPP)multi-stage pipelineSelective Search2KRegion Proposallog lossfine-tunedeep CNNDeep CNNfeaturelinear SVMBounding Box Regression</li>\n<li>CNNRegion Proposaldeep featurelinear SVM</li>\n<li>testingDeep CNNfeaturefeaturelinear SVM</li>\n</ol>\n<p>Fast RCNN</p>\n<ol>\n<li>multi-task lossobject classificationbounding box regression</li>\n<li>Training is single stage.</li>\n<li>Higher performance than RCNN and SPPNet.</li>\n</ol>\n<h3 id=\"Details-of-Fast-RCNN\"><a href=\"#Details-of-Fast-RCNN\" class=\"headerlink\" title=\"Details of Fast RCNN\"></a>Details of Fast RCNN</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg\" alt=\"Fast RCNN\"></p>\n<p>Fast RCNN pipelinewhole image with several object proposalsCNNfeatureobject proposal<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>RoI Pooling Layerfeature vectormulti-branchclassificationbbox regression</p>\n<p>Fast RCNN<font color=\"red\">RoI Pooling</font>RoI Pooling</p>\n<h4 id=\"The-RoI-pooling-layer\"><a href=\"#The-RoI-pooling-layer\" class=\"headerlink\" title=\"The RoI pooling layer\"></a>The RoI pooling layer</h4><p>Fast RCNN</p>\n<blockquote>\n<p>RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.</p>\n<p>In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.</p>\n<p>RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].</p>\n</blockquote>\n<p>valid region proposalfeature mapmax poolingfeature map</p>\n<h4 id=\"Fine-tuning-for-detection\"><a href=\"#Fine-tuning-for-detection\" class=\"headerlink\" title=\"Fine-tuning for detection\"></a>Fine-tuning for detection</h4><h5 id=\"Multi-Task-Loss\"><a href=\"#Multi-Task-Loss\" class=\"headerlink\" title=\"Multi-Task Loss\"></a>Multi-Task Loss</h5><p>Fast RCNN$K+1$ (Kobject class + 1background) classification($p=(p_0,p_1,\\cdots,p_K)$)bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)</p>\n<p>We use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(linear regressionwidthheightlog). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:<br>$$<br>L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)<br>$$<br>$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.</p>\n<p>Loss Function(regression loss)$[u\\geq 1]$$u\\geq 1$10settingbackground$[u\\geq 1]$0regression lossregressionMSE LossMSE LossCost-sensitive LossoutliersRoss$Smooth L_1 Loss$<br>$$<br>L_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)<br>$$</p>\n<p>Smooth L1 Loss<br>$$<br>smooth_{L_1}(x)=<br>\\begin{cases}<br>0.5x^2 &amp; if |x|&lt;1\\\\<br>|x|-0.5 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>Machine LearningLoss<a href=\"https://lucasxlu.github.io/blog/2018/07/24/ml-loss/\"></a></p>\n</blockquote>\n<h5 id=\"Mini-batch-sampling\"><a href=\"#Mini-batch-sampling\" class=\"headerlink\" title=\"Mini-batch sampling\"></a>Mini-batch sampling</h5><p>Fine-tuningmini-batch$N=2$image64samplegroundtruth bbox $IOU\\geq 0.5$foreground samples[$u=1$]background samples[$u=0$]</p>\n<h3 id=\"Fast-R-CNN-detection\"><a href=\"#Fast-R-CNN-detection\" class=\"headerlink\" title=\"Fast R-CNN detection\"></a>Fast R-CNN detection</h3><p>Fully Connected LayersFC Layerstruncated SVD</p>\n<p>In this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:<br>$$<br>W\\approx U\\Sigma_t V^T<br>$$<br>$U$$W$$t$left-singular vectors$u\\times t$$\\Sigma_t$$W$$t$singular value$t\\times t$$V$$W$$t$right-singular vectors$v\\times t$Truncated SVD$uv$$t(u+v)$</p>\n<p></p>\n<ol>\n<li>$conv_1$ layers feature mapgeneraltask-independentfeature</li>\n<li>Region ProposalDetection (featuresampling mini-batchkaiming He<a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss</a>)</li>\n</ol>\n<h2 id=\"Faster-RCNN\"><a href=\"#Faster-RCNN\" class=\"headerlink\" title=\"Faster RCNN\"></a>Faster RCNN</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks</a></p>\n</blockquote>\n<p>Faster RCNNObject DetectionRegion Proposal Network(RPN)RPNRCNNSPPFast RCNNtwo-stage detector<font color=\"red\">Selective Search</font>2000Region Proposal<font color=\"red\">Selective Search</font>RPNNetwork ArchitectureRegion ProposalRPNobject boundsobjectness scoreRPNFeatureDetection NetworkRegion Proposalcost-freeFaster RCNN<strong>Faster</strong></p>\n<h3 id=\"What-is-Faster-RCNN\"><a href=\"#What-is-Faster-RCNN\" class=\"headerlink\" title=\"What is Faster RCNN?\"></a>What is Faster RCNN?</h3><p>$$<br>Faster RCNN = Fast RCNN + RPN<br>$$<br><br>detectorshared features between proposals<font color=\"red\">Region Proposal</font>RPN</p>\n<p>conv feature maps used by region-based detectorsregion proposalsconv features<font color=\"red\">RPNconv layerencodeconv feature map position(256-d)conv layerconv feature map positionkregion proposal with various scales and aspect ratiosobjectness scoreregression bounds</font>RPN</p>\n<h3 id=\"Region-Proposal-Network\"><a href=\"#Region-Proposal-Network\" class=\"headerlink\" title=\"Region Proposal Network\"></a>Region Proposal Network</h3><p>RPNimageobject proposalsobjectness scoreRPNregion proposals</p>\n<p>shared conv feature mapslideinput conv feature map$n\\times n$spatial windowsliding window256-dfeature vectorfeature vectorfully connected layersbox regressionbox classificationsliding windowfully-connected layersspatial locations$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers</p>\n<h4 id=\"Translation-Invariant-Anchors\"><a href=\"#Translation-Invariant-Anchors\" class=\"headerlink\" title=\"Translation-Invariant Anchors\"></a>Translation-Invariant Anchors</h4><p>sliding window location$k$region proposalsregression layer$4k$encoding$k$bboxoutputsclassification layer$2k$scores(region proposalobject/non-object)</p>\n<blockquote>\n<p>The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.</p>\n</blockquote>\n<p>3scales3aspect ratiossliding position$k=9$anchors$W\\times H$conv feature map$WHk$anchor<strong>translation invariant</strong></p>\n<h4 id=\"A-Loss-Function-for-Learning-Region-Proposals\"><a href=\"#A-Loss-Function-for-Learning-Region-Proposals\" class=\"headerlink\" title=\"A Loss Function for Learning Region Proposals\"></a>A Loss Function for Learning Region Proposals</h4><p>RPNanchorbinary class(object)anchorspositive label:</p>\n<ol>\n<li>groundtruth bboxIoUanchor</li>\n<li>groundtruth bbox $IoU\\geq 0.7$anchor</li>\n</ol>\n<p>groundtruth bboxanchorgroundtruth bbox $IoU\\leq 0.3$anchornegative anchorpositivenegativeanchortrainingFaster RCNNLoss Function<br>$$<br>L(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})<br>$$<br>$p_i$anchor $i$ objectanchorpositivegroundtruth label $p_i^$1anchornegative0$t_i$4bbox$t_i^{\\star}$groundtruth positive anchor$L_{cls}$Log Loss(object VS non-object)regression loss$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$$R$Smooth L1 Loss(Fast RCNN)$p_i^{\\star} L_{reg}$<font color=\"red\">positive anchor ($p_i^{\\star}=1$)($p_i^{\\star}=0$)</font></p>\n<p>Bounding Box Regressionpipeline<br>$$<br>t_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})<br>$$</p>\n<p>$$<br>t_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})<br>$$</p>\n<blockquote>\n<p>In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. <strong>To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights</strong>. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.</p>\n</blockquote>\n<h4 id=\"Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\"><a href=\"#Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\" class=\"headerlink\" title=\"Sharing Convolutional Features for Region Proposal and Object Detection\"></a>Sharing Convolutional Features for Region Proposal and Object Detection</h4><ol>\n<li>Fine-tuneImageNetPretrainRPNregion proposal task</li>\n<li>RPNregion proposaltrain Fast RCNNRPNFaster RCNN</li>\n<li>detector networkRPNfix shared conv layersfine-tuneRPNRPNFast RCNN</li>\n<li>Fixshared conv layersfine-tune Fast RCNNfc layersRPNFast RCNNunified network</li>\n</ol>\n<h2 id=\"SSD\"><a href=\"#SSD\" class=\"headerlink\" title=\"SSD\"></a>SSD</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">SSD: Single Shot MultiBox Detector</a></p>\n</blockquote>\n<p>SSDone-stage detectorone-stagetwo-stageDL DetectorRCNN/SSP/Fast RCNN/Faster RCNNtwo-stage detectors<strong>region proposalsdetectionregion proposalsclassification</strong>one-stage detectionregion proposalsbboxFaster RCNNtwo-stage detectionregion proposal generation(Selective Search)RPNregion proposals</p>\n<h3 id=\"What-is-SSD\"><a href=\"#What-is-SSD\" class=\"headerlink\" title=\"What is SSD?\"></a>What is SSD?</h3><p>SSD<strong>Convolution Filterobject categorybbox offset</strong>SSDconv filterfeature mapScale Invariant</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png\" alt=\"SSD Framework\"></p>\n<h3 id=\"Details-of-SSD\"><a href=\"#Details-of-SSD\" class=\"headerlink\" title=\"Details of SSD\"></a>Details of SSD</h3><p>SSDDCNNfeaturefixed-sizebboxbboxpresence scoreNMSFeature ExtractionDCNN</p>\n<ol>\n<li><strong>Multi-scale feature maps for detection</strong>: base feature extraction networkconv layers(multi-scalefeature maps)multi-scaledetection</li>\n<li><strong>Convolutional predictors for detection</strong>: feature layer<code>small conv filters</code>fixed-size detection predictions</li>\n<li><strong>Default boxes and aspect ratios</strong>: feature map cellcelldefault boxrelative offsetclass-score(boxclass instance)given location$k$box4bbox offset$c$class score<code>feature map location</code>$(c+4)k$filters$m\\times n$<code>feature map</code>$(c+4)kmn$outputFaster RCNNanchor box<strong>SSDresolutionfeature mapfeature mapdefault box shapeoutput box shape</strong></li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png\" alt=\"SSD and YOLO\"></p>\n<h4 id=\"Training-of-SSD\"><a href=\"#Training-of-SSD\" class=\"headerlink\" title=\"Training of SSD\"></a>Training of SSD</h4><p>SSDdefault boxvaried location/ratio/scaleboxesboxgroundtruthMatching Strategy: groundtruth boxdefault boxeslocation/ratio/scaleboxesgroundtruth boxJaccard OverlapboxesSSDLoss<br>$$<br>L(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))<br>$$<br>$N$matched default boxesLocalisation LossFaster RCNNSmooth L1$x_{ij}^p=\\{0,1\\}$$i$default box$j$groundtruth boxindicator<br>$$<br>L_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h<br>$$</p>\n<p>$$<br>\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})<br>$$</p>\n<p>$$<br>\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})<br>$$</p>\n<p>Confidence LossSoftmax Loss:<br>$$<br>L_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)<br>$$<br>$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$</p>\n<p>SSDlarge objectsmall objecthigher layersfeature mapsmall objectinput size$300\\times 300$$512\\times 512$<strong>Zoom Data Augmentation</strong>(zoom inlarge objects, zoom outsmall objects)</p>\n<h2 id=\"Light-head-RCNN\"><a href=\"#Light-head-RCNN\" class=\"headerlink\" title=\"Light-head RCNN\"></a>Light-head RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>Light-head RCNNtwo-stage detectorheavy-headtwo-stage detectortwo-stage detectorRoI Warp/ Faster RCNN2fully connected layersnRoI RecognitionRFCNscore mapsbackbone network<code>light-head</code> RCNN<code>light-head</code><code>thin feature map + cheap RCNN subnet (poolingfully connected layer)</code></p>\n<p>two-stage detectordetectionclassificationstageregion proposal (<code>body</code>)stageregion proposal (<code>head</code>)two-stage detectoraccuracyone-stage detectoraccuracyheadheavyLight-head RCNN</p>\n<blockquote>\n<p>In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce thin feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.</p>\n</blockquote>\n<h3 id=\"Delve-Into-Light-Head-RCNN\"><a href=\"#Delve-Into-Light-Head-RCNN\" class=\"headerlink\" title=\"Delve Into Light-Head RCNN\"></a>Delve Into Light-Head RCNN</h3><h4 id=\"RCNN-Subnet\"><a href=\"#RCNN-Subnet\" class=\"headerlink\" title=\"RCNN Subnet\"></a>RCNN Subnet</h4><blockquote>\n<p>Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, <strong>R-FCN first produces a set of score maps for each region, whose channel number will be $classes_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation</strong>.</p>\n</blockquote>\n<p>Faster RCNNRoI Classificationglobal average poolingfully connected layer<code>GAPspatial localization</code>Faster RCNNRoIfeedforwardRCNN subnetproposal</p>\n<h4 id=\"Thin-Feature-Maps-for-RoI-Warping\"><a href=\"#Thin-Feature-Maps-for-RoI-Warping\" class=\"headerlink\" title=\"Thin Feature Maps for RoI Warping\"></a>Thin Feature Maps for RoI Warping</h4><p>feed region proposalRCNN subnetRoI warpingfixed shapefeature mapslight-head<code>thin feature maps</code>RoI Pooling<code>RoI warping on thin feature maps</code>traininginferenceRoI poolingthin feature mapsGAPspatial information</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg\" alt=\"Light Head RCNN\"></p>\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>regression lossclassification loss<code>regression lossdoublebalance multi-task training</code></p>\n<h2 id=\"YOLO-v1\"><a href=\"#YOLO-v1\" class=\"headerlink\" title=\"YOLO v1\"></a>YOLO v1</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You Only Look Once: Unified, Real-Time Object Detection</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Girshick, Ross, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>He, Kaiming, et al. <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a> European conference on computer vision. Springer, Cham, 2014.</li>\n<li>Girshick, Ross. <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast r-cnn.</a> Proceedings of the IEEE international conference on computer vision. 2015.</li>\n<li>Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollr. <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss for Dense Object Detection.</a></li>\n<li>Ren, Shaoqing, et al. <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks.</a> Advances in neural information processing systems. 2015.</li>\n<li>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., &amp; Berg, A. C. (2016, October). <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">Ssd: Single shot multibox detector</a>. In European conference on computer vision (pp. 21-37). Springer, Cham.</li>\n<li>Redmon, Joseph, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You only look once: Unified, real-time object detection.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</li>\n<li>Li Z, Peng C, Yu G, et al. <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-head r-cnn: In defense of two-stage object detector</a>[J]. arXiv preprint arXiv:1711.07264, 2017.</li>\n</ol>\n"},{"title":"[CV] Face Recognition","date":"2018-09-03T10:08:11.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Face Recognition"],"_content":"## Introduction\n(Face Recognition)Face RecognitionDeep Learning Architecture + Loss FunctionFace RecognitionPaperReferencePaper\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n\n## Face Recognition as N-Categories Classification Problems\nMetric LearningLossFace RecognitionFace Verification/Identificationtrain  n-categories classifierinput imagedistance metric[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)PaperCVPR2014PaperMetric LearningLoss\n\n> Paper: [Deep learning face representation from predicting 10,000 classes](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n\nPaperideaFace Recognition$N$-Classification$N$datasetidentityfeature representationfacial regionconcatenation[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)Architecture\n![DeepID](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg)\n\nDeepIDconv layerfeature mapmax-poolingNetworkmulti-scaleinputskipping layer(lower level featurehigher level featurefeature fusion)hidden layer\n$$\ny_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)\n$$\n\n<font color=\"red\">identity feature representation learningperformance</font>[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)LFW97.45%\n\n\n## FaceNet\nGoogle[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)Face RecognitioninsightfulPapertriplets**Euclidean Space**feature vector[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)LFW99.63%\n\n> Paper: [Facenet: A unified embedding for face recognition and clustering.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)\n\n### What is FaceNet?\n[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)IdeaDNN**Euclidean Embedding**inter-classcompactinter-classseparable[Triplet Loss](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)\n\n![FaceNet](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg)\n\n### Details of Triplet Loss\nTriplet EmbeddingNetwork$x$$d$-$f(x)\\in \\mathbb{R}^d$Normalization$||f(x)||_2=1$Triplet Losspersonanchor $x_i^a$identitypositive images $x_i^p$closeidentitynegative imagesseparate\n$$\n||x_i^a-x_i^p||_2^2 + \\alpha < ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}\n$$\n$\\alpha$marginMinimization of Triplet Loss\n$$\n\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+\n$$\nTriplet LossTriplets\n\n### Triplet Selection\n<font color=\"red\">violate tripletconstraintanchor $x_i^a$hard positive $x_i^p$$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$hard negative $x_i^n$$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)triplet loss definition[hard negative mining](http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf)\n\ntraining set$argmax$$argmin$\n* $n$tripletsmost recent network checkpointdataset$argmax$$argmin$\n* tripletsmini-batchhard positive/negative exemplars\n\nSelecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:\n$$\n||f(x_i^a)-f(x_i^p)||_2^2 < ||f(x_i^a)-f(x_i^n)||_2^2\n$$\n<font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n### Experiments\nFace Verification Tasksquared $L_2$ distance $D(x_i,x_j)$\n* True Acceptsface pairs $(i,j)identity$:\n  $TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n* False Acceptsface pairs $(i,j)identity$:\n  $FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n\n\n## Center Loss\n> Paper: [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf)\n\nFace RecognitionNetwork ArchitectureLoss[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)FaceNet[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)intra-class more compact and inter-class more separate[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)centerdeep featuresclasscentersCNNLossSoftmax Loss[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)Softmax Lossclass[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)classdeep featurescentersjoint supervision(Softmax + Center Loss)inter-classdifferenceintra-classvariantionsdiscriminativefeature representation[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)idea\n\n### What is Center Loss?\nSoftmax Loss\n$$\n\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}\n$$\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)\n$$\n\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\ncenter vector $c_{y_i}$mini-batch(training set)center vector $c_{y_i}$class featuremislabeled samples$\\alpha$center vectorlearning rateCenter Loss\n$$\n\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}\n$$\n\n$$\n\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}\n$$\n\nwhere $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.\n$$\n\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n\n\n![Learning of Center Loss](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg)\n\n\n![Center Loss Architecture](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg)\n\nCenter Loss\n* Joint supervision of Softmax Loss and Center LossDCNNfeature learning\n* Metric LearningLoss[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf), Contractive Losspairs selectionCenter Losstriplet pairs selection\n\nFace Verification/Identification<font color=\"red\"> FC Layersfeaturefeatureconcatenationface featurePCACosine Distance, Nearest Neighbor and Threshold comparison</font>\n\n\n## NormFace\n> Paper: [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf)\n\n\n\n\n## Reference\n1. Sun, Yi, Xiaogang Wang, and Xiaoou Tang. [\"Deep learning face representation from predicting 10,000 classes.\"](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [\"Facenet: A unified embedding for face recognition and clustering.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n3. Wen Y, Zhang K, Li Z, Qiao Y. [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf). In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.\n4. Wang F, Xiang X, Cheng J, Yuille AL. [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf). InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.\n","source":"_posts/cv-face-rec.md","raw":"---\ntitle: \"[CV] Face Recognition\"\ndate: 2018-09-03 18:08:11\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Recognition\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Recognition\n---\n## Introduction\n(Face Recognition)Face RecognitionDeep Learning Architecture + Loss FunctionFace RecognitionPaperReferencePaper\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n\n## Face Recognition as N-Categories Classification Problems\nMetric LearningLossFace RecognitionFace Verification/Identificationtrain  n-categories classifierinput imagedistance metric[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)PaperCVPR2014PaperMetric LearningLoss\n\n> Paper: [Deep learning face representation from predicting 10,000 classes](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n\nPaperideaFace Recognition$N$-Classification$N$datasetidentityfeature representationfacial regionconcatenation[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)Architecture\n![DeepID](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg)\n\nDeepIDconv layerfeature mapmax-poolingNetworkmulti-scaleinputskipping layer(lower level featurehigher level featurefeature fusion)hidden layer\n$$\ny_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)\n$$\n\n<font color=\"red\">identity feature representation learningperformance</font>[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)LFW97.45%\n\n\n## FaceNet\nGoogle[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)Face RecognitioninsightfulPapertriplets**Euclidean Space**feature vector[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)LFW99.63%\n\n> Paper: [Facenet: A unified embedding for face recognition and clustering.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)\n\n### What is FaceNet?\n[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)IdeaDNN**Euclidean Embedding**inter-classcompactinter-classseparable[Triplet Loss](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)\n\n![FaceNet](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg)\n\n### Details of Triplet Loss\nTriplet EmbeddingNetwork$x$$d$-$f(x)\\in \\mathbb{R}^d$Normalization$||f(x)||_2=1$Triplet Losspersonanchor $x_i^a$identitypositive images $x_i^p$closeidentitynegative imagesseparate\n$$\n||x_i^a-x_i^p||_2^2 + \\alpha < ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}\n$$\n$\\alpha$marginMinimization of Triplet Loss\n$$\n\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+\n$$\nTriplet LossTriplets\n\n### Triplet Selection\n<font color=\"red\">violate tripletconstraintanchor $x_i^a$hard positive $x_i^p$$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$hard negative $x_i^n$$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)triplet loss definition[hard negative mining](http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf)\n\ntraining set$argmax$$argmin$\n* $n$tripletsmost recent network checkpointdataset$argmax$$argmin$\n* tripletsmini-batchhard positive/negative exemplars\n\nSelecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:\n$$\n||f(x_i^a)-f(x_i^p)||_2^2 < ||f(x_i^a)-f(x_i^n)||_2^2\n$$\n<font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n### Experiments\nFace Verification Tasksquared $L_2$ distance $D(x_i,x_j)$\n* True Acceptsface pairs $(i,j)identity$:\n  $TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n* False Acceptsface pairs $(i,j)identity$:\n  $FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n\n\n## Center Loss\n> Paper: [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf)\n\nFace RecognitionNetwork ArchitectureLoss[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)FaceNet[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)intra-class more compact and inter-class more separate[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)centerdeep featuresclasscentersCNNLossSoftmax Loss[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)Softmax Lossclass[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)classdeep featurescentersjoint supervision(Softmax + Center Loss)inter-classdifferenceintra-classvariantionsdiscriminativefeature representation[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)idea\n\n### What is Center Loss?\nSoftmax Loss\n$$\n\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}\n$$\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)\n$$\n\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\ncenter vector $c_{y_i}$mini-batch(training set)center vector $c_{y_i}$class featuremislabeled samples$\\alpha$center vectorlearning rateCenter Loss\n$$\n\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}\n$$\n\n$$\n\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}\n$$\n\nwhere $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.\n$$\n\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n\n\n![Learning of Center Loss](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg)\n\n\n![Center Loss Architecture](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg)\n\nCenter Loss\n* Joint supervision of Softmax Loss and Center LossDCNNfeature learning\n* Metric LearningLoss[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf), Contractive Losspairs selectionCenter Losstriplet pairs selection\n\nFace Verification/Identification<font color=\"red\"> FC Layersfeaturefeatureconcatenationface featurePCACosine Distance, Nearest Neighbor and Threshold comparison</font>\n\n\n## NormFace\n> Paper: [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf)\n\n\n\n\n## Reference\n1. Sun, Yi, Xiaogang Wang, and Xiaoou Tang. [\"Deep learning face representation from predicting 10,000 classes.\"](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [\"Facenet: A unified embedding for face recognition and clustering.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n3. Wen Y, Zhang K, Li Z, Qiao Y. [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf). In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.\n4. Wang F, Xiang X, Cheng J, Yuille AL. [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf). InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.\n","slug":"cv-face-rec","published":1,"updated":"2018-10-02T04:19:42.047Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03br0007608w4uz8pp8x","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>(Face Recognition)Face RecognitionDeep Learning Architecture + Loss FunctionFace RecognitionPaperReferencePaper</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"Face-Recognition-as-N-Categories-Classification-Problems\"><a href=\"#Face-Recognition-as-N-Categories-Classification-Problems\" class=\"headerlink\" title=\"Face Recognition as N-Categories Classification Problems\"></a>Face Recognition as N-Categories Classification Problems</h2><p>Metric LearningLossFace RecognitionFace Verification/Identificationtrain  n-categories classifierinput imagedistance metric<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>PaperCVPR2014PaperMetric LearningLoss</p>\n<blockquote>\n<p>Paper: <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes</a></p>\n</blockquote>\n<p>PaperideaFace Recognition$N$-Classification$N$datasetidentityfeature representationfacial regionconcatenation<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>Architecture<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg\" alt=\"DeepID\"></p>\n<p>DeepIDconv layerfeature mapmax-poolingNetworkmulti-scaleinputskipping layer(lower level featurehigher level featurefeature fusion)hidden layer<br>$$<br>y_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)<br>$$</p>\n<p><font color=\"red\">identity feature representation learningperformance</font><a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>LFW97.45%</p>\n<h2 id=\"FaceNet\"><a href=\"#FaceNet\" class=\"headerlink\" title=\"FaceNet\"></a>FaceNet</h2><p>Google<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>Face RecognitioninsightfulPapertriplets<strong>Euclidean Space</strong>feature vector<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>LFW99.63%</p>\n<blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a></p>\n</blockquote>\n<h3 id=\"What-is-FaceNet\"><a href=\"#What-is-FaceNet\" class=\"headerlink\" title=\"What is FaceNet?\"></a>What is FaceNet?</h3><p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>IdeaDNN<strong>Euclidean Embedding</strong>inter-classcompactinter-classseparable<a href=\"https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg\" alt=\"FaceNet\"></p>\n<h3 id=\"Details-of-Triplet-Loss\"><a href=\"#Details-of-Triplet-Loss\" class=\"headerlink\" title=\"Details of Triplet Loss\"></a>Details of Triplet Loss</h3><p>Triplet EmbeddingNetwork$x$$d$-$f(x)\\in \\mathbb{R}^d$Normalization$||f(x)||_2=1$Triplet Losspersonanchor $x_i^a$identitypositive images $x_i^p$closeidentitynegative imagesseparate<br>$$<br>||x_i^a-x_i^p||_2^2 + \\alpha &lt; ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}<br>$$<br>$\\alpha$marginMinimization of Triplet Loss<br>$$<br>\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+<br>$$<br>Triplet LossTriplets</p>\n<h3 id=\"Triplet-Selection\"><a href=\"#Triplet-Selection\" class=\"headerlink\" title=\"Triplet Selection\"></a>Triplet Selection</h3><font color=\"red\">violate tripletconstraintanchor $x_i^a$hard positive $x_i^p$$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$hard negative $x_i^n$$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font><br>&gt; <a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>triplet loss definition<a href=\"http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf\" target=\"_blank\" rel=\"noopener\">hard negative mining</a><br><br>training set$argmax$$argmin$<br><em> $n$tripletsmost recent network checkpointdataset$argmax$$argmin$\n</em> tripletsmini-batchhard positive/negative exemplars<br><br>Selecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:<br>$$<br>||f(x_i^a)-f(x_i^p)||_2^2 &lt; ||f(x_i^a)-f(x_i^n)||_2^2<br>$$<br><font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>Face Verification Tasksquared $L_2$ distance $D(x_i,x_j)$</p>\n<ul>\n<li>True Acceptsface pairs $(i,j)identity$:<br>$TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n<li>False Acceptsface pairs $(i,j)identity$:<br>$FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n</ul>\n<h2 id=\"Center-Loss\"><a href=\"#Center-Loss\" class=\"headerlink\" title=\"Center Loss\"></a>Center Loss</h2><blockquote>\n<p>Paper: <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a></p>\n</blockquote>\n<p>Face RecognitionNetwork ArchitectureLoss<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>FaceNet<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>intra-class more compact and inter-class more separate<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a></p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>centerdeep featuresclasscentersCNNLossSoftmax Loss<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>Softmax Lossclass<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>classdeep featurescentersjoint supervision(Softmax + Center Loss)inter-classdifferenceintra-classvariantionsdiscriminativefeature representation<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>idea</p>\n<h3 id=\"What-is-Center-Loss\"><a href=\"#What-is-Center-Loss\" class=\"headerlink\" title=\"What is Center Loss?\"></a>What is Center Loss?</h3><p>Softmax Loss<br>$$<br>\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}<br>$$</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a><br>$$<br>\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$<br>center vector $c_{y_i}$mini-batch(training set)center vector $c_{y_i}$class featuremislabeled samples$\\alpha$center vectorlearning rateCenter Loss<br>$$<br>\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}<br>$$</p>\n<p>$$<br>\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}<br>$$</p>\n<p>where $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.<br>$$<br>\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$</p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg\" alt=\"Learning of Center Loss\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg\" alt=\"Center Loss Architecture\"></p>\n<p>Center Loss</p>\n<ul>\n<li>Joint supervision of Softmax Loss and Center LossDCNNfeature learning</li>\n<li>Metric LearningLoss<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>, Contractive Losspairs selectionCenter Losstriplet pairs selection</li>\n</ul>\n<p>Face Verification/Identification<font color=\"red\"> FC Layersfeaturefeatureconcatenationface featurePCACosine Distance, Nearest Neighbor and Threshold comparison</font></p>\n<h2 id=\"NormFace\"><a href=\"#NormFace\" class=\"headerlink\" title=\"NormFace\"></a>NormFace</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Sun, Yi, Xiaogang Wang, and Xiaoou Tang. <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>Schroff, Florian, Dmitry Kalenichenko, and James Philbin. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n<li>Wen Y, Zhang K, Li Z, Qiao Y. <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a>. In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.</li>\n<li>Wang F, Xiang X, Cheng J, Yuille AL. <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a>. InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>(Face Recognition)Face RecognitionDeep Learning Architecture + Loss FunctionFace RecognitionPaperReferencePaper</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"Face-Recognition-as-N-Categories-Classification-Problems\"><a href=\"#Face-Recognition-as-N-Categories-Classification-Problems\" class=\"headerlink\" title=\"Face Recognition as N-Categories Classification Problems\"></a>Face Recognition as N-Categories Classification Problems</h2><p>Metric LearningLossFace RecognitionFace Verification/Identificationtrain  n-categories classifierinput imagedistance metric<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>PaperCVPR2014PaperMetric LearningLoss</p>\n<blockquote>\n<p>Paper: <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes</a></p>\n</blockquote>\n<p>PaperideaFace Recognition$N$-Classification$N$datasetidentityfeature representationfacial regionconcatenation<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>Architecture<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg\" alt=\"DeepID\"></p>\n<p>DeepIDconv layerfeature mapmax-poolingNetworkmulti-scaleinputskipping layer(lower level featurehigher level featurefeature fusion)hidden layer<br>$$<br>y_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)<br>$$</p>\n<p><font color=\"red\">identity feature representation learningperformance</font><a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>LFW97.45%</p>\n<h2 id=\"FaceNet\"><a href=\"#FaceNet\" class=\"headerlink\" title=\"FaceNet\"></a>FaceNet</h2><p>Google<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>Face RecognitioninsightfulPapertriplets<strong>Euclidean Space</strong>feature vector<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>LFW99.63%</p>\n<blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a></p>\n</blockquote>\n<h3 id=\"What-is-FaceNet\"><a href=\"#What-is-FaceNet\" class=\"headerlink\" title=\"What is FaceNet?\"></a>What is FaceNet?</h3><p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>IdeaDNN<strong>Euclidean Embedding</strong>inter-classcompactinter-classseparable<a href=\"https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg\" alt=\"FaceNet\"></p>\n<h3 id=\"Details-of-Triplet-Loss\"><a href=\"#Details-of-Triplet-Loss\" class=\"headerlink\" title=\"Details of Triplet Loss\"></a>Details of Triplet Loss</h3><p>Triplet EmbeddingNetwork$x$$d$-$f(x)\\in \\mathbb{R}^d$Normalization$||f(x)||_2=1$Triplet Losspersonanchor $x_i^a$identitypositive images $x_i^p$closeidentitynegative imagesseparate<br>$$<br>||x_i^a-x_i^p||_2^2 + \\alpha &lt; ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}<br>$$<br>$\\alpha$marginMinimization of Triplet Loss<br>$$<br>\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+<br>$$<br>Triplet LossTriplets</p>\n<h3 id=\"Triplet-Selection\"><a href=\"#Triplet-Selection\" class=\"headerlink\" title=\"Triplet Selection\"></a>Triplet Selection</h3><font color=\"red\">violate tripletconstraintanchor $x_i^a$hard positive $x_i^p$$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$hard negative $x_i^n$$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font><br>&gt; <a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>triplet loss definition<a href=\"http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf\" target=\"_blank\" rel=\"noopener\">hard negative mining</a><br><br>training set$argmax$$argmin$<br><em> $n$tripletsmost recent network checkpointdataset$argmax$$argmin$\n</em> tripletsmini-batchhard positive/negative exemplars<br><br>Selecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:<br>$$<br>||f(x_i^a)-f(x_i^p)||_2^2 &lt; ||f(x_i^a)-f(x_i^n)||_2^2<br>$$<br><font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>Face Verification Tasksquared $L_2$ distance $D(x_i,x_j)$</p>\n<ul>\n<li>True Acceptsface pairs $(i,j)identity$:<br>$TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n<li>False Acceptsface pairs $(i,j)identity$:<br>$FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n</ul>\n<h2 id=\"Center-Loss\"><a href=\"#Center-Loss\" class=\"headerlink\" title=\"Center Loss\"></a>Center Loss</h2><blockquote>\n<p>Paper: <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a></p>\n</blockquote>\n<p>Face RecognitionNetwork ArchitectureLoss<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>FaceNet<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>intra-class more compact and inter-class more separate<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a></p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>centerdeep featuresclasscentersCNNLossSoftmax Loss<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>Softmax Lossclass<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>classdeep featurescentersjoint supervision(Softmax + Center Loss)inter-classdifferenceintra-classvariantionsdiscriminativefeature representation<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>idea</p>\n<h3 id=\"What-is-Center-Loss\"><a href=\"#What-is-Center-Loss\" class=\"headerlink\" title=\"What is Center Loss?\"></a>What is Center Loss?</h3><p>Softmax Loss<br>$$<br>\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}<br>$$</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a><br>$$<br>\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$<br>center vector $c_{y_i}$mini-batch(training set)center vector $c_{y_i}$class featuremislabeled samples$\\alpha$center vectorlearning rateCenter Loss<br>$$<br>\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}<br>$$</p>\n<p>$$<br>\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}<br>$$</p>\n<p>where $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.<br>$$<br>\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$</p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg\" alt=\"Learning of Center Loss\"></p>\n<p><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg\" alt=\"Center Loss Architecture\"></p>\n<p>Center Loss</p>\n<ul>\n<li>Joint supervision of Softmax Loss and Center LossDCNNfeature learning</li>\n<li>Metric LearningLoss<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>, Contractive Losspairs selectionCenter Losstriplet pairs selection</li>\n</ul>\n<p>Face Verification/Identification<font color=\"red\"> FC Layersfeaturefeatureconcatenationface featurePCACosine Distance, Nearest Neighbor and Threshold comparison</font></p>\n<h2 id=\"NormFace\"><a href=\"#NormFace\" class=\"headerlink\" title=\"NormFace\"></a>NormFace</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Sun, Yi, Xiaogang Wang, and Xiaoou Tang. <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>Schroff, Florian, Dmitry Kalenichenko, and James Philbin. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n<li>Wen Y, Zhang K, Li Z, Qiao Y. <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a>. In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.</li>\n<li>Wang F, Xiang X, Cheng J, Yuille AL. <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a>. InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.</li>\n</ol>\n"},{"title":"[CV] Image Quality Assessment","date":"2018-11-04T08:32:11.000Z","mathjax":true,"catagories":["Computer Vision","Machine Learning","Deep Learning","Digital Image Processing"],"_content":"## Introduction\nImage Quality Assessment (IQA) ((reflection)(blur))IQA[Face Anti-Spoofing](https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/)print/replay attack/ ()\n\nIQA3(1) distorted imageoriginal image*full reference*(2) reference image*no-reference*(3) reference image*reduced reference*\n\nIQAMetric*MSE*, *PSNR (Peak Signal-to-Noise Ratio)*  *SSMI (structural similarity)*\n\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n\n## Reference\n1. Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, [\"Image quality assessment: From error visibility to structural similarity,\"](http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf) IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n2. Talebi, Hossein, and Peyman Milanfar. [\"Nima: Neural image assessment.\"](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8352823) IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.","source":"_posts/cv-iqa.md","raw":"---\ntitle: \"[CV] Image Quality Assessment\"\ndate: 2018-11-04 16:32:11\nmathjax: true\ntags:\n- Computer Vision\n- Machine Learning\n- Deep Learning\n- Digital Image Processing\ncatagories:\n- Computer Vision\n- Machine Learning\n- Deep Learning\n- Digital Image Processing\n---\n## Introduction\nImage Quality Assessment (IQA) ((reflection)(blur))IQA[Face Anti-Spoofing](https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/)print/replay attack/ ()\n\nIQA3(1) distorted imageoriginal image*full reference*(2) reference image*no-reference*(3) reference image*reduced reference*\n\nIQAMetric*MSE*, *PSNR (Peak Signal-to-Noise Ratio)*  *SSMI (structural similarity)*\n\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)\n\n\n## Reference\n1. Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, [\"Image quality assessment: From error visibility to structural similarity,\"](http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf) IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n2. Talebi, Hossein, and Peyman Milanfar. [\"Nima: Neural image assessment.\"](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8352823) IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.","slug":"cv-iqa","published":1,"updated":"2018-11-04T14:13:47.400Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bt0009608whetyugdt","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Image Quality Assessment (IQA) ((reflection)(blur))IQA<a href=\"https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/\">Face Anti-Spoofing</a>print/replay attack/ ()</p>\n<p>IQA3(1) distorted imageoriginal image<em>full reference</em>(2) reference image<em>no-reference</em>(3) reference image<em>reduced reference</em></p>\n<p>IQAMetric<em>MSE</em>, <em>PSNR (Peak Signal-to-Noise Ratio)</em>  <em>SSMI (structural similarity)</em></p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, <a href=\"http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\" target=\"_blank\" rel=\"noopener\">Image quality assessment: From error visibility to structural similarity,</a> IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</li>\n<li>Talebi, Hossein, and Peyman Milanfar. <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8352823\" target=\"_blank\" rel=\"noopener\">Nima: Neural image assessment.</a> IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Image Quality Assessment (IQA) ((reflection)(blur))IQA<a href=\"https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/\">Face Anti-Spoofing</a>print/replay attack/ ()</p>\n<p>IQA3(1) distorted imageoriginal image<em>full reference</em>(2) reference image<em>no-reference</em>(3) reference image<em>reduced reference</em></p>\n<p>IQAMetric<em>MSE</em>, <em>PSNR (Peak Signal-to-Noise Ratio)</em>  <em>SSMI (structural similarity)</em></p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, <a href=\"http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\" target=\"_blank\" rel=\"noopener\">Image quality assessment: From error visibility to structural similarity,</a> IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</li>\n<li>Talebi, Hossein, and Peyman Milanfar. <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8352823\" target=\"_blank\" rel=\"noopener\">Nima: Neural image assessment.</a> IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.</li>\n</ol>\n"},{"title":"[DIP] Image Feature","date":"2018-08-22T07:53:01.000Z","mathjax":true,"catagories":["Digital Image Processing","Computer Vision"],"_content":"## Introduction\nComputer VisionDeep LearningDNNLarge Scale Labeled DatasetDeep Learning\n\n> TPAMI[A Performance Evaluation of Local Descriptors](https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf)\n\n## Image Descriptors\n### Image Pixels\nPCA\n\n### Distribution-Based Descriptors\nA simple descriptor is the distribution of the pixel intensities represented by a histogram. \n\n#### SIFT\nThe descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. **The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection**. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods **compute a histogram describing the edge distribution in a region**. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.\n\n## EXPERIMENTAL SETUP\n### Support Regions\nLindeberg [23] has developed a scaleinvariant \"blob\" detector, where **a \"blob\" is defined by a maximum of the normalized Laplacian in scale-space**. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Garding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.\n\n#### Region Detectors\n**Harris points** [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.\n\n**Harris-Laplace regions** [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.\n\n**Hessian-Laplace regions** [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.\n\n**Harris-Affine regions** [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. \n\n**Hessian-Affine regions** [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.\n\n**Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries**. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.\n\n### Descriptors\n#### SIFT \nSIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.\n\nEach orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.\n\n#### Gradient location-orientation histogram (GLOH)\nGradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.\n\n#### Shape context\nShape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.\n\n#### PCA-SIFT\nPCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.\n\n#### Spin image\nSpin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.\n\n#### Cross correlation\nCross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.\n\n## DISCUSSION AND CONCLUSIONS\nIn most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.\n\n## Appendix\n### \n BinColor Quantization BinBin  Bin  Bin  Bin  Bin\n\n### \n 256 \n\n### LBP\n**LBP(Local  Binary  Pattern LBP)**LBP  ** $3\\times 3$  8  1 03*3  8  8 bit  LBP**\n LBP  256  LBP \n\n### SIFT\nSIFT SIFT  SIFT \n\n**SIFT**\n\n### HOG\nHistogram of Oriented Gradients HOG \n\nHOG Appearance and Shape BlockContrast-NormalizedBlockHOG : ** HOG GeometricPhotometric**\n","source":"_posts/dip-image-feature.md","raw":"---\ntitle: \"[DIP] Image Feature\"\ndate: 2018-08-22 15:53:01\nmathjax: true\ntags:\n- Digital Image Processing\n- Computer Vision\ncatagories:\n- Digital Image Processing\n- Computer Vision\n---\n## Introduction\nComputer VisionDeep LearningDNNLarge Scale Labeled DatasetDeep Learning\n\n> TPAMI[A Performance Evaluation of Local Descriptors](https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf)\n\n## Image Descriptors\n### Image Pixels\nPCA\n\n### Distribution-Based Descriptors\nA simple descriptor is the distribution of the pixel intensities represented by a histogram. \n\n#### SIFT\nThe descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. **The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection**. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods **compute a histogram describing the edge distribution in a region**. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.\n\n## EXPERIMENTAL SETUP\n### Support Regions\nLindeberg [23] has developed a scaleinvariant \"blob\" detector, where **a \"blob\" is defined by a maximum of the normalized Laplacian in scale-space**. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Garding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.\n\n#### Region Detectors\n**Harris points** [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.\n\n**Harris-Laplace regions** [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.\n\n**Hessian-Laplace regions** [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.\n\n**Harris-Affine regions** [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. \n\n**Hessian-Affine regions** [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.\n\n**Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries**. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.\n\n### Descriptors\n#### SIFT \nSIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.\n\nEach orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.\n\n#### Gradient location-orientation histogram (GLOH)\nGradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.\n\n#### Shape context\nShape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.\n\n#### PCA-SIFT\nPCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.\n\n#### Spin image\nSpin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.\n\n#### Cross correlation\nCross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.\n\n## DISCUSSION AND CONCLUSIONS\nIn most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.\n\n## Appendix\n### \n BinColor Quantization BinBin  Bin  Bin  Bin  Bin\n\n### \n 256 \n\n### LBP\n**LBP(Local  Binary  Pattern LBP)**LBP  ** $3\\times 3$  8  1 03*3  8  8 bit  LBP**\n LBP  256  LBP \n\n### SIFT\nSIFT SIFT  SIFT \n\n**SIFT**\n\n### HOG\nHistogram of Oriented Gradients HOG \n\nHOG Appearance and Shape BlockContrast-NormalizedBlockHOG : ** HOG GeometricPhotometric**\n","slug":"dip-image-feature","published":1,"updated":"2018-10-01T04:40:08.551Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bv000a608w1fey66vl","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Computer VisionDeep LearningDNNLarge Scale Labeled DatasetDeep Learning</p>\n<blockquote>\n<p>TPAMI<a href=\"https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf\" target=\"_blank\" rel=\"noopener\">A Performance Evaluation of Local Descriptors</a></p>\n</blockquote>\n<h2 id=\"Image-Descriptors\"><a href=\"#Image-Descriptors\" class=\"headerlink\" title=\"Image Descriptors\"></a>Image Descriptors</h2><h3 id=\"Image-Pixels\"><a href=\"#Image-Pixels\" class=\"headerlink\" title=\"Image Pixels\"></a>Image Pixels</h3><p>PCA</p>\n<h3 id=\"Distribution-Based-Descriptors\"><a href=\"#Distribution-Based-Descriptors\" class=\"headerlink\" title=\"Distribution-Based Descriptors\"></a>Distribution-Based Descriptors</h3><p>A simple descriptor is the distribution of the pixel intensities represented by a histogram. </p>\n<h4 id=\"SIFT\"><a href=\"#SIFT\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>The descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. <strong>The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection</strong>. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods <strong>compute a histogram describing the edge distribution in a region</strong>. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.</p>\n<h2 id=\"EXPERIMENTAL-SETUP\"><a href=\"#EXPERIMENTAL-SETUP\" class=\"headerlink\" title=\"EXPERIMENTAL SETUP\"></a>EXPERIMENTAL SETUP</h2><h3 id=\"Support-Regions\"><a href=\"#Support-Regions\" class=\"headerlink\" title=\"Support Regions\"></a>Support Regions</h3><p>Lindeberg [23] has developed a scaleinvariant blob detector, where <strong>a blob is defined by a maximum of the normalized Laplacian in scale-space</strong>. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Garding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.</p>\n<h4 id=\"Region-Detectors\"><a href=\"#Region-Detectors\" class=\"headerlink\" title=\"Region Detectors\"></a>Region Detectors</h4><p><strong>Harris points</strong> [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.</p>\n<p><strong>Harris-Laplace regions</strong> [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.</p>\n<p><strong>Hessian-Laplace regions</strong> [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.</p>\n<p><strong>Harris-Affine regions</strong> [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. </p>\n<p><strong>Hessian-Affine regions</strong> [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.</p>\n<p><strong>Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries</strong>. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.</p>\n<h3 id=\"Descriptors\"><a href=\"#Descriptors\" class=\"headerlink\" title=\"Descriptors\"></a>Descriptors</h3><h4 id=\"SIFT-1\"><a href=\"#SIFT-1\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>SIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.</p>\n<p>Each orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.</p>\n<h4 id=\"Gradient-location-orientation-histogram-GLOH\"><a href=\"#Gradient-location-orientation-histogram-GLOH\" class=\"headerlink\" title=\"Gradient location-orientation histogram (GLOH)\"></a>Gradient location-orientation histogram (GLOH)</h4><p>Gradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.</p>\n<h4 id=\"Shape-context\"><a href=\"#Shape-context\" class=\"headerlink\" title=\"Shape context\"></a>Shape context</h4><p>Shape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.</p>\n<h4 id=\"PCA-SIFT\"><a href=\"#PCA-SIFT\" class=\"headerlink\" title=\"PCA-SIFT\"></a>PCA-SIFT</h4><p>PCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.</p>\n<h4 id=\"Spin-image\"><a href=\"#Spin-image\" class=\"headerlink\" title=\"Spin image\"></a>Spin image</h4><p>Spin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.</p>\n<h4 id=\"Cross-correlation\"><a href=\"#Cross-correlation\" class=\"headerlink\" title=\"Cross correlation\"></a>Cross correlation</h4><p>Cross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.</p>\n<h2 id=\"DISCUSSION-AND-CONCLUSIONS\"><a href=\"#DISCUSSION-AND-CONCLUSIONS\" class=\"headerlink\" title=\"DISCUSSION AND CONCLUSIONS\"></a>DISCUSSION AND CONCLUSIONS</h2><p>In most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> BinColor Quantization BinBin  Bin  Bin  Bin  Bin</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 256 </p>\n<h3 id=\"LBP\"><a href=\"#LBP\" class=\"headerlink\" title=\"LBP\"></a>LBP</h3><p><strong>LBP(Local  Binary  Pattern LBP)</strong>LBP  <strong> $3\\times 3$  8  1 03*3  8  8 bit  LBP</strong><br> LBP  256  LBP </p>\n<h3 id=\"SIFT-2\"><a href=\"#SIFT-2\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h3><p>SIFT SIFT  SIFT </p>\n<p><strong>SIFT</strong></p>\n<h3 id=\"HOG\"><a href=\"#HOG\" class=\"headerlink\" title=\"HOG\"></a>HOG</h3><p>Histogram of Oriented Gradients HOG </p>\n<p>HOG Appearance and Shape BlockContrast-NormalizedBlockHOG : <strong> HOG GeometricPhotometric</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Computer VisionDeep LearningDNNLarge Scale Labeled DatasetDeep Learning</p>\n<blockquote>\n<p>TPAMI<a href=\"https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf\" target=\"_blank\" rel=\"noopener\">A Performance Evaluation of Local Descriptors</a></p>\n</blockquote>\n<h2 id=\"Image-Descriptors\"><a href=\"#Image-Descriptors\" class=\"headerlink\" title=\"Image Descriptors\"></a>Image Descriptors</h2><h3 id=\"Image-Pixels\"><a href=\"#Image-Pixels\" class=\"headerlink\" title=\"Image Pixels\"></a>Image Pixels</h3><p>PCA</p>\n<h3 id=\"Distribution-Based-Descriptors\"><a href=\"#Distribution-Based-Descriptors\" class=\"headerlink\" title=\"Distribution-Based Descriptors\"></a>Distribution-Based Descriptors</h3><p>A simple descriptor is the distribution of the pixel intensities represented by a histogram. </p>\n<h4 id=\"SIFT\"><a href=\"#SIFT\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>The descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. <strong>The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection</strong>. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods <strong>compute a histogram describing the edge distribution in a region</strong>. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.</p>\n<h2 id=\"EXPERIMENTAL-SETUP\"><a href=\"#EXPERIMENTAL-SETUP\" class=\"headerlink\" title=\"EXPERIMENTAL SETUP\"></a>EXPERIMENTAL SETUP</h2><h3 id=\"Support-Regions\"><a href=\"#Support-Regions\" class=\"headerlink\" title=\"Support Regions\"></a>Support Regions</h3><p>Lindeberg [23] has developed a scaleinvariant blob detector, where <strong>a blob is defined by a maximum of the normalized Laplacian in scale-space</strong>. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Garding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.</p>\n<h4 id=\"Region-Detectors\"><a href=\"#Region-Detectors\" class=\"headerlink\" title=\"Region Detectors\"></a>Region Detectors</h4><p><strong>Harris points</strong> [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.</p>\n<p><strong>Harris-Laplace regions</strong> [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.</p>\n<p><strong>Hessian-Laplace regions</strong> [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.</p>\n<p><strong>Harris-Affine regions</strong> [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. </p>\n<p><strong>Hessian-Affine regions</strong> [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.</p>\n<p><strong>Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries</strong>. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.</p>\n<h3 id=\"Descriptors\"><a href=\"#Descriptors\" class=\"headerlink\" title=\"Descriptors\"></a>Descriptors</h3><h4 id=\"SIFT-1\"><a href=\"#SIFT-1\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>SIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.</p>\n<p>Each orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.</p>\n<h4 id=\"Gradient-location-orientation-histogram-GLOH\"><a href=\"#Gradient-location-orientation-histogram-GLOH\" class=\"headerlink\" title=\"Gradient location-orientation histogram (GLOH)\"></a>Gradient location-orientation histogram (GLOH)</h4><p>Gradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.</p>\n<h4 id=\"Shape-context\"><a href=\"#Shape-context\" class=\"headerlink\" title=\"Shape context\"></a>Shape context</h4><p>Shape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.</p>\n<h4 id=\"PCA-SIFT\"><a href=\"#PCA-SIFT\" class=\"headerlink\" title=\"PCA-SIFT\"></a>PCA-SIFT</h4><p>PCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.</p>\n<h4 id=\"Spin-image\"><a href=\"#Spin-image\" class=\"headerlink\" title=\"Spin image\"></a>Spin image</h4><p>Spin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.</p>\n<h4 id=\"Cross-correlation\"><a href=\"#Cross-correlation\" class=\"headerlink\" title=\"Cross correlation\"></a>Cross correlation</h4><p>Cross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.</p>\n<h2 id=\"DISCUSSION-AND-CONCLUSIONS\"><a href=\"#DISCUSSION-AND-CONCLUSIONS\" class=\"headerlink\" title=\"DISCUSSION AND CONCLUSIONS\"></a>DISCUSSION AND CONCLUSIONS</h2><p>In most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> BinColor Quantization BinBin  Bin  Bin  Bin  Bin</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 256 </p>\n<h3 id=\"LBP\"><a href=\"#LBP\" class=\"headerlink\" title=\"LBP\"></a>LBP</h3><p><strong>LBP(Local  Binary  Pattern LBP)</strong>LBP  <strong> $3\\times 3$  8  1 03*3  8  8 bit  LBP</strong><br> LBP  256  LBP </p>\n<h3 id=\"SIFT-2\"><a href=\"#SIFT-2\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h3><p>SIFT SIFT  SIFT </p>\n<p><strong>SIFT</strong></p>\n<h3 id=\"HOG\"><a href=\"#HOG\" class=\"headerlink\" title=\"HOG\"></a>HOG</h3><p>Histogram of Oriented Gradients HOG </p>\n<p>HOG Appearance and Shape BlockContrast-NormalizedBlockHOG : <strong> HOG GeometricPhotometric</strong></p>\n"},{"title":"[DL] Auto Encoder","date":"2018-08-22T11:17:56.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Auto Encoder"],"_content":"## Introduction\nAuto Encoder([repository](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)Deep AutoEncoder300word2vecdiscriminative)GANAutoEncodergenerative model\n\n## Undercomplete AutoEncoder\nAutoEncoder$h$$x$AutoEncoderUndercomplete AutoEncoderundercomplete representationAutoEncodertraining data\n\nDecoderLossMSEUndercomplete AutoEncoderPCAAutoEncodertraining datanon-linear Encodernon-linear DecoderAutoEncoderPCA\n\nEncoderDecoderAutoEncoder\n\n## Regularized AutoEncoder\novercompletelinear Encoderlinear Decoder\n\nRegularized AutoEncoderLoss Function()EncoderDecoder\n\nSparse AutoEncoderEncoder$\\Omega(h)$\n$$\nL(x,g(f(x)))+\\Omega(h)\n$$\nSparse AutoEncoder\n\nCost FunctionRegularizationAutoEncoder\n\nDenoising AutoEncoder\n$$\nL(x,g(f(\\tilde{x})))\n$$\n$\\tilde{x}$DAE\n\nRegularized AutoEncoderSparse AutoEncoder$\\Omega$\n$$\nL(x,g(f(x)))+\\Omega(h,x)\n$$\n$\\Omega$\n$$\n\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2\n$$\n$x$training dataAutoEncodertraining data distribution informationAutoEncoderContractive AutoEncoder(CAE)\n\n## Details of Denosing AutoEncoder\nDAEAutoEncoderDAE$C(\\tilde{x}|x)$$x$$\\tilde{x}$$(x,\\tilde{x})$$p_{reconstruct}(x|\\tilde{x})$:\n1. training data$x$\n2. $C(\\tilde{x}|x=x)$$\\tilde{x}$\n3. $(x,\\tilde{x})$AutoEncoder$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$$h$Encoder$f(\\tilde{x})$$p_{decoder}$$g(h)$\n\n\n","source":"_posts/dl-ae.md","raw":"---\ntitle: \"[DL] Auto Encoder\"\ndate: 2018-08-22 19:17:56\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Data Science\n- Auto Encoder\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Auto Encoder\n---\n## Introduction\nAuto Encoder([repository](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)Deep AutoEncoder300word2vecdiscriminative)GANAutoEncodergenerative model\n\n## Undercomplete AutoEncoder\nAutoEncoder$h$$x$AutoEncoderUndercomplete AutoEncoderundercomplete representationAutoEncodertraining data\n\nDecoderLossMSEUndercomplete AutoEncoderPCAAutoEncodertraining datanon-linear Encodernon-linear DecoderAutoEncoderPCA\n\nEncoderDecoderAutoEncoder\n\n## Regularized AutoEncoder\novercompletelinear Encoderlinear Decoder\n\nRegularized AutoEncoderLoss Function()EncoderDecoder\n\nSparse AutoEncoderEncoder$\\Omega(h)$\n$$\nL(x,g(f(x)))+\\Omega(h)\n$$\nSparse AutoEncoder\n\nCost FunctionRegularizationAutoEncoder\n\nDenoising AutoEncoder\n$$\nL(x,g(f(\\tilde{x})))\n$$\n$\\tilde{x}$DAE\n\nRegularized AutoEncoderSparse AutoEncoder$\\Omega$\n$$\nL(x,g(f(x)))+\\Omega(h,x)\n$$\n$\\Omega$\n$$\n\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2\n$$\n$x$training dataAutoEncodertraining data distribution informationAutoEncoderContractive AutoEncoder(CAE)\n\n## Details of Denosing AutoEncoder\nDAEAutoEncoderDAE$C(\\tilde{x}|x)$$x$$\\tilde{x}$$(x,\\tilde{x})$$p_{reconstruct}(x|\\tilde{x})$:\n1. training data$x$\n2. $C(\\tilde{x}|x=x)$$\\tilde{x}$\n3. $(x,\\tilde{x})$AutoEncoder$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$$h$Encoder$f(\\tilde{x})$$p_{decoder}$$g(h)$\n\n\n","slug":"dl-ae","published":1,"updated":"2018-10-01T04:40:08.655Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bx000c608w18wlry9o","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Auto Encoder(<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">repository</a>Deep AutoEncoder300word2vecdiscriminative)GANAutoEncodergenerative model</p>\n<h2 id=\"Undercomplete-AutoEncoder\"><a href=\"#Undercomplete-AutoEncoder\" class=\"headerlink\" title=\"Undercomplete AutoEncoder\"></a>Undercomplete AutoEncoder</h2><p>AutoEncoder$h$$x$AutoEncoderUndercomplete AutoEncoderundercomplete representationAutoEncodertraining data</p>\n<p>DecoderLossMSEUndercomplete AutoEncoderPCAAutoEncodertraining datanon-linear Encodernon-linear DecoderAutoEncoderPCA</p>\n<p>EncoderDecoderAutoEncoder</p>\n<h2 id=\"Regularized-AutoEncoder\"><a href=\"#Regularized-AutoEncoder\" class=\"headerlink\" title=\"Regularized AutoEncoder\"></a>Regularized AutoEncoder</h2><p>overcompletelinear Encoderlinear Decoder</p>\n<p>Regularized AutoEncoderLoss Function()EncoderDecoder</p>\n<p>Sparse AutoEncoderEncoder$\\Omega(h)$<br>$$<br>L(x,g(f(x)))+\\Omega(h)<br>$$<br>Sparse AutoEncoder</p>\n<p>Cost FunctionRegularizationAutoEncoder</p>\n<p>Denoising AutoEncoder<br>$$<br>L(x,g(f(\\tilde{x})))<br>$$<br>$\\tilde{x}$DAE</p>\n<p>Regularized AutoEncoderSparse AutoEncoder$\\Omega$<br>$$<br>L(x,g(f(x)))+\\Omega(h,x)<br>$$<br>$\\Omega$<br>$$<br>\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2<br>$$<br>$x$training dataAutoEncodertraining data distribution informationAutoEncoderContractive AutoEncoder(CAE)</p>\n<h2 id=\"Details-of-Denosing-AutoEncoder\"><a href=\"#Details-of-Denosing-AutoEncoder\" class=\"headerlink\" title=\"Details of Denosing AutoEncoder\"></a>Details of Denosing AutoEncoder</h2><p>DAEAutoEncoderDAE$C(\\tilde{x}|x)$$x$$\\tilde{x}$$(x,\\tilde{x})$$p_{reconstruct}(x|\\tilde{x})$:</p>\n<ol>\n<li>training data$x$</li>\n<li>$C(\\tilde{x}|x=x)$$\\tilde{x}$</li>\n<li>$(x,\\tilde{x})$AutoEncoder$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$$h$Encoder$f(\\tilde{x})$$p_{decoder}$$g(h)$</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Auto Encoder(<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">repository</a>Deep AutoEncoder300word2vecdiscriminative)GANAutoEncodergenerative model</p>\n<h2 id=\"Undercomplete-AutoEncoder\"><a href=\"#Undercomplete-AutoEncoder\" class=\"headerlink\" title=\"Undercomplete AutoEncoder\"></a>Undercomplete AutoEncoder</h2><p>AutoEncoder$h$$x$AutoEncoderUndercomplete AutoEncoderundercomplete representationAutoEncodertraining data</p>\n<p>DecoderLossMSEUndercomplete AutoEncoderPCAAutoEncodertraining datanon-linear Encodernon-linear DecoderAutoEncoderPCA</p>\n<p>EncoderDecoderAutoEncoder</p>\n<h2 id=\"Regularized-AutoEncoder\"><a href=\"#Regularized-AutoEncoder\" class=\"headerlink\" title=\"Regularized AutoEncoder\"></a>Regularized AutoEncoder</h2><p>overcompletelinear Encoderlinear Decoder</p>\n<p>Regularized AutoEncoderLoss Function()EncoderDecoder</p>\n<p>Sparse AutoEncoderEncoder$\\Omega(h)$<br>$$<br>L(x,g(f(x)))+\\Omega(h)<br>$$<br>Sparse AutoEncoder</p>\n<p>Cost FunctionRegularizationAutoEncoder</p>\n<p>Denoising AutoEncoder<br>$$<br>L(x,g(f(\\tilde{x})))<br>$$<br>$\\tilde{x}$DAE</p>\n<p>Regularized AutoEncoderSparse AutoEncoder$\\Omega$<br>$$<br>L(x,g(f(x)))+\\Omega(h,x)<br>$$<br>$\\Omega$<br>$$<br>\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2<br>$$<br>$x$training dataAutoEncodertraining data distribution informationAutoEncoderContractive AutoEncoder(CAE)</p>\n<h2 id=\"Details-of-Denosing-AutoEncoder\"><a href=\"#Details-of-Denosing-AutoEncoder\" class=\"headerlink\" title=\"Details of Denosing AutoEncoder\"></a>Details of Denosing AutoEncoder</h2><p>DAEAutoEncoderDAE$C(\\tilde{x}|x)$$x$$\\tilde{x}$$(x,\\tilde{x})$$p_{reconstruct}(x|\\tilde{x})$:</p>\n<ol>\n<li>training data$x$</li>\n<li>$C(\\tilde{x}|x=x)$$\\tilde{x}$</li>\n<li>$(x,\\tilde{x})$AutoEncoder$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$$h$Encoder$f(\\tilde{x})$$p_{decoder}$$g(h)$</li>\n</ol>\n"},{"title":"[DL] Architecture","date":"2018-11-18T14:29:40.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Image Classification","Network Architecture"],"_content":"## Introduction\nDeep LearningNetwork ArchitectureLoss Function and OptimizationOptimizationPaperNetwork ArchitectureLoss FunctionIan GoodfellowDeep LearningAlexNetCliqueNetwork\n> [@LucasX](https://www.zhihu.com/people/xulu-0620)[](https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/)\n\n## AlexNet\n> Paper: [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\nAlexNetDeep LearningLarge Scale Image Classification TaskAlexNetComputer Vision Researcherhand-crafted featuresAlexNetwork\n\nAlexNet5conv + 3FC + SoftmaxAlexNetReLUSigmoidnon-linearity transformationGPUData AugmentationDropout\n\n## VGG\n> Paper: [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)\n\nVGGVGGAlexNetVGGFilter($3\\times 3$)<font color=\"red\">basic block</font>(GoogLeNetResNetResNeXtDenseNetblock)\n\n$3\\times 3$\n1. $3\\times 3$$5\\times 5$receptive field$3\\times 3$$7\\times 7$receptive field$7\\times 7$3$3\\times 3$<font color=\"red\">non-linearity transformationdiscriminative</font>\n2. 3channel$C$$3\\times 3$: $3(3^2C^2)=27C^2$channel$C$$7\\times 7$: $7^2C^2=49C^2$\n    > This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)\n\n\n## GoogLeNet\n> Paper: [Going deeper with convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n\nDCNNCVhand-crafted featuresnetwork architectureGoogLeNet**Multi-branch + Feature Concatenation**GoogLeNet$1\\times 1$ conv ($1\\times 1$ conv[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))\n* dimension reductionremove computational bottlenecks\n* computational bottlenecksFLOPsdeeprepresentation learning\n\nInception Module\n![Inception Module](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg)\n> $3\\times 3$$5\\times 5$ conv$1\\times 1$ conv**dimension reduction****non-linearity transformation**representation learning ability([Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))\n\nGoogLeNetInception Module(VGG/ResNet/ResNeXtblock)GoogLeNetMulti-branchclassification layersupervisiongradient flow(DeepID)\n\n\n## ResNet\n> Paper: [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\nResNetAlexNetDeep LearninginsightfulideaResNetshortcutDeep Architecture(DenseNet, CliqueNet, Deep Layer Aggregation)\nResNetKaiming HePaperNetworkAccuracy<font color=\"red\">overfitting</font>\n\n### What is Residual Network?\nDNNfunction $\\mathcal{H}(x)$DNN Architecturefunctionfunction $\\mathcal{F}(x):=\\mathcal{H}(x) - x$$\\mathcal{H}(x)$<font color=\"red\">$\\mathcal{F}(x)+x$</font>$\\mathcal{H}(x)$\n> identity mappingpush0non-linearity transformationidentity mapping\n \nShortcut\n$$\ny=\\mathcal{F}(x, \\{W_i\\}) + x\n$$\n$\\mathcal{F}(x, \\{W_i\\})$conv layersfeature mapchannel by channel element-wise \n\n[VGG](https://arxiv.org/pdf/1409.1556v6.pdf)$3\\times 3$ filters\n1. feature map sizefilter\n2. feature map sizefiltertime complexity\n\nfeature map dimensionelement-wise additionfeature map dimension doublezero paddingdimension$1\\times 1$ conv\n\nResNetclassification/detection/segmentation taskstate-of-the-artResNetideaKaiming\n\n\n## ShuffleNet\nComputer VisionAlexNetVGGGoogLeNetResNetDenseNetCliqueNet[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n> Paper: [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n\n### What is ShuffleNet?\n[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)**Pointwise Group Convolution****Channel Shuffle**\n\n![Channel Shuffle](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg)\n\n![ShuffleNet Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg)\n\n> Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.\n\n#### Advantages of Point-Wise Convolution\nNote that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.\n\n#### Channel Shuffle vs. No Shuffle\nThe purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.\n\n\n## MobileNet V1\n> Paper: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861v1.pdf)\n\nCNNResNetInceptionVGGFLOP[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)\n\n### Depth-wise Separable Convolution\nMobileNet**Depth-wise Separable Convolution**DW Convmodel size:\n$D_F\\times D_F\\times M$feature map$D_F\\times D_F\\times N$feature mapsize$D_K\\times D_K\\times M\\times N$$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$Computational Costinput channel $M$output channel $N$$D_K\\times D_K$feature map$D_F\\times D_F$\n\nMobileNetDepth-wise ConvKernel SizeOutput ChannelConv Operationfiltersfeaturesfilter(Combinations)representationsFilteringCombinationsDW Separable Conv\n\n**Depth-wise Separable ConvolutionDepth-wise Conv + Point-wise Conv**DW ConvchannelfilterPW Conv (a simple $1\\times 1$ Conv)DW layerlinear combination**DW ConvComputational Cost: $D_K\\times D_K\\times M\\times D_F\\times D_F$**Conv$N$\n\nDW Convfilterinput channel__combination__layerDW Convfeaturelinear combinationrepresentation$1\\times 1$ ConvPW Conv\n\nThe combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.\n\nDW Separable ConvComputational Cost\n$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$DW ConvcostPW Convcost\n\nBy expressing convolution as a two step process of filtering and combining we get a reduction in computation of:\n$$\n\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}\n$$\nDepth-wise Separable ConvConv$\\frac{1}{N} + \\frac{1}{D_K^2}$\n![DW Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png)\n\n### Width Multiplier: Thinner Models\nIn order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier  is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:\n\n$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$\n\nwhere $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha < 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.\n\n### Resolution Multiplier: Reduced Representation\nThe second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:\n \n$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$\n\nwhere $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and  < 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.\n\n\n## MobileNet V2\n> Paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)\n\n[MobileNet V2](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[CVPR2018](http://openaccess.thecvf.com/CVPR2018.py)PaperMobileNet V2contribution: \n\n> Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.\n\n### Preliminaries, discussion and intuition\n#### Depthwise Separable Convolutions\nLight-weighted ArchitectureDW ConvMobileNet V2DW Conv\n\n> The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.\n\nConv$h_i\\times w_i\\times d_i$$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$$h_i\\times w_i\\times d_j$ConvComputational Cost$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$DW Separable ConvComputational Cost$h_i\\times w_i\\times d_i(k^2+d_j)$__$k^2$__\n\n#### Linear Bottlenecks\n> It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.\n\nDCNNConv + ReLU + (Pool) + (FC) + SoftmaxDNNnon-linearity transformationgradient vanishing/explodingSigmoidReLUReLU\n\n> It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. **In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.**\n\n> On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.\n\nReLu\n1. Manifold of InterestReLU\n2. ReLUinput manifold**input manifoldinput space**\n\nMobileNet V2high-dimensional hidden representationslow-dimensional embeddinghigh-dimensional\n\n![Evolution of Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png)\n\n#### Inverted residuals\n> The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.\n\n[MobileNet V1](https://arxiv.org/pdf/1704.04861v1.pdf)feedforwad networkshortcut[ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)effectiveMobileNet V2skip connection\n\n#### MobileNet V2 Architecture\n> We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.\n\n![DCNN Architecture Comparison](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png)\n\n\n## ResNeXt\n> Paper: [Aggregated Residual Transformations for Deep Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)\n\n### Introduction\nAlexNetDeep Learning(VGGInceptionResNet)ResNeXtResNetInception**split-transform-merge**\n\n\n### The Core of ResNeXt\n> Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 11 convolutions), transformed by a set of specialized filters (33, 55, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 55) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.\n\nInception**split-transform-merge**(VGGResNetblock stacking)ImageNetdatasetResNeXtVGG/ResNetstacking blockInceptionsplit-transform-mergeResNeXtideacardinality (the size of the set of transformations)performancewidthdepth\n\n> Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view\nour method as ensembling, because the members to be aggregated\nare trained jointly, not independently.\n\n> The above operation can be recast as a combination of\nsplitting, transforming, and aggregating. \n1. Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. \n2. Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.\n3. Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.\n\n$W$function mapping: $\\mathcal{T}(x)$aggregated transformations:\n$$\n\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n$\\mathcal{T}_i$$x$$C$transformationsize**cardinality**\n\n> In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.\n\n![ResNeXt Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg)\n\nResNetidentical mapping\n$$\ny=x+\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n\n> **Relation to Grouped Convolutions**. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.\n\n![Equivalent Building Blocks of ResNeXt](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg)\n\n\n## DenseNet\n> Paper: [Densely Connected Convolutional Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n\n### What is DenseNet?\nDenseNetCVPR2017 Best PaperResNet[](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)ResNetgradient vanishingResNetidentical mappingDenseNet\n\nDenseNetResNetshortcutsDCNNAlexNet/VGG/GoogLeNetfeedforward network**BP**gradient vanishingKaiming Heskip connection$i$identical mapping$i+t$\n> : ResNet[](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)\n\n![Dense Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg)\n\nDenseNetskip connectionlayerDenseNetskip connectionResNetDW SummationDenseNetconcatenatefeatures (ResNetDW Summation)\n\n**DenseNetResNet**dense connectionfeature maps\nevery layerlossearly layer**deep supervision**dense connectionsRegularizationoverfitting\n\n\n* gradient vanishing\n* \n* feature reuse\n* \n\n### Delve into DenseNet\n#### Dense connectivity\nTo further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the th layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:\n$$\nx_l=H_l([x_0,x_1,\\cdots,x_{l-1}])\n$$\nwhere $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l1$.\n\n#### Pooling layers\nThe concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.\n\n#### Growth rate\nIf each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l1)$ input\nfeature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.\n\n**One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's \"collective knowledge\". One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer**.\n\n#### Bottleneck layers\nAlthough each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.\n\n\n## Identity Mappings in Deep Residual Networks\n> Paper: [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n\n### Introduction\nResNetCVKaiming HeResNetshortcutDCNNshortcutworkResNetECCV'16Paperidentical mappingworkidentical mappingpre-activation\n\n### Delve Into ResNet and Identical Mapping\nResNet\n$$\ny_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\n\n$$\nx_{l+1} = f(y_l)\n$$\n$\\mathcal{F}$residual function$h(x_l)=x_l$identical mapping$f$ReLU\n\n**$h(x_l)$$f(y_l)$identical mappingunitunitsforwardbackward**\n\n![Proposed Residual Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg)\n\nidentical mapping $f(y_l)=y_l$activation function (ReLU and BN)weight layers```pre-activation```\n\n### Analysis of Deep Residual Networks\nCVPR'15 Best PaperResNet Unit\n$$\ny_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)\n$$\n\n$$\nx_{l+1}=f(y_l)\n$$\n$f$identical mapping: $x_{l+1}\\equiv y_l$\n$$\nx_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\nRecursively\n$$\nx_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})\n$$\n\n$$\nx_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\n$$\ndeep$L$shallow$l$$x_L$**shallow unit feature $x_l$residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$**\n1. **units $L$$l$ residual function**\n2. $x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$proceeding residual functionssummation$x_0$\n\nBPchain rule(loss function$\\epsilon$)\n$$\n\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))\n$$\n$\\frac{\\partial \\epsilon}{\\partial x_l}$\n1. $\\frac{\\partial \\epsilon}{\\partial x_L}$\n2. $\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$weight layers\n\n#### Discussions\nPaperidentical mappingscaling, gating, $1\\times 1$ convolutions, and dropout**$1\\times 1$ conv**representation learning abilityResNetperformance droprepresentation ability\n\n> The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations\n(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.\n\nBN + ReLUpre-activationperformance\n1. $f$identical mappingoptimization\n2. BNpre-activationregularizationBNregularizationCVPR'15ResNetBN normalizeshortcutBN normalizesignalmergepre-activationweight layersinputnormalize\n\n\n## Reference\n1. Krizhevsky A, Sutskever I, Hinton G E. [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[C]//Advances in neural information processing systems. 2012: 1097-1105.\n2. Simonyan K, Zisserman A. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)[J]. arXiv preprint arXiv:1409.1556, 2014.\n3. Lin M, Chen Q, Yan S. [Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[J]. arXiv preprint arXiv:1312.4400, 2013.\n4. He K, Zhang X, Ren S, Sun J. [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).\n5. Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n6. Chollet, Francois. [\"Xception: Deep Learning with Depthwise Separable Convolutions.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.\n7. Howard, Andrew G., et al. [\"Mobilenets: Efficient convolutional neural networks for mobile vision applications.\"](https://arxiv.org/pdf/1704.04861v1.pdf) arXiv preprint arXiv:1704.04861 (2017).\n8. Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. [\"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\"](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n9. Xie, Saining, et al. [\"Aggregated residual transformations for deep neural networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf) Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.\n10. Huang, Gao, et al. [\"Densely Connected Convolutional Networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) CVPR. Vol. 1. No. 2. 2017.\n11. He K, Zhang X, Ren S, et al. [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.\n12. Szegedy, Christian, et al. [\"Going deeper with convolutions.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n","source":"_posts/dl-architecture.md","raw":"---\ntitle: \"[DL] Architecture\"\ndate: 2018-11-18 22:29:40\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Image Classification\n- Network Architecture\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Image Classification\n- Network Architecture\n---\n## Introduction\nDeep LearningNetwork ArchitectureLoss Function and OptimizationOptimizationPaperNetwork ArchitectureLoss FunctionIan GoodfellowDeep LearningAlexNetCliqueNetwork\n> [@LucasX](https://www.zhihu.com/people/xulu-0620)[](https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/)\n\n## AlexNet\n> Paper: [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\nAlexNetDeep LearningLarge Scale Image Classification TaskAlexNetComputer Vision Researcherhand-crafted featuresAlexNetwork\n\nAlexNet5conv + 3FC + SoftmaxAlexNetReLUSigmoidnon-linearity transformationGPUData AugmentationDropout\n\n## VGG\n> Paper: [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)\n\nVGGVGGAlexNetVGGFilter($3\\times 3$)<font color=\"red\">basic block</font>(GoogLeNetResNetResNeXtDenseNetblock)\n\n$3\\times 3$\n1. $3\\times 3$$5\\times 5$receptive field$3\\times 3$$7\\times 7$receptive field$7\\times 7$3$3\\times 3$<font color=\"red\">non-linearity transformationdiscriminative</font>\n2. 3channel$C$$3\\times 3$: $3(3^2C^2)=27C^2$channel$C$$7\\times 7$: $7^2C^2=49C^2$\n    > This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)\n\n\n## GoogLeNet\n> Paper: [Going deeper with convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n\nDCNNCVhand-crafted featuresnetwork architectureGoogLeNet**Multi-branch + Feature Concatenation**GoogLeNet$1\\times 1$ conv ($1\\times 1$ conv[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))\n* dimension reductionremove computational bottlenecks\n* computational bottlenecksFLOPsdeeprepresentation learning\n\nInception Module\n![Inception Module](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg)\n> $3\\times 3$$5\\times 5$ conv$1\\times 1$ conv**dimension reduction****non-linearity transformation**representation learning ability([Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))\n\nGoogLeNetInception Module(VGG/ResNet/ResNeXtblock)GoogLeNetMulti-branchclassification layersupervisiongradient flow(DeepID)\n\n\n## ResNet\n> Paper: [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\nResNetAlexNetDeep LearninginsightfulideaResNetshortcutDeep Architecture(DenseNet, CliqueNet, Deep Layer Aggregation)\nResNetKaiming HePaperNetworkAccuracy<font color=\"red\">overfitting</font>\n\n### What is Residual Network?\nDNNfunction $\\mathcal{H}(x)$DNN Architecturefunctionfunction $\\mathcal{F}(x):=\\mathcal{H}(x) - x$$\\mathcal{H}(x)$<font color=\"red\">$\\mathcal{F}(x)+x$</font>$\\mathcal{H}(x)$\n> identity mappingpush0non-linearity transformationidentity mapping\n \nShortcut\n$$\ny=\\mathcal{F}(x, \\{W_i\\}) + x\n$$\n$\\mathcal{F}(x, \\{W_i\\})$conv layersfeature mapchannel by channel element-wise \n\n[VGG](https://arxiv.org/pdf/1409.1556v6.pdf)$3\\times 3$ filters\n1. feature map sizefilter\n2. feature map sizefiltertime complexity\n\nfeature map dimensionelement-wise additionfeature map dimension doublezero paddingdimension$1\\times 1$ conv\n\nResNetclassification/detection/segmentation taskstate-of-the-artResNetideaKaiming\n\n\n## ShuffleNet\nComputer VisionAlexNetVGGGoogLeNetResNetDenseNetCliqueNet[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n> Paper: [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n\n### What is ShuffleNet?\n[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)**Pointwise Group Convolution****Channel Shuffle**\n\n![Channel Shuffle](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg)\n\n![ShuffleNet Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg)\n\n> Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.\n\n#### Advantages of Point-Wise Convolution\nNote that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.\n\n#### Channel Shuffle vs. No Shuffle\nThe purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.\n\n\n## MobileNet V1\n> Paper: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861v1.pdf)\n\nCNNResNetInceptionVGGFLOP[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)\n\n### Depth-wise Separable Convolution\nMobileNet**Depth-wise Separable Convolution**DW Convmodel size:\n$D_F\\times D_F\\times M$feature map$D_F\\times D_F\\times N$feature mapsize$D_K\\times D_K\\times M\\times N$$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$Computational Costinput channel $M$output channel $N$$D_K\\times D_K$feature map$D_F\\times D_F$\n\nMobileNetDepth-wise ConvKernel SizeOutput ChannelConv Operationfiltersfeaturesfilter(Combinations)representationsFilteringCombinationsDW Separable Conv\n\n**Depth-wise Separable ConvolutionDepth-wise Conv + Point-wise Conv**DW ConvchannelfilterPW Conv (a simple $1\\times 1$ Conv)DW layerlinear combination**DW ConvComputational Cost: $D_K\\times D_K\\times M\\times D_F\\times D_F$**Conv$N$\n\nDW Convfilterinput channel__combination__layerDW Convfeaturelinear combinationrepresentation$1\\times 1$ ConvPW Conv\n\nThe combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.\n\nDW Separable ConvComputational Cost\n$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$DW ConvcostPW Convcost\n\nBy expressing convolution as a two step process of filtering and combining we get a reduction in computation of:\n$$\n\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}\n$$\nDepth-wise Separable ConvConv$\\frac{1}{N} + \\frac{1}{D_K^2}$\n![DW Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png)\n\n### Width Multiplier: Thinner Models\nIn order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier  is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:\n\n$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$\n\nwhere $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha < 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.\n\n### Resolution Multiplier: Reduced Representation\nThe second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:\n \n$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$\n\nwhere $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and  < 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.\n\n\n## MobileNet V2\n> Paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)\n\n[MobileNet V2](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[CVPR2018](http://openaccess.thecvf.com/CVPR2018.py)PaperMobileNet V2contribution: \n\n> Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.\n\n### Preliminaries, discussion and intuition\n#### Depthwise Separable Convolutions\nLight-weighted ArchitectureDW ConvMobileNet V2DW Conv\n\n> The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.\n\nConv$h_i\\times w_i\\times d_i$$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$$h_i\\times w_i\\times d_j$ConvComputational Cost$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$DW Separable ConvComputational Cost$h_i\\times w_i\\times d_i(k^2+d_j)$__$k^2$__\n\n#### Linear Bottlenecks\n> It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.\n\nDCNNConv + ReLU + (Pool) + (FC) + SoftmaxDNNnon-linearity transformationgradient vanishing/explodingSigmoidReLUReLU\n\n> It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. **In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.**\n\n> On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.\n\nReLu\n1. Manifold of InterestReLU\n2. ReLUinput manifold**input manifoldinput space**\n\nMobileNet V2high-dimensional hidden representationslow-dimensional embeddinghigh-dimensional\n\n![Evolution of Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png)\n\n#### Inverted residuals\n> The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.\n\n[MobileNet V1](https://arxiv.org/pdf/1704.04861v1.pdf)feedforwad networkshortcut[ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)effectiveMobileNet V2skip connection\n\n#### MobileNet V2 Architecture\n> We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.\n\n![DCNN Architecture Comparison](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png)\n\n\n## ResNeXt\n> Paper: [Aggregated Residual Transformations for Deep Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)\n\n### Introduction\nAlexNetDeep Learning(VGGInceptionResNet)ResNeXtResNetInception**split-transform-merge**\n\n\n### The Core of ResNeXt\n> Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 11 convolutions), transformed by a set of specialized filters (33, 55, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 55) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.\n\nInception**split-transform-merge**(VGGResNetblock stacking)ImageNetdatasetResNeXtVGG/ResNetstacking blockInceptionsplit-transform-mergeResNeXtideacardinality (the size of the set of transformations)performancewidthdepth\n\n> Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view\nour method as ensembling, because the members to be aggregated\nare trained jointly, not independently.\n\n> The above operation can be recast as a combination of\nsplitting, transforming, and aggregating. \n1. Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. \n2. Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.\n3. Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.\n\n$W$function mapping: $\\mathcal{T}(x)$aggregated transformations:\n$$\n\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n$\\mathcal{T}_i$$x$$C$transformationsize**cardinality**\n\n> In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.\n\n![ResNeXt Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg)\n\nResNetidentical mapping\n$$\ny=x+\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n\n> **Relation to Grouped Convolutions**. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.\n\n![Equivalent Building Blocks of ResNeXt](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg)\n\n\n## DenseNet\n> Paper: [Densely Connected Convolutional Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n\n### What is DenseNet?\nDenseNetCVPR2017 Best PaperResNet[](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)ResNetgradient vanishingResNetidentical mappingDenseNet\n\nDenseNetResNetshortcutsDCNNAlexNet/VGG/GoogLeNetfeedforward network**BP**gradient vanishingKaiming Heskip connection$i$identical mapping$i+t$\n> : ResNet[](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)\n\n![Dense Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg)\n\nDenseNetskip connectionlayerDenseNetskip connectionResNetDW SummationDenseNetconcatenatefeatures (ResNetDW Summation)\n\n**DenseNetResNet**dense connectionfeature maps\nevery layerlossearly layer**deep supervision**dense connectionsRegularizationoverfitting\n\n\n* gradient vanishing\n* \n* feature reuse\n* \n\n### Delve into DenseNet\n#### Dense connectivity\nTo further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the th layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:\n$$\nx_l=H_l([x_0,x_1,\\cdots,x_{l-1}])\n$$\nwhere $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l1$.\n\n#### Pooling layers\nThe concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.\n\n#### Growth rate\nIf each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l1)$ input\nfeature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.\n\n**One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's \"collective knowledge\". One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer**.\n\n#### Bottleneck layers\nAlthough each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.\n\n\n## Identity Mappings in Deep Residual Networks\n> Paper: [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n\n### Introduction\nResNetCVKaiming HeResNetshortcutDCNNshortcutworkResNetECCV'16Paperidentical mappingworkidentical mappingpre-activation\n\n### Delve Into ResNet and Identical Mapping\nResNet\n$$\ny_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\n\n$$\nx_{l+1} = f(y_l)\n$$\n$\\mathcal{F}$residual function$h(x_l)=x_l$identical mapping$f$ReLU\n\n**$h(x_l)$$f(y_l)$identical mappingunitunitsforwardbackward**\n\n![Proposed Residual Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg)\n\nidentical mapping $f(y_l)=y_l$activation function (ReLU and BN)weight layers```pre-activation```\n\n### Analysis of Deep Residual Networks\nCVPR'15 Best PaperResNet Unit\n$$\ny_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)\n$$\n\n$$\nx_{l+1}=f(y_l)\n$$\n$f$identical mapping: $x_{l+1}\\equiv y_l$\n$$\nx_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\nRecursively\n$$\nx_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})\n$$\n\n$$\nx_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\n$$\ndeep$L$shallow$l$$x_L$**shallow unit feature $x_l$residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$**\n1. **units $L$$l$ residual function**\n2. $x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$proceeding residual functionssummation$x_0$\n\nBPchain rule(loss function$\\epsilon$)\n$$\n\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))\n$$\n$\\frac{\\partial \\epsilon}{\\partial x_l}$\n1. $\\frac{\\partial \\epsilon}{\\partial x_L}$\n2. $\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$weight layers\n\n#### Discussions\nPaperidentical mappingscaling, gating, $1\\times 1$ convolutions, and dropout**$1\\times 1$ conv**representation learning abilityResNetperformance droprepresentation ability\n\n> The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations\n(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.\n\nBN + ReLUpre-activationperformance\n1. $f$identical mappingoptimization\n2. BNpre-activationregularizationBNregularizationCVPR'15ResNetBN normalizeshortcutBN normalizesignalmergepre-activationweight layersinputnormalize\n\n\n## Reference\n1. Krizhevsky A, Sutskever I, Hinton G E. [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[C]//Advances in neural information processing systems. 2012: 1097-1105.\n2. Simonyan K, Zisserman A. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)[J]. arXiv preprint arXiv:1409.1556, 2014.\n3. Lin M, Chen Q, Yan S. [Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[J]. arXiv preprint arXiv:1312.4400, 2013.\n4. He K, Zhang X, Ren S, Sun J. [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).\n5. Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n6. Chollet, Francois. [\"Xception: Deep Learning with Depthwise Separable Convolutions.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.\n7. Howard, Andrew G., et al. [\"Mobilenets: Efficient convolutional neural networks for mobile vision applications.\"](https://arxiv.org/pdf/1704.04861v1.pdf) arXiv preprint arXiv:1704.04861 (2017).\n8. Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. [\"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\"](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n9. Xie, Saining, et al. [\"Aggregated residual transformations for deep neural networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf) Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.\n10. Huang, Gao, et al. [\"Densely Connected Convolutional Networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) CVPR. Vol. 1. No. 2. 2017.\n11. He K, Zhang X, Ren S, et al. [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.\n12. Szegedy, Christian, et al. [\"Going deeper with convolutions.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n","slug":"dl-architecture","published":1,"updated":"2018-11-18T14:29:16.777Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bz000d608wtw5onf9s","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep LearningNetwork ArchitectureLoss Function and OptimizationOptimizationPaperNetwork ArchitectureLoss FunctionIan GoodfellowDeep LearningAlexNetCliqueNetwork</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620\" target=\"_blank\" rel=\"noopener\">@LucasX</a><a href=\"https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/\"></a></p>\n</blockquote>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a></p>\n</blockquote>\n<p>AlexNetDeep LearningLarge Scale Image Classification TaskAlexNetComputer Vision Researcherhand-crafted featuresAlexNetwork</p>\n<p>AlexNet5conv + 3FC + SoftmaxAlexNetReLUSigmoidnon-linearity transformationGPUData AugmentationDropout</p>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a></p>\n</blockquote>\n<p>VGGVGGAlexNetVGGFilter($3\\times 3$)<font color=\"red\">basic block</font>(GoogLeNetResNetResNeXtDenseNetblock)</p>\n<p>$3\\times 3$</p>\n<ol>\n<li>$3\\times 3$$5\\times 5$receptive field$3\\times 3$$7\\times 7$receptive field$7\\times 7$3$3\\times 3$<font color=\"red\">non-linearity transformationdiscriminative</font></li>\n<li>3channel$C$$3\\times 3$: $3(3^2C^2)=27C^2$channel$C$$7\\times 7$: $7^2C^2=49C^2$<blockquote>\n<p>This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"GoogLeNet\"><a href=\"#GoogLeNet\" class=\"headerlink\" title=\"GoogLeNet\"></a>GoogLeNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions</a></p>\n</blockquote>\n<p>DCNNCVhand-crafted featuresnetwork architectureGoogLeNet<strong>Multi-branch + Feature Concatenation</strong>GoogLeNet$1\\times 1$ conv ($1\\times 1$ conv<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)</p>\n<ul>\n<li>dimension reductionremove computational bottlenecks</li>\n<li>computational bottlenecksFLOPsdeeprepresentation learning</li>\n</ul>\n<p>Inception Module<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg\" alt=\"Inception Module\"></p>\n<blockquote>\n<p>$3\\times 3$$5\\times 5$ conv$1\\times 1$ conv<strong>dimension reduction</strong><strong>non-linearity transformation</strong>representation learning ability(<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a><a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)</p>\n</blockquote>\n<p>GoogLeNetInception Module(VGG/ResNet/ResNeXtblock)GoogLeNetMulti-branchclassification layersupervisiongradient flow(DeepID)</p>\n<h2 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep Residual Learning for Image Recognition</a></p>\n</blockquote>\n<p>ResNetAlexNetDeep LearninginsightfulideaResNetshortcutDeep Architecture(DenseNet, CliqueNet, Deep Layer Aggregation)<br>ResNetKaiming HePaperNetworkAccuracy<font color=\"red\">overfitting</font></p>\n<h3 id=\"What-is-Residual-Network\"><a href=\"#What-is-Residual-Network\" class=\"headerlink\" title=\"What is Residual Network?\"></a>What is Residual Network?</h3><p>DNNfunction $\\mathcal{H}(x)$DNN Architecturefunctionfunction $\\mathcal{F}(x):=\\mathcal{H}(x) - x$$\\mathcal{H}(x)$<font color=\"red\">$\\mathcal{F}(x)+x$</font>$\\mathcal{H}(x)$</p>\n<blockquote>\n<p>identity mappingpush0non-linearity transformationidentity mapping</p>\n</blockquote>\n<p>Shortcut<br>$$<br>y=\\mathcal{F}(x, \\{W_i\\}) + x<br>$$<br>$\\mathcal{F}(x, \\{W_i\\})$conv layersfeature mapchannel by channel element-wise </p>\n<p><a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">VGG</a>$3\\times 3$ filters</p>\n<ol>\n<li>feature map sizefilter</li>\n<li>feature map sizefiltertime complexity</li>\n</ol>\n<p>feature map dimensionelement-wise additionfeature map dimension doublezero paddingdimension$1\\times 1$ conv</p>\n<p>ResNetclassification/detection/segmentation taskstate-of-the-artResNetideaKaiming</p>\n<h2 id=\"ShuffleNet\"><a href=\"#ShuffleNet\" class=\"headerlink\" title=\"ShuffleNet\"></a>ShuffleNet</h2><p>Computer VisionAlexNetVGGGoogLeNetResNetDenseNetCliqueNet<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a></p>\n<blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p>\n</blockquote>\n<h3 id=\"What-is-ShuffleNet\"><a href=\"#What-is-ShuffleNet\" class=\"headerlink\" title=\"What is ShuffleNet?\"></a>What is ShuffleNet?</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><strong>Pointwise Group Convolution</strong><strong>Channel Shuffle</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg\" alt=\"Channel Shuffle\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg\" alt=\"ShuffleNet Unit\"></p>\n<blockquote>\n<p>Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.</p>\n</blockquote>\n<h4 id=\"Advantages-of-Point-Wise-Convolution\"><a href=\"#Advantages-of-Point-Wise-Convolution\" class=\"headerlink\" title=\"Advantages of Point-Wise Convolution\"></a>Advantages of Point-Wise Convolution</h4><p>Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.</p>\n<h4 id=\"Channel-Shuffle-vs-No-Shuffle\"><a href=\"#Channel-Shuffle-vs-No-Shuffle\" class=\"headerlink\" title=\"Channel Shuffle vs. No Shuffle\"></a>Channel Shuffle vs. No Shuffle</h4><p>The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.</p>\n<h2 id=\"MobileNet-V1\"><a href=\"#MobileNet-V1\" class=\"headerlink\" title=\"MobileNet V1\"></a>MobileNet V1</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>\n</blockquote>\n<p>CNNResNetInceptionVGGFLOP<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a></p>\n<h3 id=\"Depth-wise-Separable-Convolution\"><a href=\"#Depth-wise-Separable-Convolution\" class=\"headerlink\" title=\"Depth-wise Separable Convolution\"></a>Depth-wise Separable Convolution</h3><p>MobileNet<strong>Depth-wise Separable Convolution</strong>DW Convmodel size:<br>$D_F\\times D_F\\times M$feature map$D_F\\times D_F\\times N$feature mapsize$D_K\\times D_K\\times M\\times N$$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$Computational Costinput channel $M$output channel $N$$D_K\\times D_K$feature map$D_F\\times D_F$</p>\n<p>MobileNetDepth-wise ConvKernel SizeOutput ChannelConv Operationfiltersfeaturesfilter(Combinations)representationsFilteringCombinationsDW Separable Conv</p>\n<p><strong>Depth-wise Separable ConvolutionDepth-wise Conv + Point-wise Conv</strong>DW ConvchannelfilterPW Conv (a simple $1\\times 1$ Conv)DW layerlinear combination<strong>DW ConvComputational Cost: $D_K\\times D_K\\times M\\times D_F\\times D_F$</strong>Conv$N$</p>\n<p>DW Convfilterinput channel<strong>combination</strong>layerDW Convfeaturelinear combinationrepresentation$1\\times 1$ ConvPW Conv</p>\n<p>The combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.</p>\n<p>DW Separable ConvComputational Cost<br>$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$DW ConvcostPW Convcost</p>\n<p>By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:<br>$$<br>\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}<br>$$<br>Depth-wise Separable ConvConv$\\frac{1}{N} + \\frac{1}{D_K^2}$<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png\" alt=\"DW Separable Conv\"></p>\n<h3 id=\"Width-Multiplier-Thinner-Models\"><a href=\"#Width-Multiplier-Thinner-Models\" class=\"headerlink\" title=\"Width Multiplier: Thinner Models\"></a>Width Multiplier: Thinner Models</h3><p>In order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier  is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$</p>\n<p>where $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha &lt; 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.</p>\n<h3 id=\"Resolution-Multiplier-Reduced-Representation\"><a href=\"#Resolution-Multiplier-Reduced-Representation\" class=\"headerlink\" title=\"Resolution Multiplier: Reduced Representation\"></a>Resolution Multiplier: Reduced Representation</h3><p>The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$</p>\n<p>where $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and  &lt; 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.</p>\n<h2 id=\"MobileNet-V2\"><a href=\"#MobileNet-V2\" class=\"headerlink\" title=\"MobileNet V2\"></a>MobileNet V2</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>\n</blockquote>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V2</a><a href=\"http://openaccess.thecvf.com/CVPR2018.py\" target=\"_blank\" rel=\"noopener\">CVPR2018</a>PaperMobileNet V2contribution: </p>\n<blockquote>\n<p>Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.</p>\n</blockquote>\n<h3 id=\"Preliminaries-discussion-and-intuition\"><a href=\"#Preliminaries-discussion-and-intuition\" class=\"headerlink\" title=\"Preliminaries, discussion and intuition\"></a>Preliminaries, discussion and intuition</h3><h4 id=\"Depthwise-Separable-Convolutions\"><a href=\"#Depthwise-Separable-Convolutions\" class=\"headerlink\" title=\"Depthwise Separable Convolutions\"></a>Depthwise Separable Convolutions</h4><p>Light-weighted ArchitectureDW ConvMobileNet V2DW Conv</p>\n<blockquote>\n<p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.</p>\n</blockquote>\n<p>Conv$h_i\\times w_i\\times d_i$$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$$h_i\\times w_i\\times d_j$ConvComputational Cost$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$DW Separable ConvComputational Cost$h_i\\times w_i\\times d_i(k^2+d_j)$<strong>$k^2$</strong></p>\n<h4 id=\"Linear-Bottlenecks\"><a href=\"#Linear-Bottlenecks\" class=\"headerlink\" title=\"Linear Bottlenecks\"></a>Linear Bottlenecks</h4><blockquote>\n<p>It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.</p>\n</blockquote>\n<p>DCNNConv + ReLU + (Pool) + (FC) + SoftmaxDNNnon-linearity transformationgradient vanishing/explodingSigmoidReLUReLU</p>\n<blockquote>\n<p>It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. <strong>In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.</strong></p>\n</blockquote>\n<blockquote>\n<p>On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.</p>\n</blockquote>\n<p>ReLu</p>\n<ol>\n<li>Manifold of InterestReLU</li>\n<li>ReLUinput manifold<strong>input manifoldinput space</strong></li>\n</ol>\n<p>MobileNet V2high-dimensional hidden representationslow-dimensional embeddinghigh-dimensional</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png\" alt=\"Evolution of Separable Conv\"></p>\n<h4 id=\"Inverted-residuals\"><a href=\"#Inverted-residuals\" class=\"headerlink\" title=\"Inverted residuals\"></a>Inverted residuals</h4><blockquote>\n<p>The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V1</a>feedforwad networkshortcut<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>effectiveMobileNet V2skip connection</p>\n<h4 id=\"MobileNet-V2-Architecture\"><a href=\"#MobileNet-V2-Architecture\" class=\"headerlink\" title=\"MobileNet V2 Architecture\"></a>MobileNet V2 Architecture</h4><blockquote>\n<p>We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png\" alt=\"DCNN Architecture Comparison\"></p>\n<h2 id=\"ResNeXt\"><a href=\"#ResNeXt\" class=\"headerlink\" title=\"ResNeXt\"></a>ResNeXt</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated Residual Transformations for Deep Neural Networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>AlexNetDeep Learning(VGGInceptionResNet)ResNeXtResNetInception<strong>split-transform-merge</strong></p>\n<h3 id=\"The-Core-of-ResNeXt\"><a href=\"#The-Core-of-ResNeXt\" class=\"headerlink\" title=\"The Core of ResNeXt\"></a>The Core of ResNeXt</h3><blockquote>\n<p>Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 11 convolutions), transformed by a set of specialized filters (33, 55, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 55) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.</p>\n</blockquote>\n<p>Inception<strong>split-transform-merge</strong>(VGGResNetblock stacking)ImageNetdatasetResNeXtVGG/ResNetstacking blockInceptionsplit-transform-mergeResNeXtideacardinality (the size of the set of transformations)performancewidthdepth</p>\n<blockquote>\n<p>Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view<br>our method as ensembling, because the members to be aggregated<br>are trained jointly, not independently.</p>\n</blockquote>\n<blockquote>\n<p>The above operation can be recast as a combination of<br>splitting, transforming, and aggregating. </p>\n<ol>\n<li>Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. </li>\n<li>Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.</li>\n<li>Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.</li>\n</ol>\n</blockquote>\n<p>$W$function mapping: $\\mathcal{T}(x)$aggregated transformations:<br>$$<br>\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$<br>$\\mathcal{T}_i$$x$$C$transformationsize<strong>cardinality</strong></p>\n<blockquote>\n<p>In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg\" alt=\"ResNeXt Block\"></p>\n<p>ResNetidentical mapping<br>$$<br>y=x+\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$</p>\n<blockquote>\n<p><strong>Relation to Grouped Convolutions</strong>. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg\" alt=\"Equivalent Building Blocks of ResNeXt\"></p>\n<h2 id=\"DenseNet\"><a href=\"#DenseNet\" class=\"headerlink\" title=\"DenseNet\"></a>DenseNet</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n</blockquote>\n<h3 id=\"What-is-DenseNet\"><a href=\"#What-is-DenseNet\" class=\"headerlink\" title=\"What is DenseNet?\"></a>What is DenseNet?</h3><p>DenseNetCVPR2017 Best PaperResNet<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\"></a>ResNetgradient vanishingResNetidentical mappingDenseNet</p>\n<p>DenseNetResNetshortcutsDCNNAlexNet/VGG/GoogLeNetfeedforward network<strong>BP</strong>gradient vanishingKaiming Heskip connection$i$identical mapping$i+t$</p>\n<blockquote>\n<p>: ResNet<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\"></a></p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg\" alt=\"Dense Block\"></p>\n<p>DenseNetskip connectionlayerDenseNetskip connectionResNetDW SummationDenseNetconcatenatefeatures (ResNetDW Summation)</p>\n<p><strong>DenseNetResNet</strong>dense connectionfeature maps<br>every layerlossearly layer<strong>deep supervision</strong>dense connectionsRegularizationoverfitting</p>\n<p></p>\n<ul>\n<li>gradient vanishing</li>\n<li></li>\n<li>feature reuse</li>\n<li></li>\n</ul>\n<h3 id=\"Delve-into-DenseNet\"><a href=\"#Delve-into-DenseNet\" class=\"headerlink\" title=\"Delve into DenseNet\"></a>Delve into DenseNet</h3><h4 id=\"Dense-connectivity\"><a href=\"#Dense-connectivity\" class=\"headerlink\" title=\"Dense connectivity\"></a>Dense connectivity</h4><p>To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the th layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:<br>$$<br>x_l=H_l([x_0,x_1,\\cdots,x_{l-1}])<br>$$<br>where $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l1$.</p>\n<h4 id=\"Pooling-layers\"><a href=\"#Pooling-layers\" class=\"headerlink\" title=\"Pooling layers\"></a>Pooling layers</h4><p>The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.</p>\n<h4 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h4><p>If each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l1)$ input<br>feature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and<br>existing network architectures is that DenseNet can have<br>very narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.</p>\n<p><strong>One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the networks collective knowledge. One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer</strong>.</p>\n<h4 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h4><p>Although each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.</p>\n<h2 id=\"Identity-Mappings-in-Deep-Residual-Networks\"><a href=\"#Identity-Mappings-in-Deep-Residual-Networks\" class=\"headerlink\" title=\"Identity Mappings in Deep Residual Networks\"></a>Identity Mappings in Deep Residual Networks</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-2\"><a href=\"#Introduction-2\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>ResNetCVKaiming HeResNetshortcutDCNNshortcutworkResNetECCV16Paperidentical mappingworkidentical mappingpre-activation</p>\n<h3 id=\"Delve-Into-ResNet-and-Identical-Mapping\"><a href=\"#Delve-Into-ResNet-and-Identical-Mapping\" class=\"headerlink\" title=\"Delve Into ResNet and Identical Mapping\"></a>Delve Into ResNet and Identical Mapping</h3><p>ResNet<br>$$<br>y_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1} = f(y_l)<br>$$<br>$\\mathcal{F}$residual function$h(x_l)=x_l$identical mapping$f$ReLU</p>\n<p><strong>$h(x_l)$$f(y_l)$identical mappingunitunitsforwardbackward</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg\" alt=\"Proposed Residual Unit\"></p>\n<p>identical mapping $f(y_l)=y_l$activation function (ReLU and BN)weight layers<code>pre-activation</code></p>\n<h3 id=\"Analysis-of-Deep-Residual-Networks\"><a href=\"#Analysis-of-Deep-Residual-Networks\" class=\"headerlink\" title=\"Analysis of Deep Residual Networks\"></a>Analysis of Deep Residual Networks</h3><p>CVPR15 Best PaperResNet Unit<br>$$<br>y_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>$f$identical mapping: $x_{l+1}\\equiv y_l$<br>$$<br>x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$<br>Recursively<br>$$<br>x_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})<br>$$<br><br>$$<br>x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)<br>$$<br>deep$L$shallow$l$$x_L$<strong>shallow unit feature $x_l$residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$</strong></p>\n<ol>\n<li><strong>units $L$$l$ residual function</strong></li>\n<li>$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$proceeding residual functionssummation$x_0$</li>\n</ol>\n<p>BPchain rule(loss function$\\epsilon$)<br>$$<br>\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))<br>$$<br>$\\frac{\\partial \\epsilon}{\\partial x_l}$</p>\n<ol>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}$</li>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$weight layers</li>\n</ol>\n<h4 id=\"Discussions\"><a href=\"#Discussions\" class=\"headerlink\" title=\"Discussions\"></a>Discussions</h4><p>Paperidentical mappingscaling, gating, $1\\times 1$ convolutions, and dropout<strong>$1\\times 1$ conv</strong>representation learning abilityResNetperformance droprepresentation ability</p>\n<blockquote>\n<p>The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations<br>(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.</p>\n</blockquote>\n<p>BN + ReLUpre-activationperformance</p>\n<ol>\n<li>$f$identical mappingoptimization</li>\n<li>BNpre-activationregularizationBNregularizationCVPR15ResNetBN normalizeshortcutBN normalizesignalmergepre-activationweight layersinputnormalize</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Krizhevsky A, Sutskever I, Hinton G E. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a>[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>\n<li>Simonyan K, Zisserman A. <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a>[J]. arXiv preprint arXiv:1409.1556, 2014.</li>\n<li>Lin M, Chen Q, Yan S. <a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>[J]. arXiv preprint arXiv:1312.4400, 2013.</li>\n<li>He K, Zhang X, Ren S, Sun J. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep residual learning for image recognition</a>. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</li>\n<li>Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Chollet, Francois. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Xception: Deep Learning with Depthwise Separable Convolutions.</a> 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.</li>\n<li>Howard, Andrew G., et al. <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">Mobilenets: Efficient convolutional neural networks for mobile vision applications.</a> arXiv preprint arXiv:1704.04861 (2017).</li>\n<li>Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks.</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Xie, Saining, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated residual transformations for deep neural networks.</a> Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.</li>\n<li>Huang, Gao, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks.</a> CVPR. Vol. 1. No. 2. 2017.</li>\n<li>He K, Zhang X, Ren S, et al. <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a>[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.</li>\n<li>Szegedy, Christian, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep LearningNetwork ArchitectureLoss Function and OptimizationOptimizationPaperNetwork ArchitectureLoss FunctionIan GoodfellowDeep LearningAlexNetCliqueNetwork</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620\" target=\"_blank\" rel=\"noopener\">@LucasX</a><a href=\"https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/\"></a></p>\n</blockquote>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a></p>\n</blockquote>\n<p>AlexNetDeep LearningLarge Scale Image Classification TaskAlexNetComputer Vision Researcherhand-crafted featuresAlexNetwork</p>\n<p>AlexNet5conv + 3FC + SoftmaxAlexNetReLUSigmoidnon-linearity transformationGPUData AugmentationDropout</p>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a></p>\n</blockquote>\n<p>VGGVGGAlexNetVGGFilter($3\\times 3$)<font color=\"red\">basic block</font>(GoogLeNetResNetResNeXtDenseNetblock)</p>\n<p>$3\\times 3$</p>\n<ol>\n<li>$3\\times 3$$5\\times 5$receptive field$3\\times 3$$7\\times 7$receptive field$7\\times 7$3$3\\times 3$<font color=\"red\">non-linearity transformationdiscriminative</font></li>\n<li>3channel$C$$3\\times 3$: $3(3^2C^2)=27C^2$channel$C$$7\\times 7$: $7^2C^2=49C^2$<blockquote>\n<p>This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"GoogLeNet\"><a href=\"#GoogLeNet\" class=\"headerlink\" title=\"GoogLeNet\"></a>GoogLeNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions</a></p>\n</blockquote>\n<p>DCNNCVhand-crafted featuresnetwork architectureGoogLeNet<strong>Multi-branch + Feature Concatenation</strong>GoogLeNet$1\\times 1$ conv ($1\\times 1$ conv<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)</p>\n<ul>\n<li>dimension reductionremove computational bottlenecks</li>\n<li>computational bottlenecksFLOPsdeeprepresentation learning</li>\n</ul>\n<p>Inception Module<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg\" alt=\"Inception Module\"></p>\n<blockquote>\n<p>$3\\times 3$$5\\times 5$ conv$1\\times 1$ conv<strong>dimension reduction</strong><strong>non-linearity transformation</strong>representation learning ability(<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a><a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)</p>\n</blockquote>\n<p>GoogLeNetInception Module(VGG/ResNet/ResNeXtblock)GoogLeNetMulti-branchclassification layersupervisiongradient flow(DeepID)</p>\n<h2 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep Residual Learning for Image Recognition</a></p>\n</blockquote>\n<p>ResNetAlexNetDeep LearninginsightfulideaResNetshortcutDeep Architecture(DenseNet, CliqueNet, Deep Layer Aggregation)<br>ResNetKaiming HePaperNetworkAccuracy<font color=\"red\">overfitting</font></p>\n<h3 id=\"What-is-Residual-Network\"><a href=\"#What-is-Residual-Network\" class=\"headerlink\" title=\"What is Residual Network?\"></a>What is Residual Network?</h3><p>DNNfunction $\\mathcal{H}(x)$DNN Architecturefunctionfunction $\\mathcal{F}(x):=\\mathcal{H}(x) - x$$\\mathcal{H}(x)$<font color=\"red\">$\\mathcal{F}(x)+x$</font>$\\mathcal{H}(x)$</p>\n<blockquote>\n<p>identity mappingpush0non-linearity transformationidentity mapping</p>\n</blockquote>\n<p>Shortcut<br>$$<br>y=\\mathcal{F}(x, \\{W_i\\}) + x<br>$$<br>$\\mathcal{F}(x, \\{W_i\\})$conv layersfeature mapchannel by channel element-wise </p>\n<p><a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">VGG</a>$3\\times 3$ filters</p>\n<ol>\n<li>feature map sizefilter</li>\n<li>feature map sizefiltertime complexity</li>\n</ol>\n<p>feature map dimensionelement-wise additionfeature map dimension doublezero paddingdimension$1\\times 1$ conv</p>\n<p>ResNetclassification/detection/segmentation taskstate-of-the-artResNetideaKaiming</p>\n<h2 id=\"ShuffleNet\"><a href=\"#ShuffleNet\" class=\"headerlink\" title=\"ShuffleNet\"></a>ShuffleNet</h2><p>Computer VisionAlexNetVGGGoogLeNetResNetDenseNetCliqueNet<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a></p>\n<blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p>\n</blockquote>\n<h3 id=\"What-is-ShuffleNet\"><a href=\"#What-is-ShuffleNet\" class=\"headerlink\" title=\"What is ShuffleNet?\"></a>What is ShuffleNet?</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><strong>Pointwise Group Convolution</strong><strong>Channel Shuffle</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg\" alt=\"Channel Shuffle\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg\" alt=\"ShuffleNet Unit\"></p>\n<blockquote>\n<p>Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.</p>\n</blockquote>\n<h4 id=\"Advantages-of-Point-Wise-Convolution\"><a href=\"#Advantages-of-Point-Wise-Convolution\" class=\"headerlink\" title=\"Advantages of Point-Wise Convolution\"></a>Advantages of Point-Wise Convolution</h4><p>Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.</p>\n<h4 id=\"Channel-Shuffle-vs-No-Shuffle\"><a href=\"#Channel-Shuffle-vs-No-Shuffle\" class=\"headerlink\" title=\"Channel Shuffle vs. No Shuffle\"></a>Channel Shuffle vs. No Shuffle</h4><p>The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.</p>\n<h2 id=\"MobileNet-V1\"><a href=\"#MobileNet-V1\" class=\"headerlink\" title=\"MobileNet V1\"></a>MobileNet V1</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>\n</blockquote>\n<p>CNNResNetInceptionVGGFLOP<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a><a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a></p>\n<h3 id=\"Depth-wise-Separable-Convolution\"><a href=\"#Depth-wise-Separable-Convolution\" class=\"headerlink\" title=\"Depth-wise Separable Convolution\"></a>Depth-wise Separable Convolution</h3><p>MobileNet<strong>Depth-wise Separable Convolution</strong>DW Convmodel size:<br>$D_F\\times D_F\\times M$feature map$D_F\\times D_F\\times N$feature mapsize$D_K\\times D_K\\times M\\times N$$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$Computational Costinput channel $M$output channel $N$$D_K\\times D_K$feature map$D_F\\times D_F$</p>\n<p>MobileNetDepth-wise ConvKernel SizeOutput ChannelConv Operationfiltersfeaturesfilter(Combinations)representationsFilteringCombinationsDW Separable Conv</p>\n<p><strong>Depth-wise Separable ConvolutionDepth-wise Conv + Point-wise Conv</strong>DW ConvchannelfilterPW Conv (a simple $1\\times 1$ Conv)DW layerlinear combination<strong>DW ConvComputational Cost: $D_K\\times D_K\\times M\\times D_F\\times D_F$</strong>Conv$N$</p>\n<p>DW Convfilterinput channel<strong>combination</strong>layerDW Convfeaturelinear combinationrepresentation$1\\times 1$ ConvPW Conv</p>\n<p>The combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.</p>\n<p>DW Separable ConvComputational Cost<br>$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$DW ConvcostPW Convcost</p>\n<p>By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:<br>$$<br>\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}<br>$$<br>Depth-wise Separable ConvConv$\\frac{1}{N} + \\frac{1}{D_K^2}$<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png\" alt=\"DW Separable Conv\"></p>\n<h3 id=\"Width-Multiplier-Thinner-Models\"><a href=\"#Width-Multiplier-Thinner-Models\" class=\"headerlink\" title=\"Width Multiplier: Thinner Models\"></a>Width Multiplier: Thinner Models</h3><p>In order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier  is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$</p>\n<p>where $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha &lt; 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.</p>\n<h3 id=\"Resolution-Multiplier-Reduced-Representation\"><a href=\"#Resolution-Multiplier-Reduced-Representation\" class=\"headerlink\" title=\"Resolution Multiplier: Reduced Representation\"></a>Resolution Multiplier: Reduced Representation</h3><p>The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$</p>\n<p>where $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and  &lt; 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.</p>\n<h2 id=\"MobileNet-V2\"><a href=\"#MobileNet-V2\" class=\"headerlink\" title=\"MobileNet V2\"></a>MobileNet V2</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>\n</blockquote>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V2</a><a href=\"http://openaccess.thecvf.com/CVPR2018.py\" target=\"_blank\" rel=\"noopener\">CVPR2018</a>PaperMobileNet V2contribution: </p>\n<blockquote>\n<p>Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.</p>\n</blockquote>\n<h3 id=\"Preliminaries-discussion-and-intuition\"><a href=\"#Preliminaries-discussion-and-intuition\" class=\"headerlink\" title=\"Preliminaries, discussion and intuition\"></a>Preliminaries, discussion and intuition</h3><h4 id=\"Depthwise-Separable-Convolutions\"><a href=\"#Depthwise-Separable-Convolutions\" class=\"headerlink\" title=\"Depthwise Separable Convolutions\"></a>Depthwise Separable Convolutions</h4><p>Light-weighted ArchitectureDW ConvMobileNet V2DW Conv</p>\n<blockquote>\n<p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.</p>\n</blockquote>\n<p>Conv$h_i\\times w_i\\times d_i$$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$$h_i\\times w_i\\times d_j$ConvComputational Cost$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$DW Separable ConvComputational Cost$h_i\\times w_i\\times d_i(k^2+d_j)$<strong>$k^2$</strong></p>\n<h4 id=\"Linear-Bottlenecks\"><a href=\"#Linear-Bottlenecks\" class=\"headerlink\" title=\"Linear Bottlenecks\"></a>Linear Bottlenecks</h4><blockquote>\n<p>It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.</p>\n</blockquote>\n<p>DCNNConv + ReLU + (Pool) + (FC) + SoftmaxDNNnon-linearity transformationgradient vanishing/explodingSigmoidReLUReLU</p>\n<blockquote>\n<p>It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. <strong>In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.</strong></p>\n</blockquote>\n<blockquote>\n<p>On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.</p>\n</blockquote>\n<p>ReLu</p>\n<ol>\n<li>Manifold of InterestReLU</li>\n<li>ReLUinput manifold<strong>input manifoldinput space</strong></li>\n</ol>\n<p>MobileNet V2high-dimensional hidden representationslow-dimensional embeddinghigh-dimensional</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png\" alt=\"Evolution of Separable Conv\"></p>\n<h4 id=\"Inverted-residuals\"><a href=\"#Inverted-residuals\" class=\"headerlink\" title=\"Inverted residuals\"></a>Inverted residuals</h4><blockquote>\n<p>The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V1</a>feedforwad networkshortcut<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>effectiveMobileNet V2skip connection</p>\n<h4 id=\"MobileNet-V2-Architecture\"><a href=\"#MobileNet-V2-Architecture\" class=\"headerlink\" title=\"MobileNet V2 Architecture\"></a>MobileNet V2 Architecture</h4><blockquote>\n<p>We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png\" alt=\"DCNN Architecture Comparison\"></p>\n<h2 id=\"ResNeXt\"><a href=\"#ResNeXt\" class=\"headerlink\" title=\"ResNeXt\"></a>ResNeXt</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated Residual Transformations for Deep Neural Networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>AlexNetDeep Learning(VGGInceptionResNet)ResNeXtResNetInception<strong>split-transform-merge</strong></p>\n<h3 id=\"The-Core-of-ResNeXt\"><a href=\"#The-Core-of-ResNeXt\" class=\"headerlink\" title=\"The Core of ResNeXt\"></a>The Core of ResNeXt</h3><blockquote>\n<p>Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 11 convolutions), transformed by a set of specialized filters (33, 55, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 55) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.</p>\n</blockquote>\n<p>Inception<strong>split-transform-merge</strong>(VGGResNetblock stacking)ImageNetdatasetResNeXtVGG/ResNetstacking blockInceptionsplit-transform-mergeResNeXtideacardinality (the size of the set of transformations)performancewidthdepth</p>\n<blockquote>\n<p>Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view<br>our method as ensembling, because the members to be aggregated<br>are trained jointly, not independently.</p>\n</blockquote>\n<blockquote>\n<p>The above operation can be recast as a combination of<br>splitting, transforming, and aggregating. </p>\n<ol>\n<li>Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. </li>\n<li>Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.</li>\n<li>Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.</li>\n</ol>\n</blockquote>\n<p>$W$function mapping: $\\mathcal{T}(x)$aggregated transformations:<br>$$<br>\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$<br>$\\mathcal{T}_i$$x$$C$transformationsize<strong>cardinality</strong></p>\n<blockquote>\n<p>In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg\" alt=\"ResNeXt Block\"></p>\n<p>ResNetidentical mapping<br>$$<br>y=x+\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$</p>\n<blockquote>\n<p><strong>Relation to Grouped Convolutions</strong>. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg\" alt=\"Equivalent Building Blocks of ResNeXt\"></p>\n<h2 id=\"DenseNet\"><a href=\"#DenseNet\" class=\"headerlink\" title=\"DenseNet\"></a>DenseNet</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n</blockquote>\n<h3 id=\"What-is-DenseNet\"><a href=\"#What-is-DenseNet\" class=\"headerlink\" title=\"What is DenseNet?\"></a>What is DenseNet?</h3><p>DenseNetCVPR2017 Best PaperResNet<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\"></a>ResNetgradient vanishingResNetidentical mappingDenseNet</p>\n<p>DenseNetResNetshortcutsDCNNAlexNet/VGG/GoogLeNetfeedforward network<strong>BP</strong>gradient vanishingKaiming Heskip connection$i$identical mapping$i+t$</p>\n<blockquote>\n<p>: ResNet<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\"></a></p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg\" alt=\"Dense Block\"></p>\n<p>DenseNetskip connectionlayerDenseNetskip connectionResNetDW SummationDenseNetconcatenatefeatures (ResNetDW Summation)</p>\n<p><strong>DenseNetResNet</strong>dense connectionfeature maps<br>every layerlossearly layer<strong>deep supervision</strong>dense connectionsRegularizationoverfitting</p>\n<p></p>\n<ul>\n<li>gradient vanishing</li>\n<li></li>\n<li>feature reuse</li>\n<li></li>\n</ul>\n<h3 id=\"Delve-into-DenseNet\"><a href=\"#Delve-into-DenseNet\" class=\"headerlink\" title=\"Delve into DenseNet\"></a>Delve into DenseNet</h3><h4 id=\"Dense-connectivity\"><a href=\"#Dense-connectivity\" class=\"headerlink\" title=\"Dense connectivity\"></a>Dense connectivity</h4><p>To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the th layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:<br>$$<br>x_l=H_l([x_0,x_1,\\cdots,x_{l-1}])<br>$$<br>where $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l1$.</p>\n<h4 id=\"Pooling-layers\"><a href=\"#Pooling-layers\" class=\"headerlink\" title=\"Pooling layers\"></a>Pooling layers</h4><p>The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.</p>\n<h4 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h4><p>If each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l1)$ input<br>feature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and<br>existing network architectures is that DenseNet can have<br>very narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.</p>\n<p><strong>One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the networks collective knowledge. One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer</strong>.</p>\n<h4 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h4><p>Although each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.</p>\n<h2 id=\"Identity-Mappings-in-Deep-Residual-Networks\"><a href=\"#Identity-Mappings-in-Deep-Residual-Networks\" class=\"headerlink\" title=\"Identity Mappings in Deep Residual Networks\"></a>Identity Mappings in Deep Residual Networks</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-2\"><a href=\"#Introduction-2\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>ResNetCVKaiming HeResNetshortcutDCNNshortcutworkResNetECCV16Paperidentical mappingworkidentical mappingpre-activation</p>\n<h3 id=\"Delve-Into-ResNet-and-Identical-Mapping\"><a href=\"#Delve-Into-ResNet-and-Identical-Mapping\" class=\"headerlink\" title=\"Delve Into ResNet and Identical Mapping\"></a>Delve Into ResNet and Identical Mapping</h3><p>ResNet<br>$$<br>y_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1} = f(y_l)<br>$$<br>$\\mathcal{F}$residual function$h(x_l)=x_l$identical mapping$f$ReLU</p>\n<p><strong>$h(x_l)$$f(y_l)$identical mappingunitunitsforwardbackward</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg\" alt=\"Proposed Residual Unit\"></p>\n<p>identical mapping $f(y_l)=y_l$activation function (ReLU and BN)weight layers<code>pre-activation</code></p>\n<h3 id=\"Analysis-of-Deep-Residual-Networks\"><a href=\"#Analysis-of-Deep-Residual-Networks\" class=\"headerlink\" title=\"Analysis of Deep Residual Networks\"></a>Analysis of Deep Residual Networks</h3><p>CVPR15 Best PaperResNet Unit<br>$$<br>y_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>$f$identical mapping: $x_{l+1}\\equiv y_l$<br>$$<br>x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$<br>Recursively<br>$$<br>x_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})<br>$$<br><br>$$<br>x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)<br>$$<br>deep$L$shallow$l$$x_L$<strong>shallow unit feature $x_l$residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$</strong></p>\n<ol>\n<li><strong>units $L$$l$ residual function</strong></li>\n<li>$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$proceeding residual functionssummation$x_0$</li>\n</ol>\n<p>BPchain rule(loss function$\\epsilon$)<br>$$<br>\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))<br>$$<br>$\\frac{\\partial \\epsilon}{\\partial x_l}$</p>\n<ol>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}$</li>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$weight layers</li>\n</ol>\n<h4 id=\"Discussions\"><a href=\"#Discussions\" class=\"headerlink\" title=\"Discussions\"></a>Discussions</h4><p>Paperidentical mappingscaling, gating, $1\\times 1$ convolutions, and dropout<strong>$1\\times 1$ conv</strong>representation learning abilityResNetperformance droprepresentation ability</p>\n<blockquote>\n<p>The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations<br>(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.</p>\n</blockquote>\n<p>BN + ReLUpre-activationperformance</p>\n<ol>\n<li>$f$identical mappingoptimization</li>\n<li>BNpre-activationregularizationBNregularizationCVPR15ResNetBN normalizeshortcutBN normalizesignalmergepre-activationweight layersinputnormalize</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Krizhevsky A, Sutskever I, Hinton G E. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a>[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>\n<li>Simonyan K, Zisserman A. <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a>[J]. arXiv preprint arXiv:1409.1556, 2014.</li>\n<li>Lin M, Chen Q, Yan S. <a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>[J]. arXiv preprint arXiv:1312.4400, 2013.</li>\n<li>He K, Zhang X, Ren S, Sun J. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep residual learning for image recognition</a>. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</li>\n<li>Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Chollet, Francois. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Xception: Deep Learning with Depthwise Separable Convolutions.</a> 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.</li>\n<li>Howard, Andrew G., et al. <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">Mobilenets: Efficient convolutional neural networks for mobile vision applications.</a> arXiv preprint arXiv:1704.04861 (2017).</li>\n<li>Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks.</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Xie, Saining, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated residual transformations for deep neural networks.</a> Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.</li>\n<li>Huang, Gao, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks.</a> CVPR. Vol. 1. No. 2. 2017.</li>\n<li>He K, Zhang X, Ren S, et al. <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a>[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.</li>\n<li>Szegedy, Christian, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions.</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n</ol>\n"},{"title":"[DL] Batch Normalization","date":"2018-11-20T15:18:25.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n[Batch Normalization](http://proceedings.mlr.press/v37/ioffe15.pdf)ResNetskip connectioninsightfulideaBatchNorm\n\n> Paper: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n\nDeep Modelsfeedforward networkdistributionSigmoidnon-linearity transformationsaturation(sigmoid)\n\nBatchNormmini-batch samplesnormalizationinternal covariate shiftlearning rateoverfittingDropoutparam initialization\n\nDNNoptimization object\n$$\n\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)\n$$\n\nSGDsamplemini-batchmini-batchone-by-one\n* batchgradienttraining setestimationbatch sizeestimation\n* mini-batchgradientone-by-one sample\n\nSGDfeedforward networkinputDNN\n\nlearning systeminputsystem**covariate shift**\n\nBatchNorminputnormalizationinternal covariate shiftBatchNormgradientgradient flowDNNBNlearning ratesigmoid activation function (saturation)**BatchNorm**()overfittingDropout\n\n## Normalization via Mini-Batch Statistics\ninput$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$\n$E[x^{(k)}]$$Var[x^{(k)}]$training set\n\nnormalizationlayer**scale and shift the normalized value**\n$$\ny^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}\n$$\n$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$$\\beta^{(k)}=E[x^{(k)}]$\n\n![BatchNorm Transform](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg)\n\nnormalized activation $\\hat{x}^{(k)}$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$\n\nBN\n![Update of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg)\n\nBN\n![Training of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg)\n\n### BatchNorm in CNN\nBNCNNMLPCNNBatchNorm Transformchannel$x=Wu$channel$\\gamma^{(k)}$$\\beta^{(k)}$\n\n### Batch Normalization enables higher learning rates\nactivationsDNNBNparameters scalelearning rateDNNBPgradient explosiondivergenceBNBPparameters scale\n\n$$\nBN(Wu)=BN((aW)u)\n$$\nBP$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$larger weightssmaller gradients\n\n\n## Reference\n1. Ioffe S, Szegedy C. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)[C]//International Conference on Machine Learning. 2015: 448-456.","source":"_posts/dl-bn.md","raw":"---\ntitle: \"[DL] Batch Normalization\"\ndate: 2018-11-20 23:18:25\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n[Batch Normalization](http://proceedings.mlr.press/v37/ioffe15.pdf)ResNetskip connectioninsightfulideaBatchNorm\n\n> Paper: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n\nDeep Modelsfeedforward networkdistributionSigmoidnon-linearity transformationsaturation(sigmoid)\n\nBatchNormmini-batch samplesnormalizationinternal covariate shiftlearning rateoverfittingDropoutparam initialization\n\nDNNoptimization object\n$$\n\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)\n$$\n\nSGDsamplemini-batchmini-batchone-by-one\n* batchgradienttraining setestimationbatch sizeestimation\n* mini-batchgradientone-by-one sample\n\nSGDfeedforward networkinputDNN\n\nlearning systeminputsystem**covariate shift**\n\nBatchNorminputnormalizationinternal covariate shiftBatchNormgradientgradient flowDNNBNlearning ratesigmoid activation function (saturation)**BatchNorm**()overfittingDropout\n\n## Normalization via Mini-Batch Statistics\ninput$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$\n$E[x^{(k)}]$$Var[x^{(k)}]$training set\n\nnormalizationlayer**scale and shift the normalized value**\n$$\ny^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}\n$$\n$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$$\\beta^{(k)}=E[x^{(k)}]$\n\n![BatchNorm Transform](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg)\n\nnormalized activation $\\hat{x}^{(k)}$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$\n\nBN\n![Update of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg)\n\nBN\n![Training of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg)\n\n### BatchNorm in CNN\nBNCNNMLPCNNBatchNorm Transformchannel$x=Wu$channel$\\gamma^{(k)}$$\\beta^{(k)}$\n\n### Batch Normalization enables higher learning rates\nactivationsDNNBNparameters scalelearning rateDNNBPgradient explosiondivergenceBNBPparameters scale\n\n$$\nBN(Wu)=BN((aW)u)\n$$\nBP$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$larger weightssmaller gradients\n\n\n## Reference\n1. Ioffe S, Szegedy C. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)[C]//International Conference on Machine Learning. 2015: 448-456.","slug":"dl-bn","published":1,"updated":"2018-11-20T16:17:05.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c1000f608wrmkhhxc2","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p><a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization</a>ResNetskip connectioninsightfulideaBatchNorm</p>\n<blockquote>\n<p>Paper: <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>\n</blockquote>\n<p>Deep Modelsfeedforward networkdistributionSigmoidnon-linearity transformationsaturation(sigmoid)</p>\n<p>BatchNormmini-batch samplesnormalizationinternal covariate shiftlearning rateoverfittingDropoutparam initialization</p>\n<p>DNNoptimization object<br>$$<br>\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)<br>$$</p>\n<p>SGDsamplemini-batchmini-batchone-by-one</p>\n<ul>\n<li>batchgradienttraining setestimationbatch sizeestimation</li>\n<li>mini-batchgradientone-by-one sample</li>\n</ul>\n<p>SGDfeedforward networkinputDNN</p>\n<p>learning systeminputsystem<strong>covariate shift</strong></p>\n<p>BatchNorminputnormalizationinternal covariate shiftBatchNormgradientgradient flowDNNBNlearning ratesigmoid activation function (saturation)<strong>BatchNorm</strong>()overfittingDropout</p>\n<h2 id=\"Normalization-via-Mini-Batch-Statistics\"><a href=\"#Normalization-via-Mini-Batch-Statistics\" class=\"headerlink\" title=\"Normalization via Mini-Batch Statistics\"></a>Normalization via Mini-Batch Statistics</h2><p>input$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$<br>$E[x^{(k)}]$$Var[x^{(k)}]$training set</p>\n<p>normalizationlayer<strong>scale and shift the normalized value</strong><br>$$<br>y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}<br>$$<br>$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$$\\beta^{(k)}=E[x^{(k)}]$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg\" alt=\"BatchNorm Transform\"></p>\n<p>normalized activation $\\hat{x}^{(k)}$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$</p>\n<p>BN<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg\" alt=\"Update of BN Layer\"></p>\n<p>BN<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg\" alt=\"Training of BN Layer\"></p>\n<h3 id=\"BatchNorm-in-CNN\"><a href=\"#BatchNorm-in-CNN\" class=\"headerlink\" title=\"BatchNorm in CNN\"></a>BatchNorm in CNN</h3><p>BNCNNMLPCNNBatchNorm Transformchannel$x=Wu$channel$\\gamma^{(k)}$$\\beta^{(k)}$</p>\n<h3 id=\"Batch-Normalization-enables-higher-learning-rates\"><a href=\"#Batch-Normalization-enables-higher-learning-rates\" class=\"headerlink\" title=\"Batch Normalization enables higher learning rates\"></a>Batch Normalization enables higher learning rates</h3><p>activationsDNNBNparameters scalelearning rateDNNBPgradient explosiondivergenceBNBPparameters scale<br><br>$$<br>BN(Wu)=BN((aW)u)<br>$$<br>BP$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$larger weightssmaller gradients</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Ioffe S, Szegedy C. <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>[C]//International Conference on Machine Learning. 2015: 448-456.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p><a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization</a>ResNetskip connectioninsightfulideaBatchNorm</p>\n<blockquote>\n<p>Paper: <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>\n</blockquote>\n<p>Deep Modelsfeedforward networkdistributionSigmoidnon-linearity transformationsaturation(sigmoid)</p>\n<p>BatchNormmini-batch samplesnormalizationinternal covariate shiftlearning rateoverfittingDropoutparam initialization</p>\n<p>DNNoptimization object<br>$$<br>\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)<br>$$</p>\n<p>SGDsamplemini-batchmini-batchone-by-one</p>\n<ul>\n<li>batchgradienttraining setestimationbatch sizeestimation</li>\n<li>mini-batchgradientone-by-one sample</li>\n</ul>\n<p>SGDfeedforward networkinputDNN</p>\n<p>learning systeminputsystem<strong>covariate shift</strong></p>\n<p>BatchNorminputnormalizationinternal covariate shiftBatchNormgradientgradient flowDNNBNlearning ratesigmoid activation function (saturation)<strong>BatchNorm</strong>()overfittingDropout</p>\n<h2 id=\"Normalization-via-Mini-Batch-Statistics\"><a href=\"#Normalization-via-Mini-Batch-Statistics\" class=\"headerlink\" title=\"Normalization via Mini-Batch Statistics\"></a>Normalization via Mini-Batch Statistics</h2><p>input$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$<br>$E[x^{(k)}]$$Var[x^{(k)}]$training set</p>\n<p>normalizationlayer<strong>scale and shift the normalized value</strong><br>$$<br>y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}<br>$$<br>$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$$\\beta^{(k)}=E[x^{(k)}]$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg\" alt=\"BatchNorm Transform\"></p>\n<p>normalized activation $\\hat{x}^{(k)}$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$</p>\n<p>BN<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg\" alt=\"Update of BN Layer\"></p>\n<p>BN<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg\" alt=\"Training of BN Layer\"></p>\n<h3 id=\"BatchNorm-in-CNN\"><a href=\"#BatchNorm-in-CNN\" class=\"headerlink\" title=\"BatchNorm in CNN\"></a>BatchNorm in CNN</h3><p>BNCNNMLPCNNBatchNorm Transformchannel$x=Wu$channel$\\gamma^{(k)}$$\\beta^{(k)}$</p>\n<h3 id=\"Batch-Normalization-enables-higher-learning-rates\"><a href=\"#Batch-Normalization-enables-higher-learning-rates\" class=\"headerlink\" title=\"Batch Normalization enables higher learning rates\"></a>Batch Normalization enables higher learning rates</h3><p>activationsDNNBNparameters scalelearning rateDNNBPgradient explosiondivergenceBNBPparameters scale<br><br>$$<br>BN(Wu)=BN((aW)u)<br>$$<br>BP$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$larger weightssmaller gradients</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Ioffe S, Szegedy C. <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>[C]//International Conference on Machine Learning. 2015: 448-456.</li>\n</ol>\n"},{"title":"[DL] BackPropogation","date":"2018-07-25T06:33:25.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization"],"_content":"## Introduction\nTensorflowcomputational graphStanford CS231n Spring,2017  \ncomputational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)  \n![Fig. 1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png)\n\n$q=x+y$$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ \n\n$f=qz$$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$\n\n![Fig. 2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png)\n\n$\\frac{\\partial f}{\\partial f}=1$\n\n![Fig. 3](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png)\n\n\n\n$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$\n\n$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$\n\n![Fig. 4.1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png)\n\n![Fig. 4.2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png)\n\n\n\ncomputational graph\n$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$\n\n$\\frac{\\partial f}{\\partial f}=1$$\\frac{1}{x}$(x)\n\n![Fig. 5](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png)\n\n![Fig. 6](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png)\n\n1+x(x)\n\n![Fig. 7](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png)\n\n\n\n![Fig. 8](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png)\n\n![Fig. 9](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png)\n\n$-(w_0 x_0+w_1 x_1+w_2)$$e^{-1}$$-(w_0 x_0+w_1 x_1+w_2)=-1$\n\n$-x$($w_0 x_0+w_1 x_1+w_2$$x$)  \n\n![Fig. 10](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png)\n\n$w_0 x_0+w_1 x_1+w_2$(1)1$w_2$1$10.2=0.2$1$X=w_0 x_0+w_1 x_1$1$10.2=0.2$\n\n![Fig. 11](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png)\n\n$w_0 x_0+w_1 x_1$\n\n![Fig. 12](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n$w_0 x_0$$w_1 x_1$\n\n![Fig. 13](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\nSigmoidSigmoid$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$\n\n\n## Reference\n1. http://cs231n.stanford.edu/syllabus.html\n2. http://cs231n.github.io/optimization-2/\n3. http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\n4. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n5. [Derivation of Backpropagation in Convolutional Neural Network (CNN)](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf)\n6. [Backpropagation In Convolutional Neural Networks](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf)","source":"_posts/dl-bp.md","raw":"---\ntitle: \"[DL] BackPropogation\"\ndate: 2018-07-25 14:33:25\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n---\n## Introduction\nTensorflowcomputational graphStanford CS231n Spring,2017  \ncomputational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)  \n![Fig. 1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png)\n\n$q=x+y$$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ \n\n$f=qz$$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$\n\n![Fig. 2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png)\n\n$\\frac{\\partial f}{\\partial f}=1$\n\n![Fig. 3](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png)\n\n\n\n$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$\n\n$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$\n\n![Fig. 4.1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png)\n\n![Fig. 4.2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png)\n\n\n\ncomputational graph\n$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$\n\n$\\frac{\\partial f}{\\partial f}=1$$\\frac{1}{x}$(x)\n\n![Fig. 5](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png)\n\n![Fig. 6](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png)\n\n1+x(x)\n\n![Fig. 7](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png)\n\n\n\n![Fig. 8](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png)\n\n![Fig. 9](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png)\n\n$-(w_0 x_0+w_1 x_1+w_2)$$e^{-1}$$-(w_0 x_0+w_1 x_1+w_2)=-1$\n\n$-x$($w_0 x_0+w_1 x_1+w_2$$x$)  \n\n![Fig. 10](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png)\n\n$w_0 x_0+w_1 x_1+w_2$(1)1$w_2$1$10.2=0.2$1$X=w_0 x_0+w_1 x_1$1$10.2=0.2$\n\n![Fig. 11](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png)\n\n$w_0 x_0+w_1 x_1$\n\n![Fig. 12](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n$w_0 x_0$$w_1 x_1$\n\n![Fig. 13](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\nSigmoidSigmoid$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$\n\n\n## Reference\n1. http://cs231n.stanford.edu/syllabus.html\n2. http://cs231n.github.io/optimization-2/\n3. http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\n4. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n5. [Derivation of Backpropagation in Convolutional Neural Network (CNN)](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf)\n6. [Backpropagation In Convolutional Neural Networks](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf)","slug":"dl-bp","published":1,"updated":"2018-10-01T04:40:08.673Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c3000h608wvxqe83el","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Tensorflowcomputational graphStanford CS231n Spring,2017<br>computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png\" alt=\"Fig. 1\"></p>\n<p>$q=x+y$$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ </p>\n<p>$f=qz$$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png\" alt=\"Fig. 2\"></p>\n<p>$\\frac{\\partial f}{\\partial f}=1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png\" alt=\"Fig. 3\"></p>\n<p></p>\n<p>$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$</p>\n<p>$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png\" alt=\"Fig. 4.1\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png\" alt=\"Fig. 4.2\"></p>\n<p></p>\n<p>computational graph<br>$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$</p>\n<p>$\\frac{\\partial f}{\\partial f}=1$$\\frac{1}{x}$(x)</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png\" alt=\"Fig. 5\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png\" alt=\"Fig. 6\"></p>\n<p>1+x(x)</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png\" alt=\"Fig. 7\"></p>\n<p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png\" alt=\"Fig. 8\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png\" alt=\"Fig. 9\"></p>\n<p>$-(w_0 x_0+w_1 x_1+w_2)$$e^{-1}$$-(w_0 x_0+w_1 x_1+w_2)=-1$</p>\n<p>$-x$($w_0 x_0+w_1 x_1+w_2$$x$)  </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png\" alt=\"Fig. 10\"></p>\n<p>$w_0 x_0+w_1 x_1+w_2$(1)1$w_2$1$10.2=0.2$1$X=w_0 x_0+w_1 x_1$1$10.2=0.2$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png\" alt=\"Fig. 11\"></p>\n<p>$w_0 x_0+w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 12\"></p>\n<p>$w_0 x_0$$w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 13\"></p>\n<p>SigmoidSigmoid$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"http://cs231n.stanford.edu/syllabus.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/syllabus.html</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a></li>\n<li><a href=\"http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf</a></li>\n<li><a href=\"https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\" target=\"_blank\" rel=\"noopener\">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf\" target=\"_blank\" rel=\"noopener\">Derivation of Backpropagation in Convolutional Neural Network (CNN)</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf\" target=\"_blank\" rel=\"noopener\">Backpropagation In Convolutional Neural Networks</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Tensorflowcomputational graphStanford CS231n Spring,2017<br>computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png\" alt=\"Fig. 1\"></p>\n<p>$q=x+y$$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ </p>\n<p>$f=qz$$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png\" alt=\"Fig. 2\"></p>\n<p>$\\frac{\\partial f}{\\partial f}=1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png\" alt=\"Fig. 3\"></p>\n<p></p>\n<p>$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$</p>\n<p>$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png\" alt=\"Fig. 4.1\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png\" alt=\"Fig. 4.2\"></p>\n<p></p>\n<p>computational graph<br>$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$</p>\n<p>$\\frac{\\partial f}{\\partial f}=1$$\\frac{1}{x}$(x)</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png\" alt=\"Fig. 5\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png\" alt=\"Fig. 6\"></p>\n<p>1+x(x)</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png\" alt=\"Fig. 7\"></p>\n<p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png\" alt=\"Fig. 8\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png\" alt=\"Fig. 9\"></p>\n<p>$-(w_0 x_0+w_1 x_1+w_2)$$e^{-1}$$-(w_0 x_0+w_1 x_1+w_2)=-1$</p>\n<p>$-x$($w_0 x_0+w_1 x_1+w_2$$x$)  </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png\" alt=\"Fig. 10\"></p>\n<p>$w_0 x_0+w_1 x_1+w_2$(1)1$w_2$1$10.2=0.2$1$X=w_0 x_0+w_1 x_1$1$10.2=0.2$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png\" alt=\"Fig. 11\"></p>\n<p>$w_0 x_0+w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 12\"></p>\n<p>$w_0 x_0$$w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 13\"></p>\n<p>SigmoidSigmoid$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"http://cs231n.stanford.edu/syllabus.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/syllabus.html</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a></li>\n<li><a href=\"http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf</a></li>\n<li><a href=\"https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\" target=\"_blank\" rel=\"noopener\">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf\" target=\"_blank\" rel=\"noopener\">Derivation of Backpropagation in Convolutional Neural Network (CNN)</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf\" target=\"_blank\" rel=\"noopener\">Backpropagation In Convolutional Neural Networks</a></li>\n</ol>\n"},{"title":"[DL] CNN","date":"2018-07-27T08:14:38.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","CNN","Computer Vision"],"_content":"## Introduction\nVisionConvolutional Neural Networks (CNN)CNNLeNetResNetDenseNetCliqueNetCNNCNN\n\n## Convolution\n1. VALIDzero paddingfilter\n2. SAMEzero padding\n3. FULLzero padding$k$$m+k-1$\n\n\n## Pooling\nPoolingPooling ____Shift InvariantPoolingMaxPoolingPooling\n\n## Reference\n1. [Deep Learning--CNN](https://www.deeplearningbook.org/contents/convnets.html)","source":"_posts/dl-cnn.md","raw":"---\ntitle: \"[DL] CNN\"\ndate: 2018-07-27 16:14:38\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- CNN\n- Data Science\n- Computer Vision\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- CNN\n- Computer Vision\n---\n## Introduction\nVisionConvolutional Neural Networks (CNN)CNNLeNetResNetDenseNetCliqueNetCNNCNN\n\n## Convolution\n1. VALIDzero paddingfilter\n2. SAMEzero padding\n3. FULLzero padding$k$$m+k-1$\n\n\n## Pooling\nPoolingPooling ____Shift InvariantPoolingMaxPoolingPooling\n\n## Reference\n1. [Deep Learning--CNN](https://www.deeplearningbook.org/contents/convnets.html)","slug":"dl-cnn","published":1,"updated":"2018-10-01T04:40:08.836Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c5000k608wl1cri8at","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>VisionConvolutional Neural Networks (CNN)CNNLeNetResNetDenseNetCliqueNetCNNCNN</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><ol>\n<li>VALIDzero paddingfilter</li>\n<li>SAMEzero padding</li>\n<li>FULLzero padding$k$$m+k-1$</li>\n</ol>\n<h2 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h2><p>PoolingPooling <strong></strong>Shift InvariantPoolingMaxPoolingPooling</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.deeplearningbook.org/contents/convnets.html\" target=\"_blank\" rel=\"noopener\">Deep LearningCNN</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>VisionConvolutional Neural Networks (CNN)CNNLeNetResNetDenseNetCliqueNetCNNCNN</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><ol>\n<li>VALIDzero paddingfilter</li>\n<li>SAMEzero padding</li>\n<li>FULLzero padding$k$$m+k-1$</li>\n</ol>\n<h2 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h2><p>PoolingPooling <strong></strong>Shift InvariantPoolingMaxPoolingPooling</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.deeplearningbook.org/contents/convnets.html\" target=\"_blank\" rel=\"noopener\">Deep LearningCNN</a></li>\n</ol>\n"},{"title":"[DL] Data Augmentation","date":"2018-11-10T12:08:23.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Data Augmentation"],"_content":"## Introduction\nDeep Learning******GPU******Deep Learningoverfittingtraining setData Augmentation\n\n## Mixup\n> Paper: [mixup: Beyond Empirical Risk Minimization](https://openreview.net/pdf?id=r1Ddp1-Rb)\n\nMixupidea\n$$\n\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j\n$$\n\n$$\n\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j\n$$\n$x_i, x_j$raw input vectors$y_i, y_j$one-hot encodings\n\n> Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.\n\nMixupPyTorch\n```python\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam * x1 + (1. - lam) * x2)\n    y = Variable(lam * y1 + (1. - lam) * y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\n\n### What is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam's razor, since it is one of the simplest possible behaviors.\n\nmixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.\n\n\n## Reference\n1. Zhang, Hongyi, et al. [\"mixup: Beyond empirical risk minimization.\"](https://openreview.net/pdf?id=r1Ddp1-Rb) International Conference on Learning Representations (2018).","source":"_posts/dl-data-augmentation.md","raw":"---\ntitle: \"[DL] Data Augmentation\"\ndate: 2018-11-10 20:08:23\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Data Augmentation\ncatagories:\n- Machine Learning\n- Deep Learning\n- Data Augmentation\n---\n## Introduction\nDeep Learning******GPU******Deep Learningoverfittingtraining setData Augmentation\n\n## Mixup\n> Paper: [mixup: Beyond Empirical Risk Minimization](https://openreview.net/pdf?id=r1Ddp1-Rb)\n\nMixupidea\n$$\n\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j\n$$\n\n$$\n\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j\n$$\n$x_i, x_j$raw input vectors$y_i, y_j$one-hot encodings\n\n> Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.\n\nMixupPyTorch\n```python\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam * x1 + (1. - lam) * x2)\n    y = Variable(lam * y1 + (1. - lam) * y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\n\n### What is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam's razor, since it is one of the simplest possible behaviors.\n\nmixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.\n\n\n## Reference\n1. Zhang, Hongyi, et al. [\"mixup: Beyond empirical risk minimization.\"](https://openreview.net/pdf?id=r1Ddp1-Rb) International Conference on Learning Representations (2018).","slug":"dl-data-augmentation","published":1,"updated":"2018-11-10T15:03:11.194Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c7000m608wiwmlwpvc","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep Learning<strong></strong><strong>GPU</strong><strong></strong>Deep Learningoverfittingtraining setData Augmentation</p>\n<h2 id=\"Mixup\"><a href=\"#Mixup\" class=\"headerlink\" title=\"Mixup\"></a>Mixup</h2><blockquote>\n<p>Paper: <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond Empirical Risk Minimization</a></p>\n</blockquote>\n<p>Mixupidea<br>$$<br>\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j<br>$$</p>\n<p>$$<br>\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j<br>$$<br>$x_i, x_j$raw input vectors$y_i, y_j$one-hot encodings</p>\n<blockquote>\n<p>Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.</p>\n</blockquote>\n<p>MixupPyTorch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y1, y2 should be one-hot vectors</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (x1, y1), (x2, y2) <span class=\"keyword\">in</span> zip(loader1, loader2):</span><br><span class=\"line\">    lam = numpy.random.beta(alpha, alpha)</span><br><span class=\"line\">    x = Variable(lam * x1 + (<span class=\"number\">1.</span> - lam) * x2)</span><br><span class=\"line\">    y = Variable(lam * y1 + (<span class=\"number\">1.</span> - lam) * y2)</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss(net(x), y).backward()</span><br><span class=\"line\">    optimizer.step()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"What-is-mixup-doing\"><a href=\"#What-is-mixup-doing\" class=\"headerlink\" title=\"What is mixup doing?\"></a>What is mixup doing?</h3><p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occams razor, since it is one of the simplest possible behaviors.</p>\n<p>mixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Zhang, Hongyi, et al. <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond empirical risk minimization.</a> International Conference on Learning Representations (2018).</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep Learning<strong></strong><strong>GPU</strong><strong></strong>Deep Learningoverfittingtraining setData Augmentation</p>\n<h2 id=\"Mixup\"><a href=\"#Mixup\" class=\"headerlink\" title=\"Mixup\"></a>Mixup</h2><blockquote>\n<p>Paper: <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond Empirical Risk Minimization</a></p>\n</blockquote>\n<p>Mixupidea<br>$$<br>\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j<br>$$</p>\n<p>$$<br>\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j<br>$$<br>$x_i, x_j$raw input vectors$y_i, y_j$one-hot encodings</p>\n<blockquote>\n<p>Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.</p>\n</blockquote>\n<p>MixupPyTorch<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y1, y2 should be one-hot vectors</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (x1, y1), (x2, y2) <span class=\"keyword\">in</span> zip(loader1, loader2):</span><br><span class=\"line\">    lam = numpy.random.beta(alpha, alpha)</span><br><span class=\"line\">    x = Variable(lam * x1 + (<span class=\"number\">1.</span> - lam) * x2)</span><br><span class=\"line\">    y = Variable(lam * y1 + (<span class=\"number\">1.</span> - lam) * y2)</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss(net(x), y).backward()</span><br><span class=\"line\">    optimizer.step()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"What-is-mixup-doing\"><a href=\"#What-is-mixup-doing\" class=\"headerlink\" title=\"What is mixup doing?\"></a>What is mixup doing?</h3><p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occams razor, since it is one of the simplest possible behaviors.</p>\n<p>mixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Zhang, Hongyi, et al. <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond empirical risk minimization.</a> International Conference on Learning Representations (2018).</li>\n</ol>\n"},{"title":"[DL] Optimization Algorithm in Deep Learning","catalog":false,"date":"2018-07-20T03:46:37.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization"],"_content":"## \n()Deep Learning31) 2) Loss Function, 3) PaperNetworks Architecture + Loss FunctionSGD/Adam Optimizer\n\n> [Deep Learning](https://www.deeplearningbook.org/)\n\n## Background\n* Gradient Descent\"\"0\n* Gradient ClippingGradient DescentGradient Clipping\n* $W$$t$$W^t$$W=V diag(\\lambda)V^{-1}$\n$$\nW^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}\n$$\n$\\lambda_i$ $1$1Gradient Exploding$1$Gradient Vanishing\n\n## Basic Algorithm\n### SGD\nSGD$m$ mini-batch ()mini-batch\n\n![SGD](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg)\n\n### Momentum\nMomentumMomentumHessian\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\\n\\theta\\leftarrow \\theta + v\n$$\n\n![Momentum](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg)\nSGDMomentum $g$,$-g$ \n$$\n\\frac{\\epsilon||g||}{1-\\alpha}\n$$\nMomentum$\\frac{1}{1-\\alpha}$$\\alpha=0.9$ 10Gradient Descent\n\n### Nesterov\n\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\\n\\theta\\leftarrow \\theta+v\n$$\nNesterovMomentumNesterovMomentum\n\n![Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg)\n\n### AdaGrad\nLearning rate\n\nAdaGrad\n\n![AdaGrad](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg)\n\n### RMSProp\nRMSPropAdaGradAdaGradAdaGrad__RMSProp__\n\n* Standard RMSProp\n![RMSProp](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg)\n\n* RMSProp with Nesterov\n![RMSProp with Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg)\n\n### Adam\nAdam()RMSPropAdam()\n\n![Adam](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg)\n\n## \n### \n$\\theta_0$$J(\\theta)$\n$$\nJ(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)\n$$\n\n\n$$\n\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n__($H$)$H^{-1}$__\n\n![Newton's Method](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg)\n\nDeep LearningLoss Function()Newton's MethodHessian MatrixNewton's MethodHessian MatrixHessian Matrix$\\alpha$\n$$\n\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n\n### \nHessian Matrix\n\n### BFGS\nBFGS$M_t$$H^{-1}$Hessian$M_t$$\\rho_t$$\\rho_t=M_tg_t$$\\epsilon^{\\star}$\n$$\n\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t\n$$\nBFGSBFGSHessian $M$$O(n^2)$BFGSDeep Model\n\n## \n### Batch Normalization\n$H$mini batch$H$\n$$\nH^{'}=\\frac{H-\\mu}{\\sigma}\n$$\n\n$$\n\\mu=\\frac{1}{m}\\sum_i H_{i,:}\n$$\n\n$$\n\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}\n$$\n$\\delta$$\\sqrt{z}$$z=0$\n\n$\\mu$$\\sigma$$H$$h_i$BatchNorm\n\n$\\mu$$\\sigma$mini-batch$\\mu$$\\sigma$","source":"_posts/dl-optimization.md","raw":"---\ntitle: \"[DL] Optimization Algorithm in Deep Learning\"\ncatalog: false\ndate: 2018-07-20 11:46:37\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n---\n## \n()Deep Learning31) 2) Loss Function, 3) PaperNetworks Architecture + Loss FunctionSGD/Adam Optimizer\n\n> [Deep Learning](https://www.deeplearningbook.org/)\n\n## Background\n* Gradient Descent\"\"0\n* Gradient ClippingGradient DescentGradient Clipping\n* $W$$t$$W^t$$W=V diag(\\lambda)V^{-1}$\n$$\nW^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}\n$$\n$\\lambda_i$ $1$1Gradient Exploding$1$Gradient Vanishing\n\n## Basic Algorithm\n### SGD\nSGD$m$ mini-batch ()mini-batch\n\n![SGD](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg)\n\n### Momentum\nMomentumMomentumHessian\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\\n\\theta\\leftarrow \\theta + v\n$$\n\n![Momentum](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg)\nSGDMomentum $g$,$-g$ \n$$\n\\frac{\\epsilon||g||}{1-\\alpha}\n$$\nMomentum$\\frac{1}{1-\\alpha}$$\\alpha=0.9$ 10Gradient Descent\n\n### Nesterov\n\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\\n\\theta\\leftarrow \\theta+v\n$$\nNesterovMomentumNesterovMomentum\n\n![Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg)\n\n### AdaGrad\nLearning rate\n\nAdaGrad\n\n![AdaGrad](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg)\n\n### RMSProp\nRMSPropAdaGradAdaGradAdaGrad__RMSProp__\n\n* Standard RMSProp\n![RMSProp](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg)\n\n* RMSProp with Nesterov\n![RMSProp with Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg)\n\n### Adam\nAdam()RMSPropAdam()\n\n![Adam](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg)\n\n## \n### \n$\\theta_0$$J(\\theta)$\n$$\nJ(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)\n$$\n\n\n$$\n\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n__($H$)$H^{-1}$__\n\n![Newton's Method](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg)\n\nDeep LearningLoss Function()Newton's MethodHessian MatrixNewton's MethodHessian MatrixHessian Matrix$\\alpha$\n$$\n\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n\n### \nHessian Matrix\n\n### BFGS\nBFGS$M_t$$H^{-1}$Hessian$M_t$$\\rho_t$$\\rho_t=M_tg_t$$\\epsilon^{\\star}$\n$$\n\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t\n$$\nBFGSBFGSHessian $M$$O(n^2)$BFGSDeep Model\n\n## \n### Batch Normalization\n$H$mini batch$H$\n$$\nH^{'}=\\frac{H-\\mu}{\\sigma}\n$$\n\n$$\n\\mu=\\frac{1}{m}\\sum_i H_{i,:}\n$$\n\n$$\n\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}\n$$\n$\\delta$$\\sqrt{z}$$z=0$\n\n$\\mu$$\\sigma$$H$$h_i$BatchNorm\n\n$\\mu$$\\sigma$mini-batch$\\mu$$\\sigma$","slug":"dl-optimization","published":1,"updated":"2018-10-01T04:40:08.837Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c9000n608w4okd7r5h","content":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>()Deep Learning31) 2) Loss Function, 3) PaperNetworks Architecture + Loss FunctionSGD/Adam Optimizer</p>\n<blockquote>\n<p><a href=\"https://www.deeplearningbook.org/\" target=\"_blank\" rel=\"noopener\">Deep Learning</a></p>\n</blockquote>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>Gradient Descent0</li>\n<li>Gradient ClippingGradient DescentGradient Clipping</li>\n<li>$W$$t$$W^t$$W=V diag(\\lambda)V^{-1}$<br>$$<br>W^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}<br>$$<br>$\\lambda_i$ $1$1Gradient Exploding$1$Gradient Vanishing</li>\n</ul>\n<h2 id=\"Basic-Algorithm\"><a href=\"#Basic-Algorithm\" class=\"headerlink\" title=\"Basic Algorithm\"></a>Basic Algorithm</h2><h3 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h3><p>SGD$m$ mini-batch ()mini-batch</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg\" alt=\"SGD\"></p>\n<h3 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h3><p>MomentumMomentumHessian<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta + v<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg\" alt=\"Momentum\"><br>SGDMomentum $g$,$-g$ <br>$$<br>\\frac{\\epsilon||g||}{1-\\alpha}<br>$$<br>Momentum$\\frac{1}{1-\\alpha}$$\\alpha=0.9$ 10Gradient Descent</p>\n<h3 id=\"Nesterov\"><a href=\"#Nesterov\" class=\"headerlink\" title=\"Nesterov\"></a>Nesterov</h3><p><br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta+v<br>$$<br>NesterovMomentumNesterovMomentum</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg\" alt=\"Nesterov\"></p>\n<h3 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h3><p>Learning rate</p>\n<p>AdaGrad</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg\" alt=\"AdaGrad\"></p>\n<h3 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h3><p>RMSPropAdaGradAdaGradAdaGrad<strong>RMSProp</strong></p>\n<ul>\n<li><p>Standard RMSProp<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg\" alt=\"RMSProp\"></p>\n</li>\n<li><p>RMSProp with Nesterov<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg\" alt=\"RMSProp with Nesterov\"></p>\n</li>\n</ul>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>Adam()RMSPropAdam()</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg\" alt=\"Adam\"></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$\\theta_0$$J(\\theta)$<br>$$<br>J(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)<br>$$</p>\n<p><br>$$<br>\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$<br><strong>($H$)$H^{-1}$</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg\" alt=\"Newton&#39;s Method\"></p>\n<p>Deep LearningLoss Function()Newtons MethodHessian MatrixNewtons MethodHessian MatrixHessian Matrix$\\alpha$<br>$$<br>\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Hessian Matrix</p>\n<h3 id=\"BFGS\"><a href=\"#BFGS\" class=\"headerlink\" title=\"BFGS\"></a>BFGS</h3><p>BFGS$M_t$$H^{-1}$Hessian$M_t$$\\rho_t$$\\rho_t=M_tg_t$$\\epsilon^{\\star}$<br>$$<br>\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t<br>$$<br>BFGSBFGSHessian $M$$O(n^2)$BFGSDeep Model</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><p>$H$mini batch$H$<br>$$<br>H^{}=\\frac{H-\\mu}{\\sigma}<br>$$</p>\n<p>$$<br>\\mu=\\frac{1}{m}\\sum_i H_{i,:}<br>$$</p>\n<p>$$<br>\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}<br>$$<br>$\\delta$$\\sqrt{z}$$z=0$</p>\n<p>$\\mu$$\\sigma$$H$$h_i$BatchNorm</p>\n<p>$\\mu$$\\sigma$mini-batch$\\mu$$\\sigma$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>()Deep Learning31) 2) Loss Function, 3) PaperNetworks Architecture + Loss FunctionSGD/Adam Optimizer</p>\n<blockquote>\n<p><a href=\"https://www.deeplearningbook.org/\" target=\"_blank\" rel=\"noopener\">Deep Learning</a></p>\n</blockquote>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>Gradient Descent0</li>\n<li>Gradient ClippingGradient DescentGradient Clipping</li>\n<li>$W$$t$$W^t$$W=V diag(\\lambda)V^{-1}$<br>$$<br>W^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}<br>$$<br>$\\lambda_i$ $1$1Gradient Exploding$1$Gradient Vanishing</li>\n</ul>\n<h2 id=\"Basic-Algorithm\"><a href=\"#Basic-Algorithm\" class=\"headerlink\" title=\"Basic Algorithm\"></a>Basic Algorithm</h2><h3 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h3><p>SGD$m$ mini-batch ()mini-batch</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg\" alt=\"SGD\"></p>\n<h3 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h3><p>MomentumMomentumHessian<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta + v<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg\" alt=\"Momentum\"><br>SGDMomentum $g$,$-g$ <br>$$<br>\\frac{\\epsilon||g||}{1-\\alpha}<br>$$<br>Momentum$\\frac{1}{1-\\alpha}$$\\alpha=0.9$ 10Gradient Descent</p>\n<h3 id=\"Nesterov\"><a href=\"#Nesterov\" class=\"headerlink\" title=\"Nesterov\"></a>Nesterov</h3><p><br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta+v<br>$$<br>NesterovMomentumNesterovMomentum</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg\" alt=\"Nesterov\"></p>\n<h3 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h3><p>Learning rate</p>\n<p>AdaGrad</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg\" alt=\"AdaGrad\"></p>\n<h3 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h3><p>RMSPropAdaGradAdaGradAdaGrad<strong>RMSProp</strong></p>\n<ul>\n<li><p>Standard RMSProp<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg\" alt=\"RMSProp\"></p>\n</li>\n<li><p>RMSProp with Nesterov<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg\" alt=\"RMSProp with Nesterov\"></p>\n</li>\n</ul>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>Adam()RMSPropAdam()</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg\" alt=\"Adam\"></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>$\\theta_0$$J(\\theta)$<br>$$<br>J(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)<br>$$</p>\n<p><br>$$<br>\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$<br><strong>($H$)$H^{-1}$</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg\" alt=\"Newton&#39;s Method\"></p>\n<p>Deep LearningLoss Function()Newtons MethodHessian MatrixNewtons MethodHessian MatrixHessian Matrix$\\alpha$<br>$$<br>\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Hessian Matrix</p>\n<h3 id=\"BFGS\"><a href=\"#BFGS\" class=\"headerlink\" title=\"BFGS\"></a>BFGS</h3><p>BFGS$M_t$$H^{-1}$Hessian$M_t$$\\rho_t$$\\rho_t=M_tg_t$$\\epsilon^{\\star}$<br>$$<br>\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t<br>$$<br>BFGSBFGSHessian $M$$O(n^2)$BFGSDeep Model</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><p>$H$mini batch$H$<br>$$<br>H^{}=\\frac{H-\\mu}{\\sigma}<br>$$</p>\n<p>$$<br>\\mu=\\frac{1}{m}\\sum_i H_{i,:}<br>$$</p>\n<p>$$<br>\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}<br>$$<br>$\\delta$$\\sqrt{z}$$z=0$</p>\n<p>$\\mu$$\\sigma$$H$$h_i$BatchNorm</p>\n<p>$\\mu$$\\sigma$mini-batch$\\mu$$\\sigma$</p>\n"},{"title":"[DL] Regularization","date":"2018-08-06T09:13:07.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization","Regularization"],"_content":"## Introduction\nRegularizationMachine LearningOverfittingDNNoverfittingsmall modelmodel __ + __ Deep Learning\n\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)\n$$\n\n## \nweightbiasbiasweightweightbias\n\nDNNNetwork$\\alpha$weight decay\n\n### $L_2$ Regularization\nDNNbias$\\theta$$w$Loss Function\n$$\n\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)\n$$\n\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)\n$$\nSGD\n$$\nw\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))\n$$\n\n$$\nw\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)\n$$\nweight decaySGD __()__\n\nLinear RegressionCost FunctionMSE:\n$$\n(Xw-y)^T(Xw-y)\n$$\n$L_2$ RegularizationCost Function:\n$$\n(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw\n$$\n $w=(X^TX)^{-1}X^Ty$  $w=(X^TX+\\alpha I)^{-1}X^Ty$$X^TX$$\\frac{1}{m}X^TX$$L_2$ Regularization$(X^TX+\\alpha I)^{-1}$$\\alpha$$L_2$ Regularization$x$()\n\n### $L_1$ Regularization\nDNNbias$\\theta$$w$Loss Function\n$$\n\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)\n$$\n:\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)\n$$\n__$w_i$$sign(w_i)$$J(X,y;w)$($L_2$)__\n\n$L_1$ Regularization Cost Function\n$$\n\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]\n$$\n($i$)Cost Function\n$$\nw_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}\n$$\n \n1) $w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$$w_i$0$i$$J(w;X,y)$$\\hat{J}(w;X,y)$$L_1$ Regularization$w_i$0\n2) $w_i^{\\star}> \\frac{\\alpha}{H_{i,i}}$$w_i$0$\\frac{\\alpha}{H_{i,i}}$\n\n$L_2$ Regularization$L_1$ Regularization(0)\n\n## \n$$\n\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)\n$$\n$\\Omega$$L_2$$L_2$$\\Omega$$L_1$$L_1$\n\n## Data Augmentation\nNNData AugmentationNNrobustNN robustnessUnsupervised Learning AlgorithmDenoise Auto Encoderhidden layerData Augmentation\n\n## Robustness of Noise\n____hidden units\n\n## Multi-Task Learning\nMTL(____)____\n\n## Early Stopping\nvalidation set errorvalidation set(test set error)validation setvalidation set errorEarly Stopping\n\nweight decayweight decay____\n\nEarly Stoppingvalidation settraining samplesEarly Stoppingtraining data\n* Early Stopping __Epoch__\n* ____validation setvalidation setlossEarly Stopping\n\n![Early Stopping](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg)\n\n### Early StoppingRegularization\nBishop __Early Stopping$\\theta_0$__Linear ModelGradient DescendEarly Stopping$L_2$ Regularization\n\n![Early Stopping As Regularization](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg)\n\n## \nmodel$\\hat{y}^{(A)}=f(w^{(A)},x)$$\\hat{y}^{(B)}=f(w^{(B)},x)$\n\n$\\forall_i,w_i^{(A)}$$w_i^{(B)}$$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$$L_2$ RegularizationRegularization\n\n()\n\nCNN ____(hidden units)$i$$i+1$feature detector\n\n## Sparse Representation\nWeight decayNN\n\nLoss Function $J$ $\\Omega(h)$\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)\n$$\n$L_1$$\\Omega(h)=||h||_1=\\sum_i|h_i|$\n\n$x$$h$:\n$$\n\\mathop{argmin} \\limits_{h,||h||_0< k} ||x-Wh||^2\n$$\n$||h||_0$$h$$W$\n\n## Bagging\n### Why Model Averaging Works?\n$k$regression modelmodel$\\epsilon_i$$\\mathbb{E}[\\epsilon_i^2]=v$$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$$\\frac{1}{k}\\sum_i\\epsilon_i$MSE\n$$\n\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c\n$$\n$c=v$MSE$v$$c=0$MSE$\\frac{v}{k}$MSEensemble modelensemble\n\nNNModel Averaging())NNensemble\n\n## Dropout\nDropoutDNNBaggingDropoutensemblebase NN\n\nDropoutBaggingBaggingDropout\n\n0.5keep_prob22\n\nDropoutRegularizationDropoutLinear Regressionweight decay$L_2$ weight decayweight decayLinear ModelDeep ModelDropoutweight decay\n\n__DropConnect__ Dropouthidden unit\n\n__Batch Normalization__ hidden unitBatchNormDropout\n\n## Adverserial Training\nDNNrobust ____DNN$\\epsilon$$w$$\\epsilon||w||_1$$w$Adverserial trainingNN\n\nsemi-supervised learninglabel$x$label $\\hat{y}$label $\\hat{y}$ label$\\hat{y}$$x^{'}$$y^{'}$$y^{'}\\neq y$labelmodellabeladverserial samples$x$$x^{'}$__classifierrobust__","source":"_posts/dl-regularization.md","raw":"---\ntitle: \"[DL] Regularization\"\ndate: 2018-08-06 17:13:07\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Regularization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n- Regularization\n---\n## Introduction\nRegularizationMachine LearningOverfittingDNNoverfittingsmall modelmodel __ + __ Deep Learning\n\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)\n$$\n\n## \nweightbiasbiasweightweightbias\n\nDNNNetwork$\\alpha$weight decay\n\n### $L_2$ Regularization\nDNNbias$\\theta$$w$Loss Function\n$$\n\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)\n$$\n\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)\n$$\nSGD\n$$\nw\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))\n$$\n\n$$\nw\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)\n$$\nweight decaySGD __()__\n\nLinear RegressionCost FunctionMSE:\n$$\n(Xw-y)^T(Xw-y)\n$$\n$L_2$ RegularizationCost Function:\n$$\n(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw\n$$\n $w=(X^TX)^{-1}X^Ty$  $w=(X^TX+\\alpha I)^{-1}X^Ty$$X^TX$$\\frac{1}{m}X^TX$$L_2$ Regularization$(X^TX+\\alpha I)^{-1}$$\\alpha$$L_2$ Regularization$x$()\n\n### $L_1$ Regularization\nDNNbias$\\theta$$w$Loss Function\n$$\n\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)\n$$\n:\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)\n$$\n__$w_i$$sign(w_i)$$J(X,y;w)$($L_2$)__\n\n$L_1$ Regularization Cost Function\n$$\n\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]\n$$\n($i$)Cost Function\n$$\nw_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}\n$$\n \n1) $w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$$w_i$0$i$$J(w;X,y)$$\\hat{J}(w;X,y)$$L_1$ Regularization$w_i$0\n2) $w_i^{\\star}> \\frac{\\alpha}{H_{i,i}}$$w_i$0$\\frac{\\alpha}{H_{i,i}}$\n\n$L_2$ Regularization$L_1$ Regularization(0)\n\n## \n$$\n\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)\n$$\n$\\Omega$$L_2$$L_2$$\\Omega$$L_1$$L_1$\n\n## Data Augmentation\nNNData AugmentationNNrobustNN robustnessUnsupervised Learning AlgorithmDenoise Auto Encoderhidden layerData Augmentation\n\n## Robustness of Noise\n____hidden units\n\n## Multi-Task Learning\nMTL(____)____\n\n## Early Stopping\nvalidation set errorvalidation set(test set error)validation setvalidation set errorEarly Stopping\n\nweight decayweight decay____\n\nEarly Stoppingvalidation settraining samplesEarly Stoppingtraining data\n* Early Stopping __Epoch__\n* ____validation setvalidation setlossEarly Stopping\n\n![Early Stopping](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg)\n\n### Early StoppingRegularization\nBishop __Early Stopping$\\theta_0$__Linear ModelGradient DescendEarly Stopping$L_2$ Regularization\n\n![Early Stopping As Regularization](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg)\n\n## \nmodel$\\hat{y}^{(A)}=f(w^{(A)},x)$$\\hat{y}^{(B)}=f(w^{(B)},x)$\n\n$\\forall_i,w_i^{(A)}$$w_i^{(B)}$$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$$L_2$ RegularizationRegularization\n\n()\n\nCNN ____(hidden units)$i$$i+1$feature detector\n\n## Sparse Representation\nWeight decayNN\n\nLoss Function $J$ $\\Omega(h)$\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)\n$$\n$L_1$$\\Omega(h)=||h||_1=\\sum_i|h_i|$\n\n$x$$h$:\n$$\n\\mathop{argmin} \\limits_{h,||h||_0< k} ||x-Wh||^2\n$$\n$||h||_0$$h$$W$\n\n## Bagging\n### Why Model Averaging Works?\n$k$regression modelmodel$\\epsilon_i$$\\mathbb{E}[\\epsilon_i^2]=v$$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$$\\frac{1}{k}\\sum_i\\epsilon_i$MSE\n$$\n\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c\n$$\n$c=v$MSE$v$$c=0$MSE$\\frac{v}{k}$MSEensemble modelensemble\n\nNNModel Averaging())NNensemble\n\n## Dropout\nDropoutDNNBaggingDropoutensemblebase NN\n\nDropoutBaggingBaggingDropout\n\n0.5keep_prob22\n\nDropoutRegularizationDropoutLinear Regressionweight decay$L_2$ weight decayweight decayLinear ModelDeep ModelDropoutweight decay\n\n__DropConnect__ Dropouthidden unit\n\n__Batch Normalization__ hidden unitBatchNormDropout\n\n## Adverserial Training\nDNNrobust ____DNN$\\epsilon$$w$$\\epsilon||w||_1$$w$Adverserial trainingNN\n\nsemi-supervised learninglabel$x$label $\\hat{y}$label $\\hat{y}$ label$\\hat{y}$$x^{'}$$y^{'}$$y^{'}\\neq y$labelmodellabeladverserial samples$x$$x^{'}$__classifierrobust__","slug":"dl-regularization","published":1,"updated":"2018-10-01T04:40:08.901Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cb000q608wl2b759r4","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RegularizationMachine LearningOverfittingDNNoverfittingsmall modelmodel <strong> + </strong> Deep Learning</p>\n<p>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>weightbiasbiasweightweightbias</p>\n<p>DNNNetwork$\\alpha$weight decay</p>\n<h3 id=\"L-2-Regularization\"><a href=\"#L-2-Regularization\" class=\"headerlink\" title=\"$L_2$ Regularization\"></a>$L_2$ Regularization</h3><p>DNNbias$\\theta$$w$Loss Function<br>$$<br>\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)<br>$$<br><br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)<br>$$<br>SGD<br>$$<br>w\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))<br>$$<br><br>$$<br>w\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)<br>$$<br>weight decaySGD <strong>()</strong></p>\n<p>Linear RegressionCost FunctionMSE:<br>$$<br>(Xw-y)^T(Xw-y)<br>$$<br>$L_2$ RegularizationCost Function:<br>$$<br>(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw<br>$$<br> $w=(X^TX)^{-1}X^Ty$  $w=(X^TX+\\alpha I)^{-1}X^Ty$$X^TX$$\\frac{1}{m}X^TX$$L_2$ Regularization$(X^TX+\\alpha I)^{-1}$$\\alpha$$L_2$ Regularization$x$()</p>\n<h3 id=\"L-1-Regularization\"><a href=\"#L-1-Regularization\" class=\"headerlink\" title=\"$L_1$ Regularization\"></a>$L_1$ Regularization</h3><p>DNNbias$\\theta$$w$Loss Function<br>$$<br>\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)<br>$$<br>:<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)<br>$$<br><strong>$w_i$$sign(w_i)$$J(X,y;w)$($L_2$)</strong></p>\n<p>$L_1$ Regularization Cost Function<br>$$<br>\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]<br>$$<br>($i$)Cost Function<br>$$<br>w_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}<br>$$<br><br>1) $w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$$w_i$0$i$$J(w;X,y)$$\\hat{J}(w;X,y)$$L_1$ Regularization$w_i$0<br>2) $w_i^{\\star}&gt; \\frac{\\alpha}{H_{i,i}}$$w_i$0$\\frac{\\alpha}{H_{i,i}}$</p>\n<p>$L_2$ Regularization$L_1$ Regularization(0)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)<br>$$<br>$\\Omega$$L_2$$L_2$$\\Omega$$L_1$$L_1$</p>\n<h2 id=\"Data-Augmentation\"><a href=\"#Data-Augmentation\" class=\"headerlink\" title=\"Data Augmentation\"></a>Data Augmentation</h2><p>NNData AugmentationNNrobustNN robustnessUnsupervised Learning AlgorithmDenoise Auto Encoderhidden layerData Augmentation</p>\n<h2 id=\"Robustness-of-Noise\"><a href=\"#Robustness-of-Noise\" class=\"headerlink\" title=\"Robustness of Noise\"></a>Robustness of Noise</h2><p><strong></strong>hidden units</p>\n<h2 id=\"Multi-Task-Learning\"><a href=\"#Multi-Task-Learning\" class=\"headerlink\" title=\"Multi-Task Learning\"></a>Multi-Task Learning</h2><p>MTL(<strong></strong>)<strong></strong></p>\n<h2 id=\"Early-Stopping\"><a href=\"#Early-Stopping\" class=\"headerlink\" title=\"Early Stopping\"></a>Early Stopping</h2><p>validation set errorvalidation set(test set error)validation setvalidation set errorEarly Stopping</p>\n<p>weight decayweight decay<strong></strong></p>\n<p>Early Stoppingvalidation settraining samplesEarly Stoppingtraining data</p>\n<ul>\n<li>Early Stopping <strong>Epoch</strong></li>\n<li><strong></strong>validation setvalidation setlossEarly Stopping</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg\" alt=\"Early Stopping\"></p>\n<h3 id=\"Early-StoppingRegularization\"><a href=\"#Early-StoppingRegularization\" class=\"headerlink\" title=\"Early StoppingRegularization\"></a>Early StoppingRegularization</h3><p>Bishop <strong>Early Stopping$\\theta_0$</strong>Linear ModelGradient DescendEarly Stopping$L_2$ Regularization</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg\" alt=\"Early Stopping As Regularization\"></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>model$\\hat{y}^{(A)}=f(w^{(A)},x)$$\\hat{y}^{(B)}=f(w^{(B)},x)$</p>\n<p>$\\forall_i,w_i^{(A)}$$w_i^{(B)}$$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$$L_2$ RegularizationRegularization</p>\n<p>()</p>\n<p>CNN <strong></strong>(hidden units)$i$$i+1$feature detector</p>\n<h2 id=\"Sparse-Representation\"><a href=\"#Sparse-Representation\" class=\"headerlink\" title=\"Sparse Representation\"></a>Sparse Representation</h2><p>Weight decayNN</p>\n<p>Loss Function $J$ $\\Omega(h)$<br>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)<br>$$<br>$L_1$$\\Omega(h)=||h||_1=\\sum_i|h_i|$</p>\n<p>$x$$h$:<br>$$<br>\\mathop{argmin} \\limits_{h,||h||_0&lt; k} ||x-Wh||^2<br>$$<br>$||h||_0$$h$$W$</p>\n<h2 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h2><h3 id=\"Why-Model-Averaging-Works\"><a href=\"#Why-Model-Averaging-Works\" class=\"headerlink\" title=\"Why Model Averaging Works?\"></a>Why Model Averaging Works?</h3><p>$k$regression modelmodel$\\epsilon_i$$\\mathbb{E}[\\epsilon_i^2]=v$$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$$\\frac{1}{k}\\sum_i\\epsilon_i$MSE<br>$$<br>\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c<br>$$<br>$c=v$MSE$v$$c=0$MSE$\\frac{v}{k}$MSEensemble modelensemble</p>\n<p>NNModel Averaging())NNensemble</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>DropoutDNNBaggingDropoutensemblebase NN</p>\n<p>DropoutBaggingBaggingDropout</p>\n<p>0.5keep_prob22</p>\n<p>DropoutRegularizationDropoutLinear Regressionweight decay$L_2$ weight decayweight decayLinear ModelDeep ModelDropoutweight decay</p>\n<p><strong>DropConnect</strong> Dropouthidden unit</p>\n<p><strong>Batch Normalization</strong> hidden unitBatchNormDropout</p>\n<h2 id=\"Adverserial-Training\"><a href=\"#Adverserial-Training\" class=\"headerlink\" title=\"Adverserial Training\"></a>Adverserial Training</h2><p>DNNrobust <strong></strong>DNN$\\epsilon$$w$$\\epsilon||w||_1$$w$Adverserial trainingNN</p>\n<p>semi-supervised learninglabel$x$label $\\hat{y}$label $\\hat{y}$ label$\\hat{y}$$x^{}$$y^{}$$y^{}\\neq y$labelmodellabeladverserial samples$x$$x^{}$<strong>classifierrobust</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RegularizationMachine LearningOverfittingDNNoverfittingsmall modelmodel <strong> + </strong> Deep Learning</p>\n<p>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>weightbiasbiasweightweightbias</p>\n<p>DNNNetwork$\\alpha$weight decay</p>\n<h3 id=\"L-2-Regularization\"><a href=\"#L-2-Regularization\" class=\"headerlink\" title=\"$L_2$ Regularization\"></a>$L_2$ Regularization</h3><p>DNNbias$\\theta$$w$Loss Function<br>$$<br>\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)<br>$$<br><br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)<br>$$<br>SGD<br>$$<br>w\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))<br>$$<br><br>$$<br>w\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)<br>$$<br>weight decaySGD <strong>()</strong></p>\n<p>Linear RegressionCost FunctionMSE:<br>$$<br>(Xw-y)^T(Xw-y)<br>$$<br>$L_2$ RegularizationCost Function:<br>$$<br>(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw<br>$$<br> $w=(X^TX)^{-1}X^Ty$  $w=(X^TX+\\alpha I)^{-1}X^Ty$$X^TX$$\\frac{1}{m}X^TX$$L_2$ Regularization$(X^TX+\\alpha I)^{-1}$$\\alpha$$L_2$ Regularization$x$()</p>\n<h3 id=\"L-1-Regularization\"><a href=\"#L-1-Regularization\" class=\"headerlink\" title=\"$L_1$ Regularization\"></a>$L_1$ Regularization</h3><p>DNNbias$\\theta$$w$Loss Function<br>$$<br>\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)<br>$$<br>:<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)<br>$$<br><strong>$w_i$$sign(w_i)$$J(X,y;w)$($L_2$)</strong></p>\n<p>$L_1$ Regularization Cost Function<br>$$<br>\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]<br>$$<br>($i$)Cost Function<br>$$<br>w_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}<br>$$<br><br>1) $w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$$w_i$0$i$$J(w;X,y)$$\\hat{J}(w;X,y)$$L_1$ Regularization$w_i$0<br>2) $w_i^{\\star}&gt; \\frac{\\alpha}{H_{i,i}}$$w_i$0$\\frac{\\alpha}{H_{i,i}}$</p>\n<p>$L_2$ Regularization$L_1$ Regularization(0)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)<br>$$<br>$\\Omega$$L_2$$L_2$$\\Omega$$L_1$$L_1$</p>\n<h2 id=\"Data-Augmentation\"><a href=\"#Data-Augmentation\" class=\"headerlink\" title=\"Data Augmentation\"></a>Data Augmentation</h2><p>NNData AugmentationNNrobustNN robustnessUnsupervised Learning AlgorithmDenoise Auto Encoderhidden layerData Augmentation</p>\n<h2 id=\"Robustness-of-Noise\"><a href=\"#Robustness-of-Noise\" class=\"headerlink\" title=\"Robustness of Noise\"></a>Robustness of Noise</h2><p><strong></strong>hidden units</p>\n<h2 id=\"Multi-Task-Learning\"><a href=\"#Multi-Task-Learning\" class=\"headerlink\" title=\"Multi-Task Learning\"></a>Multi-Task Learning</h2><p>MTL(<strong></strong>)<strong></strong></p>\n<h2 id=\"Early-Stopping\"><a href=\"#Early-Stopping\" class=\"headerlink\" title=\"Early Stopping\"></a>Early Stopping</h2><p>validation set errorvalidation set(test set error)validation setvalidation set errorEarly Stopping</p>\n<p>weight decayweight decay<strong></strong></p>\n<p>Early Stoppingvalidation settraining samplesEarly Stoppingtraining data</p>\n<ul>\n<li>Early Stopping <strong>Epoch</strong></li>\n<li><strong></strong>validation setvalidation setlossEarly Stopping</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg\" alt=\"Early Stopping\"></p>\n<h3 id=\"Early-StoppingRegularization\"><a href=\"#Early-StoppingRegularization\" class=\"headerlink\" title=\"Early StoppingRegularization\"></a>Early StoppingRegularization</h3><p>Bishop <strong>Early Stopping$\\theta_0$</strong>Linear ModelGradient DescendEarly Stopping$L_2$ Regularization</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg\" alt=\"Early Stopping As Regularization\"></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>model$\\hat{y}^{(A)}=f(w^{(A)},x)$$\\hat{y}^{(B)}=f(w^{(B)},x)$</p>\n<p>$\\forall_i,w_i^{(A)}$$w_i^{(B)}$$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$$L_2$ RegularizationRegularization</p>\n<p>()</p>\n<p>CNN <strong></strong>(hidden units)$i$$i+1$feature detector</p>\n<h2 id=\"Sparse-Representation\"><a href=\"#Sparse-Representation\" class=\"headerlink\" title=\"Sparse Representation\"></a>Sparse Representation</h2><p>Weight decayNN</p>\n<p>Loss Function $J$ $\\Omega(h)$<br>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)<br>$$<br>$L_1$$\\Omega(h)=||h||_1=\\sum_i|h_i|$</p>\n<p>$x$$h$:<br>$$<br>\\mathop{argmin} \\limits_{h,||h||_0&lt; k} ||x-Wh||^2<br>$$<br>$||h||_0$$h$$W$</p>\n<h2 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h2><h3 id=\"Why-Model-Averaging-Works\"><a href=\"#Why-Model-Averaging-Works\" class=\"headerlink\" title=\"Why Model Averaging Works?\"></a>Why Model Averaging Works?</h3><p>$k$regression modelmodel$\\epsilon_i$$\\mathbb{E}[\\epsilon_i^2]=v$$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$$\\frac{1}{k}\\sum_i\\epsilon_i$MSE<br>$$<br>\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c<br>$$<br>$c=v$MSE$v$$c=0$MSE$\\frac{v}{k}$MSEensemble modelensemble</p>\n<p>NNModel Averaging())NNensemble</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>DropoutDNNBaggingDropoutensemblebase NN</p>\n<p>DropoutBaggingBaggingDropout</p>\n<p>0.5keep_prob22</p>\n<p>DropoutRegularizationDropoutLinear Regressionweight decay$L_2$ weight decayweight decayLinear ModelDeep ModelDropoutweight decay</p>\n<p><strong>DropConnect</strong> Dropouthidden unit</p>\n<p><strong>Batch Normalization</strong> hidden unitBatchNormDropout</p>\n<h2 id=\"Adverserial-Training\"><a href=\"#Adverserial-Training\" class=\"headerlink\" title=\"Adverserial Training\"></a>Adverserial Training</h2><p>DNNrobust <strong></strong>DNN$\\epsilon$$w$$\\epsilon||w||_1$$w$Adverserial trainingNN</p>\n<p>semi-supervised learninglabel$x$label $\\hat{y}$label $\\hat{y}$ label$\\hat{y}$$x^{}$$y^{}$$y^{}\\neq y$labelmodellabeladverserial samples$x$$x^{}$<strong>classifierrobust</strong></p>\n"},{"title":"[DL] RNN","date":"2018-08-10T06:18:19.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","RNN","NLP"],"_content":"## Introduction\nRNN(Recurrent Neural Network)RNNNLPDeep LearningRNN\n\n## Computational Graph\n![Computational Graph Unfold](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg)\n\n$g^{(t)}$ $t$ \n$$\nh^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)\n$$\n1. model\n2.  $f$\n\n(not appear in training data)training samples\n\n## Recurrent Neural Network\nRNN\n1. hidden units  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg)\n\n2. hidden units  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg)\n\n3. Hidden unitssequence  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg)\n\n $o$ Softmax$\\hat{y}$RNN$h^{(0)}$$t=1$$t=\\tau$\n$$\na^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\\nh^{(t)}=tanh(a^{(t)}) \\\\\no^{(t)}=Vh^{(t)}+c \\\\\n\\hat{y}^{(t)}=softmax(o^{(t)})\n$$\n$U, V, W$ input layers to hidden layershidden layers to output layershidden to next hidden layersRNNinput sequenceoutput sequence__ $x$  $y$ LossLoss__\n\n### Teacher Forcing and Networks with Output Recurrence\nmodelteacher forcingteacher forcing $t+1$  $y^{(t)}$ \n$$\nlog p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})\n$$\n\n![Teacher Forcing](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg)\n\n\n### Computing the Gradient in RNN\nLoss\n$\\frac{\\partial L}{\\partial L^{(t)}}=1$\n\n $i,t$  $t$  $\\triangledown_{o^{(t)}}L$   \n$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$\n\n $\\tau, h^{(\\tau)}$  $o^{(\\tau)}$   \n$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$\n\n$t=\\tau -1$$t=1$ $h^{(t)} (t<\\tau)$ $o^{(t)}$  $h^{(t+1)}$  \n$$\n\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)\n$$\n\n\n\n![RNN-BP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg)\n\n### Recurrent Networks as Directed Graphical Models\n","source":"_posts/dl-rnn.md","raw":"---\ntitle: \"[DL] RNN\"\ndate: 2018-08-10 14:18:19\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- RNN\n- Data Science\n- NLP\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- RNN\n- NLP\n---\n## Introduction\nRNN(Recurrent Neural Network)RNNNLPDeep LearningRNN\n\n## Computational Graph\n![Computational Graph Unfold](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg)\n\n$g^{(t)}$ $t$ \n$$\nh^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)\n$$\n1. model\n2.  $f$\n\n(not appear in training data)training samples\n\n## Recurrent Neural Network\nRNN\n1. hidden units  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg)\n\n2. hidden units  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg)\n\n3. Hidden unitssequence  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg)\n\n $o$ Softmax$\\hat{y}$RNN$h^{(0)}$$t=1$$t=\\tau$\n$$\na^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\\nh^{(t)}=tanh(a^{(t)}) \\\\\no^{(t)}=Vh^{(t)}+c \\\\\n\\hat{y}^{(t)}=softmax(o^{(t)})\n$$\n$U, V, W$ input layers to hidden layershidden layers to output layershidden to next hidden layersRNNinput sequenceoutput sequence__ $x$  $y$ LossLoss__\n\n### Teacher Forcing and Networks with Output Recurrence\nmodelteacher forcingteacher forcing $t+1$  $y^{(t)}$ \n$$\nlog p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})\n$$\n\n![Teacher Forcing](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg)\n\n\n### Computing the Gradient in RNN\nLoss\n$\\frac{\\partial L}{\\partial L^{(t)}}=1$\n\n $i,t$  $t$  $\\triangledown_{o^{(t)}}L$   \n$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$\n\n $\\tau, h^{(\\tau)}$  $o^{(\\tau)}$   \n$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$\n\n$t=\\tau -1$$t=1$ $h^{(t)} (t<\\tau)$ $o^{(t)}$  $h^{(t+1)}$  \n$$\n\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)\n$$\n\n\n\n![RNN-BP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg)\n\n### Recurrent Networks as Directed Graphical Models\n","slug":"dl-rnn","published":1,"updated":"2018-10-01T04:40:08.921Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cd000s608wvyyxpd8f","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RNN(Recurrent Neural Network)RNNNLPDeep LearningRNN</p>\n<h2 id=\"Computational-Graph\"><a href=\"#Computational-Graph\" class=\"headerlink\" title=\"Computational Graph\"></a>Computational Graph</h2><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg\" alt=\"Computational Graph Unfold\"></p>\n<p>$g^{(t)}$ $t$ <br>$$<br>h^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)<br>$$</p>\n<ol>\n<li>model</li>\n<li> $f$</li>\n</ol>\n<p>(not appear in training data)training samples</p>\n<h2 id=\"Recurrent-Neural-Network\"><a href=\"#Recurrent-Neural-Network\" class=\"headerlink\" title=\"Recurrent Neural Network\"></a>Recurrent Neural Network</h2><p>RNN</p>\n<ol>\n<li><p>hidden units<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>hidden units<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>Hidden unitssequence<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg\" alt=\"RNN\"></p>\n</li>\n</ol>\n<p> $o$ Softmax$\\hat{y}$RNN$h^{(0)}$$t=1$$t=\\tau$<br>$$<br>a^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\<br>h^{(t)}=tanh(a^{(t)}) \\\\<br>o^{(t)}=Vh^{(t)}+c \\\\<br>\\hat{y}^{(t)}=softmax(o^{(t)})<br>$$<br>$U, V, W$ input layers to hidden layershidden layers to output layershidden to next hidden layersRNNinput sequenceoutput sequence<strong> $x$  $y$ LossLoss</strong></p>\n<h3 id=\"Teacher-Forcing-and-Networks-with-Output-Recurrence\"><a href=\"#Teacher-Forcing-and-Networks-with-Output-Recurrence\" class=\"headerlink\" title=\"Teacher Forcing and Networks with Output Recurrence\"></a>Teacher Forcing and Networks with Output Recurrence</h3><p>modelteacher forcingteacher forcing $t+1$  $y^{(t)}$ <br>$$<br>log p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg\" alt=\"Teacher Forcing\"></p>\n<h3 id=\"Computing-the-Gradient-in-RNN\"><a href=\"#Computing-the-Gradient-in-RNN\" class=\"headerlink\" title=\"Computing the Gradient in RNN\"></a>Computing the Gradient in RNN</h3><p>Loss<br>$\\frac{\\partial L}{\\partial L^{(t)}}=1$</p>\n<p> $i,t$  $t$  $\\triangledown_{o^{(t)}}L$ <br>$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$</p>\n<p> $\\tau, h^{(\\tau)}$  $o^{(\\tau)}$ <br>$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$</p>\n<p>$t=\\tau -1$$t=1$ $h^{(t)} (t&lt;\\tau)$ $o^{(t)}$  $h^{(t+1)}$<br>$$<br>\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)<br>$$</p>\n<p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg\" alt=\"RNN-BP\"></p>\n<h3 id=\"Recurrent-Networks-as-Directed-Graphical-Models\"><a href=\"#Recurrent-Networks-as-Directed-Graphical-Models\" class=\"headerlink\" title=\"Recurrent Networks as Directed Graphical Models\"></a>Recurrent Networks as Directed Graphical Models</h3>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RNN(Recurrent Neural Network)RNNNLPDeep LearningRNN</p>\n<h2 id=\"Computational-Graph\"><a href=\"#Computational-Graph\" class=\"headerlink\" title=\"Computational Graph\"></a>Computational Graph</h2><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg\" alt=\"Computational Graph Unfold\"></p>\n<p>$g^{(t)}$ $t$ <br>$$<br>h^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)<br>$$</p>\n<ol>\n<li>model</li>\n<li> $f$</li>\n</ol>\n<p>(not appear in training data)training samples</p>\n<h2 id=\"Recurrent-Neural-Network\"><a href=\"#Recurrent-Neural-Network\" class=\"headerlink\" title=\"Recurrent Neural Network\"></a>Recurrent Neural Network</h2><p>RNN</p>\n<ol>\n<li><p>hidden units<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>hidden units<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>Hidden unitssequence<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg\" alt=\"RNN\"></p>\n</li>\n</ol>\n<p> $o$ Softmax$\\hat{y}$RNN$h^{(0)}$$t=1$$t=\\tau$<br>$$<br>a^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\<br>h^{(t)}=tanh(a^{(t)}) \\\\<br>o^{(t)}=Vh^{(t)}+c \\\\<br>\\hat{y}^{(t)}=softmax(o^{(t)})<br>$$<br>$U, V, W$ input layers to hidden layershidden layers to output layershidden to next hidden layersRNNinput sequenceoutput sequence<strong> $x$  $y$ LossLoss</strong></p>\n<h3 id=\"Teacher-Forcing-and-Networks-with-Output-Recurrence\"><a href=\"#Teacher-Forcing-and-Networks-with-Output-Recurrence\" class=\"headerlink\" title=\"Teacher Forcing and Networks with Output Recurrence\"></a>Teacher Forcing and Networks with Output Recurrence</h3><p>modelteacher forcingteacher forcing $t+1$  $y^{(t)}$ <br>$$<br>log p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg\" alt=\"Teacher Forcing\"></p>\n<h3 id=\"Computing-the-Gradient-in-RNN\"><a href=\"#Computing-the-Gradient-in-RNN\" class=\"headerlink\" title=\"Computing the Gradient in RNN\"></a>Computing the Gradient in RNN</h3><p>Loss<br>$\\frac{\\partial L}{\\partial L^{(t)}}=1$</p>\n<p> $i,t$  $t$  $\\triangledown_{o^{(t)}}L$ <br>$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$</p>\n<p> $\\tau, h^{(\\tau)}$  $o^{(\\tau)}$ <br>$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$</p>\n<p>$t=\\tau -1$$t=1$ $h^{(t)} (t&lt;\\tau)$ $o^{(t)}$  $h^{(t+1)}$<br>$$<br>\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)<br>$$</p>\n<p></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg\" alt=\"RNN-BP\"></p>\n<h3 id=\"Recurrent-Networks-as-Directed-Graphical-Models\"><a href=\"#Recurrent-Networks-as-Directed-Graphical-Models\" class=\"headerlink\" title=\"Recurrent Networks as Directed Graphical Models\"></a>Recurrent Networks as Directed Graphical Models</h3>"},{"title":"[ML] Clustering","date":"2018-07-30T14:01:45.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Performance Metric\n\n\n$D=\\{x_1,x_2,\\cdots,x_m\\}$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$$\\lambda$$\\lambda^{\\star}$$\\mathcal{C}$$\\mathcal{C}^{\\star}$\n$$\na=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nb=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\} \\\\\nc=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nd=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\}\n$$\n$SS$$\\mathcal{C}$$\\mathcal{C}^{\\star}$$SD$$\\mathcal{C}$$\\mathcal{C}^{\\star}$\n$$\na+b+c+d=\\frac{m(m-1)}{2}\n$$\n\n\n* Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$\n* FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$\n* Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$\n\n$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$\n* $AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i<j \\leq |C|}} dist(x_i,x_j)$\n* $diam(C)=\\mathop{max} \\limits_{1\\leq i < j \\leq |C|} dist(x_i,x_j)$\n* $d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$\n* $d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$\n\n\n* DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$\n* Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$\n\n## Distance Metric\nVDM(Value Difference Metric)$m_{u,a}$$u$$a$$m_{u,a,i}$$i$$u$$a$$k$$u$$a$$b$VDM\n$$\nVDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p\n$$\n\n$L_P$ DistanceVDM$n_c$$n-n_c$\n$$\nMinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}\n$$\n\n$$\ndist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}\n$$\n\n## Prototype-based Clustering\n### KMeans\nKMeansMSE Loss\n$$\nE=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2\n$$\n$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$$C_i$\n\n### Learning Vector Quantization\nLVQ$\\{p_1,p_2,\\cdots,p_q\\}$$\\chi$$x$$p_i$$R_i$$p_i$$p_{i^{'}}(i^{'}\\neq i)$\n$$\nR_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{'}}||_2,i\\neq i^{'}\\}\n$$\n\n### Mixture of Gaussian\nKMeansLVQMixture-of-Gaussian\n\n$n$$\\chi$$x$$x$\n$$\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n$$\n$\\mu$$n$$\\Sigma$$n\\times n$$\\mu$$\\Sigma$$p(x|\\mu,\\Sigma)$\n\n\n$$\np_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)\n$$\n$k$$\\mu_i$$\\Sigma_i$$i$$\\alpha_i>0$$\\sum_{i=1}^k \\alpha_i=1$\n\n$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$$\\alpha_i$$i$\n\ntraining set $D=\\{x_1,x_2,\\cdots,x_m\\}$$z_j\\in \\{1,2,\\cdots,k\\}$$x_j$Bayesian Theorem$z_j$\n$$\np_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}\n$$\n$p_{\\mathcal{M}}(z_j=i|x_j)$$x_j$$i$$\\gamma_{ji}$\n\n$D$$k$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$x_j$\n$$\n\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}\n$$\n\n## Density-based Clustering\nDBSCAN$(\\epsilon, MinPts)$$D=\\{x_1,x_2,\\cdots,x_m\\}$\n* $\\epsilon-$$x_j\\in D$$\\epsilon-$$D$$x_j$$\\epsilon$$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$\n* $x_j$$\\epsilon-$$MinPts$$|N_{\\epsilon}(x_j)|\\geq MinPts$$x_j$\n* $x_j$$x_i$$\\epsilon-$$x_i$$x_j$$x_i$\n* $x_i$$x_j$$p_1,p_2,\\cdots,p_n$$p_1=x_i,p_2=x_j$$p_{i+1}$$p_i$$x_j$$x_i$\n* $x_i$$x_j$$x_k$$x_i$$x_j$$x_k$$x_i$$x_j$\n\nDBSCAN$(\\epsilon,MinPts)$$C\\subseteq D$\n* $x_i\\in C,x_j \\in C, \\implies x_i$$x_j$\n* $x_i\\in C, x_j$$x_i$$\\implies$ $x_j \\in C$\n\n## Hierarchical Clustering\n\n\n## Tips\n$k$$k$","source":"_posts/ml-clustering.md","raw":"---\ntitle: \"[ML] Clustering\"\ndate: 2018-07-30 22:01:45\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Performance Metric\n\n\n$D=\\{x_1,x_2,\\cdots,x_m\\}$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$$\\lambda$$\\lambda^{\\star}$$\\mathcal{C}$$\\mathcal{C}^{\\star}$\n$$\na=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nb=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\} \\\\\nc=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nd=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\}\n$$\n$SS$$\\mathcal{C}$$\\mathcal{C}^{\\star}$$SD$$\\mathcal{C}$$\\mathcal{C}^{\\star}$\n$$\na+b+c+d=\\frac{m(m-1)}{2}\n$$\n\n\n* Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$\n* FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$\n* Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$\n\n$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$\n* $AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i<j \\leq |C|}} dist(x_i,x_j)$\n* $diam(C)=\\mathop{max} \\limits_{1\\leq i < j \\leq |C|} dist(x_i,x_j)$\n* $d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$\n* $d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$\n\n\n* DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$\n* Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$\n\n## Distance Metric\nVDM(Value Difference Metric)$m_{u,a}$$u$$a$$m_{u,a,i}$$i$$u$$a$$k$$u$$a$$b$VDM\n$$\nVDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p\n$$\n\n$L_P$ DistanceVDM$n_c$$n-n_c$\n$$\nMinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}\n$$\n\n$$\ndist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}\n$$\n\n## Prototype-based Clustering\n### KMeans\nKMeansMSE Loss\n$$\nE=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2\n$$\n$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$$C_i$\n\n### Learning Vector Quantization\nLVQ$\\{p_1,p_2,\\cdots,p_q\\}$$\\chi$$x$$p_i$$R_i$$p_i$$p_{i^{'}}(i^{'}\\neq i)$\n$$\nR_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{'}}||_2,i\\neq i^{'}\\}\n$$\n\n### Mixture of Gaussian\nKMeansLVQMixture-of-Gaussian\n\n$n$$\\chi$$x$$x$\n$$\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n$$\n$\\mu$$n$$\\Sigma$$n\\times n$$\\mu$$\\Sigma$$p(x|\\mu,\\Sigma)$\n\n\n$$\np_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)\n$$\n$k$$\\mu_i$$\\Sigma_i$$i$$\\alpha_i>0$$\\sum_{i=1}^k \\alpha_i=1$\n\n$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$$\\alpha_i$$i$\n\ntraining set $D=\\{x_1,x_2,\\cdots,x_m\\}$$z_j\\in \\{1,2,\\cdots,k\\}$$x_j$Bayesian Theorem$z_j$\n$$\np_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}\n$$\n$p_{\\mathcal{M}}(z_j=i|x_j)$$x_j$$i$$\\gamma_{ji}$\n\n$D$$k$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$x_j$\n$$\n\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}\n$$\n\n## Density-based Clustering\nDBSCAN$(\\epsilon, MinPts)$$D=\\{x_1,x_2,\\cdots,x_m\\}$\n* $\\epsilon-$$x_j\\in D$$\\epsilon-$$D$$x_j$$\\epsilon$$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$\n* $x_j$$\\epsilon-$$MinPts$$|N_{\\epsilon}(x_j)|\\geq MinPts$$x_j$\n* $x_j$$x_i$$\\epsilon-$$x_i$$x_j$$x_i$\n* $x_i$$x_j$$p_1,p_2,\\cdots,p_n$$p_1=x_i,p_2=x_j$$p_{i+1}$$p_i$$x_j$$x_i$\n* $x_i$$x_j$$x_k$$x_i$$x_j$$x_k$$x_i$$x_j$\n\nDBSCAN$(\\epsilon,MinPts)$$C\\subseteq D$\n* $x_i\\in C,x_j \\in C, \\implies x_i$$x_j$\n* $x_i\\in C, x_j$$x_i$$\\implies$ $x_j \\in C$\n\n## Hierarchical Clustering\n\n\n## Tips\n$k$$k$","slug":"ml-clustering","published":1,"updated":"2018-10-01T04:40:08.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cf000u608w5h4oygdy","content":"<h2 id=\"Performance-Metric\"><a href=\"#Performance-Metric\" class=\"headerlink\" title=\"Performance Metric\"></a>Performance Metric</h2><p></p>\n<p>$D=\\{x_1,x_2,\\cdots,x_m\\}$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$$\\lambda$$\\lambda^{\\star}$$\\mathcal{C}$$\\mathcal{C}^{\\star}$<br>$$<br>a=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>b=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\} \\\\<br>c=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>d=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\}<br>$$<br>$SS$$\\mathcal{C}$$\\mathcal{C}^{\\star}$$SD$$\\mathcal{C}$$\\mathcal{C}^{\\star}$<br>$$<br>a+b+c+d=\\frac{m(m-1)}{2}<br>$$</p>\n<p></p>\n<ul>\n<li>Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$</li>\n<li>FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$</li>\n<li>Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$</li>\n</ul>\n<p>$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$</p>\n<ul>\n<li>$AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i&lt;j \\leq |C|}} dist(x_i,x_j)$</li>\n<li>$diam(C)=\\mathop{max} \\limits_{1\\leq i &lt; j \\leq |C|} dist(x_i,x_j)$</li>\n<li>$d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$</li>\n<li>$d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$</li>\n</ul>\n<p></p>\n<ul>\n<li>DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$</li>\n<li>Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$</li>\n</ul>\n<h2 id=\"Distance-Metric\"><a href=\"#Distance-Metric\" class=\"headerlink\" title=\"Distance Metric\"></a>Distance Metric</h2><p>VDM(Value Difference Metric)$m_{u,a}$$u$$a$$m_{u,a,i}$$i$$u$$a$$k$$u$$a$$b$VDM<br>$$<br>VDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p<br>$$</p>\n<p>$L_P$ DistanceVDM$n_c$$n-n_c$<br>$$<br>MinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}<br>$$<br><br>$$<br>dist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}<br>$$</p>\n<h2 id=\"Prototype-based-Clustering\"><a href=\"#Prototype-based-Clustering\" class=\"headerlink\" title=\"Prototype-based Clustering\"></a>Prototype-based Clustering</h2><h3 id=\"KMeans\"><a href=\"#KMeans\" class=\"headerlink\" title=\"KMeans\"></a>KMeans</h3><p>KMeansMSE Loss<br>$$<br>E=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2<br>$$<br>$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$$C_i$</p>\n<h3 id=\"Learning-Vector-Quantization\"><a href=\"#Learning-Vector-Quantization\" class=\"headerlink\" title=\"Learning Vector Quantization\"></a>Learning Vector Quantization</h3><p>LVQ$\\{p_1,p_2,\\cdots,p_q\\}$$\\chi$$x$$p_i$$R_i$$p_i$$p_{i^{}}(i^{}\\neq i)$<br>$$<br>R_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{}}||_2,i\\neq i^{}\\}<br>$$</p>\n<h3 id=\"Mixture-of-Gaussian\"><a href=\"#Mixture-of-Gaussian\" class=\"headerlink\" title=\"Mixture of Gaussian\"></a>Mixture of Gaussian</h3><p>KMeansLVQMixture-of-Gaussian</p>\n<p>$n$$\\chi$$x$$x$<br>$$<br>p(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}<br>$$<br>$\\mu$$n$$\\Sigma$$n\\times n$$\\mu$$\\Sigma$$p(x|\\mu,\\Sigma)$</p>\n<p><br>$$<br>p_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)<br>$$<br>$k$$\\mu_i$$\\Sigma_i$$i$$\\alpha_i&gt;0$$\\sum_{i=1}^k \\alpha_i=1$</p>\n<p>$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$$\\alpha_i$$i$</p>\n<p>training set $D=\\{x_1,x_2,\\cdots,x_m\\}$$z_j\\in \\{1,2,\\cdots,k\\}$$x_j$Bayesian Theorem$z_j$<br>$$<br>p_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}<br>$$<br>$p_{\\mathcal{M}}(z_j=i|x_j)$$x_j$$i$$\\gamma_{ji}$</p>\n<p>$D$$k$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$x_j$<br>$$<br>\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}<br>$$</p>\n<h2 id=\"Density-based-Clustering\"><a href=\"#Density-based-Clustering\" class=\"headerlink\" title=\"Density-based Clustering\"></a>Density-based Clustering</h2><p>DBSCAN$(\\epsilon, MinPts)$$D=\\{x_1,x_2,\\cdots,x_m\\}$</p>\n<ul>\n<li>$\\epsilon-$$x_j\\in D$$\\epsilon-$$D$$x_j$$\\epsilon$$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$</li>\n<li>$x_j$$\\epsilon-$$MinPts$$|N_{\\epsilon}(x_j)|\\geq MinPts$$x_j$</li>\n<li>$x_j$$x_i$$\\epsilon-$$x_i$$x_j$$x_i$</li>\n<li>$x_i$$x_j$$p_1,p_2,\\cdots,p_n$$p_1=x_i,p_2=x_j$$p_{i+1}$$p_i$$x_j$$x_i$</li>\n<li>$x_i$$x_j$$x_k$$x_i$$x_j$$x_k$$x_i$$x_j$</li>\n</ul>\n<p>DBSCAN$(\\epsilon,MinPts)$$C\\subseteq D$</p>\n<ul>\n<li>$x_i\\in C,x_j \\in C, \\implies x_i$$x_j$</li>\n<li>$x_i\\in C, x_j$$x_i$$\\implies$ $x_j \\in C$</li>\n</ul>\n<h2 id=\"Hierarchical-Clustering\"><a href=\"#Hierarchical-Clustering\" class=\"headerlink\" title=\"Hierarchical Clustering\"></a>Hierarchical Clustering</h2><p></p>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><p>$k$$k$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Performance-Metric\"><a href=\"#Performance-Metric\" class=\"headerlink\" title=\"Performance Metric\"></a>Performance Metric</h2><p></p>\n<p>$D=\\{x_1,x_2,\\cdots,x_m\\}$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$$\\lambda$$\\lambda^{\\star}$$\\mathcal{C}$$\\mathcal{C}^{\\star}$<br>$$<br>a=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>b=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\} \\\\<br>c=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>d=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\}<br>$$<br>$SS$$\\mathcal{C}$$\\mathcal{C}^{\\star}$$SD$$\\mathcal{C}$$\\mathcal{C}^{\\star}$<br>$$<br>a+b+c+d=\\frac{m(m-1)}{2}<br>$$</p>\n<p></p>\n<ul>\n<li>Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$</li>\n<li>FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$</li>\n<li>Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$</li>\n</ul>\n<p>$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$</p>\n<ul>\n<li>$AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i&lt;j \\leq |C|}} dist(x_i,x_j)$</li>\n<li>$diam(C)=\\mathop{max} \\limits_{1\\leq i &lt; j \\leq |C|} dist(x_i,x_j)$</li>\n<li>$d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$</li>\n<li>$d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$</li>\n</ul>\n<p></p>\n<ul>\n<li>DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$</li>\n<li>Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$</li>\n</ul>\n<h2 id=\"Distance-Metric\"><a href=\"#Distance-Metric\" class=\"headerlink\" title=\"Distance Metric\"></a>Distance Metric</h2><p>VDM(Value Difference Metric)$m_{u,a}$$u$$a$$m_{u,a,i}$$i$$u$$a$$k$$u$$a$$b$VDM<br>$$<br>VDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p<br>$$</p>\n<p>$L_P$ DistanceVDM$n_c$$n-n_c$<br>$$<br>MinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}<br>$$<br><br>$$<br>dist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}<br>$$</p>\n<h2 id=\"Prototype-based-Clustering\"><a href=\"#Prototype-based-Clustering\" class=\"headerlink\" title=\"Prototype-based Clustering\"></a>Prototype-based Clustering</h2><h3 id=\"KMeans\"><a href=\"#KMeans\" class=\"headerlink\" title=\"KMeans\"></a>KMeans</h3><p>KMeansMSE Loss<br>$$<br>E=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2<br>$$<br>$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$$C_i$</p>\n<h3 id=\"Learning-Vector-Quantization\"><a href=\"#Learning-Vector-Quantization\" class=\"headerlink\" title=\"Learning Vector Quantization\"></a>Learning Vector Quantization</h3><p>LVQ$\\{p_1,p_2,\\cdots,p_q\\}$$\\chi$$x$$p_i$$R_i$$p_i$$p_{i^{}}(i^{}\\neq i)$<br>$$<br>R_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{}}||_2,i\\neq i^{}\\}<br>$$</p>\n<h3 id=\"Mixture-of-Gaussian\"><a href=\"#Mixture-of-Gaussian\" class=\"headerlink\" title=\"Mixture of Gaussian\"></a>Mixture of Gaussian</h3><p>KMeansLVQMixture-of-Gaussian</p>\n<p>$n$$\\chi$$x$$x$<br>$$<br>p(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}<br>$$<br>$\\mu$$n$$\\Sigma$$n\\times n$$\\mu$$\\Sigma$$p(x|\\mu,\\Sigma)$</p>\n<p><br>$$<br>p_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)<br>$$<br>$k$$\\mu_i$$\\Sigma_i$$i$$\\alpha_i&gt;0$$\\sum_{i=1}^k \\alpha_i=1$</p>\n<p>$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$$\\alpha_i$$i$</p>\n<p>training set $D=\\{x_1,x_2,\\cdots,x_m\\}$$z_j\\in \\{1,2,\\cdots,k\\}$$x_j$Bayesian Theorem$z_j$<br>$$<br>p_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}<br>$$<br>$p_{\\mathcal{M}}(z_j=i|x_j)$$x_j$$i$$\\gamma_{ji}$</p>\n<p>$D$$k$$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$$x_j$<br>$$<br>\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}<br>$$</p>\n<h2 id=\"Density-based-Clustering\"><a href=\"#Density-based-Clustering\" class=\"headerlink\" title=\"Density-based Clustering\"></a>Density-based Clustering</h2><p>DBSCAN$(\\epsilon, MinPts)$$D=\\{x_1,x_2,\\cdots,x_m\\}$</p>\n<ul>\n<li>$\\epsilon-$$x_j\\in D$$\\epsilon-$$D$$x_j$$\\epsilon$$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$</li>\n<li>$x_j$$\\epsilon-$$MinPts$$|N_{\\epsilon}(x_j)|\\geq MinPts$$x_j$</li>\n<li>$x_j$$x_i$$\\epsilon-$$x_i$$x_j$$x_i$</li>\n<li>$x_i$$x_j$$p_1,p_2,\\cdots,p_n$$p_1=x_i,p_2=x_j$$p_{i+1}$$p_i$$x_j$$x_i$</li>\n<li>$x_i$$x_j$$x_k$$x_i$$x_j$$x_k$$x_i$$x_j$</li>\n</ul>\n<p>DBSCAN$(\\epsilon,MinPts)$$C\\subseteq D$</p>\n<ul>\n<li>$x_i\\in C,x_j \\in C, \\implies x_i$$x_j$</li>\n<li>$x_i\\in C, x_j$$x_i$$\\implies$ $x_j \\in C$</li>\n</ul>\n<h2 id=\"Hierarchical-Clustering\"><a href=\"#Hierarchical-Clustering\" class=\"headerlink\" title=\"Hierarchical Clustering\"></a>Hierarchical Clustering</h2><p></p>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><p>$k$$k$</p>\n"},{"title":"[ML] Dimension Reduction and Metric Learning","date":"2018-08-20T14:02:55.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Low-Dimension Embedding\nMLcurse of dimensionality\n\n**Why dimension reduction works?** \nEmbedding\n\n### Multiple Dimensional Scaling\nMDS$d^{'}$$Z\\in \\mathbb{R}^{d^{'}\\times m},d^{'}\\leq d$$d^{'}$$||z_i - z_j||=dist_{ij}$\n\n$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$$B$$b_{ij}=z_i^Tz_j$\n$$\ndist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j\n$$\n\n$Z$$\\sum_{i=1}^mz_i=0$$B$0\n$$\n\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\\n\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\\n\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)\n$$\n\n$tr(B)=\\sum_{i=1}^m||z_i||^2$\n$$\ndist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\\n$$\n\n\n$$\nb_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)\n$$\n\n$D$$B$\n\n$B$$B=V\\bigwedge V^T$$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$$V$$d^{\\star}$$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$$V_{\\star}$$Z$\n$$\nZ=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}\n$$\n\n ****$d^{'}\\ll d$$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{'}})$$\\tilde{V}$$Z$\n$$\nZ=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{'}\\times m}\n$$\n\n## PCA\n$\\sum_i x_i=0$$\\{w_1,w_2,\\cdots,w_d\\}$$w_i$$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$$d^{'}< d$$x_i$$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{'}})$$z_{ij}=w_j^Tx_i$$x_i$$j$$z_i$$x_i$$\\hat{x}_i=\\sum_{j=1}^{d^{'}z_{ij}w_j}$\n\n$$\n\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n$x_i$$W^Tx_i$ ****\n\n\n$$\n\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n\n$$\nXX^TW=\\lambda W\n$$\n\n$XX^T$$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$$d^{'}$$W=(w_1,w_2,\\cdots,w_{d^{'}})$PCA\n\n$d-d^{'}$\n\n## Kernel Linear Dimension Reduction\n\n\nKernel Tricks(SVMSVMKernel Tricks)\n\nKernel PCA$W$PCA\n$$\n(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W\n$$\n\n$z_i$$x_i$\n$$\nW=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i\n$$\n$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$$z_i$$x_i$$\\phi$$z_i=\\phi(x_i),i=1,2,\\cdots,m$$\\phi$PCA\n$$\n(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W\n$$\n\n$$\nW=\\sum_{i=1}^m \\phi(x_i) \\alpha_i\n$$\n\n$\\phi$\n$$\n\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)\n$$\n\n\n$$\nKA=\\lambda A\n$$\n$K$$\\mathcal{k}$$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$$K$$d^{'}$\n\n$x$$j (j=1,2,\\cdots,d^{'})$\n$$\nz_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)\n$$\n$\\alpha_i$$\\alpha_i^j$$\\alpha_i$$j$\n\n## Manifold Learning\nManifold<font color=\"red\"></font><font color=\"red\"></font>\n\n### Isometric Mapping\nIsomap\n\n<font color=\"red\"></font><font color=\"red\"></font>\n\n$k$$k$$\\epsilon$$\\epsilon$$\\epsilon$\n\n### Locally Linear Embedding (LLE)\nIsomap<font color=\"red\">LLE</font>$x_i$$x_j,x_k,x_l$\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE$x_i$$Q_i$$Q_i$$x_i$$w_i$:\n$$\n\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1\n$$\n$x_i$$x_j$$C_{jk}=(x_i-x_j)^T(x_i-x_k)$$w_{ij}$close-form solution\n$$\nw_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}\n$$\nLLE$w_i$$x_i$$z_i$:\n$$\n\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2\n$$\n\n$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{'}\\times m}, (W)_{ij}=w_{ij}$\n$$\nM=(I-W)^T(I-W)\n$$\n\n$$\n\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I\n$$\n$M$$d^{'}$$Z^T$\n\n## Metric Learning\nMachine LearningDistance MetricMetric Learningidea\n\n$d$$x_i$$x_j$$L_2$ Distance\n$$\ndist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2\n$$\n$dist_{ij,k}^2$$x_i$$x_j$$k$$w$\n$$\ndist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)\n$$\n$w_i\\geq 0, W=diag(w)$$(W)_{ii}=w_i$\n\nError Ratemust-link$\\mathcal{M}$cannot-link$\\mathcal{C}$$(x_i,x_j)\\in \\mathcal{M}$$x_i$$x_j$$(x_i,x_k)\\in \\mathcal{C}$$x_i$$x_k$\n$$\n\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0\n$$\n$M\\succeq 0$M1\n\n<font color=\"red\">$M$$M$$M$$rank(M)$$d$Metric Learning$P\\in \\mathbb{R}^{d\\times rank(M)}$</font>","source":"_posts/ml-dimen-red-metric-learning.md","raw":"---\ntitle: \"[ML] Dimension Reduction and Metric Learning\"\ndate: 2018-08-20 22:02:55\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Low-Dimension Embedding\nMLcurse of dimensionality\n\n**Why dimension reduction works?** \nEmbedding\n\n### Multiple Dimensional Scaling\nMDS$d^{'}$$Z\\in \\mathbb{R}^{d^{'}\\times m},d^{'}\\leq d$$d^{'}$$||z_i - z_j||=dist_{ij}$\n\n$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$$B$$b_{ij}=z_i^Tz_j$\n$$\ndist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j\n$$\n\n$Z$$\\sum_{i=1}^mz_i=0$$B$0\n$$\n\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\\n\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\\n\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)\n$$\n\n$tr(B)=\\sum_{i=1}^m||z_i||^2$\n$$\ndist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\\n$$\n\n\n$$\nb_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)\n$$\n\n$D$$B$\n\n$B$$B=V\\bigwedge V^T$$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$$V$$d^{\\star}$$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$$V_{\\star}$$Z$\n$$\nZ=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}\n$$\n\n ****$d^{'}\\ll d$$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{'}})$$\\tilde{V}$$Z$\n$$\nZ=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{'}\\times m}\n$$\n\n## PCA\n$\\sum_i x_i=0$$\\{w_1,w_2,\\cdots,w_d\\}$$w_i$$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$$d^{'}< d$$x_i$$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{'}})$$z_{ij}=w_j^Tx_i$$x_i$$j$$z_i$$x_i$$\\hat{x}_i=\\sum_{j=1}^{d^{'}z_{ij}w_j}$\n\n$$\n\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n$x_i$$W^Tx_i$ ****\n\n\n$$\n\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n\n$$\nXX^TW=\\lambda W\n$$\n\n$XX^T$$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$$d^{'}$$W=(w_1,w_2,\\cdots,w_{d^{'}})$PCA\n\n$d-d^{'}$\n\n## Kernel Linear Dimension Reduction\n\n\nKernel Tricks(SVMSVMKernel Tricks)\n\nKernel PCA$W$PCA\n$$\n(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W\n$$\n\n$z_i$$x_i$\n$$\nW=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i\n$$\n$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$$z_i$$x_i$$\\phi$$z_i=\\phi(x_i),i=1,2,\\cdots,m$$\\phi$PCA\n$$\n(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W\n$$\n\n$$\nW=\\sum_{i=1}^m \\phi(x_i) \\alpha_i\n$$\n\n$\\phi$\n$$\n\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)\n$$\n\n\n$$\nKA=\\lambda A\n$$\n$K$$\\mathcal{k}$$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$$K$$d^{'}$\n\n$x$$j (j=1,2,\\cdots,d^{'})$\n$$\nz_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)\n$$\n$\\alpha_i$$\\alpha_i^j$$\\alpha_i$$j$\n\n## Manifold Learning\nManifold<font color=\"red\"></font><font color=\"red\"></font>\n\n### Isometric Mapping\nIsomap\n\n<font color=\"red\"></font><font color=\"red\"></font>\n\n$k$$k$$\\epsilon$$\\epsilon$$\\epsilon$\n\n### Locally Linear Embedding (LLE)\nIsomap<font color=\"red\">LLE</font>$x_i$$x_j,x_k,x_l$\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE$x_i$$Q_i$$Q_i$$x_i$$w_i$:\n$$\n\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1\n$$\n$x_i$$x_j$$C_{jk}=(x_i-x_j)^T(x_i-x_k)$$w_{ij}$close-form solution\n$$\nw_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}\n$$\nLLE$w_i$$x_i$$z_i$:\n$$\n\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2\n$$\n\n$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{'}\\times m}, (W)_{ij}=w_{ij}$\n$$\nM=(I-W)^T(I-W)\n$$\n\n$$\n\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I\n$$\n$M$$d^{'}$$Z^T$\n\n## Metric Learning\nMachine LearningDistance MetricMetric Learningidea\n\n$d$$x_i$$x_j$$L_2$ Distance\n$$\ndist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2\n$$\n$dist_{ij,k}^2$$x_i$$x_j$$k$$w$\n$$\ndist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)\n$$\n$w_i\\geq 0, W=diag(w)$$(W)_{ii}=w_i$\n\nError Ratemust-link$\\mathcal{M}$cannot-link$\\mathcal{C}$$(x_i,x_j)\\in \\mathcal{M}$$x_i$$x_j$$(x_i,x_k)\\in \\mathcal{C}$$x_i$$x_k$\n$$\n\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0\n$$\n$M\\succeq 0$M1\n\n<font color=\"red\">$M$$M$$M$$rank(M)$$d$Metric Learning$P\\in \\mathbb{R}^{d\\times rank(M)}$</font>","slug":"ml-dimen-red-metric-learning","published":1,"updated":"2018-10-01T04:40:08.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cf000v608wanrj1kpm","content":"<h2 id=\"Low-Dimension-Embedding\"><a href=\"#Low-Dimension-Embedding\" class=\"headerlink\" title=\"Low-Dimension Embedding\"></a>Low-Dimension Embedding</h2><p>MLcurse of dimensionality</p>\n<p><strong>Why dimension reduction works?</strong><br>Embedding</p>\n<h3 id=\"Multiple-Dimensional-Scaling\"><a href=\"#Multiple-Dimensional-Scaling\" class=\"headerlink\" title=\"Multiple Dimensional Scaling\"></a>Multiple Dimensional Scaling</h3><p>MDS$d^{}$$Z\\in \\mathbb{R}^{d^{}\\times m},d^{}\\leq d$$d^{}$$||z_i - z_j||=dist_{ij}$</p>\n<p>$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$$B$$b_{ij}=z_i^Tz_j$<br>$$<br>dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j<br>$$</p>\n<p>$Z$$\\sum_{i=1}^mz_i=0$$B$0<br>$$<br>\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\<br>\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\<br>\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)<br>$$</p>\n<p>$tr(B)=\\sum_{i=1}^m||z_i||^2$<br>$$<br>dist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\<br>$$</p>\n<p><br>$$<br>b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)<br>$$</p>\n<p>$D$$B$</p>\n<p>$B$$B=V\\bigwedge V^T$$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$$V$$d^{\\star}$$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$$V_{\\star}$$Z$<br>$$<br>Z=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}<br>$$</p>\n<p> <strong></strong>$d^{}\\ll d$$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{}})$$\\tilde{V}$$Z$<br>$$<br>Z=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{}\\times m}<br>$$</p>\n<h2 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h2><p>$\\sum_i x_i=0$$\\{w_1,w_2,\\cdots,w_d\\}$$w_i$$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$$d^{}&lt; d$$x_i$$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{}})$$z_{ij}=w_j^Tx_i$$x_i$$j$$z_i$$x_i$$\\hat{x}_i=\\sum_{j=1}^{d^{}z_{ij}w_j}$</p>\n<p>$$<br>\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>$x_i$$W^Tx_i$ <strong></strong></p>\n<p><br>$$<br>\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p><br>$$<br>XX^TW=\\lambda W<br>$$</p>\n<p>$XX^T$$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$$d^{}$$W=(w_1,w_2,\\cdots,w_{d^{}})$PCA</p>\n<p>$d-d^{}$</p>\n<h2 id=\"Kernel-Linear-Dimension-Reduction\"><a href=\"#Kernel-Linear-Dimension-Reduction\" class=\"headerlink\" title=\"Kernel Linear Dimension Reduction\"></a>Kernel Linear Dimension Reduction</h2><p></p>\n<p>Kernel Tricks(SVMSVMKernel Tricks)</p>\n<p>Kernel PCA$W$PCA<br>$$<br>(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W<br>$$</p>\n<p>$z_i$$x_i$<br>$$<br>W=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i<br>$$<br>$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$$z_i$$x_i$$\\phi$$z_i=\\phi(x_i),i=1,2,\\cdots,m$$\\phi$PCA<br>$$<br>(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W<br>$$</p>\n<p>$$<br>W=\\sum_{i=1}^m \\phi(x_i) \\alpha_i<br>$$</p>\n<p>$\\phi$<br>$$<br>\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)<br>$$</p>\n<p><br>$$<br>KA=\\lambda A<br>$$<br>$K$$\\mathcal{k}$$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$$K$$d^{}$</p>\n<p>$x$$j (j=1,2,\\cdots,d^{})$<br>$$<br>z_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)<br>$$<br>$\\alpha_i$$\\alpha_i^j$$\\alpha_i$$j$</p>\n<h2 id=\"Manifold-Learning\"><a href=\"#Manifold-Learning\" class=\"headerlink\" title=\"Manifold Learning\"></a>Manifold Learning</h2><p>Manifold<font color=\"red\"></font><font color=\"red\"></font></p>\n<h3 id=\"Isometric-Mapping\"><a href=\"#Isometric-Mapping\" class=\"headerlink\" title=\"Isometric Mapping\"></a>Isometric Mapping</h3><p>Isomap</p>\n<p><font color=\"red\"></font><font color=\"red\"></font></p>\n<p>$k$$k$$\\epsilon$$\\epsilon$$\\epsilon$</p>\n<h3 id=\"Locally-Linear-Embedding-LLE\"><a href=\"#Locally-Linear-Embedding-LLE\" class=\"headerlink\" title=\"Locally Linear Embedding (LLE)\"></a>Locally Linear Embedding (LLE)</h3><p>Isomap<font color=\"red\">LLE</font>$x_i$$x_j,x_k,x_l$<br>$$<br>x_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l<br>$$<br>LLE$x_i$$Q_i$$Q_i$$x_i$$w_i$:<br>$$<br>\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1<br>$$<br>$x_i$$x_j$$C_{jk}=(x_i-x_j)^T(x_i-x_k)$$w_{ij}$close-form solution<br>$$<br>w_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}<br>$$<br>LLE$w_i$$x_i$$z_i$:<br>$$<br>\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2<br>$$</p>\n<p>$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{}\\times m}, (W)_{ij}=w_{ij}$<br>$$<br>M=(I-W)^T(I-W)<br>$$<br><br>$$<br>\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I<br>$$<br>$M$$d^{}$$Z^T$</p>\n<h2 id=\"Metric-Learning\"><a href=\"#Metric-Learning\" class=\"headerlink\" title=\"Metric Learning\"></a>Metric Learning</h2><p>Machine LearningDistance MetricMetric Learningidea</p>\n<p>$d$$x_i$$x_j$$L_2$ Distance<br>$$<br>dist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2<br>$$<br>$dist_{ij,k}^2$$x_i$$x_j$$k$$w$<br>$$<br>dist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)<br>$$<br>$w_i\\geq 0, W=diag(w)$$(W)_{ii}=w_i$</p>\n<p>Error Ratemust-link$\\mathcal{M}$cannot-link$\\mathcal{C}$$(x_i,x_j)\\in \\mathcal{M}$$x_i$$x_j$$(x_i,x_k)\\in \\mathcal{C}$$x_i$$x_k$<br>$$<br>\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0<br>$$<br>$M\\succeq 0$M1</p>\n<p><font color=\"red\">$M$$M$$M$$rank(M)$$d$Metric Learning$P\\in \\mathbb{R}^{d\\times rank(M)}$</font></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Low-Dimension-Embedding\"><a href=\"#Low-Dimension-Embedding\" class=\"headerlink\" title=\"Low-Dimension Embedding\"></a>Low-Dimension Embedding</h2><p>MLcurse of dimensionality</p>\n<p><strong>Why dimension reduction works?</strong><br>Embedding</p>\n<h3 id=\"Multiple-Dimensional-Scaling\"><a href=\"#Multiple-Dimensional-Scaling\" class=\"headerlink\" title=\"Multiple Dimensional Scaling\"></a>Multiple Dimensional Scaling</h3><p>MDS$d^{}$$Z\\in \\mathbb{R}^{d^{}\\times m},d^{}\\leq d$$d^{}$$||z_i - z_j||=dist_{ij}$</p>\n<p>$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$$B$$b_{ij}=z_i^Tz_j$<br>$$<br>dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j<br>$$</p>\n<p>$Z$$\\sum_{i=1}^mz_i=0$$B$0<br>$$<br>\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\<br>\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\<br>\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)<br>$$</p>\n<p>$tr(B)=\\sum_{i=1}^m||z_i||^2$<br>$$<br>dist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\<br>$$</p>\n<p><br>$$<br>b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)<br>$$</p>\n<p>$D$$B$</p>\n<p>$B$$B=V\\bigwedge V^T$$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$$V$$d^{\\star}$$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$$V_{\\star}$$Z$<br>$$<br>Z=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}<br>$$</p>\n<p> <strong></strong>$d^{}\\ll d$$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{}})$$\\tilde{V}$$Z$<br>$$<br>Z=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{}\\times m}<br>$$</p>\n<h2 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h2><p>$\\sum_i x_i=0$$\\{w_1,w_2,\\cdots,w_d\\}$$w_i$$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$$d^{}&lt; d$$x_i$$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{}})$$z_{ij}=w_j^Tx_i$$x_i$$j$$z_i$$x_i$$\\hat{x}_i=\\sum_{j=1}^{d^{}z_{ij}w_j}$</p>\n<p>$$<br>\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>$x_i$$W^Tx_i$ <strong></strong></p>\n<p><br>$$<br>\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p><br>$$<br>XX^TW=\\lambda W<br>$$</p>\n<p>$XX^T$$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$$d^{}$$W=(w_1,w_2,\\cdots,w_{d^{}})$PCA</p>\n<p>$d-d^{}$</p>\n<h2 id=\"Kernel-Linear-Dimension-Reduction\"><a href=\"#Kernel-Linear-Dimension-Reduction\" class=\"headerlink\" title=\"Kernel Linear Dimension Reduction\"></a>Kernel Linear Dimension Reduction</h2><p></p>\n<p>Kernel Tricks(SVMSVMKernel Tricks)</p>\n<p>Kernel PCA$W$PCA<br>$$<br>(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W<br>$$</p>\n<p>$z_i$$x_i$<br>$$<br>W=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i<br>$$<br>$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$$z_i$$x_i$$\\phi$$z_i=\\phi(x_i),i=1,2,\\cdots,m$$\\phi$PCA<br>$$<br>(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W<br>$$</p>\n<p>$$<br>W=\\sum_{i=1}^m \\phi(x_i) \\alpha_i<br>$$</p>\n<p>$\\phi$<br>$$<br>\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)<br>$$</p>\n<p><br>$$<br>KA=\\lambda A<br>$$<br>$K$$\\mathcal{k}$$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$$K$$d^{}$</p>\n<p>$x$$j (j=1,2,\\cdots,d^{})$<br>$$<br>z_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)<br>$$<br>$\\alpha_i$$\\alpha_i^j$$\\alpha_i$$j$</p>\n<h2 id=\"Manifold-Learning\"><a href=\"#Manifold-Learning\" class=\"headerlink\" title=\"Manifold Learning\"></a>Manifold Learning</h2><p>Manifold<font color=\"red\"></font><font color=\"red\"></font></p>\n<h3 id=\"Isometric-Mapping\"><a href=\"#Isometric-Mapping\" class=\"headerlink\" title=\"Isometric Mapping\"></a>Isometric Mapping</h3><p>Isomap</p>\n<p><font color=\"red\"></font><font color=\"red\"></font></p>\n<p>$k$$k$$\\epsilon$$\\epsilon$$\\epsilon$</p>\n<h3 id=\"Locally-Linear-Embedding-LLE\"><a href=\"#Locally-Linear-Embedding-LLE\" class=\"headerlink\" title=\"Locally Linear Embedding (LLE)\"></a>Locally Linear Embedding (LLE)</h3><p>Isomap<font color=\"red\">LLE</font>$x_i$$x_j,x_k,x_l$<br>$$<br>x_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l<br>$$<br>LLE$x_i$$Q_i$$Q_i$$x_i$$w_i$:<br>$$<br>\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1<br>$$<br>$x_i$$x_j$$C_{jk}=(x_i-x_j)^T(x_i-x_k)$$w_{ij}$close-form solution<br>$$<br>w_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}<br>$$<br>LLE$w_i$$x_i$$z_i$:<br>$$<br>\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2<br>$$</p>\n<p>$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{}\\times m}, (W)_{ij}=w_{ij}$<br>$$<br>M=(I-W)^T(I-W)<br>$$<br><br>$$<br>\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I<br>$$<br>$M$$d^{}$$Z^T$</p>\n<h2 id=\"Metric-Learning\"><a href=\"#Metric-Learning\" class=\"headerlink\" title=\"Metric Learning\"></a>Metric Learning</h2><p>Machine LearningDistance MetricMetric Learningidea</p>\n<p>$d$$x_i$$x_j$$L_2$ Distance<br>$$<br>dist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2<br>$$<br>$dist_{ij,k}^2$$x_i$$x_j$$k$$w$<br>$$<br>dist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)<br>$$<br>$w_i\\geq 0, W=diag(w)$$(W)_{ii}=w_i$</p>\n<p>Error Ratemust-link$\\mathcal{M}$cannot-link$\\mathcal{C}$$(x_i,x_j)\\in \\mathcal{M}$$x_i$$x_j$$(x_i,x_k)\\in \\mathcal{C}$$x_i$$x_k$<br>$$<br>\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0<br>$$<br>$M\\succeq 0$M1</p>\n<p><font color=\"red\">$M$$M$$M$$rank(M)$$d$Metric Learning$P\\in \\mathbb{R}^{d\\times rank(M)}$</font></p>\n"},{"title":"[ML] Decision Tree","date":"2018-07-24T02:02:19.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n\n\n## \n__Loss Function__Loss Function\n\n\n\n### \n* Entropy$X$  \n  $P(X=x_i)=p_i$\n    \n  $X$Entropy  \n  $H(X)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy$X$$X$$X$Entropy$H(p)$  \n  $H(p)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy$0\\leq H(p)\\leq logn$\n\n  $X$  \n  $P(X=1)=p, P(X=0)=1-p$  \n  Entropy  \n  $H(p)=-plog_2p-(1-p)log_2(1-p)$\n\n* $H(Y|X)$$X$$Y$$X$$Y$$X$$Y$$X$  \n  $H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$\n\n*  __$X$$Y$__  \n  $A$dataset $D$$g(D,A)$$D$$H(D)$$A$$D$$H(D|A)$  \n  $g(D,A)=H(D)-H(D|A)$  \n\n  $H(Y)$$H(Y|X)$__dataset__\n\n ____ dataset $D$$A$$H(D)$$D$$H(D|A)$$A$$D$$A$$D$\n\ntraining set $D$\n\n#### \ntraining set$D$$|D|$$K$$C_k,k=1,2,\\cdots,K$$|C_k|$$C_k$$\\sum_{k=1}^K|C_k|=|D|$$A$$n$$\\{a_1,a_2,\\cdots,a_n\\}$$A$$D$$n$$D_1,D_2,\\cdots,D_n$$|D_i|$$D_i$$\\sum_{i=1}^n|D_i|=|D|$$D_i$$C_k$$D_{ik}$$D_{ik}=D_i\\cap C_k$$|D_{ik}|$$D_{ik}$\n1. $D$$H(D)$  \n   $H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$\n\n2. $A$$D$$H(D|A)$  \n   $H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$\n\n3.   \n   $g(D,A)=H(D)-H(D|A)$\n\n ____\n\n* $A$training set $D$$g_R(D,A)$$g(D,A)$$D$$A$$H_A(D)$  \n  $g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$  \n  $H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$$n$$A$\n\n## \n### ID3\nID3 ____ __ID3__\n\n1. $D$$C_k$$T$$C_k$$T$\n2. $A=\\varnothing$$T$$D$$C_k$$T$\n3. $A$$D$ ____ $A_g$\n4. $A_g$$\\epsilon$$T$$D$$C_k$$T$\n5. $A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$\n6. $i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$\n\nID3\n\n### C4.5\nC4.5 ____ \n\n1. $D$$C_k$$T$$C_k$$T$\n2. $A=\\varnothing$$T$$D$$C_k$$T$\n3. $A$$D$ ____ ____ $A_g$\n4. $A_g$$\\epsilon$$T$$D$$C_k$$T$\n5. $A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$\n6. $i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$\n\n## \nLoss Function$T$$|T|$$t$$T$$N_t$$k$$N_{tk}$$k=1,2,\\cdots,K$$H_t(T)$$t$$\\alpha\\geq 0$Loss Function  \n$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$  \n  \n$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$  \nLoss Function  \n$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$  \n  \n$C_{\\alpha}(T)=C(T)+\\alpha|T|$\n\n$C(T)$$|T|$$\\alpha\\geq 0$$\\alpha$$\\alpha$\n\n()Loss Function\n\nLoss FunctionLoss Function\n\n### \n1. \n2.   \n   $T_B$$T_A$Loss Function$C_{\\alpha}(T_B)$$C_{\\alpha}(T_A)$ $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$\n3. 2Loss Function$T_{\\alpha}$\n\n## CART\nCART$X$$Y$CART\n\nCART  \n1. training set\n2. validation set\n\n### CART\n__MSEGini Index__ \n\n#### \n$M$$R_1,R_2,\\cdots,R_M$$R_m$$c_m$  \n$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$\n\nMSEtraining setMSE Loss$R_m$$c_m$$\\hat{c}_m$$R_m$$x_i$$y_i$  \n$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$\n\n$j$$x^{(j)}$$s$  \n$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$  $R_2(j,s)=\\{x|x^{(j)}> s\\}$  \n$j$$s$  \n$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n\n$j$$s$  \n$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$  $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$\n\n$j$$(j,s)$\n\n##### \n1. $j$$s$  \n   $\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n   \n   $j$$j$$s$$(j,s)$\n\n2. $(j,s)$  \n   $R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ $R_2(j,s)=\\{x|x^{(j)}> s\\}$\n\n   $\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$\n\n3. 12\n\n4. $M$$R_1,R_2,\\cdots,R_M$  \n   $f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$\n\n#### \nGini Index\n\n* Gini Index: $K$$k$$p_k$Gini Index  \n  $Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$\n\n  1$p$Gini Index  \n  $Gini(p)=2p(1-p)$\n  \n  $D$Gini Index  \n  $Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$\n\n  $C_k$$D$$k$$K$\n\n  $D$$A$$a$$D_1$$D_2$  \n  $D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$\n\n  $A$$D$Gini Index  \n  $Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$\n\n  Gini Index$D$Gini Index $G(D,A)$$A=a$$D$Gini IndexEntropy\n\n##### CART\n1. training set$D$Gini Index$A$$a$$A=a$$D$$D_1$$D_2$$A=a$Gini Index\n2. $A$$a$Gini Indextraining set\n3. 12\n4. CART\n\nGini Index()\n\n#### CART\nCART$T_0$$T_0$$\\{T_0,T_1,\\cdots,T_n\\}$validation set\n\n1.   \n   Loss Function  \n   $C_{\\alpha}(T)=C(T)+\\alpha |T|$  \n   $T$$C(T)$training set(Gini Index)$|T|$$\\alpha \\geq 0$\n\n   $\\alpha$$0=\\alpha_0 < \\alpha_1 < \\alpha_2 < \\cdots < \\alpha_n < +\\infty$$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$$\\{T_0,T_1,\\cdots,T_n\\}$\n\n   $T_0$$T_0$$t$$t$Loss Function  \n   $C_{\\alpha}=C(t)+\\alpha$\n\n   $t$$T_t$Loss Function  \n   $C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$\n\n   $\\alpha=0$$\\alpha$  \n   $C_{\\alpha}(T_t)<C_{\\alpha}(t)$\n\n   $\\alpha$$\\alpha$  \n   $C_{\\alpha}(T_t)=C_{\\alpha}(t)$\n\n   $\\alpha$$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$$T_t$$t$Loss Function$t$$t$$T_t$$T_t$\n\n   $T_0$$t$  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$  \n   Loss$T_0$$g(t)$$T_t$$T_1$$g(t)$$\\alpha_1$$T_1$$[\\alpha_1,\\alpha_2)$$\\alpha$\n\n2. $T_0,T_1,\\cdots,T_n$$T_{\\alpha}$\n   validation set$T_1,\\cdots,T_n$MSEGini Index$T_1,\\cdots,T_n$$\\alpha_1,\\cdots,\\alpha_n$$T_k$$\\alpha_k$\n\n##### CART\n1. $k=0,T=T_0$\n2. $\\alpha=+\\infty$\n3. $t$$C(T_t)$$|T_t|$  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$  \n   $T_t$$t$$C(T_t)$training set$|T_t|$$T_t$\n4. $t$$g(t)=\\alpha$$t$$T$\n5. $k=k+1,\\alpha_k=\\alpha,T_k=T$\n6. $T$4\n7. cross validation\n\n\n\n\n\n","source":"_posts/ml-dt.md","raw":"---\ntitle: \"[ML] Decision Tree\"\ndate: 2018-07-24 10:02:19\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n\n\n## \n__Loss Function__Loss Function\n\n\n\n### \n* Entropy$X$  \n  $P(X=x_i)=p_i$\n    \n  $X$Entropy  \n  $H(X)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy$X$$X$$X$Entropy$H(p)$  \n  $H(p)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy$0\\leq H(p)\\leq logn$\n\n  $X$  \n  $P(X=1)=p, P(X=0)=1-p$  \n  Entropy  \n  $H(p)=-plog_2p-(1-p)log_2(1-p)$\n\n* $H(Y|X)$$X$$Y$$X$$Y$$X$$Y$$X$  \n  $H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$\n\n*  __$X$$Y$__  \n  $A$dataset $D$$g(D,A)$$D$$H(D)$$A$$D$$H(D|A)$  \n  $g(D,A)=H(D)-H(D|A)$  \n\n  $H(Y)$$H(Y|X)$__dataset__\n\n ____ dataset $D$$A$$H(D)$$D$$H(D|A)$$A$$D$$A$$D$\n\ntraining set $D$\n\n#### \ntraining set$D$$|D|$$K$$C_k,k=1,2,\\cdots,K$$|C_k|$$C_k$$\\sum_{k=1}^K|C_k|=|D|$$A$$n$$\\{a_1,a_2,\\cdots,a_n\\}$$A$$D$$n$$D_1,D_2,\\cdots,D_n$$|D_i|$$D_i$$\\sum_{i=1}^n|D_i|=|D|$$D_i$$C_k$$D_{ik}$$D_{ik}=D_i\\cap C_k$$|D_{ik}|$$D_{ik}$\n1. $D$$H(D)$  \n   $H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$\n\n2. $A$$D$$H(D|A)$  \n   $H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$\n\n3.   \n   $g(D,A)=H(D)-H(D|A)$\n\n ____\n\n* $A$training set $D$$g_R(D,A)$$g(D,A)$$D$$A$$H_A(D)$  \n  $g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$  \n  $H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$$n$$A$\n\n## \n### ID3\nID3 ____ __ID3__\n\n1. $D$$C_k$$T$$C_k$$T$\n2. $A=\\varnothing$$T$$D$$C_k$$T$\n3. $A$$D$ ____ $A_g$\n4. $A_g$$\\epsilon$$T$$D$$C_k$$T$\n5. $A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$\n6. $i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$\n\nID3\n\n### C4.5\nC4.5 ____ \n\n1. $D$$C_k$$T$$C_k$$T$\n2. $A=\\varnothing$$T$$D$$C_k$$T$\n3. $A$$D$ ____ ____ $A_g$\n4. $A_g$$\\epsilon$$T$$D$$C_k$$T$\n5. $A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$\n6. $i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$\n\n## \nLoss Function$T$$|T|$$t$$T$$N_t$$k$$N_{tk}$$k=1,2,\\cdots,K$$H_t(T)$$t$$\\alpha\\geq 0$Loss Function  \n$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$  \n  \n$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$  \nLoss Function  \n$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$  \n  \n$C_{\\alpha}(T)=C(T)+\\alpha|T|$\n\n$C(T)$$|T|$$\\alpha\\geq 0$$\\alpha$$\\alpha$\n\n()Loss Function\n\nLoss FunctionLoss Function\n\n### \n1. \n2.   \n   $T_B$$T_A$Loss Function$C_{\\alpha}(T_B)$$C_{\\alpha}(T_A)$ $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$\n3. 2Loss Function$T_{\\alpha}$\n\n## CART\nCART$X$$Y$CART\n\nCART  \n1. training set\n2. validation set\n\n### CART\n__MSEGini Index__ \n\n#### \n$M$$R_1,R_2,\\cdots,R_M$$R_m$$c_m$  \n$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$\n\nMSEtraining setMSE Loss$R_m$$c_m$$\\hat{c}_m$$R_m$$x_i$$y_i$  \n$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$\n\n$j$$x^{(j)}$$s$  \n$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$  $R_2(j,s)=\\{x|x^{(j)}> s\\}$  \n$j$$s$  \n$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n\n$j$$s$  \n$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$  $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$\n\n$j$$(j,s)$\n\n##### \n1. $j$$s$  \n   $\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n   \n   $j$$j$$s$$(j,s)$\n\n2. $(j,s)$  \n   $R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ $R_2(j,s)=\\{x|x^{(j)}> s\\}$\n\n   $\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$\n\n3. 12\n\n4. $M$$R_1,R_2,\\cdots,R_M$  \n   $f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$\n\n#### \nGini Index\n\n* Gini Index: $K$$k$$p_k$Gini Index  \n  $Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$\n\n  1$p$Gini Index  \n  $Gini(p)=2p(1-p)$\n  \n  $D$Gini Index  \n  $Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$\n\n  $C_k$$D$$k$$K$\n\n  $D$$A$$a$$D_1$$D_2$  \n  $D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$\n\n  $A$$D$Gini Index  \n  $Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$\n\n  Gini Index$D$Gini Index $G(D,A)$$A=a$$D$Gini IndexEntropy\n\n##### CART\n1. training set$D$Gini Index$A$$a$$A=a$$D$$D_1$$D_2$$A=a$Gini Index\n2. $A$$a$Gini Indextraining set\n3. 12\n4. CART\n\nGini Index()\n\n#### CART\nCART$T_0$$T_0$$\\{T_0,T_1,\\cdots,T_n\\}$validation set\n\n1.   \n   Loss Function  \n   $C_{\\alpha}(T)=C(T)+\\alpha |T|$  \n   $T$$C(T)$training set(Gini Index)$|T|$$\\alpha \\geq 0$\n\n   $\\alpha$$0=\\alpha_0 < \\alpha_1 < \\alpha_2 < \\cdots < \\alpha_n < +\\infty$$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$$\\{T_0,T_1,\\cdots,T_n\\}$\n\n   $T_0$$T_0$$t$$t$Loss Function  \n   $C_{\\alpha}=C(t)+\\alpha$\n\n   $t$$T_t$Loss Function  \n   $C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$\n\n   $\\alpha=0$$\\alpha$  \n   $C_{\\alpha}(T_t)<C_{\\alpha}(t)$\n\n   $\\alpha$$\\alpha$  \n   $C_{\\alpha}(T_t)=C_{\\alpha}(t)$\n\n   $\\alpha$$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$$T_t$$t$Loss Function$t$$t$$T_t$$T_t$\n\n   $T_0$$t$  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$  \n   Loss$T_0$$g(t)$$T_t$$T_1$$g(t)$$\\alpha_1$$T_1$$[\\alpha_1,\\alpha_2)$$\\alpha$\n\n2. $T_0,T_1,\\cdots,T_n$$T_{\\alpha}$\n   validation set$T_1,\\cdots,T_n$MSEGini Index$T_1,\\cdots,T_n$$\\alpha_1,\\cdots,\\alpha_n$$T_k$$\\alpha_k$\n\n##### CART\n1. $k=0,T=T_0$\n2. $\\alpha=+\\infty$\n3. $t$$C(T_t)$$|T_t|$  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$  \n   $T_t$$t$$C(T_t)$training set$|T_t|$$T_t$\n4. $t$$g(t)=\\alpha$$t$$T$\n5. $k=k+1,\\alpha_k=\\alpha,T_k=T$\n6. $T$4\n7. cross validation\n\n\n\n\n\n","slug":"ml-dt","published":1,"updated":"2018-10-01T04:40:08.993Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03ch000y608w2p2hng7v","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong>Loss Function</strong>Loss Function</p>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ul>\n<li><p>Entropy$X$<br>$P(X=x_i)=p_i$</p>\n<p>$X$Entropy<br>$H(X)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy$X$$X$$X$Entropy$H(p)$<br>$H(p)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy$0\\leq H(p)\\leq logn$</p>\n<p>$X$<br>$P(X=1)=p, P(X=0)=1-p$<br>Entropy<br>$H(p)=-plog_2p-(1-p)log_2(1-p)$</p>\n</li>\n<li><p>$H(Y|X)$$X$$Y$$X$$Y$$X$$Y$$X$<br>$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$</p>\n</li>\n<li><p> <strong>$X$$Y$</strong><br>$A$dataset $D$$g(D,A)$$D$$H(D)$$A$$D$$H(D|A)$<br>$g(D,A)=H(D)-H(D|A)$  </p>\n<p>$H(Y)$$H(Y|X)$<strong>dataset</strong></p>\n</li>\n</ul>\n<p> <strong></strong> dataset $D$$A$$H(D)$$D$$H(D|A)$$A$$D$$A$$D$</p>\n<p>training set $D$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>training set$D$$|D|$$K$$C_k,k=1,2,\\cdots,K$$|C_k|$$C_k$$\\sum_{k=1}^K|C_k|=|D|$$A$$n$$\\{a_1,a_2,\\cdots,a_n\\}$$A$$D$$n$$D_1,D_2,\\cdots,D_n$$|D_i|$$D_i$$\\sum_{i=1}^n|D_i|=|D|$$D_i$$C_k$$D_{ik}$$D_{ik}=D_i\\cap C_k$$|D_{ik}|$$D_{ik}$</p>\n<ol>\n<li><p>$D$$H(D)$<br>$H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$</p>\n</li>\n<li><p>$A$$D$$H(D|A)$<br>$H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$</p>\n</li>\n<li><p><br>$g(D,A)=H(D)-H(D|A)$</p>\n</li>\n</ol>\n<p> <strong></strong></p>\n<ul>\n<li>$A$training set $D$$g_R(D,A)$$g(D,A)$$D$$A$$H_A(D)$<br>$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$<br>$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$$n$$A$</li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"ID3\"><a href=\"#ID3\" class=\"headerlink\" title=\"ID3\"></a>ID3</h3><p>ID3 <strong></strong> <strong>ID3</strong></p>\n<ol>\n<li>$D$$C_k$$T$$C_k$$T$</li>\n<li>$A=\\varnothing$$T$$D$$C_k$$T$</li>\n<li>$A$$D$ <strong></strong> $A_g$</li>\n<li>$A_g$$\\epsilon$$T$$D$$C_k$$T$</li>\n<li>$A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$</li>\n<li>$i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$</li>\n</ol>\n<p>ID3</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a>C4.5</h3><p>C4.5 <strong></strong> </p>\n<ol>\n<li>$D$$C_k$$T$$C_k$$T$</li>\n<li>$A=\\varnothing$$T$$D$$C_k$$T$</li>\n<li>$A$$D$ <strong></strong> <strong></strong> $A_g$</li>\n<li>$A_g$$\\epsilon$$T$$D$$C_k$$T$</li>\n<li>$A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$</li>\n<li>$i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$</li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>Loss Function$T$$|T|$$t$$T$$N_t$$k$$N_{tk}$$k=1,2,\\cdots,K$$H_t(T)$$t$$\\alpha\\geq 0$Loss Function<br>$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$<br><br>$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$<br>Loss Function<br>$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$<br><br>$C_{\\alpha}(T)=C(T)+\\alpha|T|$</p>\n<p>$C(T)$$|T|$$\\alpha\\geq 0$$\\alpha$$\\alpha$</p>\n<p>()Loss Function</p>\n<p>Loss FunctionLoss Function</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ol>\n<li></li>\n<li><br>$T_B$$T_A$Loss Function$C_{\\alpha}(T_B)$$C_{\\alpha}(T_A)$ $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$</li>\n<li>2Loss Function$T_{\\alpha}$</li>\n</ol>\n<h2 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h2><p>CART$X$$Y$CART</p>\n<p>CART  </p>\n<ol>\n<li>training set</li>\n<li>validation set</li>\n</ol>\n<h3 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h3><p><strong>MSEGini Index</strong> </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>$M$$R_1,R_2,\\cdots,R_M$$R_m$$c_m$<br>$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$</p>\n<p>MSEtraining setMSE Loss$R_m$$c_m$$\\hat{c}_m$$R_m$$x_i$$y_i$<br>$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$</p>\n<p>$j$$x^{(j)}$$s$<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$  $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$<br>$j$$s$<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>$j$$s$<br>$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$  $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$</p>\n<p>$j$$(j,s)$</p>\n<h5 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h5><ol>\n<li><p>$j$$s$<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>$j$$j$$s$$(j,s)$</p>\n</li>\n<li><p>$(j,s)$<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$</p>\n<p>$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$</p>\n</li>\n<li><p>12</p>\n</li>\n<li><p>$M$$R_1,R_2,\\cdots,R_M$<br>$f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$</p>\n</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>Gini Index</p>\n<ul>\n<li><p>Gini Index: $K$$k$$p_k$Gini Index<br>$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$</p>\n<p>1$p$Gini Index<br>$Gini(p)=2p(1-p)$</p>\n<p>$D$Gini Index<br>$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$</p>\n<p>$C_k$$D$$k$$K$</p>\n<p>$D$$A$$a$$D_1$$D_2$<br>$D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$</p>\n<p>$A$$D$Gini Index<br>$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$</p>\n<p>Gini Index$D$Gini Index $G(D,A)$$A=a$$D$Gini IndexEntropy</p>\n</li>\n</ul>\n<h5 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h5><ol>\n<li>training set$D$Gini Index$A$$a$$A=a$$D$$D_1$$D_2$$A=a$Gini Index</li>\n<li>$A$$a$Gini Indextraining set</li>\n<li>12</li>\n<li>CART</li>\n</ol>\n<p>Gini Index()</p>\n<h4 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h4><p>CART$T_0$$T_0$$\\{T_0,T_1,\\cdots,T_n\\}$validation set</p>\n<ol>\n<li><p><br>Loss Function<br>$C_{\\alpha}(T)=C(T)+\\alpha |T|$<br>$T$$C(T)$training set(Gini Index)$|T|$$\\alpha \\geq 0$</p>\n<p>$\\alpha$$0=\\alpha_0 &lt; \\alpha_1 &lt; \\alpha_2 &lt; \\cdots &lt; \\alpha_n &lt; +\\infty$$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$$\\{T_0,T_1,\\cdots,T_n\\}$</p>\n<p>$T_0$$T_0$$t$$t$Loss Function<br>$C_{\\alpha}=C(t)+\\alpha$</p>\n<p>$t$$T_t$Loss Function<br>$C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$</p>\n<p>$\\alpha=0$$\\alpha$<br>$C_{\\alpha}(T_t)&lt;C_{\\alpha}(t)$</p>\n<p>$\\alpha$$\\alpha$<br>$C_{\\alpha}(T_t)=C_{\\alpha}(t)$</p>\n<p>$\\alpha$$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$$T_t$$t$Loss Function$t$$t$$T_t$$T_t$</p>\n<p>$T_0$$t$<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$<br>Loss$T_0$$g(t)$$T_t$$T_1$$g(t)$$\\alpha_1$$T_1$$[\\alpha_1,\\alpha_2)$$\\alpha$</p>\n</li>\n<li><p>$T_0,T_1,\\cdots,T_n$$T_{\\alpha}$<br>validation set$T_1,\\cdots,T_n$MSEGini Index$T_1,\\cdots,T_n$$\\alpha_1,\\cdots,\\alpha_n$$T_k$$\\alpha_k$</p>\n</li>\n</ol>\n<h5 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h5><ol>\n<li>$k=0,T=T_0$</li>\n<li>$\\alpha=+\\infty$</li>\n<li>$t$$C(T_t)$$|T_t|$<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$<br>$T_t$$t$$C(T_t)$training set$|T_t|$$T_t$</li>\n<li>$t$$g(t)=\\alpha$$t$$T$</li>\n<li>$k=k+1,\\alpha_k=\\alpha,T_k=T$</li>\n<li>$T$4</li>\n<li>cross validation</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong>Loss Function</strong>Loss Function</p>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ul>\n<li><p>Entropy$X$<br>$P(X=x_i)=p_i$</p>\n<p>$X$Entropy<br>$H(X)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy$X$$X$$X$Entropy$H(p)$<br>$H(p)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy$0\\leq H(p)\\leq logn$</p>\n<p>$X$<br>$P(X=1)=p, P(X=0)=1-p$<br>Entropy<br>$H(p)=-plog_2p-(1-p)log_2(1-p)$</p>\n</li>\n<li><p>$H(Y|X)$$X$$Y$$X$$Y$$X$$Y$$X$<br>$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$</p>\n</li>\n<li><p> <strong>$X$$Y$</strong><br>$A$dataset $D$$g(D,A)$$D$$H(D)$$A$$D$$H(D|A)$<br>$g(D,A)=H(D)-H(D|A)$  </p>\n<p>$H(Y)$$H(Y|X)$<strong>dataset</strong></p>\n</li>\n</ul>\n<p> <strong></strong> dataset $D$$A$$H(D)$$D$$H(D|A)$$A$$D$$A$$D$</p>\n<p>training set $D$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>training set$D$$|D|$$K$$C_k,k=1,2,\\cdots,K$$|C_k|$$C_k$$\\sum_{k=1}^K|C_k|=|D|$$A$$n$$\\{a_1,a_2,\\cdots,a_n\\}$$A$$D$$n$$D_1,D_2,\\cdots,D_n$$|D_i|$$D_i$$\\sum_{i=1}^n|D_i|=|D|$$D_i$$C_k$$D_{ik}$$D_{ik}=D_i\\cap C_k$$|D_{ik}|$$D_{ik}$</p>\n<ol>\n<li><p>$D$$H(D)$<br>$H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$</p>\n</li>\n<li><p>$A$$D$$H(D|A)$<br>$H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$</p>\n</li>\n<li><p><br>$g(D,A)=H(D)-H(D|A)$</p>\n</li>\n</ol>\n<p> <strong></strong></p>\n<ul>\n<li>$A$training set $D$$g_R(D,A)$$g(D,A)$$D$$A$$H_A(D)$<br>$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$<br>$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$$n$$A$</li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"ID3\"><a href=\"#ID3\" class=\"headerlink\" title=\"ID3\"></a>ID3</h3><p>ID3 <strong></strong> <strong>ID3</strong></p>\n<ol>\n<li>$D$$C_k$$T$$C_k$$T$</li>\n<li>$A=\\varnothing$$T$$D$$C_k$$T$</li>\n<li>$A$$D$ <strong></strong> $A_g$</li>\n<li>$A_g$$\\epsilon$$T$$D$$C_k$$T$</li>\n<li>$A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$</li>\n<li>$i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$</li>\n</ol>\n<p>ID3</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a>C4.5</h3><p>C4.5 <strong></strong> </p>\n<ol>\n<li>$D$$C_k$$T$$C_k$$T$</li>\n<li>$A=\\varnothing$$T$$D$$C_k$$T$</li>\n<li>$A$$D$ <strong></strong> <strong></strong> $A_g$</li>\n<li>$A_g$$\\epsilon$$T$$D$$C_k$$T$</li>\n<li>$A_g$$a_i$$A_g=a_i$$D$$D_i$$D_i$$T$$T$</li>\n<li>$i$$D_i$$A-\\{A_g\\}$ 1~5 $T_i$$T_i$</li>\n</ol>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>Loss Function$T$$|T|$$t$$T$$N_t$$k$$N_{tk}$$k=1,2,\\cdots,K$$H_t(T)$$t$$\\alpha\\geq 0$Loss Function<br>$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$<br><br>$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$<br>Loss Function<br>$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$<br><br>$C_{\\alpha}(T)=C(T)+\\alpha|T|$</p>\n<p>$C(T)$$|T|$$\\alpha\\geq 0$$\\alpha$$\\alpha$</p>\n<p>()Loss Function</p>\n<p>Loss FunctionLoss Function</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><ol>\n<li></li>\n<li><br>$T_B$$T_A$Loss Function$C_{\\alpha}(T_B)$$C_{\\alpha}(T_A)$ $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$</li>\n<li>2Loss Function$T_{\\alpha}$</li>\n</ol>\n<h2 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h2><p>CART$X$$Y$CART</p>\n<p>CART  </p>\n<ol>\n<li>training set</li>\n<li>validation set</li>\n</ol>\n<h3 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h3><p><strong>MSEGini Index</strong> </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>$M$$R_1,R_2,\\cdots,R_M$$R_m$$c_m$<br>$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$</p>\n<p>MSEtraining setMSE Loss$R_m$$c_m$$\\hat{c}_m$$R_m$$x_i$$y_i$<br>$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$</p>\n<p>$j$$x^{(j)}$$s$<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$  $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$<br>$j$$s$<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>$j$$s$<br>$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$  $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$</p>\n<p>$j$$(j,s)$</p>\n<h5 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h5><ol>\n<li><p>$j$$s$<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>$j$$j$$s$$(j,s)$</p>\n</li>\n<li><p>$(j,s)$<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$</p>\n<p>$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$</p>\n</li>\n<li><p>12</p>\n</li>\n<li><p>$M$$R_1,R_2,\\cdots,R_M$<br>$f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$</p>\n</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>Gini Index</p>\n<ul>\n<li><p>Gini Index: $K$$k$$p_k$Gini Index<br>$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$</p>\n<p>1$p$Gini Index<br>$Gini(p)=2p(1-p)$</p>\n<p>$D$Gini Index<br>$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$</p>\n<p>$C_k$$D$$k$$K$</p>\n<p>$D$$A$$a$$D_1$$D_2$<br>$D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$</p>\n<p>$A$$D$Gini Index<br>$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$</p>\n<p>Gini Index$D$Gini Index $G(D,A)$$A=a$$D$Gini IndexEntropy</p>\n</li>\n</ul>\n<h5 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h5><ol>\n<li>training set$D$Gini Index$A$$a$$A=a$$D$$D_1$$D_2$$A=a$Gini Index</li>\n<li>$A$$a$Gini Indextraining set</li>\n<li>12</li>\n<li>CART</li>\n</ol>\n<p>Gini Index()</p>\n<h4 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h4><p>CART$T_0$$T_0$$\\{T_0,T_1,\\cdots,T_n\\}$validation set</p>\n<ol>\n<li><p><br>Loss Function<br>$C_{\\alpha}(T)=C(T)+\\alpha |T|$<br>$T$$C(T)$training set(Gini Index)$|T|$$\\alpha \\geq 0$</p>\n<p>$\\alpha$$0=\\alpha_0 &lt; \\alpha_1 &lt; \\alpha_2 &lt; \\cdots &lt; \\alpha_n &lt; +\\infty$$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$$\\{T_0,T_1,\\cdots,T_n\\}$</p>\n<p>$T_0$$T_0$$t$$t$Loss Function<br>$C_{\\alpha}=C(t)+\\alpha$</p>\n<p>$t$$T_t$Loss Function<br>$C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$</p>\n<p>$\\alpha=0$$\\alpha$<br>$C_{\\alpha}(T_t)&lt;C_{\\alpha}(t)$</p>\n<p>$\\alpha$$\\alpha$<br>$C_{\\alpha}(T_t)=C_{\\alpha}(t)$</p>\n<p>$\\alpha$$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$$T_t$$t$Loss Function$t$$t$$T_t$$T_t$</p>\n<p>$T_0$$t$<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$<br>Loss$T_0$$g(t)$$T_t$$T_1$$g(t)$$\\alpha_1$$T_1$$[\\alpha_1,\\alpha_2)$$\\alpha$</p>\n</li>\n<li><p>$T_0,T_1,\\cdots,T_n$$T_{\\alpha}$<br>validation set$T_1,\\cdots,T_n$MSEGini Index$T_1,\\cdots,T_n$$\\alpha_1,\\cdots,\\alpha_n$$T_k$$\\alpha_k$</p>\n</li>\n</ol>\n<h5 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a>CART</h5><ol>\n<li>$k=0,T=T_0$</li>\n<li>$\\alpha=+\\infty$</li>\n<li>$t$$C(T_t)$$|T_t|$<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$<br>$T_t$$t$$C(T_t)$training set$|T_t|$$T_t$</li>\n<li>$t$$g(t)=\\alpha$$t$$T$</li>\n<li>$k=k+1,\\alpha_k=\\alpha,T_k=T$</li>\n<li>$T$4</li>\n<li>cross validation</li>\n</ol>\n"},{"title":"[ML] Ensemble Learning","date":"2018-07-25T02:59:08.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nEnsemble LearningMLTopEnsemble Learning\n\nBase LearnerEnsemble Learningbase learnerbase learnerBoostingBaggingRandom Forests\n\nBoostingbiasBoostingweak learnerbaggingvarianceNNlearner\n\n## Ensemble Learning Algorithm\n### Boosting\nBoosting\n\nBoostingweak learnerweak learner(base learner)weak learnertraining settraining setweak learner algorithmweak learner\n\nAdaBoosttraining setbase learnerbase learnerbase learnerbase learnerbase learner$T$$T$base learnerAdaBoostweighted majority votingweak learnerweak learner\n\n#### AdaBoost\n1. training set\n$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$\n2. $m=1,2,\\cdots,M$\n    * $D_m$training setbase learner  \n$G_m(x):\\chi \\to \\{-1,+1\\}$\n    * $G_m(x)$training set  \n$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$  \n$w_{mi}$$m$$i$\n    * $G_m(x)$  \n$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$\n    * training set  \n$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$  \n$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$  \n    $Z_m$  \n$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$  \n$D_{mi}$\n    * base learner  \n$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$  \n  \n$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$  \n$\\alpha_m$1\n\n#### AdaBoost\nAdaBoostLoss Function\n\n##### \n  \n$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$  \n$b(x;\\gamma_m)$$\\beta_m$\n\ntraining setLoss Function $L(y,f(x))$$f(x)$Loss Function  \n$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$\n\nLoss Function  \n$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$\n\n1. $f_0(x)=0$\n2. $m=1,2,\\cdots,M$\n    * Loss  \n      $(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$  \n      $\\beta_m, \\gamma_m$\n\n    *   \n      $f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$\n3.   \n   $f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$\n\n#### Boosting Tree\nBoosting()BoostingBoosting TreeBoosting Tree  \n$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$  \n$T(x;\\Theta_m)$$\\Theta_m$$T$\n\nBoosting Tree$f_0(x)=0$$m$  \n$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n\n$f_{m-1}(x)$$\\Theta$  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$\n\n$\\chi$$J$$R_1,R_2,\\cdots,R_J$$c_j$  \n$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$  \n$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$$J$\n\nBoosting Tree  \n$$\nf_0(x)=0  \\\\\nf_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\\nf_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)\n$$\n\n$m$$f_{m-1}(x)$  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$  \n$\\hat{\\Theta}_m$$m$\n\nMSE Loss  \n$L(y,f(x))=(y-f(x))^2$\n\n  \n$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$\n\n$r=y-f_{m-1}(x)$ __Boosting Tree__\n\n##### Boosting Tree for Regression\n1. $f_0(x)=0$\n2. $m=1,2,\\cdots,M$\n   * $r_{mi}=y_i-f_{m-1}(x_i)$\n   * $T(x;\\Theta_m)$\n   * $f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n3. Boosting Tree  \n   $f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$\n\n#### Gradient Boosting (GBDT)\nBoosting TreeLoss FunctionMSELossLoss FunctionGradient BoostingGradient DescendLoss Function  \n$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$\n\n\n1. $f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$\n2. $m=1,2,\\cdots,M$\n   * $i=1,2,\\cdots,N$  \n     $r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$\n   * $r_{mi}$$m$$R_{mj},\\quad j=1,2, \\cdots,J$\n   * $j=1,2,\\cdots,J$  \n     $c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$\n   * $f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$\n3.   \n   $$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$\n\n### Bagging and Random Forests\n#### Bagging\nBaggingbootstrap sampling$m$$m$$m$\n\n$T$$m$base learnerbase learnerBaggingmajority votingaveraging\n\nAdaBoostbinary classificationBagging\nBootstrap samplingBaggingbase learner63.2%36.8%validation setout-of-bag estimateBaggingvarianceNN\n\n#### Random Forests\nRandom ForestsBaggingRFDecision Treebase learnerBaggingDecision Tree ____Decision Tree$d$Random Forestsbase decision tree$k$$k$$k=d$base learnerdecision tree$k=1$$k=log_2d$\n\nBaggingbase learner__Random Forestsbase learnerbase learner__\n\nRandom ForestsBaggingbase decision treeBaggingdecision treeRandom Forestsdecision tree\n\n### \nLearner3\n1. training setlearner\n2. learning algorithm\n3. learner\n\n* Averaging\n$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$\n\n* Weighted Averaging\n$H(x)=\\sum_{i=1}^Tw_i h_i(x)$\n$w_i$training setbase learnerWeighted Averagingbase learnerAveraging\n\n* Voting\n    * Majority Voting\n      $$\n      H(x)=\\begin{cases}\n      c_j,\\quad if \\sum_{i=1}^Th_i^j(x)>\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\\nreject, \\quad otherwise\n      \\end{cases}\n      $$\n\n    * Plurality Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$\n      \n      \n    * Weighted Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$\n\n* Stacking\nStackingtraining set learnerlearnerlearner\n\n### \n#### \n* \nNNDecision TreeSVMKNNNaive Bayes ____\n\n* \nbase learnerbase learnerbase learner\n\n* \ntraining sampleFlipping Out\n\n* \nlearnercross validationlearnerlearnerEnsemble LearninglearnerEnsemble Learninglearner","source":"_posts/ml-ensemble.md","raw":"---\ntitle: \"[ML] Ensemble Learning\"\ndate: 2018-07-25 10:59:08\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nEnsemble LearningMLTopEnsemble Learning\n\nBase LearnerEnsemble Learningbase learnerbase learnerBoostingBaggingRandom Forests\n\nBoostingbiasBoostingweak learnerbaggingvarianceNNlearner\n\n## Ensemble Learning Algorithm\n### Boosting\nBoosting\n\nBoostingweak learnerweak learner(base learner)weak learnertraining settraining setweak learner algorithmweak learner\n\nAdaBoosttraining setbase learnerbase learnerbase learnerbase learnerbase learner$T$$T$base learnerAdaBoostweighted majority votingweak learnerweak learner\n\n#### AdaBoost\n1. training set\n$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$\n2. $m=1,2,\\cdots,M$\n    * $D_m$training setbase learner  \n$G_m(x):\\chi \\to \\{-1,+1\\}$\n    * $G_m(x)$training set  \n$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$  \n$w_{mi}$$m$$i$\n    * $G_m(x)$  \n$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$\n    * training set  \n$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$  \n$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$  \n    $Z_m$  \n$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$  \n$D_{mi}$\n    * base learner  \n$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$  \n  \n$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$  \n$\\alpha_m$1\n\n#### AdaBoost\nAdaBoostLoss Function\n\n##### \n  \n$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$  \n$b(x;\\gamma_m)$$\\beta_m$\n\ntraining setLoss Function $L(y,f(x))$$f(x)$Loss Function  \n$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$\n\nLoss Function  \n$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$\n\n1. $f_0(x)=0$\n2. $m=1,2,\\cdots,M$\n    * Loss  \n      $(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$  \n      $\\beta_m, \\gamma_m$\n\n    *   \n      $f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$\n3.   \n   $f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$\n\n#### Boosting Tree\nBoosting()BoostingBoosting TreeBoosting Tree  \n$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$  \n$T(x;\\Theta_m)$$\\Theta_m$$T$\n\nBoosting Tree$f_0(x)=0$$m$  \n$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n\n$f_{m-1}(x)$$\\Theta$  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$\n\n$\\chi$$J$$R_1,R_2,\\cdots,R_J$$c_j$  \n$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$  \n$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$$J$\n\nBoosting Tree  \n$$\nf_0(x)=0  \\\\\nf_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\\nf_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)\n$$\n\n$m$$f_{m-1}(x)$  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$  \n$\\hat{\\Theta}_m$$m$\n\nMSE Loss  \n$L(y,f(x))=(y-f(x))^2$\n\n  \n$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$\n\n$r=y-f_{m-1}(x)$ __Boosting Tree__\n\n##### Boosting Tree for Regression\n1. $f_0(x)=0$\n2. $m=1,2,\\cdots,M$\n   * $r_{mi}=y_i-f_{m-1}(x_i)$\n   * $T(x;\\Theta_m)$\n   * $f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n3. Boosting Tree  \n   $f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$\n\n#### Gradient Boosting (GBDT)\nBoosting TreeLoss FunctionMSELossLoss FunctionGradient BoostingGradient DescendLoss Function  \n$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$\n\n\n1. $f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$\n2. $m=1,2,\\cdots,M$\n   * $i=1,2,\\cdots,N$  \n     $r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$\n   * $r_{mi}$$m$$R_{mj},\\quad j=1,2, \\cdots,J$\n   * $j=1,2,\\cdots,J$  \n     $c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$\n   * $f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$\n3.   \n   $$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$\n\n### Bagging and Random Forests\n#### Bagging\nBaggingbootstrap sampling$m$$m$$m$\n\n$T$$m$base learnerbase learnerBaggingmajority votingaveraging\n\nAdaBoostbinary classificationBagging\nBootstrap samplingBaggingbase learner63.2%36.8%validation setout-of-bag estimateBaggingvarianceNN\n\n#### Random Forests\nRandom ForestsBaggingRFDecision Treebase learnerBaggingDecision Tree ____Decision Tree$d$Random Forestsbase decision tree$k$$k$$k=d$base learnerdecision tree$k=1$$k=log_2d$\n\nBaggingbase learner__Random Forestsbase learnerbase learner__\n\nRandom ForestsBaggingbase decision treeBaggingdecision treeRandom Forestsdecision tree\n\n### \nLearner3\n1. training setlearner\n2. learning algorithm\n3. learner\n\n* Averaging\n$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$\n\n* Weighted Averaging\n$H(x)=\\sum_{i=1}^Tw_i h_i(x)$\n$w_i$training setbase learnerWeighted Averagingbase learnerAveraging\n\n* Voting\n    * Majority Voting\n      $$\n      H(x)=\\begin{cases}\n      c_j,\\quad if \\sum_{i=1}^Th_i^j(x)>\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\\nreject, \\quad otherwise\n      \\end{cases}\n      $$\n\n    * Plurality Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$\n      \n      \n    * Weighted Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$\n\n* Stacking\nStackingtraining set learnerlearnerlearner\n\n### \n#### \n* \nNNDecision TreeSVMKNNNaive Bayes ____\n\n* \nbase learnerbase learnerbase learner\n\n* \ntraining sampleFlipping Out\n\n* \nlearnercross validationlearnerlearnerEnsemble LearninglearnerEnsemble Learninglearner","slug":"ml-ensemble","published":1,"updated":"2018-10-01T04:40:08.997Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cj0010608wwmlxn6on","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Ensemble LearningMLTopEnsemble Learning</p>\n<p>Base LearnerEnsemble Learningbase learnerbase learnerBoostingBaggingRandom Forests</p>\n<p>BoostingbiasBoostingweak learnerbaggingvarianceNNlearner</p>\n<h2 id=\"Ensemble-Learning-Algorithm\"><a href=\"#Ensemble-Learning-Algorithm\" class=\"headerlink\" title=\"Ensemble Learning Algorithm\"></a>Ensemble Learning Algorithm</h2><h3 id=\"Boosting\"><a href=\"#Boosting\" class=\"headerlink\" title=\"Boosting\"></a>Boosting</h3><p>Boosting</p>\n<p>Boostingweak learnerweak learner(base learner)weak learnertraining settraining setweak learner algorithmweak learner</p>\n<p>AdaBoosttraining setbase learnerbase learnerbase learnerbase learnerbase learner$T$$T$base learnerAdaBoostweighted majority votingweak learnerweak learner</p>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><ol>\n<li>training set<br>$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$D_m$training setbase learner<br>$G_m(x):\\chi \\to \\{-1,+1\\}$</li>\n<li>$G_m(x)$training set<br>$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$<br>$w_{mi}$$m$$i$</li>\n<li>$G_m(x)$<br>$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$</li>\n<li>training set<br>$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$<br>$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$<br>$Z_m$<br>$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$<br>$D_{mi}$</li>\n<li>base learner<br>$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$<br><br>$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$<br>$\\alpha_m$1</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><p>AdaBoostLoss Function</p>\n<h5 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h5><p><br>$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$<br>$b(x;\\gamma_m)$$\\beta_m$</p>\n<p>training setLoss Function $L(y,f(x))$$f(x)$Loss Function<br>$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$</p>\n<p>Loss Function<br>$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$</p>\n<ol>\n<li>$f_0(x)=0$</li>\n<li><p>$m=1,2,\\cdots,M$</p>\n<ul>\n<li><p>Loss<br>$(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$<br>$\\beta_m, \\gamma_m$</p>\n</li>\n<li><p><br>$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$</p>\n</li>\n</ul>\n</li>\n<li><br>$f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$</li>\n</ol>\n<h4 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h4><p>Boosting()BoostingBoosting TreeBoosting Tree<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$<br>$T(x;\\Theta_m)$$\\Theta_m$$T$</p>\n<p>Boosting Tree$f_0(x)=0$$m$<br>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</p>\n<p>$f_{m-1}(x)$$\\Theta$<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$</p>\n<p>$\\chi$$J$$R_1,R_2,\\cdots,R_J$$c_j$<br>$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$<br>$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$$J$</p>\n<p>Boosting Tree<br>$$<br>f_0(x)=0  \\\\<br>f_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\<br>f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)<br>$$</p>\n<p>$m$$f_{m-1}(x)$<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$<br>$\\hat{\\Theta}_m$$m$</p>\n<p>MSE Loss<br>$L(y,f(x))=(y-f(x))^2$</p>\n<p><br>$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$</p>\n<p>$r=y-f_{m-1}(x)$ <strong>Boosting Tree</strong></p>\n<h5 id=\"Boosting-Tree-for-Regression\"><a href=\"#Boosting-Tree-for-Regression\" class=\"headerlink\" title=\"Boosting Tree for Regression\"></a>Boosting Tree for Regression</h5><ol>\n<li>$f_0(x)=0$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$r_{mi}=y_i-f_{m-1}(x_i)$</li>\n<li>$T(x;\\Theta_m)$</li>\n<li>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</li>\n</ul>\n</li>\n<li>Boosting Tree<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$</li>\n</ol>\n<h4 id=\"Gradient-Boosting-GBDT\"><a href=\"#Gradient-Boosting-GBDT\" class=\"headerlink\" title=\"Gradient Boosting (GBDT)\"></a>Gradient Boosting (GBDT)</h4><p>Boosting TreeLoss FunctionMSELossLoss FunctionGradient BoostingGradient DescendLoss Function<br>$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$<br></p>\n<ol>\n<li>$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$i=1,2,\\cdots,N$<br>$r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$</li>\n<li>$r_{mi}$$m$$R_{mj},\\quad j=1,2, \\cdots,J$</li>\n<li>$j=1,2,\\cdots,J$<br>$c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$</li>\n<li>$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$</li>\n</ul>\n</li>\n<li><br>$$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$</li>\n</ol>\n<h3 id=\"Bagging-and-Random-Forests\"><a href=\"#Bagging-and-Random-Forests\" class=\"headerlink\" title=\"Bagging and Random Forests\"></a>Bagging and Random Forests</h3><h4 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h4><p>Baggingbootstrap sampling$m$$m$$m$</p>\n<p>$T$$m$base learnerbase learnerBaggingmajority votingaveraging</p>\n<p>AdaBoostbinary classificationBagging<br>Bootstrap samplingBaggingbase learner63.2%36.8%validation setout-of-bag estimateBaggingvarianceNN</p>\n<h4 id=\"Random-Forests\"><a href=\"#Random-Forests\" class=\"headerlink\" title=\"Random Forests\"></a>Random Forests</h4><p>Random ForestsBaggingRFDecision Treebase learnerBaggingDecision Tree <strong></strong>Decision Tree$d$Random Forestsbase decision tree$k$$k$$k=d$base learnerdecision tree$k=1$$k=log_2d$</p>\n<p>Baggingbase learner<strong>Random Forestsbase learnerbase learner</strong></p>\n<p>Random ForestsBaggingbase decision treeBaggingdecision treeRandom Forestsdecision tree</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Learner3</p>\n<ol>\n<li>training setlearner</li>\n<li>learning algorithm</li>\n<li>learner</li>\n</ol>\n<ul>\n<li><p>Averaging<br>$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$</p>\n</li>\n<li><p>Weighted Averaging<br>$H(x)=\\sum_{i=1}^Tw_i h_i(x)$<br>$w_i$training setbase learnerWeighted Averagingbase learnerAveraging</p>\n</li>\n<li><p>Voting</p>\n<ul>\n<li><p>Majority Voting<br>$$<br>H(x)=\\begin{cases}<br>c_j,\\quad if \\sum_{i=1}^Th_i^j(x)&gt;\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\<br>reject, \\quad otherwise<br>\\end{cases}<br>$$</p>\n</li>\n<li><p>Plurality Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$<br></p>\n</li>\n<li><p>Weighted Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$</p>\n</li>\n</ul>\n</li>\n<li><p>Stacking<br>Stackingtraining set learnerlearnerlearner</p>\n</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><ul>\n<li><p><br>NNDecision TreeSVMKNNNaive Bayes <strong></strong></p>\n</li>\n<li><p><br>base learnerbase learnerbase learner</p>\n</li>\n<li><p><br>training sampleFlipping Out</p>\n</li>\n<li><p><br>learnercross validationlearnerlearnerEnsemble LearninglearnerEnsemble Learninglearner</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Ensemble LearningMLTopEnsemble Learning</p>\n<p>Base LearnerEnsemble Learningbase learnerbase learnerBoostingBaggingRandom Forests</p>\n<p>BoostingbiasBoostingweak learnerbaggingvarianceNNlearner</p>\n<h2 id=\"Ensemble-Learning-Algorithm\"><a href=\"#Ensemble-Learning-Algorithm\" class=\"headerlink\" title=\"Ensemble Learning Algorithm\"></a>Ensemble Learning Algorithm</h2><h3 id=\"Boosting\"><a href=\"#Boosting\" class=\"headerlink\" title=\"Boosting\"></a>Boosting</h3><p>Boosting</p>\n<p>Boostingweak learnerweak learner(base learner)weak learnertraining settraining setweak learner algorithmweak learner</p>\n<p>AdaBoosttraining setbase learnerbase learnerbase learnerbase learnerbase learner$T$$T$base learnerAdaBoostweighted majority votingweak learnerweak learner</p>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><ol>\n<li>training set<br>$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$D_m$training setbase learner<br>$G_m(x):\\chi \\to \\{-1,+1\\}$</li>\n<li>$G_m(x)$training set<br>$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$<br>$w_{mi}$$m$$i$</li>\n<li>$G_m(x)$<br>$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$</li>\n<li>training set<br>$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$<br>$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$<br>$Z_m$<br>$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$<br>$D_{mi}$</li>\n<li>base learner<br>$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$<br><br>$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$<br>$\\alpha_m$1</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><p>AdaBoostLoss Function</p>\n<h5 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h5><p><br>$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$<br>$b(x;\\gamma_m)$$\\beta_m$</p>\n<p>training setLoss Function $L(y,f(x))$$f(x)$Loss Function<br>$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$</p>\n<p>Loss Function<br>$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$</p>\n<ol>\n<li>$f_0(x)=0$</li>\n<li><p>$m=1,2,\\cdots,M$</p>\n<ul>\n<li><p>Loss<br>$(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$<br>$\\beta_m, \\gamma_m$</p>\n</li>\n<li><p><br>$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$</p>\n</li>\n</ul>\n</li>\n<li><br>$f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$</li>\n</ol>\n<h4 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h4><p>Boosting()BoostingBoosting TreeBoosting Tree<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$<br>$T(x;\\Theta_m)$$\\Theta_m$$T$</p>\n<p>Boosting Tree$f_0(x)=0$$m$<br>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</p>\n<p>$f_{m-1}(x)$$\\Theta$<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$</p>\n<p>$\\chi$$J$$R_1,R_2,\\cdots,R_J$$c_j$<br>$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$<br>$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$$J$</p>\n<p>Boosting Tree<br>$$<br>f_0(x)=0  \\\\<br>f_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\<br>f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)<br>$$</p>\n<p>$m$$f_{m-1}(x)$<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$<br>$\\hat{\\Theta}_m$$m$</p>\n<p>MSE Loss<br>$L(y,f(x))=(y-f(x))^2$</p>\n<p><br>$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$</p>\n<p>$r=y-f_{m-1}(x)$ <strong>Boosting Tree</strong></p>\n<h5 id=\"Boosting-Tree-for-Regression\"><a href=\"#Boosting-Tree-for-Regression\" class=\"headerlink\" title=\"Boosting Tree for Regression\"></a>Boosting Tree for Regression</h5><ol>\n<li>$f_0(x)=0$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$r_{mi}=y_i-f_{m-1}(x_i)$</li>\n<li>$T(x;\\Theta_m)$</li>\n<li>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</li>\n</ul>\n</li>\n<li>Boosting Tree<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$</li>\n</ol>\n<h4 id=\"Gradient-Boosting-GBDT\"><a href=\"#Gradient-Boosting-GBDT\" class=\"headerlink\" title=\"Gradient Boosting (GBDT)\"></a>Gradient Boosting (GBDT)</h4><p>Boosting TreeLoss FunctionMSELossLoss FunctionGradient BoostingGradient DescendLoss Function<br>$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$<br></p>\n<ol>\n<li>$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$</li>\n<li>$m=1,2,\\cdots,M$<ul>\n<li>$i=1,2,\\cdots,N$<br>$r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$</li>\n<li>$r_{mi}$$m$$R_{mj},\\quad j=1,2, \\cdots,J$</li>\n<li>$j=1,2,\\cdots,J$<br>$c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$</li>\n<li>$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$</li>\n</ul>\n</li>\n<li><br>$$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$</li>\n</ol>\n<h3 id=\"Bagging-and-Random-Forests\"><a href=\"#Bagging-and-Random-Forests\" class=\"headerlink\" title=\"Bagging and Random Forests\"></a>Bagging and Random Forests</h3><h4 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h4><p>Baggingbootstrap sampling$m$$m$$m$</p>\n<p>$T$$m$base learnerbase learnerBaggingmajority votingaveraging</p>\n<p>AdaBoostbinary classificationBagging<br>Bootstrap samplingBaggingbase learner63.2%36.8%validation setout-of-bag estimateBaggingvarianceNN</p>\n<h4 id=\"Random-Forests\"><a href=\"#Random-Forests\" class=\"headerlink\" title=\"Random Forests\"></a>Random Forests</h4><p>Random ForestsBaggingRFDecision Treebase learnerBaggingDecision Tree <strong></strong>Decision Tree$d$Random Forestsbase decision tree$k$$k$$k=d$base learnerdecision tree$k=1$$k=log_2d$</p>\n<p>Baggingbase learner<strong>Random Forestsbase learnerbase learner</strong></p>\n<p>Random ForestsBaggingbase decision treeBaggingdecision treeRandom Forestsdecision tree</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Learner3</p>\n<ol>\n<li>training setlearner</li>\n<li>learning algorithm</li>\n<li>learner</li>\n</ol>\n<ul>\n<li><p>Averaging<br>$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$</p>\n</li>\n<li><p>Weighted Averaging<br>$H(x)=\\sum_{i=1}^Tw_i h_i(x)$<br>$w_i$training setbase learnerWeighted Averagingbase learnerAveraging</p>\n</li>\n<li><p>Voting</p>\n<ul>\n<li><p>Majority Voting<br>$$<br>H(x)=\\begin{cases}<br>c_j,\\quad if \\sum_{i=1}^Th_i^j(x)&gt;\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\<br>reject, \\quad otherwise<br>\\end{cases}<br>$$</p>\n</li>\n<li><p>Plurality Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$<br></p>\n</li>\n<li><p>Weighted Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$</p>\n</li>\n</ul>\n</li>\n<li><p>Stacking<br>Stackingtraining set learnerlearnerlearner</p>\n</li>\n</ul>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><ul>\n<li><p><br>NNDecision TreeSVMKNNNaive Bayes <strong></strong></p>\n</li>\n<li><p><br>base learnerbase learnerbase learner</p>\n</li>\n<li><p><br>training sampleFlipping Out</p>\n</li>\n<li><p><br>learnercross validationlearnerlearnerEnsemble LearninglearnerEnsemble Learninglearner</p>\n</li>\n</ul>\n"},{"title":"[ML] Feature Engineering in Machine Learning","date":"2018-08-20T11:19:59.000Z","catalog":true,"mathjax":true,"catagories":["Algorithm","Machine Learning","Feature Engineering"],"_content":"## Introduction\nFeature Engineering  Machine Learning ****** + **researchML\n\n## Numerical Features\n### Scale\nExamples include **k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance**. For these models and modeling components, **it is often a good idea to normalize the features so that the output stays on an expected scale**.\n\nLogical functions, on the other hand, are not sensitive to input feature scale**. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). **Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sorteventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.\n\n### Distribution\nIt's also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. **One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian**.\n\n### Quantization\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nIn order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: **fixed-width** or **adaptive**. We will give an example of each type.\n\n### Fixed-width binning\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 09 years old in bin 1, 1019 years in bin 2, etc. **To map from the count to the bin, we simply divide by the width of the bin and take the integer part**.\n\nIts also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 09, 1099, 100999, 10009999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.\n\n### Quantile binning\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.\n\n### Log Transformation\nThe log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.\n\n### Power Transforms: Generalization of the Log\nTransform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n$$\n\\tilde{x}=\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda} & if \\lambda \\neq 0,\\\\\nln(x) & if \\lambda = 0\n\\end{cases}\n$$\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.\n\n### Feature Scaling or Normalization\nSmooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldnt care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.\n\n#### Min-Max Scaling\n$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$\n\n#### Standardization (Variance Scaling)\n$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$\n\nIt subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.\n\n> Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn't appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.\n\n#### $L^2$ Normalization\n$\\tilde{x}=\\frac{x}{||x||_2}$\n\n### Feature Selection\nFeature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training timein fact, some techniques increase overall training timebut about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\n#### Filtering\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\n#### Wrapper methods\nThese techniques are expensive, but they allow you to try out subsets of features, which means you wont accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\n#### Embedded methods\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so its also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\n## Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n### Encoding Categorical Variables\n#### One-Hot Encoding\nA better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be \"on.\" This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.\n\n#### Dummy Coding\nThe problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k1. Dummy coding removes the extra degree of freedom by using only k1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\n#### Effect Coding\nYet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all 1's.\n\n#### Pros and Cons of Categorical Variable Encodings \nOne-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all 1s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\n### Dealing with Large Categorical Variables\nExisting solutions can be categorized as follows:\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n2. Compress the features. There are two choices: \n    * Feature hashing, popular with linear models \n    * Bin counting, popular with linear models as well as trees\n\n#### Feature Hashing\nThe idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.\n\n#### What about rare categories?\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.\n\n![Black-off Bin](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png)\n\nThere is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.\n\n#### Counts without bounds\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model \"knows\" the input data up to the observed scale.\n\nFor this reason, **it is often better to use normalized counts that are guaranteed to be bounded in a known interval**. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\n### Summary\n#### Plain one-hot encoding\n**Space requirement** $O(n)$ using the sparse vector format, where n is the number of data points.\n\n**Computation requirement** $O(nk)$ under a linear model, where k is the number of categories.\n\n**Pros**\n* Easiest to implement\n* Potentially most accurate\n* Feasible for online learning\n\n**Cons**\n* Computationally inefficient\n* Does not adapt to growing categories\n* Not feasible for anything other than linear models\n* Requires large-scale distributed optimization with truly\nlarge datasets\n\n#### Feature hashing\n**Space requirement** $O(n)$ using the sparse matrix format, where n is the number of data points.\n\n**Computation requirement** $O(nm)$ under a linear or kernel model, where m is the number of hash bins.\n\n**Pros** \n* Easy to implement\n* Makes model training cheaper\n* Easily adaptable to new categories\n* Easily handles rare categories\n* Feasible for online learning\n\n**Cons**\n* Only suitable for linear or kernelized models\n* Hashed features not interpretable\n* Mixed reports of accuracy\n\n#### Bin-counting\n**Space requirement** $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.\n\n**Computation requirement** $O(n)$ for linear models; also usable for nonlinear models such as trees.\n\n**Pros** \n* Smallest computational burden at training time\n* Enables tree-based models\n* Relatively easy to adapt to new categories\n* Handles rare categories with back-off or count-min sketch\n* Interpretable\n\n**Cons**\n* Requires historical data\n* Delayed updates required, not completely suitable for online\nlearning\n* Higher potential for leakage\n\n\n## Dimensionality Reduction: Squashing the Data Pancake with PCA","source":"_posts/ml-feml.md","raw":"---\ntitle: \"[ML] Feature Engineering in Machine Learning\"\ndate: 2018-08-20 19:19:59\ncatalog: true\nmathjax: true\ntags:\n- Feature Engineering\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Feature Engineering\n---\n## Introduction\nFeature Engineering  Machine Learning ****** + **researchML\n\n## Numerical Features\n### Scale\nExamples include **k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance**. For these models and modeling components, **it is often a good idea to normalize the features so that the output stays on an expected scale**.\n\nLogical functions, on the other hand, are not sensitive to input feature scale**. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). **Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sorteventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.\n\n### Distribution\nIt's also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. **One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian**.\n\n### Quantization\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nIn order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: **fixed-width** or **adaptive**. We will give an example of each type.\n\n### Fixed-width binning\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 09 years old in bin 1, 1019 years in bin 2, etc. **To map from the count to the bin, we simply divide by the width of the bin and take the integer part**.\n\nIts also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 09, 1099, 100999, 10009999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.\n\n### Quantile binning\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.\n\n### Log Transformation\nThe log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.\n\n### Power Transforms: Generalization of the Log\nTransform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n$$\n\\tilde{x}=\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda} & if \\lambda \\neq 0,\\\\\nln(x) & if \\lambda = 0\n\\end{cases}\n$$\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.\n\n### Feature Scaling or Normalization\nSmooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldnt care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.\n\n#### Min-Max Scaling\n$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$\n\n#### Standardization (Variance Scaling)\n$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$\n\nIt subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.\n\n> Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn't appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.\n\n#### $L^2$ Normalization\n$\\tilde{x}=\\frac{x}{||x||_2}$\n\n### Feature Selection\nFeature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training timein fact, some techniques increase overall training timebut about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\n#### Filtering\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\n#### Wrapper methods\nThese techniques are expensive, but they allow you to try out subsets of features, which means you wont accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\n#### Embedded methods\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so its also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\n## Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n### Encoding Categorical Variables\n#### One-Hot Encoding\nA better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be \"on.\" This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.\n\n#### Dummy Coding\nThe problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k1. Dummy coding removes the extra degree of freedom by using only k1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\n#### Effect Coding\nYet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all 1's.\n\n#### Pros and Cons of Categorical Variable Encodings \nOne-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all 1s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\n### Dealing with Large Categorical Variables\nExisting solutions can be categorized as follows:\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n2. Compress the features. There are two choices: \n    * Feature hashing, popular with linear models \n    * Bin counting, popular with linear models as well as trees\n\n#### Feature Hashing\nThe idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.\n\n#### What about rare categories?\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.\n\n![Black-off Bin](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png)\n\nThere is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.\n\n#### Counts without bounds\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model \"knows\" the input data up to the observed scale.\n\nFor this reason, **it is often better to use normalized counts that are guaranteed to be bounded in a known interval**. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\n### Summary\n#### Plain one-hot encoding\n**Space requirement** $O(n)$ using the sparse vector format, where n is the number of data points.\n\n**Computation requirement** $O(nk)$ under a linear model, where k is the number of categories.\n\n**Pros**\n* Easiest to implement\n* Potentially most accurate\n* Feasible for online learning\n\n**Cons**\n* Computationally inefficient\n* Does not adapt to growing categories\n* Not feasible for anything other than linear models\n* Requires large-scale distributed optimization with truly\nlarge datasets\n\n#### Feature hashing\n**Space requirement** $O(n)$ using the sparse matrix format, where n is the number of data points.\n\n**Computation requirement** $O(nm)$ under a linear or kernel model, where m is the number of hash bins.\n\n**Pros** \n* Easy to implement\n* Makes model training cheaper\n* Easily adaptable to new categories\n* Easily handles rare categories\n* Feasible for online learning\n\n**Cons**\n* Only suitable for linear or kernelized models\n* Hashed features not interpretable\n* Mixed reports of accuracy\n\n#### Bin-counting\n**Space requirement** $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.\n\n**Computation requirement** $O(n)$ for linear models; also usable for nonlinear models such as trees.\n\n**Pros** \n* Smallest computational burden at training time\n* Enables tree-based models\n* Relatively easy to adapt to new categories\n* Handles rare categories with back-off or count-min sketch\n* Interpretable\n\n**Cons**\n* Requires historical data\n* Delayed updates required, not completely suitable for online\nlearning\n* Higher potential for leakage\n\n\n## Dimensionality Reduction: Squashing the Data Pancake with PCA","slug":"ml-feml","published":1,"updated":"2018-10-01T04:40:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cl0013608wsc34nbfi","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Feature Engineering  Machine Learning <em></em><em></em><em></em> + <em></em>researchML</p>\n<h2 id=\"Numerical-Features\"><a href=\"#Numerical-Features\" class=\"headerlink\" title=\"Numerical Features\"></a>Numerical Features</h2><h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>Examples include <strong>k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance</strong>. For these models and modeling components, <strong>it is often a good idea to normalize the features so that the output stays on an expected scale</strong>.</p>\n<p>Logical functions, on the other hand, are not sensitive to input feature scale<strong>. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). </strong>Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sorteventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.</p>\n<h3 id=\"Distribution\"><a href=\"#Distribution\" class=\"headerlink\" title=\"Distribution\"></a>Distribution</h3><p>Its also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. <strong>One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian</strong>.</p>\n<h3 id=\"Quantization\"><a href=\"#Quantization\" class=\"headerlink\" title=\"Quantization\"></a>Quantization</h3><p>Raw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.</p>\n<p>In order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: <strong>fixed-width</strong> or <strong>adaptive</strong>. We will give an example of each type.</p>\n<h3 id=\"Fixed-width-binning\"><a href=\"#Fixed-width-binning\" class=\"headerlink\" title=\"Fixed-width binning\"></a>Fixed-width binning</h3><p>With fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 09 years old in bin 1, 1019 years in bin 2, etc. <strong>To map from the count to the bin, we simply divide by the width of the bin and take the integer part</strong>.</p>\n<p>Its also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 09, 1099, 100999, 10009999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.</p>\n<h3 id=\"Quantile-binning\"><a href=\"#Quantile-binning\" class=\"headerlink\" title=\"Quantile binning\"></a>Quantile binning</h3><p>Fixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.</p>\n<h3 id=\"Log-Transformation\"><a href=\"#Log-Transformation\" class=\"headerlink\" title=\"Log Transformation\"></a>Log Transformation</h3><p>The log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.</p>\n<h3 id=\"Power-Transforms-Generalization-of-the-Log\"><a href=\"#Power-Transforms-Generalization-of-the-Log\" class=\"headerlink\" title=\"Power Transforms: Generalization of the Log\"></a>Power Transforms: Generalization of the Log</h3><p>Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.</p>\n<p>A simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:<br>$$<br>\\tilde{x}=<br>\\begin{cases}<br>\\frac{x^{\\lambda}-1}{\\lambda} &amp; if \\lambda \\neq 0,\\\\<br>ln(x) &amp; if \\lambda = 0<br>\\end{cases}<br>$$</p>\n<p>The Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.</p>\n<h3 id=\"Feature-Scaling-or-Normalization\"><a href=\"#Feature-Scaling-or-Normalization\" class=\"headerlink\" title=\"Feature Scaling or Normalization\"></a>Feature Scaling or Normalization</h3><p>Smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldnt care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.</p>\n<h4 id=\"Min-Max-Scaling\"><a href=\"#Min-Max-Scaling\" class=\"headerlink\" title=\"Min-Max Scaling\"></a>Min-Max Scaling</h4><p>$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$</p>\n<h4 id=\"Standardization-Variance-Scaling\"><a href=\"#Standardization-Variance-Scaling\" class=\"headerlink\" title=\"Standardization (Variance Scaling)\"></a>Standardization (Variance Scaling)</h4><p>$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$</p>\n<p>It subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.</p>\n<blockquote>\n<p>Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didnt appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.</p>\n</blockquote>\n<h4 id=\"L-2-Normalization\"><a href=\"#L-2-Normalization\" class=\"headerlink\" title=\"$L^2$ Normalization\"></a>$L^2$ Normalization</h4><p>$\\tilde{x}=\\frac{x}{||x||_2}$</p>\n<h3 id=\"Feature-Selection\"><a href=\"#Feature-Selection\" class=\"headerlink\" title=\"Feature Selection\"></a>Feature Selection</h3><p>Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training timein fact, some techniques increase overall training timebut about reducing model scoring time.</p>\n<p>Roughly speaking, feature selection techniques fall into three classes:</p>\n<h4 id=\"Filtering\"><a href=\"#Filtering\" class=\"headerlink\" title=\"Filtering\"></a>Filtering</h4><p>Filtering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.</p>\n<h4 id=\"Wrapper-methods\"><a href=\"#Wrapper-methods\" class=\"headerlink\" title=\"Wrapper methods\"></a>Wrapper methods</h4><p>These techniques are expensive, but they allow you to try out subsets of features, which means you wont accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.</p>\n<h4 id=\"Embedded-methods\"><a href=\"#Embedded-methods\" class=\"headerlink\" title=\"Embedded methods\"></a>Embedded methods</h4><p>These methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so its also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.</p>\n<h2 id=\"Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\"><a href=\"#Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\" class=\"headerlink\" title=\"Categorical Variables: Counting Eggs in the Age of Robotic Chickens\"></a>Categorical Variables: Counting Eggs in the Age of Robotic Chickens</h2><h3 id=\"Encoding-Categorical-Variables\"><a href=\"#Encoding-Categorical-Variables\" class=\"headerlink\" title=\"Encoding Categorical Variables\"></a>Encoding Categorical Variables</h3><h4 id=\"One-Hot-Encoding\"><a href=\"#One-Hot-Encoding\" class=\"headerlink\" title=\"One-Hot Encoding\"></a>One-Hot Encoding</h4><p>A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be on. This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.</p>\n<h4 id=\"Dummy-Coding\"><a href=\"#Dummy-Coding\" class=\"headerlink\" title=\"Dummy Coding\"></a>Dummy Coding</h4><p>The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k1. Dummy coding removes the extra degree of freedom by using only k1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.</p>\n<h4 id=\"Effect-Coding\"><a href=\"#Effect-Coding\" class=\"headerlink\" title=\"Effect Coding\"></a>Effect Coding</h4><p>Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all 1s.</p>\n<h4 id=\"Pros-and-Cons-of-Categorical-Variable-Encodings\"><a href=\"#Pros-and-Cons-of-Categorical-Variable-Encodings\" class=\"headerlink\" title=\"Pros and Cons of Categorical Variable Encodings\"></a>Pros and Cons of Categorical Variable Encodings</h4><p>One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all 1s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.</p>\n<h3 id=\"Dealing-with-Large-Categorical-Variables\"><a href=\"#Dealing-with-Large-Categorical-Variables\" class=\"headerlink\" title=\"Dealing with Large Categorical Variables\"></a>Dealing with Large Categorical Variables</h3><p>Existing solutions can be categorized as follows:</p>\n<ol>\n<li>Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.</li>\n<li>Compress the features. There are two choices: <ul>\n<li>Feature hashing, popular with linear models </li>\n<li>Bin counting, popular with linear models as well as trees</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"Feature-Hashing\"><a href=\"#Feature-Hashing\" class=\"headerlink\" title=\"Feature Hashing\"></a>Feature Hashing</h4><p>The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.</p>\n<h4 id=\"What-about-rare-categories\"><a href=\"#What-about-rare-categories\" class=\"headerlink\" title=\"What about rare categories?\"></a>What about rare categories?</h4><p>One way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png\" alt=\"Black-off Bin\"></p>\n<p>There is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.</p>\n<h4 id=\"Counts-without-bounds\"><a href=\"#Counts-without-bounds\" class=\"headerlink\" title=\"Counts without bounds\"></a>Counts without bounds</h4><p>If the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model knows the input data up to the observed scale.</p>\n<p>For this reason, <strong>it is often better to use normalized counts that are guaranteed to be bounded in a known interval</strong>. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.</p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><h4 id=\"Plain-one-hot-encoding\"><a href=\"#Plain-one-hot-encoding\" class=\"headerlink\" title=\"Plain one-hot encoding\"></a>Plain one-hot encoding</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse vector format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nk)$ under a linear model, where k is the number of categories.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easiest to implement</li>\n<li>Potentially most accurate</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Computationally inefficient</li>\n<li>Does not adapt to growing categories</li>\n<li>Not feasible for anything other than linear models</li>\n<li>Requires large-scale distributed optimization with truly<br>large datasets</li>\n</ul>\n<h4 id=\"Feature-hashing\"><a href=\"#Feature-hashing\" class=\"headerlink\" title=\"Feature hashing\"></a>Feature hashing</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse matrix format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nm)$ under a linear or kernel model, where m is the number of hash bins.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Easy to implement</li>\n<li>Makes model training cheaper</li>\n<li>Easily adaptable to new categories</li>\n<li>Easily handles rare categories</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Only suitable for linear or kernelized models</li>\n<li>Hashed features not interpretable</li>\n<li>Mixed reports of accuracy</li>\n</ul>\n<h4 id=\"Bin-counting\"><a href=\"#Bin-counting\" class=\"headerlink\" title=\"Bin-counting\"></a>Bin-counting</h4><p><strong>Space requirement</strong> $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.</p>\n<p><strong>Computation requirement</strong> $O(n)$ for linear models; also usable for nonlinear models such as trees.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Smallest computational burden at training time</li>\n<li>Enables tree-based models</li>\n<li>Relatively easy to adapt to new categories</li>\n<li>Handles rare categories with back-off or count-min sketch</li>\n<li>Interpretable</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Requires historical data</li>\n<li>Delayed updates required, not completely suitable for online<br>learning</li>\n<li>Higher potential for leakage</li>\n</ul>\n<h2 id=\"Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\"><a href=\"#Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\" class=\"headerlink\" title=\"Dimensionality Reduction: Squashing the Data Pancake with PCA\"></a>Dimensionality Reduction: Squashing the Data Pancake with PCA</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Feature Engineering  Machine Learning <em></em><em></em><em></em> + <em></em>researchML</p>\n<h2 id=\"Numerical-Features\"><a href=\"#Numerical-Features\" class=\"headerlink\" title=\"Numerical Features\"></a>Numerical Features</h2><h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>Examples include <strong>k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance</strong>. For these models and modeling components, <strong>it is often a good idea to normalize the features so that the output stays on an expected scale</strong>.</p>\n<p>Logical functions, on the other hand, are not sensitive to input feature scale<strong>. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). </strong>Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sorteventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.</p>\n<h3 id=\"Distribution\"><a href=\"#Distribution\" class=\"headerlink\" title=\"Distribution\"></a>Distribution</h3><p>Its also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. <strong>One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian</strong>.</p>\n<h3 id=\"Quantization\"><a href=\"#Quantization\" class=\"headerlink\" title=\"Quantization\"></a>Quantization</h3><p>Raw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.</p>\n<p>In order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: <strong>fixed-width</strong> or <strong>adaptive</strong>. We will give an example of each type.</p>\n<h3 id=\"Fixed-width-binning\"><a href=\"#Fixed-width-binning\" class=\"headerlink\" title=\"Fixed-width binning\"></a>Fixed-width binning</h3><p>With fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 09 years old in bin 1, 1019 years in bin 2, etc. <strong>To map from the count to the bin, we simply divide by the width of the bin and take the integer part</strong>.</p>\n<p>Its also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 09, 1099, 100999, 10009999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.</p>\n<h3 id=\"Quantile-binning\"><a href=\"#Quantile-binning\" class=\"headerlink\" title=\"Quantile binning\"></a>Quantile binning</h3><p>Fixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.</p>\n<h3 id=\"Log-Transformation\"><a href=\"#Log-Transformation\" class=\"headerlink\" title=\"Log Transformation\"></a>Log Transformation</h3><p>The log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.</p>\n<h3 id=\"Power-Transforms-Generalization-of-the-Log\"><a href=\"#Power-Transforms-Generalization-of-the-Log\" class=\"headerlink\" title=\"Power Transforms: Generalization of the Log\"></a>Power Transforms: Generalization of the Log</h3><p>Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.</p>\n<p>A simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:<br>$$<br>\\tilde{x}=<br>\\begin{cases}<br>\\frac{x^{\\lambda}-1}{\\lambda} &amp; if \\lambda \\neq 0,\\\\<br>ln(x) &amp; if \\lambda = 0<br>\\end{cases}<br>$$</p>\n<p>The Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.</p>\n<h3 id=\"Feature-Scaling-or-Normalization\"><a href=\"#Feature-Scaling-or-Normalization\" class=\"headerlink\" title=\"Feature Scaling or Normalization\"></a>Feature Scaling or Normalization</h3><p>Smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldnt care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.</p>\n<h4 id=\"Min-Max-Scaling\"><a href=\"#Min-Max-Scaling\" class=\"headerlink\" title=\"Min-Max Scaling\"></a>Min-Max Scaling</h4><p>$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$</p>\n<h4 id=\"Standardization-Variance-Scaling\"><a href=\"#Standardization-Variance-Scaling\" class=\"headerlink\" title=\"Standardization (Variance Scaling)\"></a>Standardization (Variance Scaling)</h4><p>$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$</p>\n<p>It subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.</p>\n<blockquote>\n<p>Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didnt appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.</p>\n</blockquote>\n<h4 id=\"L-2-Normalization\"><a href=\"#L-2-Normalization\" class=\"headerlink\" title=\"$L^2$ Normalization\"></a>$L^2$ Normalization</h4><p>$\\tilde{x}=\\frac{x}{||x||_2}$</p>\n<h3 id=\"Feature-Selection\"><a href=\"#Feature-Selection\" class=\"headerlink\" title=\"Feature Selection\"></a>Feature Selection</h3><p>Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training timein fact, some techniques increase overall training timebut about reducing model scoring time.</p>\n<p>Roughly speaking, feature selection techniques fall into three classes:</p>\n<h4 id=\"Filtering\"><a href=\"#Filtering\" class=\"headerlink\" title=\"Filtering\"></a>Filtering</h4><p>Filtering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.</p>\n<h4 id=\"Wrapper-methods\"><a href=\"#Wrapper-methods\" class=\"headerlink\" title=\"Wrapper methods\"></a>Wrapper methods</h4><p>These techniques are expensive, but they allow you to try out subsets of features, which means you wont accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.</p>\n<h4 id=\"Embedded-methods\"><a href=\"#Embedded-methods\" class=\"headerlink\" title=\"Embedded methods\"></a>Embedded methods</h4><p>These methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so its also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.</p>\n<h2 id=\"Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\"><a href=\"#Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\" class=\"headerlink\" title=\"Categorical Variables: Counting Eggs in the Age of Robotic Chickens\"></a>Categorical Variables: Counting Eggs in the Age of Robotic Chickens</h2><h3 id=\"Encoding-Categorical-Variables\"><a href=\"#Encoding-Categorical-Variables\" class=\"headerlink\" title=\"Encoding Categorical Variables\"></a>Encoding Categorical Variables</h3><h4 id=\"One-Hot-Encoding\"><a href=\"#One-Hot-Encoding\" class=\"headerlink\" title=\"One-Hot Encoding\"></a>One-Hot Encoding</h4><p>A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be on. This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.</p>\n<h4 id=\"Dummy-Coding\"><a href=\"#Dummy-Coding\" class=\"headerlink\" title=\"Dummy Coding\"></a>Dummy Coding</h4><p>The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k1. Dummy coding removes the extra degree of freedom by using only k1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.</p>\n<h4 id=\"Effect-Coding\"><a href=\"#Effect-Coding\" class=\"headerlink\" title=\"Effect Coding\"></a>Effect Coding</h4><p>Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all 1s.</p>\n<h4 id=\"Pros-and-Cons-of-Categorical-Variable-Encodings\"><a href=\"#Pros-and-Cons-of-Categorical-Variable-Encodings\" class=\"headerlink\" title=\"Pros and Cons of Categorical Variable Encodings\"></a>Pros and Cons of Categorical Variable Encodings</h4><p>One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all 1s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.</p>\n<h3 id=\"Dealing-with-Large-Categorical-Variables\"><a href=\"#Dealing-with-Large-Categorical-Variables\" class=\"headerlink\" title=\"Dealing with Large Categorical Variables\"></a>Dealing with Large Categorical Variables</h3><p>Existing solutions can be categorized as follows:</p>\n<ol>\n<li>Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.</li>\n<li>Compress the features. There are two choices: <ul>\n<li>Feature hashing, popular with linear models </li>\n<li>Bin counting, popular with linear models as well as trees</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"Feature-Hashing\"><a href=\"#Feature-Hashing\" class=\"headerlink\" title=\"Feature Hashing\"></a>Feature Hashing</h4><p>The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.</p>\n<h4 id=\"What-about-rare-categories\"><a href=\"#What-about-rare-categories\" class=\"headerlink\" title=\"What about rare categories?\"></a>What about rare categories?</h4><p>One way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png\" alt=\"Black-off Bin\"></p>\n<p>There is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.</p>\n<h4 id=\"Counts-without-bounds\"><a href=\"#Counts-without-bounds\" class=\"headerlink\" title=\"Counts without bounds\"></a>Counts without bounds</h4><p>If the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model knows the input data up to the observed scale.</p>\n<p>For this reason, <strong>it is often better to use normalized counts that are guaranteed to be bounded in a known interval</strong>. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.</p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><h4 id=\"Plain-one-hot-encoding\"><a href=\"#Plain-one-hot-encoding\" class=\"headerlink\" title=\"Plain one-hot encoding\"></a>Plain one-hot encoding</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse vector format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nk)$ under a linear model, where k is the number of categories.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easiest to implement</li>\n<li>Potentially most accurate</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Computationally inefficient</li>\n<li>Does not adapt to growing categories</li>\n<li>Not feasible for anything other than linear models</li>\n<li>Requires large-scale distributed optimization with truly<br>large datasets</li>\n</ul>\n<h4 id=\"Feature-hashing\"><a href=\"#Feature-hashing\" class=\"headerlink\" title=\"Feature hashing\"></a>Feature hashing</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse matrix format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nm)$ under a linear or kernel model, where m is the number of hash bins.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Easy to implement</li>\n<li>Makes model training cheaper</li>\n<li>Easily adaptable to new categories</li>\n<li>Easily handles rare categories</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Only suitable for linear or kernelized models</li>\n<li>Hashed features not interpretable</li>\n<li>Mixed reports of accuracy</li>\n</ul>\n<h4 id=\"Bin-counting\"><a href=\"#Bin-counting\" class=\"headerlink\" title=\"Bin-counting\"></a>Bin-counting</h4><p><strong>Space requirement</strong> $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.</p>\n<p><strong>Computation requirement</strong> $O(n)$ for linear models; also usable for nonlinear models such as trees.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Smallest computational burden at training time</li>\n<li>Enables tree-based models</li>\n<li>Relatively easy to adapt to new categories</li>\n<li>Handles rare categories with back-off or count-min sketch</li>\n<li>Interpretable</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Requires historical data</li>\n<li>Delayed updates required, not completely suitable for online<br>learning</li>\n<li>Higher potential for leakage</li>\n</ul>\n<h2 id=\"Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\"><a href=\"#Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\" class=\"headerlink\" title=\"Dimensionality Reduction: Squashing the Data Pancake with PCA\"></a>Dimensionality Reduction: Squashing the Data Pancake with PCA</h2>"},{"title":"[ML] Loss Function in ML","date":"2018-07-24T05:47:56.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nLoss FunctionML/DLLoss FunctionLoss\n* Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss\n* Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss\n\n1. __MAE and MSE Loss__  \n   MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).\n   \n   For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be __median__ of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.\n\n   One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.\n\n2. __Huber Loss, Smooth Mean Absolute Error__   \n   Huber loss is less sensitive to outliers in data than the squared error loss. Its also differentiable at 0. It's basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)\n\n   $$\n   L_{\\delta}(y,f(x))=\n   \\begin{cases}\n   \\frac{1}{2}(y-f(x))^2 & for |y-f(x)|\\leq \\delta \\\\\n   \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 & otherwise \n   \\end{cases}\n   $$\n\n   The choice of delta is critical because it determines what you're willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized \"appropriately\" with $L_2$.\n\n   __Why use Huber Loss?__  \n   One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n\n   Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And its more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n\n3. __Log-Cosh Loss__\n   Log-cosh is another function used in regression tasks that's smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.  \n   $L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ \n\n   ![Log-Cosh](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png)\n\n   __Advantage__: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it's twice differentiable everywhere,unlike Huber loss.\n\n   __Why do we need a 2nd derivative?__   \n   Many ML model implementations like XGBoost use Newton's method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.\n\n   But Log-cosh loss isn't perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.\n\n4. Quantile Loss\n   Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.\n\n   __Understanding the quantile loss function__  \n   The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median\n\n   $L_{\\gamma}(y,y^p)=\\sum_{i=y_i<y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$\n\n5. Exponential Loss  \n   $L(y,f(x))=exp(-yf(x))$","source":"_posts/ml-loss.md","raw":"---\ntitle: \"[ML] Loss Function in ML\"\ndate: 2018-07-24 13:47:56\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nLoss FunctionML/DLLoss FunctionLoss\n* Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss\n* Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss\n\n1. __MAE and MSE Loss__  \n   MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).\n   \n   For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be __median__ of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.\n\n   One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.\n\n2. __Huber Loss, Smooth Mean Absolute Error__   \n   Huber loss is less sensitive to outliers in data than the squared error loss. Its also differentiable at 0. It's basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)\n\n   $$\n   L_{\\delta}(y,f(x))=\n   \\begin{cases}\n   \\frac{1}{2}(y-f(x))^2 & for |y-f(x)|\\leq \\delta \\\\\n   \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 & otherwise \n   \\end{cases}\n   $$\n\n   The choice of delta is critical because it determines what you're willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized \"appropriately\" with $L_2$.\n\n   __Why use Huber Loss?__  \n   One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n\n   Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And its more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n\n3. __Log-Cosh Loss__\n   Log-cosh is another function used in regression tasks that's smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.  \n   $L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ \n\n   ![Log-Cosh](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png)\n\n   __Advantage__: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it's twice differentiable everywhere,unlike Huber loss.\n\n   __Why do we need a 2nd derivative?__   \n   Many ML model implementations like XGBoost use Newton's method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.\n\n   But Log-cosh loss isn't perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.\n\n4. Quantile Loss\n   Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.\n\n   __Understanding the quantile loss function__  \n   The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median\n\n   $L_{\\gamma}(y,y^p)=\\sum_{i=y_i<y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$\n\n5. Exponential Loss  \n   $L(y,f(x))=exp(-yf(x))$","slug":"ml-loss","published":1,"updated":"2018-10-01T04:40:09.048Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cm0015608wne980x56","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Loss FunctionML/DLLoss FunctionLoss</p>\n<ul>\n<li>Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss</li>\n<li>Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss</li>\n</ul>\n<ol>\n<li><p><strong>MAE and MSE Loss</strong><br>MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).</p>\n<p>For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be <strong>median</strong> of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.</p>\n<p>One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.</p>\n</li>\n<li><p><strong>Huber Loss, Smooth Mean Absolute Error</strong><br>Huber loss is less sensitive to outliers in data than the squared error loss. Its also differentiable at 0. Its basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)</p>\n<p>$$<br>L_{\\delta}(y,f(x))=<br>\\begin{cases}<br>\\frac{1}{2}(y-f(x))^2 &amp; for |y-f(x)|\\leq \\delta \\\\<br>\\delta|y-f(x)|-\\frac{1}{2}\\delta^2 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<p>The choice of delta is critical because it determines what youre willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized appropriately with $L_2$.</p>\n<p><strong>Why use Huber Loss?</strong><br>One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.</p>\n<p>Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And its more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.</p>\n</li>\n<li><p><strong>Log-Cosh Loss</strong><br>Log-cosh is another function used in regression tasks thats smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.<br>$L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png\" alt=\"Log-Cosh\"></p>\n<p><strong>Advantage</strong>: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that logcosh works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and its twice differentiable everywhere,unlike Huber loss.</p>\n<p><strong>Why do we need a 2nd derivative?</strong><br>Many ML model implementations like XGBoost use Newtons method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.</p>\n<p>But Log-cosh loss isnt perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.</p>\n</li>\n<li><p>Quantile Loss<br>Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.</p>\n<p><strong>Understanding the quantile loss function</strong><br>The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median</p>\n<p>$L_{\\gamma}(y,y^p)=\\sum_{i=y_i&lt;y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$</p>\n</li>\n<li><p>Exponential Loss<br>$L(y,f(x))=exp(-yf(x))$</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Loss FunctionML/DLLoss FunctionLoss</p>\n<ul>\n<li>Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss</li>\n<li>Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss</li>\n</ul>\n<ol>\n<li><p><strong>MAE and MSE Loss</strong><br>MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).</p>\n<p>For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be <strong>median</strong> of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.</p>\n<p>One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.</p>\n</li>\n<li><p><strong>Huber Loss, Smooth Mean Absolute Error</strong><br>Huber loss is less sensitive to outliers in data than the squared error loss. Its also differentiable at 0. Its basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)</p>\n<p>$$<br>L_{\\delta}(y,f(x))=<br>\\begin{cases}<br>\\frac{1}{2}(y-f(x))^2 &amp; for |y-f(x)|\\leq \\delta \\\\<br>\\delta|y-f(x)|-\\frac{1}{2}\\delta^2 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<p>The choice of delta is critical because it determines what youre willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized appropriately with $L_2$.</p>\n<p><strong>Why use Huber Loss?</strong><br>One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.</p>\n<p>Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And its more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.</p>\n</li>\n<li><p><strong>Log-Cosh Loss</strong><br>Log-cosh is another function used in regression tasks thats smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.<br>$L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png\" alt=\"Log-Cosh\"></p>\n<p><strong>Advantage</strong>: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that logcosh works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and its twice differentiable everywhere,unlike Huber loss.</p>\n<p><strong>Why do we need a 2nd derivative?</strong><br>Many ML model implementations like XGBoost use Newtons method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.</p>\n<p>But Log-cosh loss isnt perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.</p>\n</li>\n<li><p>Quantile Loss<br>Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.</p>\n<p><strong>Understanding the quantile loss function</strong><br>The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median</p>\n<p>$L_{\\gamma}(y,y^p)=\\sum_{i=y_i&lt;y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$</p>\n</li>\n<li><p>Exponential Loss<br>$L(y,f(x))=exp(-yf(x))$</p>\n</li>\n</ol>\n"},{"title":"[ML] KNN","catalog":false,"mathjax":true,"date":"2018-07-19T04:37:17.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## \nKNN/ ____ majority votingLabel ____ KmeanKDistance MetricKNN\n\n###  Distance Metric\n$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$\n$p=\\infty$$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$\n\n### K\n* KvarianceKbias","source":"_posts/ml-knn.md","raw":"---\ntitle: \"[ML] KNN\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 12:37:17\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## \nKNN/ ____ majority votingLabel ____ KmeanKDistance MetricKNN\n\n###  Distance Metric\n$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$\n$p=\\infty$$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$\n\n### K\n* KvarianceKbias","slug":"ml-knn","published":1,"updated":"2018-10-01T04:40:09.017Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cp0017608wxlenocz5","content":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>KNN/ <strong></strong> majority votingLabel <strong></strong> KmeanKDistance MetricKNN</p>\n<h3 id=\"-Distance-Metric\"><a href=\"#-Distance-Metric\" class=\"headerlink\" title=\" Distance Metric\"></a> Distance Metric</h3><p>$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$<br>$p=\\infty$$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$</p>\n<h3 id=\"K\"><a href=\"#K\" class=\"headerlink\" title=\"K\"></a>K</h3><ul>\n<li>KvarianceKbias</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>KNN/ <strong></strong> majority votingLabel <strong></strong> KmeanKDistance MetricKNN</p>\n<h3 id=\"-Distance-Metric\"><a href=\"#-Distance-Metric\" class=\"headerlink\" title=\" Distance Metric\"></a> Distance Metric</h3><p>$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$<br>$p=\\infty$$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$</p>\n<h3 id=\"K\"><a href=\"#K\" class=\"headerlink\" title=\"K\"></a>K</h3><ul>\n<li>KvarianceKbias</li>\n</ul>\n"},{"title":"[ML] Linear Model","catalog":false,"mathjax":true,"date":"2018-07-18T04:23:24.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Statistical Methods\n* Linear ModelLinear ModelLinear\n* $f$KNN$f$Non-Parameter MethodsParameter Methods\n\n## Linear Model\n* Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:\n\n    $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$\n\n    That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.\n\n* Standard error  \n  $H_0$: $X$$Y$$\\beta_1= 0$  \n  $H_{\\alpha}$: $X$$Y$$\\beta_1\\neq 0$  \n  \n  $\\hat{\\beta}_1$ ($\\beta$)0$SE(\\hat{\\beta}_1)$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$$\\beta_1\\neq 0$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$\n\n* RSS measures the amount of variability that is left unexplained after performing the regression.\n\n* TSS-RSS measures the amount of variability in the response that is explained by performing the regression. \n\n* $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.\n\n*  __residual plot__ Outliersoutliers\"\" __studentized residual__$e_i/SE(e_i)$\n\n  __Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.__\n\n* A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\n\n  However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this __''multi-collinearity''__.\n\n  A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.\n\n  $$\n  VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}\n  $$\n  \n  VIF 1VIF510\n\n* In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors that is called __curse of dimensionality__. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.\n\n* Generally, Parameter-Methods will tend to outperform non-parameter methods when there's a small number of observations per predictors.","source":"_posts/ml-lm.md","raw":"---\ntitle: \"[ML] Linear Model\"\ncatalog: false\nmathjax: true\ndate: 2018-07-18 12:23:24\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Statistical Methods\n* Linear ModelLinear ModelLinear\n* $f$KNN$f$Non-Parameter MethodsParameter Methods\n\n## Linear Model\n* Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:\n\n    $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$\n\n    That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.\n\n* Standard error  \n  $H_0$: $X$$Y$$\\beta_1= 0$  \n  $H_{\\alpha}$: $X$$Y$$\\beta_1\\neq 0$  \n  \n  $\\hat{\\beta}_1$ ($\\beta$)0$SE(\\hat{\\beta}_1)$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$$\\beta_1\\neq 0$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$\n\n* RSS measures the amount of variability that is left unexplained after performing the regression.\n\n* TSS-RSS measures the amount of variability in the response that is explained by performing the regression. \n\n* $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.\n\n*  __residual plot__ Outliersoutliers\"\" __studentized residual__$e_i/SE(e_i)$\n\n  __Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.__\n\n* A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\n\n  However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this __''multi-collinearity''__.\n\n  A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.\n\n  $$\n  VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}\n  $$\n  \n  VIF 1VIF510\n\n* In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors that is called __curse of dimensionality__. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.\n\n* Generally, Parameter-Methods will tend to outperform non-parameter methods when there's a small number of observations per predictors.","slug":"ml-lm","published":1,"updated":"2018-10-01T04:40:09.047Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cs001a608wk9gey017","content":"<h2 id=\"Statistical-Methods\"><a href=\"#Statistical-Methods\" class=\"headerlink\" title=\"Statistical Methods\"></a>Statistical Methods</h2><ul>\n<li>Linear ModelLinear ModelLinear</li>\n<li>$f$KNN$f$Non-Parameter MethodsParameter Methods</li>\n</ul>\n<h2 id=\"Linear-Model\"><a href=\"#Linear-Model\" class=\"headerlink\" title=\"Linear Model\"></a>Linear Model</h2><ul>\n<li><p>Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:</p>\n<p>  $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$</p>\n<p>  That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.</p>\n</li>\n<li><p>Standard error<br>$H_0$: $X$$Y$$\\beta_1= 0$<br>$H_{\\alpha}$: $X$$Y$$\\beta_1\\neq 0$  </p>\n<p>$\\hat{\\beta}_1$ ($\\beta$)0$SE(\\hat{\\beta}_1)$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$$\\beta_1\\neq 0$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$</p>\n</li>\n<li><p>RSS measures the amount of variability that is left unexplained after performing the regression.</p>\n</li>\n<li><p>TSS-RSS measures the amount of variability in the response that is explained by performing the regression. </p>\n</li>\n<li><p>$R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.</p>\n</li>\n<li><p> <strong>residual plot</strong> Outliersoutliers <strong>studentized residual</strong>$e_i/SE(e_i)$</p>\n<p><strong>Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.</strong></p>\n</li>\n<li><p>A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>\n<p>However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this <strong>multi-collinearity</strong>.</p>\n<p>A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.</p>\n<p>$$<br>VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}<br>$$</p>\n<p>VIF 1VIF510</p>\n</li>\n<li><p>In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors that is called <strong>curse of dimensionality</strong>. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.</p>\n</li>\n<li><p>Generally, Parameter-Methods will tend to outperform non-parameter methods when theres a small number of observations per predictors.</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Statistical-Methods\"><a href=\"#Statistical-Methods\" class=\"headerlink\" title=\"Statistical Methods\"></a>Statistical Methods</h2><ul>\n<li>Linear ModelLinear ModelLinear</li>\n<li>$f$KNN$f$Non-Parameter MethodsParameter Methods</li>\n</ul>\n<h2 id=\"Linear-Model\"><a href=\"#Linear-Model\" class=\"headerlink\" title=\"Linear Model\"></a>Linear Model</h2><ul>\n<li><p>Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:</p>\n<p>  $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$</p>\n<p>  That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.</p>\n</li>\n<li><p>Standard error<br>$H_0$: $X$$Y$$\\beta_1= 0$<br>$H_{\\alpha}$: $X$$Y$$\\beta_1\\neq 0$  </p>\n<p>$\\hat{\\beta}_1$ ($\\beta$)0$SE(\\hat{\\beta}_1)$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$$\\beta_1\\neq 0$$SE(\\hat{\\beta}_1)$$\\hat{\\beta}_1$</p>\n</li>\n<li><p>RSS measures the amount of variability that is left unexplained after performing the regression.</p>\n</li>\n<li><p>TSS-RSS measures the amount of variability in the response that is explained by performing the regression. </p>\n</li>\n<li><p>$R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.</p>\n</li>\n<li><p> <strong>residual plot</strong> Outliersoutliers <strong>studentized residual</strong>$e_i/SE(e_i)$</p>\n<p><strong>Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.</strong></p>\n</li>\n<li><p>A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>\n<p>However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this <strong>multi-collinearity</strong>.</p>\n<p>A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.</p>\n<p>$$<br>VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}<br>$$</p>\n<p>VIF 1VIF510</p>\n</li>\n<li><p>In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors that is called <strong>curse of dimensionality</strong>. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.</p>\n</li>\n<li><p>Generally, Parameter-Methods will tend to outperform non-parameter methods when theres a small number of observations per predictors.</p>\n</li>\n</ul>\n"},{"title":"[ML] Logistic Regression and Maximum Entropy Model","mathjax":true,"date":"2018-07-23T09:28:30.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nLogistic RegressionLogistic Regression\n\n## Logistic Regression\n* Logistic Distribution: $X$$X$Logistic Distribution$X$:\n  $F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$\n\n  $f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$\n\nLogistic Regression  \n$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$\n$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$  \n$x$$P(Y=1|X)$$P(Y=0|X)$Logistic Regression$x$\n\nOdds: (odds)$p$$\\frac{p}{1-p}$(log odds)logit:   \n$logit(p)=log\\frac{p}{1-p}$  \nLogistic Regression\n$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$\n\nLogistic Regression$Y=1$$x$$Y=1$$x$Logistic Regression\n\n### Logistic Regression\nLogistic Regression ____Logistic Regression Model\n$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$  \n\n$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$\n  \n\n$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$\n\n$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  \n\n$L(w)$$w$__Logistic Regression__\n\n#### Logistic Regression\n$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$\n\n$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$\n\n## Maximum Entropy Model\n","source":"_posts/ml-lr-me.md","raw":"---\ntitle: \"[ML] Logistic Regression and Maximum Entropy Model\"\nmathjax: true\ndate: 2018-07-23 17:28:30\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nLogistic RegressionLogistic Regression\n\n## Logistic Regression\n* Logistic Distribution: $X$$X$Logistic Distribution$X$:\n  $F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$\n\n  $f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$\n\nLogistic Regression  \n$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$\n$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$  \n$x$$P(Y=1|X)$$P(Y=0|X)$Logistic Regression$x$\n\nOdds: (odds)$p$$\\frac{p}{1-p}$(log odds)logit:   \n$logit(p)=log\\frac{p}{1-p}$  \nLogistic Regression\n$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$\n\nLogistic Regression$Y=1$$x$$Y=1$$x$Logistic Regression\n\n### Logistic Regression\nLogistic Regression ____Logistic Regression Model\n$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$  \n\n$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$\n  \n\n$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$\n\n$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  \n\n$L(w)$$w$__Logistic Regression__\n\n#### Logistic Regression\n$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$\n\n$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$\n\n## Maximum Entropy Model\n","slug":"ml-lr-me","published":1,"updated":"2018-10-01T04:40:09.052Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cu001c608w09ol81m3","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Logistic RegressionLogistic Regression</p>\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><ul>\n<li><p>Logistic Distribution: $X$$X$Logistic Distribution$X$:<br>$F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$</p>\n<p>$f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$</p>\n</li>\n</ul>\n<p>Logistic Regression<br>$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$<br>$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$<br>$x$$P(Y=1|X)$$P(Y=0|X)$Logistic Regression$x$</p>\n<p>Odds: (odds)$p$$\\frac{p}{1-p}$(log odds)logit:<br>$logit(p)=log\\frac{p}{1-p}$<br>Logistic Regression<br>$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$</p>\n<p>Logistic Regression$Y=1$$x$$Y=1$$x$Logistic Regression</p>\n<h3 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h3><p>Logistic Regression <strong></strong>Logistic Regression Model<br>$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$<br><br>$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$<br>  </p>\n<p>$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$</p>\n<p>$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  </p>\n<p>$L(w)$$w$<strong>Logistic Regression</strong></p>\n<h4 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h4><p>$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$</p>\n<p>$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$</p>\n<h2 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Logistic RegressionLogistic Regression</p>\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><ul>\n<li><p>Logistic Distribution: $X$$X$Logistic Distribution$X$:<br>$F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$</p>\n<p>$f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$</p>\n</li>\n</ul>\n<p>Logistic Regression<br>$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$<br>$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$<br>$x$$P(Y=1|X)$$P(Y=0|X)$Logistic Regression$x$</p>\n<p>Odds: (odds)$p$$\\frac{p}{1-p}$(log odds)logit:<br>$logit(p)=log\\frac{p}{1-p}$<br>Logistic Regression<br>$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$</p>\n<p>Logistic Regression$Y=1$$x$$Y=1$$x$Logistic Regression</p>\n<h3 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h3><p>Logistic Regression <strong></strong>Logistic Regression Model<br>$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$<br><br>$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$<br>  </p>\n<p>$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$</p>\n<p>$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  </p>\n<p>$L(w)$$w$<strong>Logistic Regression</strong></p>\n<h4 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h4><p>$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$</p>\n<p>$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$</p>\n<h2 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h2>"},{"title":"[ML] Model Selection and Performance Metric","catalog":false,"mathjax":true,"date":"2018-07-19T03:02:28.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n* $p>n$Backward SelectionForward Selection\n\n* $Precision=\\frac{TP}{TP+FP}$  \n  $Recall=\\frac{TP}{TP+FN}$  \n  $F_1=\\frac{2PR}{P+R}$  \n\n* confusionmatrixtraining/testconfusion matrixtraining/test$n$ confusion matrixPrecisionRecall\n    * Macro-P/Macro-R/Macro-F1: confusion matrix P/R/F1\n    $Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$  \n    $Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$  \n    $Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$\n    \n    * Micro-P/Micro-R/Micro-F1: confusion matrixTP, FP, TN, FN\n    $Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$  \n    $Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$  \n    $Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$\n\n* ROC\"\"(TPR)\"\"(FPR)\n    $TPR=\\frac{TP}{TP+FN}$  \n    $FPR=\\frac{FP}{FP+TN}$\ntest set samplesROC(TPR, FPR)()ROC\n\n* : $bias^2+variance+\\epsilon^2$\n  $bias$learner\n  $variance$training set\n  $\\epsilon$learner","source":"_posts/ml-model-selection-metric.md","raw":"---\ntitle: \"[ML] Model Selection and Performance Metric\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 11:02:28\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n* $p>n$Backward SelectionForward Selection\n\n* $Precision=\\frac{TP}{TP+FP}$  \n  $Recall=\\frac{TP}{TP+FN}$  \n  $F_1=\\frac{2PR}{P+R}$  \n\n* confusionmatrixtraining/testconfusion matrixtraining/test$n$ confusion matrixPrecisionRecall\n    * Macro-P/Macro-R/Macro-F1: confusion matrix P/R/F1\n    $Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$  \n    $Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$  \n    $Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$\n    \n    * Micro-P/Micro-R/Micro-F1: confusion matrixTP, FP, TN, FN\n    $Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$  \n    $Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$  \n    $Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$\n\n* ROC\"\"(TPR)\"\"(FPR)\n    $TPR=\\frac{TP}{TP+FN}$  \n    $FPR=\\frac{FP}{FP+TN}$\ntest set samplesROC(TPR, FPR)()ROC\n\n* : $bias^2+variance+\\epsilon^2$\n  $bias$learner\n  $variance$training set\n  $\\epsilon$learner","slug":"ml-model-selection-metric","published":1,"updated":"2018-10-01T04:40:09.054Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cw001f608wuc5jmdy4","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ul>\n<li><p>$p&gt;n$Backward SelectionForward Selection</p>\n</li>\n<li><p>$Precision=\\frac{TP}{TP+FP}$<br>$Recall=\\frac{TP}{TP+FN}$<br>$F_1=\\frac{2PR}{P+R}$  </p>\n</li>\n<li><p>confusionmatrixtraining/testconfusion matrixtraining/test$n$ confusion matrixPrecisionRecall</p>\n<ul>\n<li><p>Macro-P/Macro-R/Macro-F1: confusion matrix P/R/F1<br>$Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$<br>$Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$<br>$Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$</p>\n</li>\n<li><p>Micro-P/Micro-R/Micro-F1: confusion matrixTP, FP, TN, FN<br>$Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$<br>$Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$<br>$Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$</p>\n</li>\n</ul>\n</li>\n<li><p>ROC(TPR)(FPR)<br>  $TPR=\\frac{TP}{TP+FN}$<br>  $FPR=\\frac{FP}{FP+TN}$<br>test set samplesROC(TPR, FPR)()ROC</p>\n</li>\n<li><p>: $bias^2+variance+\\epsilon^2$<br>$bias$learner<br>$variance$training set<br>$\\epsilon$learner</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ul>\n<li><p>$p&gt;n$Backward SelectionForward Selection</p>\n</li>\n<li><p>$Precision=\\frac{TP}{TP+FP}$<br>$Recall=\\frac{TP}{TP+FN}$<br>$F_1=\\frac{2PR}{P+R}$  </p>\n</li>\n<li><p>confusionmatrixtraining/testconfusion matrixtraining/test$n$ confusion matrixPrecisionRecall</p>\n<ul>\n<li><p>Macro-P/Macro-R/Macro-F1: confusion matrix P/R/F1<br>$Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$<br>$Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$<br>$Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$</p>\n</li>\n<li><p>Micro-P/Micro-R/Micro-F1: confusion matrixTP, FP, TN, FN<br>$Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$<br>$Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$<br>$Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$</p>\n</li>\n</ul>\n</li>\n<li><p>ROC(TPR)(FPR)<br>  $TPR=\\frac{TP}{TP+FN}$<br>  $FPR=\\frac{FP}{FP+TN}$<br>test set samplesROC(TPR, FPR)()ROC</p>\n</li>\n<li><p>: $bias^2+variance+\\epsilon^2$<br>$bias$learner<br>$variance$training set<br>$\\epsilon$learner</p>\n</li>\n</ul>\n"},{"title":"[ML] SVM","mathjax":true,"date":"2018-07-22T04:42:39.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## \nSVMSVM __Hinge Loss__\n\n  \n  \nKernel Tricks\n\n __SVMSVM__\n\ntraining setMLP ____SVM ____\n\n## SVM\n$w\\cdot x+b=0$$|w\\cdot x+b|$$x$$w\\cdot x+b$$y$$y(w\\cdot x+b)$ __\"\"__\n\n* T$(w,b)$$(w,b)$$(x_i,y_i)$:  \n  $\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$\n\n  $(w,b)$T$(w,b)$T$(x_i,y_i)$  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  $w$$b$2$w$$||w||=1$ ____\n\n* T$(w,b)$$(w,b)$$(x_i,y_i)$  \n  $\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$\n\n  $(w,b)$T$(w,b)$T$(x_i,y_i)$  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  $(w,b)$$(x_i,y_i)$\n\n    \n  $\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$\n\n  $\\gamma=\\frac{\\hat{\\gamma}}{||w||}$\n\n  __$||w||=1$__$w$$b$()\n\n\n    \n$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$\n\n$(w,b)$training set$\\gamma$$(w,b)$training sample$\\gamma$\n\n\n$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$\n\n$\\frac{1}{||w||}$$\\frac{1}{2}||w||^2$SVM\n$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$\n\n* T\n\ntraining set\n$y_i(w\\cdot x_i+b)-1=0$\n\n$y_i=+1$ $H_1:w\\cdot x+b=1$$y_i=-1$ $H_2:w\\cdot x+b=-1$\"\"\n\n\n### \nSVMSVMKernel Function\n\n$\\alpha_i \\geq 0, i=1,2,\\cdots,N$\n$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$\n\n\n$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$L(w,b,\\alpha)$$w,b$$\\alpha$\n\n1. $\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$  \n   $L(w,b,\\alpha)$$w,b$0  \n   $\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$\n\n   $\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$  \n  :  \n  $w=\\sum_{i=1}^N\\alpha_i y_i x_i$  \n  $\\sum_{i=1}^N\\alpha_i y_i=0$  \n  :  \n  $\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$\n\n2. $\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$\\alpha$  \n  $$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n  \n  $$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n#### SVM\n1. :\n  $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$  \n  $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2.  $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$  $\\alpha^{\\star}$  $\\alpha_j^{\\star}>0$:  \n  $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$\n\n3.  $w^{\\star}\\cdot x+b^{\\star}=0$ $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n## SVM\n$(x_i,y_i)$1$(x_i, y_i)$$\\xi_i \\geq0$1:  \n$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i$$\\frac{1}{2}||w||^2$ $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$$C>0$$C$$C$Loss$\\frac{1}{2}||w||^2$$C$\n\nSVM  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$\n   \n### \n:\n$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$\n$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$\n\n#### SVM\n1. $C>0$  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$  \n   $s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$  \n   $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. $w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$  \n   $\\alpha^{\\star}$$\\alpha_j^{\\star}$$0<\\alpha_j^{\\star}<C$ ()  \n   $b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$\n  \n3.  $w^{\\star}\\cdot x+b^{\\star}=0$\n   $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n#### \n$x_i$  \n\n$\\alpha_i^{\\star}<C$$\\xi_i=0$$x_i$  \n$\\alpha_i^{\\star}=C, 0<\\xi_i<1$$x_i$  \n$\\alpha_i^{\\star}=C, \\xi_i=1$$x_i$  \n$\\alpha_i^{\\star}=C, \\xi_i>1$$x_i$\n\n#### Hinge Loss\nSVMLoss Function  \n$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$\n\n$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$ Hinge Loss$(x_i,y_i)$ $y_i(w\\cdot x_i+b)$10 $1-y_i(w\\cdot x_i+b)$\n\nSVM:  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$\n\n$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i\\geq 0$\n\n\n\n$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$\n\n## SVMKernel Function\n$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$()  \n$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$  \n$z=\\phi(x)$$\\chi \\subset R^2$$\\mathcal{Z}\\subset R^2$\n\n### Kernel Function\n$\\chi$($R^n$)$\\mathcal{H}$()$\\chi$$\\mathcal{H}$:  \n$\\phi(x):\\chi \\to \\mathcal{H}$\n$x,z\\in \\chi$$K(x,z)$\n$K(x,z)=\\phi(x)\\cdot \\phi(z)$  \n$K(x,z)$$\\phi(x)$$\\phi(x)\\cdot \\phi(z)$$\\phi(x)$$\\phi(z)$\n\nKernel Tricks$K(x,z)$$\\phi$$K(x,z)$$\\phi(x)$$\\phi(z)$$K(x,z)$\n\nKernel-based SVM$\\phi$$x_i\\cdot x_j$$\\phi(x_i)\\cdot \\phi(x_j)$SVMSVM\n\nKernel TricksKernel Tricks\n\n### Kernel Function\n1.   \n   $K(x,z)=(x\\cdot z+1)^p$  \n   SVM$p$:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$\n\n2.   \n   $K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$\n   SVMRBF:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$\n\n### SVM\nKernel Tricks  \n$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$\n$K(x,z)$\n\n#### SVM\n1. $K(x,z)$$C$  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$\n\n   $s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$\n   $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. $\\alpha^{\\star}$$0<\\alpha_i^{\\star}<C$ $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$\n\n3.   \n   $f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$","source":"_posts/ml-svm.md","raw":"---\ntitle: \"[ML] SVM\"\nmathjax: true\ndate: 2018-07-22 12:42:39\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## \nSVMSVM __Hinge Loss__\n\n  \n  \nKernel Tricks\n\n __SVMSVM__\n\ntraining setMLP ____SVM ____\n\n## SVM\n$w\\cdot x+b=0$$|w\\cdot x+b|$$x$$w\\cdot x+b$$y$$y(w\\cdot x+b)$ __\"\"__\n\n* T$(w,b)$$(w,b)$$(x_i,y_i)$:  \n  $\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$\n\n  $(w,b)$T$(w,b)$T$(x_i,y_i)$  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  $w$$b$2$w$$||w||=1$ ____\n\n* T$(w,b)$$(w,b)$$(x_i,y_i)$  \n  $\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$\n\n  $(w,b)$T$(w,b)$T$(x_i,y_i)$  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  $(w,b)$$(x_i,y_i)$\n\n    \n  $\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$\n\n  $\\gamma=\\frac{\\hat{\\gamma}}{||w||}$\n\n  __$||w||=1$__$w$$b$()\n\n\n    \n$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$\n\n$(w,b)$training set$\\gamma$$(w,b)$training sample$\\gamma$\n\n\n$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$\n\n$\\frac{1}{||w||}$$\\frac{1}{2}||w||^2$SVM\n$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$\n\n* T\n\ntraining set\n$y_i(w\\cdot x_i+b)-1=0$\n\n$y_i=+1$ $H_1:w\\cdot x+b=1$$y_i=-1$ $H_2:w\\cdot x+b=-1$\"\"\n\n\n### \nSVMSVMKernel Function\n\n$\\alpha_i \\geq 0, i=1,2,\\cdots,N$\n$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$\n\n\n$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$L(w,b,\\alpha)$$w,b$$\\alpha$\n\n1. $\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$  \n   $L(w,b,\\alpha)$$w,b$0  \n   $\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$\n\n   $\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$  \n  :  \n  $w=\\sum_{i=1}^N\\alpha_i y_i x_i$  \n  $\\sum_{i=1}^N\\alpha_i y_i=0$  \n  :  \n  $\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$\n\n2. $\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$\\alpha$  \n  $$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n  \n  $$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n#### SVM\n1. :\n  $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$  \n  $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2.  $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$  $\\alpha^{\\star}$  $\\alpha_j^{\\star}>0$:  \n  $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$\n\n3.  $w^{\\star}\\cdot x+b^{\\star}=0$ $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n## SVM\n$(x_i,y_i)$1$(x_i, y_i)$$\\xi_i \\geq0$1:  \n$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i$$\\frac{1}{2}||w||^2$ $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$$C>0$$C$$C$Loss$\\frac{1}{2}||w||^2$$C$\n\nSVM  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$\n   \n### \n:\n$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$\n$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$\n\n#### SVM\n1. $C>0$  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$  \n   $s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$  \n   $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. $w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$  \n   $\\alpha^{\\star}$$\\alpha_j^{\\star}$$0<\\alpha_j^{\\star}<C$ ()  \n   $b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$\n  \n3.  $w^{\\star}\\cdot x+b^{\\star}=0$\n   $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n#### \n$x_i$  \n\n$\\alpha_i^{\\star}<C$$\\xi_i=0$$x_i$  \n$\\alpha_i^{\\star}=C, 0<\\xi_i<1$$x_i$  \n$\\alpha_i^{\\star}=C, \\xi_i=1$$x_i$  \n$\\alpha_i^{\\star}=C, \\xi_i>1$$x_i$\n\n#### Hinge Loss\nSVMLoss Function  \n$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$\n\n$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$ Hinge Loss$(x_i,y_i)$ $y_i(w\\cdot x_i+b)$10 $1-y_i(w\\cdot x_i+b)$\n\nSVM:  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$\n\n$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i\\geq 0$\n\n\n\n$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$\n\n## SVMKernel Function\n$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$()  \n$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$  \n$z=\\phi(x)$$\\chi \\subset R^2$$\\mathcal{Z}\\subset R^2$\n\n### Kernel Function\n$\\chi$($R^n$)$\\mathcal{H}$()$\\chi$$\\mathcal{H}$:  \n$\\phi(x):\\chi \\to \\mathcal{H}$\n$x,z\\in \\chi$$K(x,z)$\n$K(x,z)=\\phi(x)\\cdot \\phi(z)$  \n$K(x,z)$$\\phi(x)$$\\phi(x)\\cdot \\phi(z)$$\\phi(x)$$\\phi(z)$\n\nKernel Tricks$K(x,z)$$\\phi$$K(x,z)$$\\phi(x)$$\\phi(z)$$K(x,z)$\n\nKernel-based SVM$\\phi$$x_i\\cdot x_j$$\\phi(x_i)\\cdot \\phi(x_j)$SVMSVM\n\nKernel TricksKernel Tricks\n\n### Kernel Function\n1.   \n   $K(x,z)=(x\\cdot z+1)^p$  \n   SVM$p$:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$\n\n2.   \n   $K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$\n   SVMRBF:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$\n\n### SVM\nKernel Tricks  \n$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$\n$K(x,z)$\n\n#### SVM\n1. $K(x,z)$$C$  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$\n\n   $s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$\n   $\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. $\\alpha^{\\star}$$0<\\alpha_i^{\\star}<C$ $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$\n\n3.   \n   $f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$","slug":"ml-svm","published":1,"updated":"2018-10-01T04:40:09.601Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cy001h608w6u6ihik2","content":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>SVMSVM <strong>Hinge Loss</strong></p>\n<p><br><br>Kernel Tricks</p>\n<p> <strong>SVMSVM</strong></p>\n<p>training setMLP <strong></strong>SVM <strong></strong></p>\n<h2 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h2><p>$w\\cdot x+b=0$$|w\\cdot x+b|$$x$$w\\cdot x+b$$y$$y(w\\cdot x+b)$ <strong></strong></p>\n<ul>\n<li><p>T$(w,b)$$(w,b)$$(x_i,y_i)$:<br>$\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$</p>\n<p>$(w,b)$T$(w,b)$T$(x_i,y_i)$<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>$w$$b$2$w$$||w||=1$ <strong></strong></p>\n</li>\n<li><p>T$(w,b)$$(w,b)$$(x_i,y_i)$<br>$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$</p>\n<p>$(w,b)$T$(w,b)$T$(x_i,y_i)$<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>$(w,b)$$(x_i,y_i)$</p>\n<p><br>$\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$</p>\n<p>$\\gamma=\\frac{\\hat{\\gamma}}{||w||}$</p>\n<p><strong>$||w||=1$</strong>$w$$b$()</p>\n</li>\n</ul>\n<p>  <br>$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$</p>\n<p>$(w,b)$training set$\\gamma$$(w,b)$training sample$\\gamma$</p>\n<p><br>$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$</p>\n<p>$\\frac{1}{||w||}$$\\frac{1}{2}||w||^2$SVM<br>$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$</p>\n<ul>\n<li>T</li>\n</ul>\n<p>training set<br>$y_i(w\\cdot x_i+b)-1=0$</p>\n<p>$y_i=+1$ $H_1:w\\cdot x+b=1$$y_i=-1$ $H_2:w\\cdot x+b=-1$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>SVMSVMKernel Function</p>\n<p>$\\alpha_i \\geq 0, i=1,2,\\cdots,N$<br>$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$</p>\n<p><br>$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$L(w,b,\\alpha)$$w,b$$\\alpha$</p>\n<ol>\n<li><p>$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$<br>$L(w,b,\\alpha)$$w,b$0<br>$\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$</p>\n<p>$\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$<br>:<br>$w=\\sum_{i=1}^N\\alpha_i y_i x_i$<br>$\\sum_{i=1}^N\\alpha_i y_i=0$<br>:<br>$\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$</p>\n</li>\n<li><p>$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$\\alpha$<br>$$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n<p><br>$$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n</li>\n</ol>\n<h4 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>:<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p> $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$  $\\alpha^{\\star}$  $\\alpha_j^{\\star}&gt;0$:<br>$b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$</p>\n</li>\n<li><p> $w^{\\star}\\cdot x+b^{\\star}=0$ $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h2 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h2><p>$(x_i,y_i)$1$(x_i, y_i)$$\\xi_i \\geq0$1:<br>$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i$$\\frac{1}{2}||w||^2$ $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$$C&gt;0$$C$$C$Loss$\\frac{1}{2}||w||^2$$C$</p>\n<p>SVM<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$</p>\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p>:<br>$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$<br>$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$</p>\n<h4 id=\"SVM-1\"><a href=\"#SVM-1\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>$C&gt;0$<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$<br>$s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$<br>$\\alpha^{\\star}$$\\alpha_j^{\\star}$$0&lt;\\alpha_j^{\\star}&lt;C$ ()<br>$b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$</p>\n</li>\n<li><p> $w^{\\star}\\cdot x+b^{\\star}=0$<br>$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>$x_i$  </p>\n<p>$\\alpha_i^{\\star}&lt;C$$\\xi_i=0$$x_i$<br>$\\alpha_i^{\\star}=C, 0&lt;\\xi_i&lt;1$$x_i$<br>$\\alpha_i^{\\star}=C, \\xi_i=1$$x_i$<br>$\\alpha_i^{\\star}=C, \\xi_i&gt;1$$x_i$</p>\n<h4 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h4><p>SVMLoss Function<br>$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$</p>\n<p>$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$ Hinge Loss$(x_i,y_i)$ $y_i(w\\cdot x_i+b)$10 $1-y_i(w\\cdot x_i+b)$</p>\n<p>SVM:<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$</p>\n<p>$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i\\geq 0$</p>\n<p></p>\n<p>$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$</p>\n<h2 id=\"SVMKernel-Function\"><a href=\"#SVMKernel-Function\" class=\"headerlink\" title=\"SVMKernel Function\"></a>SVMKernel Function</h2><p>$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$()<br>$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$<br>$z=\\phi(x)$$\\chi \\subset R^2$$\\mathcal{Z}\\subset R^2$</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><p>$\\chi$($R^n$)$\\mathcal{H}$()$\\chi$$\\mathcal{H}$:<br>$\\phi(x):\\chi \\to \\mathcal{H}$<br>$x,z\\in \\chi$$K(x,z)$<br>$K(x,z)=\\phi(x)\\cdot \\phi(z)$<br>$K(x,z)$$\\phi(x)$$\\phi(x)\\cdot \\phi(z)$$\\phi(x)$$\\phi(z)$</p>\n<p>Kernel Tricks$K(x,z)$$\\phi$$K(x,z)$$\\phi(x)$$\\phi(z)$$K(x,z)$</p>\n<p>Kernel-based SVM$\\phi$$x_i\\cdot x_j$$\\phi(x_i)\\cdot \\phi(x_j)$SVMSVM</p>\n<p>Kernel TricksKernel Tricks</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><ol>\n<li><p><br>$K(x,z)=(x\\cdot z+1)^p$<br>SVM$p$:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$</p>\n</li>\n<li><p><br>$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$<br>SVMRBF:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$</p>\n</li>\n</ol>\n<h3 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h3><p>Kernel Tricks<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$<br>$K(x,z)$</p>\n<h4 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>$K(x,z)$$C$<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$</p>\n<p>$s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>$\\alpha^{\\star}$$0&lt;\\alpha_i^{\\star}&lt;C$ $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$</p>\n</li>\n<li><p><br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>SVMSVM <strong>Hinge Loss</strong></p>\n<p><br><br>Kernel Tricks</p>\n<p> <strong>SVMSVM</strong></p>\n<p>training setMLP <strong></strong>SVM <strong></strong></p>\n<h2 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h2><p>$w\\cdot x+b=0$$|w\\cdot x+b|$$x$$w\\cdot x+b$$y$$y(w\\cdot x+b)$ <strong></strong></p>\n<ul>\n<li><p>T$(w,b)$$(w,b)$$(x_i,y_i)$:<br>$\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$</p>\n<p>$(w,b)$T$(w,b)$T$(x_i,y_i)$<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>$w$$b$2$w$$||w||=1$ <strong></strong></p>\n</li>\n<li><p>T$(w,b)$$(w,b)$$(x_i,y_i)$<br>$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$</p>\n<p>$(w,b)$T$(w,b)$T$(x_i,y_i)$<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>$(w,b)$$(x_i,y_i)$</p>\n<p><br>$\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$</p>\n<p>$\\gamma=\\frac{\\hat{\\gamma}}{||w||}$</p>\n<p><strong>$||w||=1$</strong>$w$$b$()</p>\n</li>\n</ul>\n<p>  <br>$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$</p>\n<p>$(w,b)$training set$\\gamma$$(w,b)$training sample$\\gamma$</p>\n<p><br>$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$</p>\n<p>$\\frac{1}{||w||}$$\\frac{1}{2}||w||^2$SVM<br>$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$</p>\n<ul>\n<li>T</li>\n</ul>\n<p>training set<br>$y_i(w\\cdot x_i+b)-1=0$</p>\n<p>$y_i=+1$ $H_1:w\\cdot x+b=1$$y_i=-1$ $H_2:w\\cdot x+b=-1$</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>SVMSVMKernel Function</p>\n<p>$\\alpha_i \\geq 0, i=1,2,\\cdots,N$<br>$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$</p>\n<p><br>$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$L(w,b,\\alpha)$$w,b$$\\alpha$</p>\n<ol>\n<li><p>$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$<br>$L(w,b,\\alpha)$$w,b$0<br>$\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$</p>\n<p>$\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$<br>:<br>$w=\\sum_{i=1}^N\\alpha_i y_i x_i$<br>$\\sum_{i=1}^N\\alpha_i y_i=0$<br>:<br>$\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$</p>\n</li>\n<li><p>$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$$\\alpha$<br>$$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n<p><br>$$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n</li>\n</ol>\n<h4 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>:<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p> $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$  $\\alpha^{\\star}$  $\\alpha_j^{\\star}&gt;0$:<br>$b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$</p>\n</li>\n<li><p> $w^{\\star}\\cdot x+b^{\\star}=0$ $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h2 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h2><p>$(x_i,y_i)$1$(x_i, y_i)$$\\xi_i \\geq0$1:<br>$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i$$\\frac{1}{2}||w||^2$ $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$$C&gt;0$$C$$C$Loss$\\frac{1}{2}||w||^2$$C$</p>\n<p>SVM<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$</p>\n<h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p>:<br>$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$<br>$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$</p>\n<h4 id=\"SVM-1\"><a href=\"#SVM-1\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>$C&gt;0$<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$<br>$s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$<br>$\\alpha^{\\star}$$\\alpha_j^{\\star}$$0&lt;\\alpha_j^{\\star}&lt;C$ ()<br>$b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$</p>\n</li>\n<li><p> $w^{\\star}\\cdot x+b^{\\star}=0$<br>$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>$x_i$  </p>\n<p>$\\alpha_i^{\\star}&lt;C$$\\xi_i=0$$x_i$<br>$\\alpha_i^{\\star}=C, 0&lt;\\xi_i&lt;1$$x_i$<br>$\\alpha_i^{\\star}=C, \\xi_i=1$$x_i$<br>$\\alpha_i^{\\star}=C, \\xi_i&gt;1$$x_i$</p>\n<h4 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h4><p>SVMLoss Function<br>$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$</p>\n<p>$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$ Hinge Loss$(x_i,y_i)$ $y_i(w\\cdot x_i+b)$10 $1-y_i(w\\cdot x_i+b)$</p>\n<p>SVM:<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$</p>\n<p>$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i\\geq 0$</p>\n<p></p>\n<p>$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$</p>\n<h2 id=\"SVMKernel-Function\"><a href=\"#SVMKernel-Function\" class=\"headerlink\" title=\"SVMKernel Function\"></a>SVMKernel Function</h2><p>$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$()<br>$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$<br>$z=\\phi(x)$$\\chi \\subset R^2$$\\mathcal{Z}\\subset R^2$</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><p>$\\chi$($R^n$)$\\mathcal{H}$()$\\chi$$\\mathcal{H}$:<br>$\\phi(x):\\chi \\to \\mathcal{H}$<br>$x,z\\in \\chi$$K(x,z)$<br>$K(x,z)=\\phi(x)\\cdot \\phi(z)$<br>$K(x,z)$$\\phi(x)$$\\phi(x)\\cdot \\phi(z)$$\\phi(x)$$\\phi(z)$</p>\n<p>Kernel Tricks$K(x,z)$$\\phi$$K(x,z)$$\\phi(x)$$\\phi(z)$$K(x,z)$</p>\n<p>Kernel-based SVM$\\phi$$x_i\\cdot x_j$$\\phi(x_i)\\cdot \\phi(x_j)$SVMSVM</p>\n<p>Kernel TricksKernel Tricks</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><ol>\n<li><p><br>$K(x,z)=(x\\cdot z+1)^p$<br>SVM$p$:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$</p>\n</li>\n<li><p><br>$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$<br>SVMRBF:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$</p>\n</li>\n</ol>\n<h3 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h3><p>Kernel Tricks<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$<br>$K(x,z)$</p>\n<h4 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h4><ol>\n<li><p>$K(x,z)$$C$<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$</p>\n<p>$s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$<br>$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>$\\alpha^{\\star}$$0&lt;\\alpha_i^{\\star}&lt;C$ $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$</p>\n</li>\n<li><p><br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$</p>\n</li>\n</ol>\n"},{"title":"[ML] Naive Bayes","catalog":false,"mathjax":true,"date":"2018-07-19T08:17:20.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n1. Naive Bayes Bayes Theorem ____  __/__$x$Bayes Theorem$y$\n\n2. Naive Bayes$P(X,Y)$Naive Bayes\n$$\nP(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)\n$$\n\nNaive Bayes ____ ____\n\n## \n$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\\n\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$\n\n$c_k$\n$$\ny=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)\n$$\n\n0-1\n$$\nL(Y,f(X))=\n\\begin{cases}\n    1, & Y\\neq f(X)\\\\\n    0, & otherwise\n\\end{cases}\n$$\n\n$$\nR_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)\n$$\n\n$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\\n=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\\n=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$\n\n\n$$\nf(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)\n$$\n\n$P(Y=c_k)$\n$$\nP(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K\n$$\n$j$$x^{(j)}$$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$$P(X^{(j)}=a_{jl}|Y=c_k)$\n$$\nP(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}\n$$\n","source":"_posts/ml-nb.md","raw":"---\ntitle: \"[ML] Naive Bayes\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 16:17:20\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n1. Naive Bayes Bayes Theorem ____  __/__$x$Bayes Theorem$y$\n\n2. Naive Bayes$P(X,Y)$Naive Bayes\n$$\nP(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)\n$$\n\nNaive Bayes ____ ____\n\n## \n$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\\n\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$\n\n$c_k$\n$$\ny=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)\n$$\n\n0-1\n$$\nL(Y,f(X))=\n\\begin{cases}\n    1, & Y\\neq f(X)\\\\\n    0, & otherwise\n\\end{cases}\n$$\n\n$$\nR_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)\n$$\n\n$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\\n=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\\n=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$\n\n\n$$\nf(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)\n$$\n\n$P(Y=c_k)$\n$$\nP(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K\n$$\n$j$$x^{(j)}$$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$$P(X^{(j)}=a_{jl}|Y=c_k)$\n$$\nP(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}\n$$\n","slug":"ml-nb","published":1,"updated":"2018-10-01T04:40:09.058Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03d0001k608wdj1ah9ts","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ol>\n<li><p>Naive Bayes Bayes Theorem <strong></strong>  <strong>/</strong>$x$Bayes Theorem$y$</p>\n</li>\n<li><p>Naive Bayes$P(X,Y)$Naive Bayes<br>$$<br>P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)<br>$$</p>\n</li>\n</ol>\n<p>Naive Bayes <strong></strong> <strong></strong></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\<br>\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$</p>\n<p>$c_k$<br>$$<br>y=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$</p>\n<p>0-1<br>$$<br>L(Y,f(X))=<br>\\begin{cases}<br>    1, &amp; Y\\neq f(X)\\\\<br>    0, &amp; otherwise<br>\\end{cases}<br>$$<br><br>$$<br>R_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)<br>$$<br><br>$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\<br>=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\<br>=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$</p>\n<p><br>$$<br>f(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)<br>$$</p>\n<p>$P(Y=c_k)$<br>$$<br>P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K<br>$$<br>$j$$x^{(j)}$$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$$P(X^{(j)}=a_{jl}|Y=c_k)$<br>$$<br>P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}<br>$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ol>\n<li><p>Naive Bayes Bayes Theorem <strong></strong>  <strong>/</strong>$x$Bayes Theorem$y$</p>\n</li>\n<li><p>Naive Bayes$P(X,Y)$Naive Bayes<br>$$<br>P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)<br>$$</p>\n</li>\n</ol>\n<p>Naive Bayes <strong></strong> <strong></strong></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\<br>\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$</p>\n<p>$c_k$<br>$$<br>y=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$</p>\n<p>0-1<br>$$<br>L(Y,f(X))=<br>\\begin{cases}<br>    1, &amp; Y\\neq f(X)\\\\<br>    0, &amp; otherwise<br>\\end{cases}<br>$$<br><br>$$<br>R_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)<br>$$<br><br>$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\<br>=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\<br>=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$</p>\n<p><br>$$<br>f(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)<br>$$</p>\n<p>$P(Y=c_k)$<br>$$<br>P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K<br>$$<br>$j$$x^{(j)}$$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$$P(X^{(j)}=a_{jl}|Y=c_k)$<br>$$<br>P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}<br>$$</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bm0004608w17rmwwmo","_id":"cjopy03c3000g608wf7lx94fn"},{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bs0008608wwz5s7ov2","_id":"cjopy03c4000i608wu5lef2l1"},{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bw000b608w44ac37bt","_id":"cjopy03c7000l608wkkf0hm63"},{"post_id":"cjopy03bi0002608wgrt7lxch","tag_id":"cjopy03c1000e608wfz56akxl","_id":"cjopy03cb000p608ws5igym68"},{"post_id":"cjopy03bi0002608wgrt7lxch","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cd000r608wm525nrix"},{"post_id":"cjopy03cf000u608w5h4oygdy","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ch000x608ws3mnmrav"},{"post_id":"cjopy03cf000u608w5h4oygdy","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ci000z608w23t1bdzw"},{"post_id":"cjopy03cf000v608wanrj1kpm","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ck0011608wgcy1nhcq"},{"post_id":"cjopy03cf000v608wanrj1kpm","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cm0014608wi0rp9jc6"},{"post_id":"cjopy03ch000y608w2p2hng7v","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03co0016608wiab3uiac"},{"post_id":"cjopy03ch000y608w2p2hng7v","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cs0019608wmr0zk1qs"},{"post_id":"cjopy03cj0010608wwmlxn6on","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ct001b608w8ongyqzh"},{"post_id":"cjopy03cj0010608wwmlxn6on","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cw001e608whlvgv9s0"},{"post_id":"cjopy03cm0015608wne980x56","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03cy001g608wkrnhbn3l"},{"post_id":"cjopy03cm0015608wne980x56","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d0001j608wk2rk2ddk"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d1001l608wnau16r6p"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03d2001n608w9joe0g42"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03d3001o608wi90wi7gl"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ck0012608ws1vrr4sc","_id":"cjopy03d3001q608wnpcqumkk"},{"post_id":"cjopy03cp0017608wxlenocz5","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d4001r608w1nj135al"},{"post_id":"cjopy03cp0017608wxlenocz5","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d5001t608wxuys423q"},{"post_id":"cjopy03cs001a608wk9gey017","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d5001u608wc7fn2d1t"},{"post_id":"cjopy03cs001a608wk9gey017","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d6001w608wpow8d2v9"},{"post_id":"cjopy03cu001c608w09ol81m3","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d6001x608w9ua0r3vv"},{"post_id":"cjopy03cu001c608w09ol81m3","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d7001z608w6ux9inhi"},{"post_id":"cjopy03cw001f608wuc5jmdy4","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d70020608w3pstwl1w"},{"post_id":"cjopy03cw001f608wuc5jmdy4","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d80022608wgof63nz0"},{"post_id":"cjopy03cy001h608w6u6ihik2","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d90023608w0yj6x1ig"},{"post_id":"cjopy03cy001h608w6u6ihik2","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03db0025608wp2a2rypb"},{"post_id":"cjopy03d0001k608wdj1ah9ts","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dc0026608w2ob5z9ye"},{"post_id":"cjopy03d0001k608wdj1ah9ts","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03de0028608w3suobw94"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03de0029608w12gqsj3d"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03df002b608webh36oev"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dg002c608wgq7qttv4"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03d2001m608w5bzwzhgj","_id":"cjopy03dg002d608wx8p2uo96"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dh002f608wfm82h97b"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03di002g608w9zn5d7n5"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dj002i608wkbj88e2u"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03d7001y608w5w80cwkc","_id":"cjopy03dj002j608wmxtsw8z3"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dk002l608wi2wc9g1z"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dk002m608w5eubtti6"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dl002o608w93ytwz4v"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03df002a608wnjm0x89p","_id":"cjopy03dm002p608w3nrpi7rz"},{"post_id":"cjopy03bv000a608w1fey66vl","tag_id":"cjopy03df002a608wnjm0x89p","_id":"cjopy03dn002r608wgunlukew"},{"post_id":"cjopy03bv000a608w1fey66vl","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dn002s608w36ul8kyt"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03do002v608wmi2fm9vk"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dp002w608wxfg0jx0k"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03dq002y608wvfn4dfxl"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03dn002t608wjl1u8sz4","_id":"cjopy03dq002z608w5s0rhc25"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dt0034608w88o3j3fe"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dt0035608w2cpxdwxm"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03du0037608wayu0mxbb"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03dr0031608wc9rz76c5","_id":"cjopy03du0038608wzwiht4m8"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ds0032608wmfiofaxk","_id":"cjopy03dv003a608w8lvjzr0k"},{"post_id":"cjopy03c1000f608wrmkhhxc2","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dw003b608wcspxxvsn"},{"post_id":"cjopy03c1000f608wrmkhhxc2","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dx003d608wy0vhhu0y"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dz003h608wcpxsj9z7"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dz003i608wy0y12wa9"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03e1003k608wqvkpg3up"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03e1003l608wcixagd7i"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03e4003q608wnppln6n5"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03e5003r608wnm6xn1pv"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03e1003m608wniu2y5nv","_id":"cjopy03e6003t608whr0wj1pz"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03e6003u608w2aq4s9od"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03e7003w608w5tpdajo3"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03e8003y608wb5dbafwi"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03e8003z608wdlxml48c"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03e6003v608w0qm0v0nv","_id":"cjopy03e90041608we3pexlfn"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03eb0044608wy4t4qrsq"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03eb0045608wfx69q3yu"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03ec0047608wkw40jckf"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ec0048608wfr6w4wbd"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ee004b608wk7sjgg3l"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03ef004c608wn4wl7cs1"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03ef004e608wlbwxfryj"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ed0049608w3gbqmkyk","_id":"cjopy03eg004f608wfxmbo0el"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03eh004h608w2cshefss"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ei004j608wa9oehtiq"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03ej004k608wnxbenx7p"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ef004d608w6zuoqhz8","_id":"cjopy03ej004l608wbg43n8n0"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ej004m608wto6npl4a"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03eg004g608w9t4zewzw","_id":"cjopy03ek004n608wuzc9bybv"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03eh004i608wpmg4jq72","_id":"cjopy03ek004o608wb8jaht27"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ek004p608wavr7vn3t"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03el004q608wcdghwkzz"}],"Tag":[{"name":"Algorithm","_id":"cjopy03bm0004608w17rmwwmo"},{"name":"Data Structure","_id":"cjopy03bs0008608wwz5s7ov2"},{"name":"Graph","_id":"cjopy03bw000b608w44ac37bt"},{"name":"Data Visualization","_id":"cjopy03c1000e608wfz56akxl"},{"name":"Data Science","_id":"cjopy03c5000j608wsgypu3v9"},{"name":"Machine Learning","_id":"cjopy03ca000o608wz3qytqs9"},{"name":"Deep Learning","_id":"cjopy03ce000t608w91zh6ojj"},{"name":"Computer Vision","_id":"cjopy03ch000w608wemv3tk1g"},{"name":"Face Anti-Spoofing","_id":"cjopy03ck0012608ws1vrr4sc"},{"name":"Object Detection","_id":"cjopy03d2001m608w5bzwzhgj"},{"name":"Face Recognition","_id":"cjopy03d7001y608w5w80cwkc"},{"name":"Digital Image Processing","_id":"cjopy03df002a608wnjm0x89p"},{"name":"Auto Encoder","_id":"cjopy03dn002t608wjl1u8sz4"},{"name":"Image Classification","_id":"cjopy03dr0031608wc9rz76c5"},{"name":"Network Architecture","_id":"cjopy03ds0032608wmfiofaxk"},{"name":"Optimization","_id":"cjopy03dx003e608wfnw34zqd"},{"name":"CNN","_id":"cjopy03e1003m608wniu2y5nv"},{"name":"Data Augmentation","_id":"cjopy03e6003v608w0qm0v0nv"},{"name":"Regularization","_id":"cjopy03ed0049608w3gbqmkyk"},{"name":"RNN","_id":"cjopy03ef004d608w6zuoqhz8"},{"name":"NLP","_id":"cjopy03eg004g608w9t4zewzw"},{"name":"Feature Engineering","_id":"cjopy03eh004i608wpmg4jq72"}]}}