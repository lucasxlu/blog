{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/about/LICENSE","path":"about/LICENSE","modified":1,"renderable":0},{"_id":"source/about/CV_LuXu.pdf","path":"about/CV_LuXu.pdf","modified":1,"renderable":0},{"_id":"source/about/LucasX.jpg","path":"about/LucasX.jpg","modified":1,"renderable":0},{"_id":"source/images/WeChatPay.png","path":"images/WeChatPay.png","modified":1,"renderable":0},{"_id":"themes/hiker/source/css/archive.css","path":"css/archive.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/dialog.css","path":"css/dialog.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/header-post.css","path":"css/header-post.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/home.css","path":"css/home.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/vdonate.css","path":"css/vdonate.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/dialog.js","path":"js/dialog.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/home.js","path":"js/home.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/scripts.js","path":"js/scripts.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/vdonate.js","path":"js/vdonate.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/browser-support.png","path":"preview/browser-support.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/donation-btn.png","path":"preview/donation-btn.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/preview-abstract.png","path":"preview/preview-abstract.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/theme-color.png","path":"preview/theme-color.png","modified":1,"renderable":1},{"_id":"source/images/Alipay.jpg","path":"images/Alipay.jpg","modified":1,"renderable":0},{"_id":"themes/hiker/source/js/bootstrap.js","path":"js/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/js/jquery-3.1.1.min.js","path":"js/jquery-3.1.1.min.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/bootstrap.css","path":"css/bootstrap.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/avatar.jpg","path":"css/images/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/homelogo.jpg","path":"css/images/homelogo.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/mylogo.jpg","path":"css/images/mylogo.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/rocket.png","path":"css/images/rocket.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/preview-mobile.png","path":"preview/preview-mobile.png","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/home-bg.jpg","path":"css/images/home-bg.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/sample.jpg","path":"css/images/sample.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/css/images/pose.jpg","path":"css/images/pose.jpg","modified":1,"renderable":1},{"_id":"themes/hiker/source/preview/code-theme.jpg","path":"preview/code-theme.jpg","modified":1,"renderable":1},{"_id":"source/projects/LagouJob.pdf","path":"projects/LagouJob.pdf","modified":1,"renderable":0},{"_id":"source/about/Presentation_TransFBP.pdf","path":"about/Presentation_TransFBP.pdf","modified":1,"renderable":0},{"_id":"themes/hiker/source/preview/preview-pc.png","path":"preview/preview-pc.png","modified":1,"renderable":1},{"_id":"source/projects/MLJob.pdf","path":"projects/MLJob.pdf","modified":1,"renderable":0},{"_id":"source/about/Research_Overview.pdf","path":"about/Research_Overview.pdf","modified":1,"renderable":0},{"_id":"source/projects/JiaYuan.pdf","path":"projects/JiaYuan.pdf","modified":1,"renderable":0},{"_id":"source/about/Presentation.pdf","path":"about/Presentation.pdf","modified":1,"renderable":0},{"_id":"source/about/DL_for_Face_Analysis.pdf","path":"about/DL_for_Face_Analysis.pdf","modified":1,"renderable":0},{"_id":"source/projects/GovReport.pdf","path":"projects/GovReport.pdf","modified":1,"renderable":0},{"_id":"source/projects/IndustryReport.pdf","path":"projects/IndustryReport.pdf","modified":1,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"c874a9c65eaf1b3e4f81bfaf1f09876474f56c50","modified":1541216985174},{"_id":"themes/hiker/.gitignore","hash":"ea2b285a29690f1eabbad0f3a158e34e9ccd1d86","modified":1538368810463},{"_id":"themes/hiker/.travis.yml","hash":"7ed5eb33c899eb49ec323f7ed7ee431bea52bf4f","modified":1538368810464},{"_id":"themes/hiker/Gruntfile.js","hash":"412e30530784993c8997aa8b1319c669b83b91c2","modified":1538368810464},{"_id":"themes/hiker/LICENSE","hash":"df913ae888823f1551d5f1837902f7ccb2634459","modified":1538368810465},{"_id":"themes/hiker/README.md","hash":"2bc2899d048e6df9dff5ad01bdf550bb6a158e24","modified":1538368810466},{"_id":"themes/hiker/_config.yml","hash":"9f5d5f747eb104f520d645b350457dfd6d4fa6d9","modified":1538368810467},{"_id":"themes/hiker/package.json","hash":"5800ffd530e6ec7fb9e3f261b5e3a1286c2b54f6","modified":1538368812127},{"_id":"source/_posts/algo-sort.md","hash":"bcc5c47d727484142d8c72356a4347910f26df09","modified":1538368808403},{"_id":"source/_posts/book-storytelling-with-data.md","hash":"d630523771033fabcfa2b7f753db3bf3fa890e44","modified":1538368808404},{"_id":"source/_posts/cv-antispoofing.md","hash":"0c3107be0dcc936ef961c349d6b50541b743be20","modified":1540830325350},{"_id":"source/_posts/cv-detection.md","hash":"306792e8ba035a6d3db84ff294e87dfbcf89b554","modified":1542367993192},{"_id":"source/_posts/cv-face-rec.md","hash":"b13cb50b6cdb39522cdc5e15d43fa12854f328a8","modified":1538453982047},{"_id":"source/_posts/cv-iqa.md","hash":"71763dccf1cfc783a873739937c1dc73990d4f59","modified":1541340827400},{"_id":"source/_posts/dip-image-feature.md","hash":"e85e7d437fa64e75c64e0b28c32316d249bf41e3","modified":1538368808551},{"_id":"source/_posts/dl-ae.md","hash":"806a6bb62806f53e9096c6432bb7932759f1f054","modified":1538368808655},{"_id":"source/_posts/dl-architecture.md","hash":"dbb386deb2134506295702b265d7ecc703530e92","modified":1542551356777},{"_id":"source/_posts/dl-bn.md","hash":"4189407c07a8ef7e8a0faf44fa5134ddec35b5f2","modified":1542730625441},{"_id":"source/_posts/dl-bp.md","hash":"694363e13949ed24cde363d87334a5c3bacf6317","modified":1538368808673},{"_id":"source/_posts/dl-cnn.md","hash":"6852d9ab047b7759dfad7fbb8a224e39ecc7e00e","modified":1538368808836},{"_id":"source/_posts/dl-data-augmentation.md","hash":"749cc06d1c62ba1543f7810cebd2b2bd8952f409","modified":1541862191194},{"_id":"source/_posts/dl-optimization.md","hash":"389df4cf7981593c24187bd077da962fcebb1dac","modified":1538368808837},{"_id":"source/_posts/dl-regularization.md","hash":"a855b01508c19acc1ed9cfd4a0a397aa10a0707b","modified":1538368808901},{"_id":"source/_posts/dl-rnn.md","hash":"0b59ced6bfcafed68ebba04698ea0d8ecb23555d","modified":1538368808921},{"_id":"source/_posts/ml-clustering.md","hash":"d77a99e767eefe7ed92e35a7e7ee85bb6bee0aba","modified":1538368808983},{"_id":"source/_posts/ml-dimen-red-metric-learning.md","hash":"8c456922432af72fe5ea7f28f173f480314f7b62","modified":1538368808986},{"_id":"source/_posts/ml-dt.md","hash":"dbf7935c4401b854ce0176ed307694b271db9889","modified":1538368808993},{"_id":"source/_posts/ml-ensemble.md","hash":"8f727f4e650947015c526f79078cce3ca1951454","modified":1538368808997},{"_id":"source/_posts/ml-feml.md","hash":"6602263e2b8165101f04cffba46af1b898441cc7","modified":1538368809000},{"_id":"source/_posts/ml-knn.md","hash":"dd539203177f55483b9acd2ef0e5004809390ae8","modified":1538368809017},{"_id":"source/_posts/ml-lm.md","hash":"d0b73db8fbf476ce95442cd8b8a0d9265b046045","modified":1538368809047},{"_id":"source/_posts/ml-loss.md","hash":"947bd90a92c907004f33082df3da0206d781a1b7","modified":1538368809048},{"_id":"source/_posts/ml-lr-me.md","hash":"140672851319d2f17fc0dab9bab351f70cd3e0b3","modified":1538368809052},{"_id":"source/_posts/ml-model-selection-metric.md","hash":"0887be5123918022d8b98b5e15d7a77ae5d7203f","modified":1538368809054},{"_id":"source/_posts/ml-nb.md","hash":"6c426058095498e2ded9386f764a623ffca2cc22","modified":1538368809058},{"_id":"source/_posts/ml-svm.md","hash":"35aa11adce9ed6048af3db16dc711b2d18a3032a","modified":1538368809601},{"_id":"source/about/LICENSE","hash":"6cfbca72152ed7d78b31fb88f23b5c6087fff34d","modified":1538368809612},{"_id":"source/about/index.md","hash":"a72099cb184ff6b3755437f4496546dc0cda7c3c","modified":1542537756880},{"_id":"source/archive/index.md","hash":"fe084b4f62929343d70032c307570be2c6cb108c","modified":1538368809635},{"_id":"source/papers/index.md","hash":"250c4e3af4d7bc8c14bdaa65dfdb46e99433b3d6","modified":1542537795437},{"_id":"source/projects/index.md","hash":"a8b840da198db5173709e9dda103f921c4ed2db6","modified":1542505627514},{"_id":"source/tags/index.md","hash":"54f6b44c51d546a9c2fef80d5628f79744ecefc9","modified":1538368810462},{"_id":"themes/hiker/languages/de.yml","hash":"2801ddc0807ce707905203a9f2eb9ccced959991","modified":1538368810940},{"_id":"themes/hiker/languages/default.yml","hash":"6bd574a83c1445fdba9f451cf2cc9b2738e4b5cf","modified":1538368810941},{"_id":"themes/hiker/languages/en.yml","hash":"570f53de74ae233b0fe07abb61d2db7bf2a3f755","modified":1538368810941},{"_id":"themes/hiker/languages/es.yml","hash":"e3b4937da4cd2d0393b8a0ba310e70fc605cc431","modified":1538368810942},{"_id":"themes/hiker/languages/fr.yml","hash":"d67ddc5a00060e67de561b4be7bd14ac4e2d7186","modified":1538368810943},{"_id":"themes/hiker/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1538368810943},{"_id":"themes/hiker/languages/no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1538368811202},{"_id":"themes/hiker/languages/pt.yml","hash":"0ec64b7e134e802846125770782fab9590495bcd","modified":1538368811205},{"_id":"themes/hiker/languages/ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1538368811210},{"_id":"themes/hiker/languages/zh-CN.yml","hash":"17d040c8a0b2964b9d5cce305057cb5264169a0d","modified":1538368811212},{"_id":"themes/hiker/languages/zh-TW.yml","hash":"34d3b2fb9cc61a3f6df7936868fc8877fe08c842","modified":1538368811214},{"_id":"themes/hiker/layout/archive.ejs","hash":"fff20dc39a59641b35bbc921866921375ef584a5","modified":1538368811765},{"_id":"themes/hiker/layout/categories.ejs","hash":"5e8ec5304c76f80bcecb710d83c3dead4cab8a2a","modified":1538368811766},{"_id":"themes/hiker/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1538368811766},{"_id":"themes/hiker/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1538368811767},{"_id":"themes/hiker/layout/layout.ejs","hash":"f4e7d8cdf47f654f8c011ae21bc77f2c71219861","modified":1538368812111},{"_id":"themes/hiker/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1538368812112},{"_id":"themes/hiker/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1538368812113},{"_id":"themes/hiker/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1538368812125},{"_id":"themes/hiker/layout/tags.ejs","hash":"f1f8530e856cb7e7bfed044d2117a4d8b374709a","modified":1538368812126},{"_id":"themes/hiker/scripts/fancybox.js","hash":"4c130fc242cf9b59b5df6ca5eae3b14302311e8c","modified":1538368812131},{"_id":"source/about/CV_LuXu.pdf","hash":"a82c924ef6afad3e065bf180e328af495df75b68","modified":1540621004844},{"_id":"source/about/LucasX.jpg","hash":"b50fed3c6eed4cd39ada918654f11a3b086d02f2","modified":1538368809615},{"_id":"source/images/WeChatPay.png","hash":"b5d525b4ea3741bdd8edc8c5679a23aa2ec86c53","modified":1538368809643},{"_id":"source/_posts/book-storytelling-with-data/bar.png","hash":"29bcf6273e43b3229a3319cb6d1a057936972690","modified":1538368808407},{"_id":"source/_posts/book-storytelling-with-data/double-y-before.png","hash":"a89b89a3650914b6e0b3809c1d85c901dccc311d","modified":1538368808413},{"_id":"source/_posts/book-storytelling-with-data/double-y-revised.png","hash":"01619ff15d1726106b1718dc03b5fcaa7b7ba585","modified":1538368808460},{"_id":"source/_posts/book-storytelling-with-data/less-is-more.png","hash":"17aaf8978cde23669ec012e3efb6f3d03d5c6449","modified":1538368808464},{"_id":"source/_posts/book-storytelling-with-data/numbers.png","hash":"9d86f9507997d383ac1f8ae170db7a835e55cd8f","modified":1538368808466},{"_id":"source/_posts/book-storytelling-with-data/one-line-highlighted.png","hash":"1502949aba98a3ead7d016cb322f6c1e03269e65","modified":1538368808468},{"_id":"source/_posts/book-storytelling-with-data/pie-chart.png","hash":"e2e068280da0937a7866355615b122c737c643bc","modified":1538368808469},{"_id":"source/_posts/book-storytelling-with-data/report_revised.png","hash":"b2f8aa914801bbc5c2a336dad8ec728d13d452b6","modified":1538368808474},{"_id":"source/_posts/book-storytelling-with-data/spaghetti-graph.png","hash":"89f2210659ece1bf9a33124e93350f5eeff18388","modified":1538368808477},{"_id":"source/_posts/book-storytelling-with-data/tables.png","hash":"865d334c5466cde940316943cf2e1855fdc2b429","modified":1538368808479},{"_id":"source/_posts/cv-detection/cw_vs_spp.jpg","hash":"a852caba3d307a51a278ea707a6b16b83472db8b","modified":1538368808489},{"_id":"source/_posts/dl-architecture/dense_block.jpg","hash":"3d8de90b4d0467bcac4ebbf2eac4f71c694e482e","modified":1541434807540},{"_id":"source/_posts/dl-architecture/inception_module.jpg","hash":"f1f60f3ec596923f2ed2800cd053db0db3fed18a","modified":1542549776309},{"_id":"source/_posts/dl-architecture/proposed_residual_unit.jpg","hash":"6018312a7c6d3cc0e77012b9eb7a462d6ab86c3c","modified":1541939890019},{"_id":"source/_posts/dl-bn/BN_update.jpg","hash":"202106fa90dba5144d85b00bb5711bebb3dd146b","modified":1542704411749},{"_id":"source/_posts/dl-bp/fig2.png","hash":"b59eb2fcd6c25e3e4b6e7f92db93c48ed257341a","modified":1538368808801},{"_id":"source/_posts/dl-bp/fig3.png","hash":"8e61dc1b02e406fe50e45afea07b415d04604781","modified":1538368808806},{"_id":"source/_posts/dl-optimization/newtons_method.jpg","hash":"ec21c305192f2bafb9194fb56b4046d5c5b2db46","modified":1538368808886},{"_id":"source/_posts/dl-regularization/es.jpg","hash":"7c9598650d06f0baee27c569e945cf4e249da104","modified":1538368808916},{"_id":"source/_posts/dl-rnn/cg-unfold.jpg","hash":"7054e032d3cbb4107e375fe1d6cd92dbfd27624e","modified":1538368808934},{"_id":"source/_posts/dl-rnn/rnn-bp.jpg","hash":"4cfd440ae9b358a8b694427c84846591e1543e46","modified":1538368808944},{"_id":"source/_posts/dl-rnn/rnn3.jpg","hash":"d7ca30133caa000822317ec7bc3772e2c72b4fcc","modified":1538368808971},{"_id":"source/_posts/dl-rnn/teacher-forcing.jpg","hash":"037418943fffc91804709ea357f088b4570cb14f","modified":1538368808979},{"_id":"source/_posts/ml-feml/blackoff_bin.png","hash":"e6df155725db92f6745df1bb77d1bc2128b083bb","modified":1538368809010},{"_id":"source/_posts/ml-loss/log-cosh.png","hash":"7efea2aaa1339485ef55a2237ba9c33f41400a70","modified":1538368809050},{"_id":"themes/hiker/layout/_partial/after-footer.ejs","hash":"cb3e2715acd7fdde1d4c7d439dec6cae71179887","modified":1538368811219},{"_id":"themes/hiker/layout/_partial/archive-post.ejs","hash":"7f17ae361df4071091b37fe1420247523d20ed90","modified":1538368811222},{"_id":"themes/hiker/layout/_partial/archive.ejs","hash":"11290e1529e50fe7c8953e26e61477635bb4d501","modified":1538368811224},{"_id":"themes/hiker/layout/_partial/article.ejs","hash":"759a140d8f1d522516f652fda348099768e71d65","modified":1538368811225},{"_id":"themes/hiker/layout/_partial/baidu-analytics.ejs","hash":"1edc20ced3c255c09405f0750b6c7bdf328af554","modified":1538368811226},{"_id":"themes/hiker/layout/_partial/busuanzi-analytics.ejs","hash":"02cf1fde1915b20fea7f4d81c34ffa55dbe56e10","modified":1538368811649},{"_id":"themes/hiker/layout/_partial/cnzz-analytics.ejs","hash":"1d6f7c86f5b0f2a7636caace94bef3ed12309dce","modified":1538368811649},{"_id":"themes/hiker/layout/_partial/comment.ejs","hash":"5102c373b39985087854091b7b9f056fe0ba9a1b","modified":1538368811650},{"_id":"themes/hiker/layout/_partial/copyright.ejs","hash":"73d1b18b1f572f10121b85795b69b947bfc56419","modified":1538368811651},{"_id":"themes/hiker/layout/_partial/dialog.ejs","hash":"1b38de12939c21906c66f2fde16b2bc86b23c673","modified":1538368811652},{"_id":"themes/hiker/layout/_partial/donate.ejs","hash":"ce2dc1ca23cc12a6b91651aaf7845508ab95832f","modified":1538368811653},{"_id":"themes/hiker/layout/_partial/facebook-sdk.ejs","hash":"8fc5cf7abbfd587057fb86ee028c7f216d30d68c","modified":1538368811655},{"_id":"themes/hiker/layout/_partial/footer.ejs","hash":"d2949d40f7aac65d2ac8426f5099eded12a5ec94","modified":1538368811656},{"_id":"themes/hiker/layout/_partial/gauges-analytics.ejs","hash":"ace3000bd3e01d03041d5be24f7640b6c003a5b5","modified":1538368811657},{"_id":"themes/hiker/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1538368811696},{"_id":"themes/hiker/layout/_partial/head.ejs","hash":"4eb047dcc24ffd60d8e3c39b9e821d5c97fcc6e0","modified":1538368811697},{"_id":"themes/hiker/layout/_partial/header-post.ejs","hash":"dd0c9881065f8da929390d7c717d63b957c6cd76","modified":1538368811698},{"_id":"themes/hiker/layout/_partial/header.ejs","hash":"7d894fc61023289609006a2b03ba255edd722efa","modified":1538368811698},{"_id":"themes/hiker/layout/_partial/mobile-nav.ejs","hash":"347cf1befd2ea637c24bd5901929d8e36e359e75","modified":1538368811699},{"_id":"themes/hiker/layout/_partial/sidebar.ejs","hash":"c70869569749a8f48cce202fa57926c06b55fdab","modified":1538368811724},{"_id":"themes/hiker/layout/_partial/tencent-analytics.ejs","hash":"6a50e6fe7701ff131b2fc0c066a4615dd2a37da7","modified":1538368811725},{"_id":"themes/hiker/layout/_widget/archive.ejs","hash":"856a6352a0f8d55f3d2965eea8ad4ec517f6af96","modified":1538368811726},{"_id":"themes/hiker/layout/_widget/category.ejs","hash":"866790acc13fed44b7ef74c3e19c300a3d6180d8","modified":1538368811727},{"_id":"themes/hiker/layout/_widget/curtains.ejs","hash":"d96be843847211664b83b786bfee43fa2f897616","modified":1538368811727},{"_id":"themes/hiker/layout/_widget/recent_posts.ejs","hash":"16800f85ffb036d2644a26e02facd61acb3706e9","modified":1538368811728},{"_id":"themes/hiker/layout/_widget/social.ejs","hash":"5719baf6cee0ddd97a0a61351d5d107257531504","modified":1538368811729},{"_id":"themes/hiker/layout/_widget/tag.ejs","hash":"6017c54a8c3c8ff8db491cfbea3100c139da75d6","modified":1538368811730},{"_id":"themes/hiker/layout/_widget/tagcloud.ejs","hash":"7259c179aa0c41c02e467ad892292e90430aaabc","modified":1538368811764},{"_id":"themes/hiker/layout/search/baidu.ejs","hash":"8cc6f6e601b14d310f20eaf29dc55d6c60ab4ee4","modified":1538368812115},{"_id":"themes/hiker/layout/search/index-mobile.ejs","hash":"8e2e28b37a908f60e4953bf9175a7af329d15d40","modified":1538368812116},{"_id":"themes/hiker/layout/search/index.ejs","hash":"48e0d133a808000a60a4ce0d673737c37be2410d","modified":1538368812117},{"_id":"themes/hiker/layout/search/insight.ejs","hash":"5205e75f0ceedc38e2bd9904464324ce179b8e25","modified":1538368812118},{"_id":"themes/hiker/layout/search/swiftype.ejs","hash":"cce9c44180d9490f45b30b8f052ac82675a9d66a","modified":1538368812124},{"_id":"themes/hiker/source/css/_extend.styl","hash":"3c4dd93884eb25385d837fb975bb311a639ecc1d","modified":1538368812132},{"_id":"themes/hiker/source/css/_variables.styl","hash":"cbb703c722fb98b2f08cc803e8b954acdf7b5522","modified":1538368812253},{"_id":"themes/hiker/source/css/archive.css","hash":"6e7e24bc6c356fe83833ceb73986036248d2892c","modified":1538368812253},{"_id":"themes/hiker/source/css/dialog.css","hash":"c248717aecf61b42e2bbfece61140a5c44911b4b","modified":1538368812763},{"_id":"themes/hiker/source/css/header-post.css","hash":"18e6e212ae21df9dfa9a7f20b5eba798b151230c","modified":1538368812774},{"_id":"themes/hiker/source/css/home.css","hash":"5c80e0c163533afa5a6605818510ed2d95939613","modified":1538368812775},{"_id":"themes/hiker/source/css/style.styl","hash":"4704ff34504b467252f767ce23477d801bb47cd6","modified":1538368813316},{"_id":"themes/hiker/source/css/vdonate.css","hash":"b43acd89a3d87725d6496a4f780dcf5eb576f866","modified":1538368813316},{"_id":"themes/hiker/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1538368813317},{"_id":"themes/hiker/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1538368813318},{"_id":"themes/hiker/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1538368813319},{"_id":"themes/hiker/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1538368813319},{"_id":"themes/hiker/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1538368813320},{"_id":"themes/hiker/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1538368813331},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.css","hash":"2e54d51d21e68ebc4bb870f6e57d3bfb660d4f9c","modified":1538368813871},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.js","hash":"58193c802f307ec9bc9e586c0e8a13ebef45d2f8","modified":1538368813872},{"_id":"themes/hiker/source/fancybox/jquery.fancybox.pack.js","hash":"2da892a02778236b64076e5e8802ef0566e1d9e8","modified":1538368813873},{"_id":"themes/hiker/source/js/dialog.js","hash":"f982934062fd6b2b385d1b6a6f7fbe4510087210","modified":1538368813875},{"_id":"themes/hiker/source/js/home.js","hash":"6534b583d310c57c49cb16aa75f6f33e9af41a92","modified":1538368813876},{"_id":"themes/hiker/source/js/insight.js","hash":"4b21b86ea9554d9f5a5da675f72632d9310b26d1","modified":1538368813877},{"_id":"themes/hiker/source/js/scripts.js","hash":"7b8531f6fdfeb1590cf096fbf913a63c66e5bbef","modified":1538368813878},{"_id":"themes/hiker/source/js/totop.js","hash":"aefa54321fdacd48537be444215bcb8df9190c3b","modified":1538368813891},{"_id":"themes/hiker/source/js/vdonate.js","hash":"a3482fae4f782028e7faa090f2f001ad9bca05e5","modified":1538368813892},{"_id":"themes/hiker/source/preview/browser-support.png","hash":"a6d8498553550c6b18a8f22bcd2f53c993c7d677","modified":1538368813893},{"_id":"themes/hiker/source/preview/donation-btn.png","hash":"ad78b1605b162e2399a1cdc5232f6a44298dba6c","modified":1538368813901},{"_id":"themes/hiker/source/preview/preview-abstract.png","hash":"3d18ad9ba38fe24770ba6758d8f1bb242f669ce3","modified":1538368813902},{"_id":"themes/hiker/source/preview/theme-color.png","hash":"725130ceea5e41bb2cc60b31e45275b4b0cc77b3","modified":1538368814205},{"_id":"source/_posts/book-storytelling-with-data/color.png","hash":"5a1ce74b4183937f2e29c59238579bd4f918f547","modified":1538368808409},{"_id":"source/_posts/book-storytelling-with-data/combined-approach-h.png","hash":"35e8e856207eb7edf0ac239b7c7923cbf83c89ec","modified":1538368808410},{"_id":"source/_posts/book-storytelling-with-data/combined-approach-v.png","hash":"04fa8fffc91eae0392946004514bd34297c602f3","modified":1538368808412},{"_id":"source/_posts/book-storytelling-with-data/h-bar.png","hash":"f276c5fed12fe9de8f4c46cbd81f44d4885cb1d2","modified":1538368808461},{"_id":"source/_posts/book-storytelling-with-data/heatmap.png","hash":"1fb509a1953858d2f2c4e74c163f05cdd48b24cd","modified":1538368808463},{"_id":"source/_posts/book-storytelling-with-data/no-attr.png","hash":"182fb755d50ee8337da97bde62b663a6186fe8b9","modified":1538368808465},{"_id":"source/_posts/book-storytelling-with-data/preatten-attr.png","hash":"ce99de1444d6007d917364b35f6c1724f22ccbab","modified":1538368808471},{"_id":"source/_posts/book-storytelling-with-data/report_original.png","hash":"e00799e3a98f098836ae894207e4b93bbd1ce472","modified":1538368808473},{"_id":"source/_posts/book-storytelling-with-data/slope.png","hash":"8fc7fb91dff0150bd15ae53a6f2704cee3c3640a","modified":1538368808476},{"_id":"source/_posts/book-storytelling-with-data/v-apart.png","hash":"20867199dc16d73f75508f56d96cc4f8fb1b6f40","modified":1538368808481},{"_id":"source/_posts/book-storytelling-with-data/with-attr.png","hash":"1f2b6ac983f833d16ed9d2bbfe93f9aab319adc0","modified":1538368808484},{"_id":"source/_posts/cv-detection/fastrcnn.jpg","hash":"05a2fb87e6a7ef2618a297ee502f6ab1b13138cd","modified":1538368808494},{"_id":"source/_posts/cv-detection/light_head_rcnn.jpg","hash":"8ffc357126b9026fc0078569a84e772e66400910","modified":1541940215601},{"_id":"source/_posts/cv-detection/pooling.jpg","hash":"74894b1fb4a0b636f3bbfa707279d43cfe9eea55","modified":1538368808511},{"_id":"source/_posts/cv-detection/spp_layer.jpg","hash":"0eee7ec1dc1fa32f5d2af5f9eda3febe265ee0ec","modified":1538368808529},{"_id":"source/_posts/cv-face-rec/centerloss_nn.jpg","hash":"2ce7356a98e3464de4af2ed44d480224df9cedce","modified":1538368808544},{"_id":"source/_posts/cv-face-rec/deepid.jpg","hash":"fb73c15181f5450b529db346bcfcc2f542f5a2c6","modified":1538368808548},{"_id":"source/_posts/cv-face-rec/facenet.jpg","hash":"ceb05829a1e56471a5b66566c8e39fd62d6b6da8","modified":1538368808550},{"_id":"source/_posts/dl-architecture/channel_shuffle.jpg","hash":"a2d3da16544ed4565d5c7fb60e8d22622ee269b1","modified":1538368808665},{"_id":"source/_posts/dl-architecture/dw-sep-conv.png","hash":"95a5cd9c35ae075a90ffbc16dc31527e63a0242a","modified":1538713506501},{"_id":"source/_posts/dl-architecture/resnext_block.jpg","hash":"5feb89892d4355f9983e7ab9985a592ccad5ab45","modified":1540183161260},{"_id":"source/_posts/dl-architecture/shufflenet_unit.jpg","hash":"b8ca7d6b4c4b016ca85fa2bac3102d51d00e0847","modified":1538368808672},{"_id":"source/_posts/dl-bn/BN_training.jpg","hash":"b832ccb3f8aa9f221117522d06d0aa2a53bb5ef5","modified":1542706110398},{"_id":"source/_posts/dl-bn/BN_transform.jpg","hash":"aaf729e340e62edd6a9ac8a195f2edb89c93efd5","modified":1542703570321},{"_id":"source/_posts/dl-bp/fig1.png","hash":"73b699a23997702d9991119d3025ab7adadd7475","modified":1538368808744},{"_id":"source/_posts/dl-bp/fig10.png","hash":"010d33e29baf56939eeb9a277ff435db5a64ee26","modified":1538368808763},{"_id":"source/_posts/dl-bp/fig13.png","hash":"96d0fbfcb7073c64556c1217c6b5160150057636","modified":1538368808791},{"_id":"source/_posts/dl-bp/fig4-1.png","hash":"9f2b40babb7d8b8a70feefbd153bae10dad4cf2e","modified":1538368808812},{"_id":"source/_posts/dl-bp/fig4-2.png","hash":"a8ccd9624d96756dbf1b0fe4257820e53607a529","modified":1538368808813},{"_id":"source/_posts/dl-bp/fig5.png","hash":"0b91ef06706cdba959ca68b7e8d137f5d40e4f49","modified":1538368808816},{"_id":"source/_posts/dl-bp/fig8.png","hash":"4dc393ded207bd643490e5b85581700527f3c73d","modified":1538368808833},{"_id":"source/_posts/dl-optimization/adagrad.jpg","hash":"56161cdc47bc8e5092c9ef4e9c59b880539682d6","modified":1538368808852},{"_id":"source/_posts/dl-optimization/adam.jpg","hash":"980b6d37b7dffe60122670c1120e2fb8f0a9846c","modified":1538368808856},{"_id":"source/_posts/dl-optimization/momentum.jpg","hash":"2e117a48a019bb5551e17f51ec235f037ca49fd9","modified":1538368808875},{"_id":"source/_posts/dl-optimization/nesterov.jpg","hash":"de21073a1690fe3f59080776ddaad52a4361e7d6","modified":1538368808882},{"_id":"source/_posts/dl-optimization/rmsprop.jpg","hash":"cea37d95e782d5def3453620abec49ae1651d1cd","modified":1538368808888},{"_id":"source/_posts/dl-optimization/rmsprop_with_nesterov.jpg","hash":"dd795b39039619772a7ceb986de5a1cc298a36c8","modified":1538368808895},{"_id":"source/_posts/dl-optimization/sgd.jpg","hash":"50eaee0a935720185f8e5fe08cbd70a5d6e1ce30","modified":1538368808898},{"_id":"source/_posts/dl-regularization/early_stopping.jpg","hash":"dd7ebb6c9c89abfa2cbd513244a09bb51577ac24","modified":1538368808910},{"_id":"source/_posts/dl-rnn/rnn1.jpg","hash":"dd64113bb658848050330391b2dff84c366aab60","modified":1538368808950},{"_id":"source/_posts/dl-rnn/rnn2.jpg","hash":"e8bf1cc6c955d81950b8ee3fac75f89feb15d144","modified":1538368808964},{"_id":"source/images/Alipay.jpg","hash":"8474d7edead93f8f283efd1c5d728fede56bea1e","modified":1538368809641},{"_id":"themes/hiker/source/js/bootstrap.js","hash":"474b25cebd06d57a38090c6716d5dfaa5591baad","modified":1538368813874},{"_id":"themes/hiker/source/js/jquery-3.1.1.min.js","hash":"042dd055cd289215835a58507c9531f808e1648a","modified":1538368813878},{"_id":"source/_posts/cv-detection/rcnn.png","hash":"4629ce79e338fa3fe8a6fbe90fe22ad9c7c1dc5b","modified":1538368808519},{"_id":"source/_posts/cv-face-rec/centerloss_update.jpg","hash":"4284af764a471431ae26d16d5bf5d6cb1852455a","modified":1538368808547},{"_id":"source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg","hash":"c105b82e5dde9cce7836cb4d1622b1d7a49bd261","modified":1540183672089},{"_id":"source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png","hash":"92f94c4c0e69016e86cbae2cf9e7b67af54fd7b1","modified":1538731370906},{"_id":"source/_posts/dl-bp/fig11.png","hash":"552a615d425cbdfc613bd4a70973584ca6555bd2","modified":1538368808773},{"_id":"source/_posts/dl-bp/fig12.png","hash":"e8ea9cf22191ce7eedaac6f7c5bae706e8b5d339","modified":1538368808789},{"_id":"source/_posts/dl-bp/fig14.png","hash":"a5423a0d44870f7f7dd482c9277bafb56f14fe8c","modified":1538368808794},{"_id":"source/_posts/dl-bp/fig6.png","hash":"faabe1d06f8419ce9ed187bafa27087e878efdb2","modified":1538368808818},{"_id":"source/_posts/dl-bp/fig7.png","hash":"59bf734c9c01c35c337908d6fd5edfffa786dbbb","modified":1538368808820},{"_id":"source/_posts/dl-bp/fig9.png","hash":"be4ed1dacabedf01f7af809eff84d8318d1d2859","modified":1538368808835},{"_id":"themes/hiker/layout/_partial/post/busuanzi-analytics.ejs","hash":"b282fb7e646dfe02dc89e7e786f233a6700dc808","modified":1538368811700},{"_id":"themes/hiker/layout/_partial/post/category.ejs","hash":"692c4fab11b31adce8f724f51203bec6ea759b9a","modified":1538368811703},{"_id":"themes/hiker/layout/_partial/post/date.ejs","hash":"cc160cd537a966c6ecb3c73c4a44207f254b0461","modified":1538368811704},{"_id":"themes/hiker/layout/_partial/post/gallery.ejs","hash":"bfde040b4c4a8ce43645e0783cdd2b944269ec80","modified":1538368811707},{"_id":"themes/hiker/layout/_partial/post/mathjax.ejs","hash":"ae998e014666cb354ded5a11a8a6bc7eae3e3c34","modified":1538368811708},{"_id":"themes/hiker/layout/_partial/post/nav.ejs","hash":"cbb3819ce512bd24db8bad41b8617d46eba82fdc","modified":1538368811710},{"_id":"themes/hiker/layout/_partial/post/tag.ejs","hash":"694b5101bcc44c9f9c1cc62e5ad2fdfb4b7c7a07","modified":1538368811721},{"_id":"themes/hiker/layout/_partial/post/title.ejs","hash":"9ee31f67ad337d5dcaaa10aa8ba55c7c22074b1c","modified":1538368811722},{"_id":"themes/hiker/layout/_partial/post/urlconvert.ejs","hash":"2133f1029632417f9043b9d4749d580ed0c75db0","modified":1538368811723},{"_id":"themes/hiker/source/css/_partial/archive.styl","hash":"b16abc653626645f570d48aca094e6d0ca1f231d","modified":1538368812134},{"_id":"themes/hiker/source/css/_partial/article.styl","hash":"720aff632a75f785751e7d17d5ff23ff4bbe96cf","modified":1538368812135},{"_id":"themes/hiker/source/css/_partial/comment.styl","hash":"43279aaaa00cc07b4a65b13ef01c391355e82717","modified":1538368812136},{"_id":"themes/hiker/source/css/_partial/footer.styl","hash":"203c03d670a6fd434efdb1d9047166db7d6d874b","modified":1538368812142},{"_id":"themes/hiker/source/css/_partial/header-post.styl","hash":"81539c15e42c40e1ffbc36952e4617e17fdbba9c","modified":1538368812143},{"_id":"themes/hiker/source/css/_partial/header.styl","hash":"5686f062d84928ceec78efbd01dba59609437efb","modified":1538368812144},{"_id":"themes/hiker/source/css/_partial/highlight.styl","hash":"a12d13407903d4847c4207e2cd62190367acae64","modified":1538368812246},{"_id":"themes/hiker/source/css/_partial/insight.styl","hash":"3b4042ebff85ded51eccd6183bad812d9aeef844","modified":1538368812247},{"_id":"themes/hiker/source/css/_partial/mobile.styl","hash":"3920e4c3cd11f294d3e7835ed628f169cbea6b21","modified":1538368812248},{"_id":"themes/hiker/source/css/_partial/sidebar-aside.styl","hash":"4f323b13a7594a46b5b21a9052bbdb7fb5144411","modified":1538368812248},{"_id":"themes/hiker/source/css/_partial/sidebar-bottom.styl","hash":"73909106254b7ec312367079687c0de37740bb31","modified":1538368812249},{"_id":"themes/hiker/source/css/_partial/sidebar.styl","hash":"a0b01d34317f4b17ce0f8c14cd978798ad2fbecc","modified":1538368812250},{"_id":"themes/hiker/source/css/_util/grid.styl","hash":"1aa883ab432d9e4139c89dcbd40ae2bd1528d029","modified":1538368812251},{"_id":"themes/hiker/source/css/_util/mixin.styl","hash":"429bad87fc156eacf226c5e35b0eafc277f2504b","modified":1538368812252},{"_id":"themes/hiker/source/css/bootstrap.css","hash":"41abf4a39af0a19b6e08b7e2d6ddacbd3892daaf","modified":1538368812257},{"_id":"themes/hiker/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1538368812765},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1538368812766},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1538368812773},{"_id":"themes/hiker/source/css/images/avatar.jpg","hash":"d685c61842df6ff3f44a96ba4ac15117502168ee","modified":1538368812776},{"_id":"themes/hiker/source/css/images/homelogo.jpg","hash":"4bfc9650c4fd6e60b09ae29f888a819cdf88e9fe","modified":1538368812781},{"_id":"themes/hiker/source/css/images/mylogo.jpg","hash":"c6bd830b95d59ad28489f949135640f401ee53e8","modified":1542367956368},{"_id":"themes/hiker/source/css/images/rocket.png","hash":"6dee0406955aa9b7a261161d30f2538a671e806b","modified":1538368812790},{"_id":"themes/hiker/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1538368813332},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1538368813333},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"4c9c395d705d22af7da06870d18f434e2a2eeaf9","modified":1538368813333},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-media.js","hash":"e14c32cc6823b81b2f758512f13ed8eb9ef2b454","modified":1538368813334},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1538368813335},{"_id":"themes/hiker/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"83cdfea43632b613771691a11f56f99d85fb6dbd","modified":1538368813870},{"_id":"source/_posts/cv-detection/SSD_YOLO.png","hash":"494ee85e902567768c8105c75c10ba581d5f482b","modified":1540712145230},{"_id":"source/_posts/dl-architecture/evolution-of-separable-conv.png","hash":"9bcb851cc454591a3fedbbdd951b67bb6ecf6d12","modified":1538728571022},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1538368812772},{"_id":"source/_posts/cv-detection/SSD.png","hash":"9e96555172b3a1bd7aed7a514b301fa669f86e3b","modified":1540710106536},{"_id":"themes/hiker/source/css/fonts/fontawesome-webfont.svg","hash":"a275426daefd3716c53561fad121d258a7f05b47","modified":1538368812769},{"_id":"source/_posts/dl-optimization/deep-learning.png","hash":"715462fb653522db028a15b762c0805613348aa1","modified":1538368808868},{"_id":"themes/hiker/source/preview/preview-mobile.png","hash":"7679a50aef93fc364cbcee4a52cd84604bf741b1","modified":1538368813905},{"_id":"themes/hiker/source/css/images/home-bg.jpg","hash":"cd9b73e3aeefd17f4a16528f783ac66093b66f5e","modified":1538728781274},{"_id":"themes/hiker/source/css/images/sample.jpg","hash":"02f5c2de79d093b94f72f5a361528dbbc7645d9e","modified":1538728732551},{"_id":"themes/hiker/source/css/images/pose.jpg","hash":"4d394a662e7d6d2de6a4598688c7cd3dba5c2fa3","modified":1538368812788},{"_id":"source/_posts/dl-bp/Derivation-of-CNN.pdf","hash":"5d7911c93ddcb34cac088d99bd0cae9124e5dcd1","modified":1538368808734},{"_id":"source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf","hash":"616cca53572c50c8e4911be9ccdaebfc1cfbca41","modified":1538368808709},{"_id":"themes/hiker/source/preview/code-theme.jpg","hash":"8c8512fd04e6106033656d10e92d51de76cca6d8","modified":1538368813900},{"_id":"source/projects/LagouJob.pdf","hash":"29dfa5252ec3448a86636005696b353f312fdba5","modified":1538368810298},{"_id":"source/about/Presentation_TransFBP.pdf","hash":"df22e0411b115a8dc5386d05cd264e0a90c85922","modified":1542537559268},{"_id":"themes/hiker/source/preview/preview-pc.png","hash":"2471e0697938721e4c5d7c66940e165c59a31a0f","modified":1538368814203},{"_id":"source/projects/MLJob.pdf","hash":"bb695129aca0adc5ac3b53408bee869170c912ad","modified":1538368810318},{"_id":"source/about/Research_Overview.pdf","hash":"2afc75056ddff10d4a03769c5040f2dc22057aa4","modified":1539788622175},{"_id":"source/projects/JiaYuan.pdf","hash":"cba0c80dd81fc63b8bea31e5c80a026eecc189de","modified":1538368810285},{"_id":"source/about/Presentation.pdf","hash":"a1bc00bb925f8fed059aac4f76ccf01c3d2f44f7","modified":1538368809631},{"_id":"source/_posts/ml-knn/Demo.png","hash":"d9fa12f1e40924a0db57761d09d52dce450b4f7b","modified":1538368809042},{"_id":"source/about/DL_for_Face_Analysis.pdf","hash":"4b4ffd9019ad773656ec509e2f84011e51b3e361","modified":1542505372193},{"_id":"source/_posts/dip-image-feature/TPAMI-A performance evaluation of local descriptors.pdf","hash":"934f349ee27cddc019ffd0db86bb28f980fad4e2","modified":1538368808647},{"_id":"source/projects/GovReport.pdf","hash":"c4fc0d750fb44b19a1c5c228a0b5757ac74bf2a7","modified":1538368810174},{"_id":"source/projects/IndustryReport.pdf","hash":"53d00824c8253eef8933bcc2d9dd1915565562cd","modified":1538368810269},{"_id":"public/archive/index.html","hash":"1176dfddf7a592a92f7526c838aabee82e6ef704","modified":1542730662460},{"_id":"public/papers/index.html","hash":"0eb265d8293156b61ed32e434151126cc1514d6d","modified":1542730662460},{"_id":"public/tags/Algorithm/index.html","hash":"16fa72d893be9d1abc1eb4770088ee7d58f3ac1d","modified":1542730662461},{"_id":"public/tags/Data-Structure/index.html","hash":"1dc9ba3438c2c03f579bc222260cc3d0518577c2","modified":1542730662461},{"_id":"public/tags/Graph/index.html","hash":"55ae50412497b2f5e9708f26056b35b4d93ad6e7","modified":1542730662462},{"_id":"public/tags/Data-Visualization/index.html","hash":"ee4b563554cda6e43a2ac058baa85639c66e060c","modified":1542730662462},{"_id":"public/tags/Machine-Learning/page/3/index.html","hash":"d1e24cef552682119f571baa3d7adae4b61a801d","modified":1542730662462},{"_id":"public/tags/Deep-Learning/page/2/index.html","hash":"2d7222f765db790816ca7f07cdcb26fc78f84816","modified":1542730662463},{"_id":"public/tags/Computer-Vision/index.html","hash":"1ccf1a747297d628b95b3e22db67f7f65a10c14b","modified":1542730662463},{"_id":"public/tags/Face-Anti-Spoofing/index.html","hash":"898d2286934f616c754d549dad05de56dee8403b","modified":1542730662463},{"_id":"public/tags/Object-Detection/index.html","hash":"13a7a6f943e108972965814a3fb01a84f65c662f","modified":1542730662463},{"_id":"public/tags/Face-Recognition/index.html","hash":"fa12f0babf701dcb744e71cbdef148a09bb61971","modified":1542730662463},{"_id":"public/tags/Digital-Image-Processing/index.html","hash":"777af8edbc020fabe7f4ecf6a0e87312e6b0517b","modified":1542730662463},{"_id":"public/tags/Auto-Encoder/index.html","hash":"9b2be6275768e4d157ca67f04efbcf455042b25c","modified":1542730662463},{"_id":"public/tags/Image-Classification/index.html","hash":"db47f6a54d9b3f1bdd1c314f551f2b3e86e70732","modified":1542730662463},{"_id":"public/tags/Network-Architecture/index.html","hash":"711228566448262c7bf1922a9307d24bbc3db5e0","modified":1542730662463},{"_id":"public/tags/Optimization/index.html","hash":"ed3aa8aad5d9a16ff49c83ac33d688233f4e90bd","modified":1542730662464},{"_id":"public/tags/CNN/index.html","hash":"b8a716b3823e358ebb6b9ed1bb90f7a8b3b551f5","modified":1542730662464},{"_id":"public/tags/Data-Augmentation/index.html","hash":"6cb5f969e85b7b9a223e96ef1da24c6488d6c104","modified":1542730662464},{"_id":"public/tags/Regularization/index.html","hash":"42382534dd308cbbc798586b1732c1d80aea7d8f","modified":1542730662464},{"_id":"public/tags/RNN/index.html","hash":"107f27949d53e13c58057b30038fa74a3043c7d8","modified":1542730662464},{"_id":"public/tags/NLP/index.html","hash":"070d7410a33709904a67a5bb099f3f6b7f9cab19","modified":1542730662464},{"_id":"public/tags/Feature-Engineering/index.html","hash":"dce8fb15ebdb40b4e50312428f0aa9a5c481b7f9","modified":1542730662464},{"_id":"public/about/index.html","hash":"dd24381953cbb09a7556a0a1799235dee905943f","modified":1542730662464},{"_id":"public/projects/index.html","hash":"9e9fdb37c0df3d270f37a35324e7f08d3be96616","modified":1542730662464},{"_id":"public/tags/index.html","hash":"6e1683baf8388c892b415a6a9a73945731fd324f","modified":1542730662464},{"_id":"public/2018/11/20/dl-bn/index.html","hash":"edf6f6c5384b917d7749a19f3016032ca721f107","modified":1542730662464},{"_id":"public/2018/11/18/dl-architecture/index.html","hash":"03cf4e82c9ffbd9b7dff3e3faa80f6f7bf0e28bf","modified":1542730662465},{"_id":"public/2018/11/11/cv-detection/index.html","hash":"762eb11a959022e077c37a42617059b78afc1819","modified":1542730662465},{"_id":"public/2018/11/10/dl-data-augmentation/index.html","hash":"4092b4d61d0e14828cc578dd91819f76c4ef0176","modified":1542730662465},{"_id":"public/2018/11/04/cv-iqa/index.html","hash":"2373ebc31864774fb910e8aeebcd62a274ffab0e","modified":1542730662465},{"_id":"public/2018/10/30/cv-antispoofing/index.html","hash":"83edd8805c7d0b0b41df44c566406bb6b9113b65","modified":1542730662465},{"_id":"public/2018/09/03/cv-face-rec/index.html","hash":"9822db074a90c381e55d4beecdbf2e4bcea3b451","modified":1542730662465},{"_id":"public/2018/08/22/dl-ae/index.html","hash":"7024bf23b610ba3b405c71bc0f0035d046a31fe5","modified":1542730662465},{"_id":"public/2018/08/22/dip-image-feature/index.html","hash":"56b1adee737bc03fa087ad8bd48255735691fbd3","modified":1542730662465},{"_id":"public/2018/08/20/ml-dimen-red-metric-learning/index.html","hash":"8199bbc270f132109ad116b8d65ff3c8bca27784","modified":1542730662465},{"_id":"public/2018/08/20/ml-feml/index.html","hash":"7f5780dc9d436091d67708a07bcce69472dea104","modified":1542730662466},{"_id":"public/2018/08/11/book-storytelling-with-data/index.html","hash":"9e00e407e6296a34c6128b34df4e018aed2cc9f1","modified":1542730662466},{"_id":"public/2018/08/10/dl-rnn/index.html","hash":"2d67e53251e0d7dc215901256d0f30fe740d4e40","modified":1542730662466},{"_id":"public/2018/08/06/dl-regularization/index.html","hash":"3e1865a8c7b687170c7251b3bf0e1c2b53dc37df","modified":1542730662466},{"_id":"public/2018/08/02/algo-sort/index.html","hash":"704a011e27aa88db3f90dc4a026ec5de3500ef5d","modified":1542730662466},{"_id":"public/2018/07/30/ml-clustering/index.html","hash":"4e89efeca36a69c2741d4cbb49db3c1414d97a92","modified":1542730662466},{"_id":"public/2018/07/27/dl-cnn/index.html","hash":"77ad7783d3eb9cba7e5b1de0cf5efb3d94989159","modified":1542730662466},{"_id":"public/2018/07/25/dl-bp/index.html","hash":"be343bdf299a8e7c24bf0951d756b7280d04790f","modified":1542730662466},{"_id":"public/2018/07/25/ml-ensemble/index.html","hash":"41f908812fbad7b4fff9a0d1a9969f54025b4b0b","modified":1542730662466},{"_id":"public/2018/07/24/ml-loss/index.html","hash":"eff199dfd431abf0464fa4ae34be4a5ebea1e8e1","modified":1542730662466},{"_id":"public/2018/07/24/ml-dt/index.html","hash":"1de70be3ab88a5a822a17693196e40cc856a548f","modified":1542730662467},{"_id":"public/2018/07/23/ml-lr-me/index.html","hash":"f24f6c185a060abafffb9164acce89489e124a0b","modified":1542730662467},{"_id":"public/2018/07/22/ml-svm/index.html","hash":"e0b4a3362e3a6e4a8dee2836c8b72c70c6cdf352","modified":1542730662467},{"_id":"public/2018/07/20/dl-optimization/index.html","hash":"66226b0c80b43ba819b8135328fd91a5a3f92545","modified":1542730662467},{"_id":"public/2018/07/19/ml-nb/index.html","hash":"fc080ef8f36370a1e6b963412ba0a63e11b274ef","modified":1542730662467},{"_id":"public/2018/07/19/ml-knn/index.html","hash":"de1ababe7dd7bfa712830d251c0b65ba4b2602f1","modified":1542730662467},{"_id":"public/2018/07/19/ml-model-selection-metric/index.html","hash":"cfc8c19021440b019f605097da8e83f917c7a896","modified":1542730662468},{"_id":"public/2018/07/18/ml-lm/index.html","hash":"7f9d057110320d5706404739aa0eeb2f89536e7c","modified":1542730662468},{"_id":"public/archives/index.html","hash":"6bf55548a8371da3b54130d542a850d904ab7abe","modified":1542730662468},{"_id":"public/archives/page/2/index.html","hash":"2c7fa693e412a837c73c7fa0ccf23cbb45b0bd62","modified":1542730662468},{"_id":"public/archives/page/3/index.html","hash":"80edcb4cd062230514296972ba1e1f7acc797055","modified":1542730662468},{"_id":"public/archives/2018/index.html","hash":"ce17ca219c4665ced3200baec794b47f59ac578c","modified":1542730662468},{"_id":"public/archives/2018/page/2/index.html","hash":"04160895de4a53b5683de12c6178dda89d62ecfd","modified":1542730662469},{"_id":"public/archives/2018/page/3/index.html","hash":"12dbd709e839ce164368a386c020aa8a2dc81bc4","modified":1542730662469},{"_id":"public/archives/2018/07/index.html","hash":"8932b9f0a2ad5e3aab8308e3d7ae444d16022830","modified":1542730662469},{"_id":"public/archives/2018/07/page/2/index.html","hash":"ada29ebd093cda06173511da5d824e2c6dfdc790","modified":1542730662469},{"_id":"public/archives/2018/08/index.html","hash":"9396f0a0050fd549dd4db87351bbdb7972cb59e2","modified":1542730662469},{"_id":"public/archives/2018/09/index.html","hash":"fb39708edc24cb61ff6197bc8b788e346899fcff","modified":1542730662469},{"_id":"public/archives/2018/10/index.html","hash":"87cb35dbb1093ca157ddcc880b761e2ff1bcda1d","modified":1542730662469},{"_id":"public/archives/2018/11/index.html","hash":"ea8f54f69619f68c13760fc1f53be619c1ccaffe","modified":1542730662470},{"_id":"public/page/2/index.html","hash":"250d584e62fec513e27ba0a4a2622826096da6a2","modified":1542730662470},{"_id":"public/index.html","hash":"7336cfe50e797f5dcfb87219586da605e62c3afa","modified":1542730662470},{"_id":"public/page/3/index.html","hash":"15ec187c7712ce0069d4a2dd6f577e9e7a7ff895","modified":1542730662470},{"_id":"public/tags/Data-Science/index.html","hash":"5a99f5552f16b42c369dddec4f8d7f0ce336254f","modified":1542730662470},{"_id":"public/tags/Data-Science/page/2/index.html","hash":"e1f44b3d647ccbab556a331d0508ba185077d25a","modified":1542730662470},{"_id":"public/tags/Machine-Learning/index.html","hash":"4262f11bc1168ca88c08e05150a488ab69710593","modified":1542730662470},{"_id":"public/tags/Machine-Learning/page/2/index.html","hash":"6de93c82dcbc7954679c38af99e671d3ec14bb4c","modified":1542730662470},{"_id":"public/tags/Deep-Learning/index.html","hash":"95b0091172748bb19c06ad338cd9ddedb406c913","modified":1542730662470},{"_id":"public/CNAME","hash":"c874a9c65eaf1b3e4f81bfaf1f09876474f56c50","modified":1542730662495},{"_id":"public/about/LICENSE","hash":"6cfbca72152ed7d78b31fb88f23b5c6087fff34d","modified":1542730662495},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1542730662495},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1542730662495},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1542730662495},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1542730662495},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1542730662495},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1542730662496},{"_id":"public/preview/browser-support.png","hash":"a6d8498553550c6b18a8f22bcd2f53c993c7d677","modified":1542730662496},{"_id":"public/preview/donation-btn.png","hash":"ad78b1605b162e2399a1cdc5232f6a44298dba6c","modified":1542730662497},{"_id":"public/preview/preview-abstract.png","hash":"3d18ad9ba38fe24770ba6758d8f1bb242f669ce3","modified":1542730662497},{"_id":"public/preview/theme-color.png","hash":"725130ceea5e41bb2cc60b31e45275b4b0cc77b3","modified":1542730662498},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1542730662498},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1542730662498},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1542730662498},{"_id":"public/css/images/avatar.jpg","hash":"d685c61842df6ff3f44a96ba4ac15117502168ee","modified":1542730662498},{"_id":"public/css/images/homelogo.jpg","hash":"4bfc9650c4fd6e60b09ae29f888a819cdf88e9fe","modified":1542730662498},{"_id":"public/css/images/mylogo.jpg","hash":"c6bd830b95d59ad28489f949135640f401ee53e8","modified":1542730662498},{"_id":"public/css/images/rocket.png","hash":"6dee0406955aa9b7a261161d30f2538a671e806b","modified":1542730662498},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1542730662499},{"_id":"public/about/CV_LuXu.pdf","hash":"a82c924ef6afad3e065bf180e328af495df75b68","modified":1542730663364},{"_id":"public/about/LucasX.jpg","hash":"b50fed3c6eed4cd39ada918654f11a3b086d02f2","modified":1542730663365},{"_id":"public/images/WeChatPay.png","hash":"b5d525b4ea3741bdd8edc8c5679a23aa2ec86c53","modified":1542730663371},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1542730663371},{"_id":"public/css/archive.css","hash":"17cc72203cad1b0be66008d662c7494507aaee8b","modified":1542730663422},{"_id":"public/css/dialog.css","hash":"5e0333adf3f496e0d443767fe228a1d4b1a2bafc","modified":1542730663422},{"_id":"public/css/header-post.css","hash":"3f6d1f5593a353b4b05a67ee14e04ec9c986db21","modified":1542730663422},{"_id":"public/css/home.css","hash":"2a7bfef438468b5b8be3f84ea6818156d371077d","modified":1542730663423},{"_id":"public/css/vdonate.css","hash":"bca2d291a71e7358654c51f23e8bfb467b2bc8b2","modified":1542730663423},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1542730663423},{"_id":"public/js/dialog.js","hash":"01e8b337c1721e0486fd5044f98b233e84ba1985","modified":1542730663423},{"_id":"public/js/home.js","hash":"e403c3290d76c5f58571cbfe4414236e41a7ac94","modified":1542730663423},{"_id":"public/js/scripts.js","hash":"e06a8948375df71cbf77abf8617db438ece811b3","modified":1542730663423},{"_id":"public/js/insight.js","hash":"f79ab175d1c8c4fb59328ee4fd9eb95808eb0be5","modified":1542730663423},{"_id":"public/js/totop.js","hash":"29bb40144ac238d22b25d59df465aff8dc38bfd0","modified":1542730663424},{"_id":"public/js/vdonate.js","hash":"5738414c642d30e43943a69287b3d25a0b6be135","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1542730663424},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1542730663425},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1542730663425},{"_id":"public/css/style.css","hash":"924f37d406a58b29c37b06a10e8783438c0fbc71","modified":1542730663425},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1542730663425},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1542730663425},{"_id":"public/js/bootstrap.js","hash":"3b965a36a6b08854ad6eddedf85c5319fd392b4a","modified":1542730663426},{"_id":"public/js/jquery-3.1.1.min.js","hash":"f647a6d37dc4ca055ced3cf64bbc1f490070acba","modified":1542730663426},{"_id":"public/css/bootstrap.css","hash":"64fdc2e7c3f8a164d21c5632b5adbbb9990ea802","modified":1542730663427},{"_id":"public/images/Alipay.jpg","hash":"8474d7edead93f8f283efd1c5d728fede56bea1e","modified":1542730663491},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"a275426daefd3716c53561fad121d258a7f05b47","modified":1542730663491},{"_id":"public/css/images/home-bg.jpg","hash":"cd9b73e3aeefd17f4a16528f783ac66093b66f5e","modified":1542730663543},{"_id":"public/preview/preview-mobile.png","hash":"7679a50aef93fc364cbcee4a52cd84604bf741b1","modified":1542730663567},{"_id":"public/css/images/sample.jpg","hash":"02f5c2de79d093b94f72f5a361528dbbc7645d9e","modified":1542730663568},{"_id":"public/css/images/pose.jpg","hash":"4d394a662e7d6d2de6a4598688c7cd3dba5c2fa3","modified":1542730663570},{"_id":"public/preview/code-theme.jpg","hash":"8c8512fd04e6106033656d10e92d51de76cca6d8","modified":1542730663696},{"_id":"public/projects/LagouJob.pdf","hash":"29dfa5252ec3448a86636005696b353f312fdba5","modified":1542730663721},{"_id":"public/preview/preview-pc.png","hash":"2471e0697938721e4c5d7c66940e165c59a31a0f","modified":1542730663736},{"_id":"public/about/Presentation_TransFBP.pdf","hash":"df22e0411b115a8dc5386d05cd264e0a90c85922","modified":1542730663745},{"_id":"public/projects/MLJob.pdf","hash":"bb695129aca0adc5ac3b53408bee869170c912ad","modified":1542730663769},{"_id":"public/about/Research_Overview.pdf","hash":"2afc75056ddff10d4a03769c5040f2dc22057aa4","modified":1542730663774},{"_id":"public/projects/JiaYuan.pdf","hash":"cba0c80dd81fc63b8bea31e5c80a026eecc189de","modified":1542730663774},{"_id":"public/about/Presentation.pdf","hash":"a1bc00bb925f8fed059aac4f76ccf01c3d2f44f7","modified":1542730663794},{"_id":"public/about/DL_for_Face_Analysis.pdf","hash":"4b4ffd9019ad773656ec509e2f84011e51b3e361","modified":1542730663824},{"_id":"public/projects/GovReport.pdf","hash":"c4fc0d750fb44b19a1c5c228a0b5757ac74bf2a7","modified":1542730663892},{"_id":"public/projects/IndustryReport.pdf","hash":"53d00824c8253eef8933bcc2d9dd1915565562cd","modified":1542730663914}],"Category":[],"Data":[],"Page":[{"layout":"archives","title":"Archives","header-img":null,"comments":0,"date":"2018-07-18T04:28:56.000Z","description":"All posted archives.","_content":"","source":"archive/index.md","raw":"---\nlayout: \"archives\"\ntitle: \"Archives\"\nheader-img: \ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"All posted archives.\"\n---\n","updated":"2018-10-01T04:40:09.635Z","path":"archive/index.html","_id":"cjopy03bh0001608wjqgww8ux","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"About Me","description":"Self introduction and personal information.","header-img":"img/header_img/archive-bg.png","_content":"## Self Introduction\nI am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. \n\n![LucasX](https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg)\n\n__Curriculum Vitae__: [[PDF](./CV_LuXu.pdf)] \n\n## Academic Papers\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](./Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]  \n\n## Projects\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]  \n  An AI cloud platform based on Java & Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.\n* [Lagou Job](https://github.com/lucasxlu/LagouJob.git)  \n  A repository for job data crawling and data analysis based on Python3. This repo has received over **200 stars** and **120+ forks**. More details can be read at [here](https://www.zhihu.com/question/36132174/answer/94392659).\n\n## Competition\n* My algorithm named **TreeCNN** ranks the 1st place on **Facial Expression Recognition Challenge** on [Real-word Affective Face Database (RAF-DB)](http://www.whdeng.cn/raf/model1.html).\n\n\n## Experience\n* Image Quality Assessment & Automatic License Examination & Face Analysis\n  During my intern at [DiDi](https://www.didiglobal.com/), we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.\n  * Building deep models for IQA with 93.15% precision and 98.35% recall\n  * Developing and training deep models for license type classification with 99.72% precision and 99.73% recall \n  * Developing deep models for Facial Beauty Prediction with 0.89 PC\n  * Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall\n  * Writing 16 patents about security products about DiDi\n\n\n* [Pig Face Recognition](http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html)  \n  Our team developed a **Pig Face Recognition System** with [Guangzhou Yingzi Technology Co.,Ltd](http://www.yingzi.com/), to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   \n  * Building deep model and training on Amazon S3 GPU server.\n  * Encapsulate RESTful API and web backend development.\n  \n  \n* Mal-Image Recognition System  \n  I developed a Mal-Image Recognition System with [Wuhan ZhiLiFeng Information .,Ltd.](http://zlfinfo.com.cn/), the model achieves a mAP with 92.17% on detecting mal-images.  \n  * Image data crawling and preprocessing.\n  * Building machine learning algorithms.\n  * Model training and deployment.\n\n\n* Web Crawling and Java Developer  \n  During working at [Beijing Jiewen Technology Co.,Ltd.](http://www.jiewen.com.cn/), Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.\n  * Developing and maintaining web information system based on Java SSM framework and MySQL database.\n  * Developing web crawlers to collect data from O2O websites (e.g. [Ctrip](http://www.ctrip.com/), [Dazhong Dianping](http://www.dianping.com/), [AMap](https://www.amap.com/)), and storing them in MySQL and MongoDB.\n  * Data statistics, analysis and visualization.\n\n## Skills\n* English: (CET-6: 575/710); Proficiency in Reading and Writing    \n* Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    \n* Programming: Java == Python > R > C++  \n* Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  \n* Database: MongoDB/MySQL/Oracle  \n* Others: Web Crawler/Web Development/PPT\n\n## Award\n* 1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [[Slides](./Presentation.pdf)]\n* 1st Scholar Prize [[Slides](./Research_Overview.pdf)]\n* 2nd Scholar Prize  \n* 3rd Prize in NECCS (national level)  \n\n## Report & Tutorial\n* Deep Learning for Face Analysis [[Slides](./DL_for_Face_Analysis.pdf)]\n\n## Interests\n* Coding  \n* Reading and Writing  \n* Design \n\n\n## Contact\n* [Zhihu](https://www.zhihu.com/people/xulu-0620/activities)\n* [Github](https://github.com/lucasxlu)  \n* Email: xulu0620@gmail.com","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About Me\"\ndescription: \"Self introduction and personal information.\"\nheader-img: \"img/header_img/archive-bg.png\"\n---\n## Self Introduction\nI am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. \n\n![LucasX](https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg)\n\n__Curriculum Vitae__: [[PDF](./CV_LuXu.pdf)] \n\n## Academic Papers\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](./Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]  \n\n## Projects\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]  \n  An AI cloud platform based on Java & Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.\n* [Lagou Job](https://github.com/lucasxlu/LagouJob.git)  \n  A repository for job data crawling and data analysis based on Python3. This repo has received over **200 stars** and **120+ forks**. More details can be read at [here](https://www.zhihu.com/question/36132174/answer/94392659).\n\n## Competition\n* My algorithm named **TreeCNN** ranks the 1st place on **Facial Expression Recognition Challenge** on [Real-word Affective Face Database (RAF-DB)](http://www.whdeng.cn/raf/model1.html).\n\n\n## Experience\n* Image Quality Assessment & Automatic License Examination & Face Analysis\n  During my intern at [DiDi](https://www.didiglobal.com/), we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.\n  * Building deep models for IQA with 93.15% precision and 98.35% recall\n  * Developing and training deep models for license type classification with 99.72% precision and 99.73% recall \n  * Developing deep models for Facial Beauty Prediction with 0.89 PC\n  * Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall\n  * Writing 16 patents about security products about DiDi\n\n\n* [Pig Face Recognition](http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html)  \n  Our team developed a **Pig Face Recognition System** with [Guangzhou Yingzi Technology Co.,Ltd](http://www.yingzi.com/), to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   \n  * Building deep model and training on Amazon S3 GPU server.\n  * Encapsulate RESTful API and web backend development.\n  \n  \n* Mal-Image Recognition System  \n  I developed a Mal-Image Recognition System with [Wuhan ZhiLiFeng Information .,Ltd.](http://zlfinfo.com.cn/), the model achieves a mAP with 92.17% on detecting mal-images.  \n  * Image data crawling and preprocessing.\n  * Building machine learning algorithms.\n  * Model training and deployment.\n\n\n* Web Crawling and Java Developer  \n  During working at [Beijing Jiewen Technology Co.,Ltd.](http://www.jiewen.com.cn/), Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.\n  * Developing and maintaining web information system based on Java SSM framework and MySQL database.\n  * Developing web crawlers to collect data from O2O websites (e.g. [Ctrip](http://www.ctrip.com/), [Dazhong Dianping](http://www.dianping.com/), [AMap](https://www.amap.com/)), and storing them in MySQL and MongoDB.\n  * Data statistics, analysis and visualization.\n\n## Skills\n* English: (CET-6: 575/710); Proficiency in Reading and Writing    \n* Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    \n* Programming: Java == Python > R > C++  \n* Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  \n* Database: MongoDB/MySQL/Oracle  \n* Others: Web Crawler/Web Development/PPT\n\n## Award\n* 1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [[Slides](./Presentation.pdf)]\n* 1st Scholar Prize [[Slides](./Research_Overview.pdf)]\n* 2nd Scholar Prize  \n* 3rd Prize in NECCS (national level)  \n\n## Report & Tutorial\n* Deep Learning for Face Analysis [[Slides](./DL_for_Face_Analysis.pdf)]\n\n## Interests\n* Coding  \n* Reading and Writing  \n* Design \n\n\n## Contact\n* [Zhihu](https://www.zhihu.com/people/xulu-0620/activities)\n* [Github](https://github.com/lucasxlu)  \n* Email: xulu0620@gmail.com","date":"2018-11-18T10:42:36.880Z","updated":"2018-11-18T10:42:36.880Z","path":"about/index.html","comments":1,"_id":"cjopy03bk0003608w96m0ditw","content":"<h2 id=\"Self-Introduction\"><a href=\"#Self-Introduction\" class=\"headerlink\" title=\"Self Introduction\"></a>Self Introduction</h2><p>I am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg\" alt=\"LucasX\"></p>\n<p><strong>Curriculum Vitae</strong>: [<a href=\"./CV_LuXu.pdf\">PDF</a>] </p>\n<h2 id=\"Academic-Papers\"><a href=\"#Academic-Papers\" class=\"headerlink\" title=\"Academic Papers\"></a>Academic Papers</h2><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]  </li>\n</ul>\n<h2 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h2><ul>\n<li>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]<br>An AI cloud platform based on Java &amp; Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.</li>\n<li><a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Lagou Job</a><br>A repository for job data crawling and data analysis based on Python3. This repo has received over <strong>200 stars</strong> and <strong>120+ forks</strong>. More details can be read at <a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">here</a>.</li>\n</ul>\n<h2 id=\"Competition\"><a href=\"#Competition\" class=\"headerlink\" title=\"Competition\"></a>Competition</h2><ul>\n<li>My algorithm named <strong>TreeCNN</strong> ranks the 1st place on <strong>Facial Expression Recognition Challenge</strong> on <a href=\"http://www.whdeng.cn/raf/model1.html\" target=\"_blank\" rel=\"noopener\">Real-word Affective Face Database (RAF-DB)</a>.</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Image Quality Assessment &amp; Automatic License Examination &amp; Face Analysis<br>During my intern at <a href=\"https://www.didiglobal.com/\" target=\"_blank\" rel=\"noopener\">DiDi</a>, we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.<ul>\n<li>Building deep models for IQA with 93.15% precision and 98.35% recall</li>\n<li>Developing and training deep models for license type classification with 99.72% precision and 99.73% recall </li>\n<li>Developing deep models for Facial Beauty Prediction with 0.89 PC</li>\n<li>Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall</li>\n<li>Writing 16 patents about security products about DiDi</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><a href=\"http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html\" target=\"_blank\" rel=\"noopener\">Pig Face Recognition</a><br>Our team developed a <strong>Pig Face Recognition System</strong> with <a href=\"http://www.yingzi.com/\" target=\"_blank\" rel=\"noopener\">Guangzhou Yingzi Technology Co.,Ltd</a>, to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   <ul>\n<li>Building deep model and training on Amazon S3 GPU server.</li>\n<li>Encapsulate RESTful API and web backend development.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Mal-Image Recognition System<br>I developed a Mal-Image Recognition System with <a href=\"http://zlfinfo.com.cn/\" target=\"_blank\" rel=\"noopener\">Wuhan ZhiLiFeng Information .,Ltd.</a>, the model achieves a mAP with 92.17% on detecting mal-images.  <ul>\n<li>Image data crawling and preprocessing.</li>\n<li>Building machine learning algorithms.</li>\n<li>Model training and deployment.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Web Crawling and Java Developer<br>During working at <a href=\"http://www.jiewen.com.cn/\" target=\"_blank\" rel=\"noopener\">Beijing Jiewen Technology Co.,Ltd.</a>, Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.<ul>\n<li>Developing and maintaining web information system based on Java SSM framework and MySQL database.</li>\n<li>Developing web crawlers to collect data from O2O websites (e.g. <a href=\"http://www.ctrip.com/\" target=\"_blank\" rel=\"noopener\">Ctrip</a>, <a href=\"http://www.dianping.com/\" target=\"_blank\" rel=\"noopener\">Dazhong Dianping</a>, <a href=\"https://www.amap.com/\" target=\"_blank\" rel=\"noopener\">AMap</a>), and storing them in MySQL and MongoDB.</li>\n<li>Data statistics, analysis and visualization.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Skills\"><a href=\"#Skills\" class=\"headerlink\" title=\"Skills\"></a>Skills</h2><ul>\n<li>English: (CET-6: 575/710); Proficiency in Reading and Writing    </li>\n<li>Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    </li>\n<li>Programming: Java == Python &gt; R &gt; C++  </li>\n<li>Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  </li>\n<li>Database: MongoDB/MySQL/Oracle  </li>\n<li>Others: Web Crawler/Web Development/PPT</li>\n</ul>\n<h2 id=\"Award\"><a href=\"#Award\" class=\"headerlink\" title=\"Award\"></a>Award</h2><ul>\n<li>1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [<a href=\"./Presentation.pdf\">Slides</a>]</li>\n<li>1st Scholar Prize [<a href=\"./Research_Overview.pdf\">Slides</a>]</li>\n<li>2nd Scholar Prize  </li>\n<li>3rd Prize in NECCS (national level)  </li>\n</ul>\n<h2 id=\"Report-amp-Tutorial\"><a href=\"#Report-amp-Tutorial\" class=\"headerlink\" title=\"Report &amp; Tutorial\"></a>Report &amp; Tutorial</h2><ul>\n<li>Deep Learning for Face Analysis [<a href=\"./DL_for_Face_Analysis.pdf\">Slides</a>]</li>\n</ul>\n<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h2><ul>\n<li>Coding  </li>\n<li>Reading and Writing  </li>\n<li>Design </li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><ul>\n<li><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">Zhihu</a></li>\n<li><a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">Github</a>  </li>\n<li>Email: <a href=\"mailto:xulu0620@gmail.com\" target=\"_blank\" rel=\"noopener\">xulu0620@gmail.com</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Self-Introduction\"><a href=\"#Self-Introduction\" class=\"headerlink\" title=\"Self Introduction\"></a>Self Introduction</h2><p>I am a third-year master major in machine learning and computer vision. My research interests include Facial Attribute Analysis, Deep Learning and Data Science. </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/about/LucasX.jpg\" alt=\"LucasX\"></p>\n<p><strong>Curriculum Vitae</strong>: [<a href=\"./CV_LuXu.pdf\">PDF</a>] </p>\n<h2 id=\"Academic-Papers\"><a href=\"#Academic-Papers\" class=\"headerlink\" title=\"Academic Papers\"></a>Academic Papers</h2><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]  </li>\n</ul>\n<h2 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h2><ul>\n<li>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]<br>An AI cloud platform based on Java &amp; Python, to provide RESTful APIs in web data crawling, text classification, sentiment analysis, document   similarity measuring, image classification and HTML5-based data visualization.</li>\n<li><a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Lagou Job</a><br>A repository for job data crawling and data analysis based on Python3. This repo has received over <strong>200 stars</strong> and <strong>120+ forks</strong>. More details can be read at <a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">here</a>.</li>\n</ul>\n<h2 id=\"Competition\"><a href=\"#Competition\" class=\"headerlink\" title=\"Competition\"></a>Competition</h2><ul>\n<li>My algorithm named <strong>TreeCNN</strong> ranks the 1st place on <strong>Facial Expression Recognition Challenge</strong> on <a href=\"http://www.whdeng.cn/raf/model1.html\" target=\"_blank\" rel=\"noopener\">Real-word Affective Face Database (RAF-DB)</a>.</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Image Quality Assessment &amp; Automatic License Examination &amp; Face Analysis<br>During my intern at <a href=\"https://www.didiglobal.com/\" target=\"_blank\" rel=\"noopener\">DiDi</a>, we are working at developing deep learning models to recognize the image quality (blur/reflection/normal), and facial beauty prediction algorithm.<ul>\n<li>Building deep models for IQA with 93.15% precision and 98.35% recall</li>\n<li>Developing and training deep models for license type classification with 99.72% precision and 99.73% recall </li>\n<li>Developing deep models for Facial Beauty Prediction with 0.89 PC</li>\n<li>Developing and training deep models for scene recognition (in/out car) with 98.94% precision and 99.27% recall</li>\n<li>Writing 16 patents about security products about DiDi</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><a href=\"http://gd.people.com.cn/n2/2018/0323/c123932-31374601.html\" target=\"_blank\" rel=\"noopener\">Pig Face Recognition</a><br>Our team developed a <strong>Pig Face Recognition System</strong> with <a href=\"http://www.yingzi.com/\" target=\"_blank\" rel=\"noopener\">Guangzhou Yingzi Technology Co.,Ltd</a>, to accurately recognize a pig and get its detailed information through captured photo. It achieves a mAP over 95%.   <ul>\n<li>Building deep model and training on Amazon S3 GPU server.</li>\n<li>Encapsulate RESTful API and web backend development.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Mal-Image Recognition System<br>I developed a Mal-Image Recognition System with <a href=\"http://zlfinfo.com.cn/\" target=\"_blank\" rel=\"noopener\">Wuhan ZhiLiFeng Information .,Ltd.</a>, the model achieves a mAP with 92.17% on detecting mal-images.  <ul>\n<li>Image data crawling and preprocessing.</li>\n<li>Building machine learning algorithms.</li>\n<li>Model training and deployment.</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Web Crawling and Java Developer<br>During working at <a href=\"http://www.jiewen.com.cn/\" target=\"_blank\" rel=\"noopener\">Beijing Jiewen Technology Co.,Ltd.</a>, Huazhong Developing Center. My duties are developing and maintaining financial information system, and developing web crawling system based on Java and Python.<ul>\n<li>Developing and maintaining web information system based on Java SSM framework and MySQL database.</li>\n<li>Developing web crawlers to collect data from O2O websites (e.g. <a href=\"http://www.ctrip.com/\" target=\"_blank\" rel=\"noopener\">Ctrip</a>, <a href=\"http://www.dianping.com/\" target=\"_blank\" rel=\"noopener\">Dazhong Dianping</a>, <a href=\"https://www.amap.com/\" target=\"_blank\" rel=\"noopener\">AMap</a>), and storing them in MySQL and MongoDB.</li>\n<li>Data statistics, analysis and visualization.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Skills\"><a href=\"#Skills\" class=\"headerlink\" title=\"Skills\"></a>Skills</h2><ul>\n<li>English: (CET-6: 575/710); Proficiency in Reading and Writing    </li>\n<li>Algorithm: Deep Learning/Machine Learning/Computer Vision/Data Mining/Data Analysis    </li>\n<li>Programming: Java == Python &gt; R &gt; C++  </li>\n<li>Framework: PyTorch/TensorFlow/Scikit-Learn/OpenCV  </li>\n<li>Database: MongoDB/MySQL/Oracle  </li>\n<li>Others: Web Crawler/Web Development/PPT</li>\n</ul>\n<h2 id=\"Award\"><a href=\"#Award\" class=\"headerlink\" title=\"Award\"></a>Award</h2><ul>\n<li>1st Prize in Academic Research Report (outperform all Ph.Ds). Combining Machine Learning and Data-driven Approaches for AI Services. [<a href=\"./Presentation.pdf\">Slides</a>]</li>\n<li>1st Scholar Prize [<a href=\"./Research_Overview.pdf\">Slides</a>]</li>\n<li>2nd Scholar Prize  </li>\n<li>3rd Prize in NECCS (national level)  </li>\n</ul>\n<h2 id=\"Report-amp-Tutorial\"><a href=\"#Report-amp-Tutorial\" class=\"headerlink\" title=\"Report &amp; Tutorial\"></a>Report &amp; Tutorial</h2><ul>\n<li>Deep Learning for Face Analysis [<a href=\"./DL_for_Face_Analysis.pdf\">Slides</a>]</li>\n</ul>\n<h2 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h2><ul>\n<li>Coding  </li>\n<li>Reading and Writing  </li>\n<li>Design </li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><ul>\n<li><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">Zhihu</a></li>\n<li><a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">Github</a>  </li>\n<li>Email: <a href=\"mailto:xulu0620@gmail.com\" target=\"_blank\" rel=\"noopener\">xulu0620@gmail.com</a></li>\n</ul>\n"},{"layout":"papers","title":"Papers","header-img":"img/header_img/archive-bg.png","comments":0,"date":"2018-07-18T04:28:56.000Z","description":"Academic papers list.","_content":"# Academic Papers List\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](../about/Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]   \n","source":"papers/index.md","raw":"---\nlayout: \"papers\"\ntitle: \"Papers\"\nheader-img: \"img/header_img/archive-bg.png\"\ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"Academic papers list.\"\n---\n# Academic Papers List\n* **Xu L**, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61)] [[Code](https://github.com/lucasxlu/CRNet.git)]\n* Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  \n* Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis\n* Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]    \n* Transferring Rich Deep Features for Facial Beauty Prediction ([Computers and Electrical Engineering](https://www.journals.elsevier.com/computers-and-electrical-engineering). SCI) [[ArXiv](https://arxiv.org/abs/1803.07253)] [[Code](https://github.com/lucasxlu/TransFBP.git)] [[Slides](../about/Presentation_TransFBP.pdf)]\n* An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)\n* Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as [Excellent Paper](http://www.hbe.gov.cn/content.php?id=12717) (ID: [2024](http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls)) among all papers of Hubei Province in 2015. Chinese) [[Code](https://github.com/xuludev/System.git)]   \n","updated":"2018-11-18T10:43:15.437Z","path":"papers/index.html","_id":"cjopy03fp004r608wvt8gmo5n","content":"<h1 id=\"Academic-Papers-List\"><a href=\"#Academic-Papers-List\" class=\"headerlink\" title=\"Academic Papers List\"></a>Academic Papers List</h1><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"../about/Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]   </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Academic-Papers-List\"><a href=\"#Academic-Papers-List\" class=\"headerlink\" title=\"Academic Papers List\"></a>Academic Papers List</h1><ul>\n<li><strong>Xu L</strong>, Xiang J, Yuan X. CRNet: Classification and Regression Neural Network for Facial Beauty Prediction[C]//Pacific Rim Conference on Multimedia. Springer, Cham, 2018: 661-671. [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-00764-5_61\" target=\"_blank\" rel=\"noopener\">Paper</a>] [<a href=\"https://github.com/lucasxlu/CRNet.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</li>\n<li>Hierarchical Multi-task Networks for Race, Gender and Facial Attractiveness Recognition  </li>\n<li>Multi-Task Tree Convolutional Neural Network for Facial Expression Recognition and Face Analysis</li>\n<li>Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]    </li>\n<li>Transferring Rich Deep Features for Facial Beauty Prediction (<a href=\"https://www.journals.elsevier.com/computers-and-electrical-engineering\" target=\"_blank\" rel=\"noopener\">Computers and Electrical Engineering</a>. SCI) [<a href=\"https://arxiv.org/abs/1803.07253\" target=\"_blank\" rel=\"noopener\">ArXiv</a>] [<a href=\"https://github.com/lucasxlu/TransFBP.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"../about/Presentation_TransFBP.pdf\">Slides</a>]</li>\n<li>An Automatic Method for Internet Terror Information Classification based on Deep Learning and Random Forests (Chinese)</li>\n<li>Research on Hot Topic Detection and Tracking based on Incremental Clustering (Awarded as <a href=\"http://www.hbe.gov.cn/content.php?id=12717\" target=\"_blank\" rel=\"noopener\">Excellent Paper</a> (ID: <a href=\"http://hbxw.e21.edu.cn/e21sqlimg//file/201512/fff20151224164931_675715070.xls\" target=\"_blank\" rel=\"noopener\">2024</a>) among all papers of Hubei Province in 2015. Chinese) [<a href=\"https://github.com/xuludev/System.git\" target=\"_blank\" rel=\"noopener\">Code</a>]   </li>\n</ul>\n"},{"layout":"projects","title":"Projects","header-img":"img/header_img/archive-bg.png","comments":0,"date":"2018-07-18T04:28:56.000Z","description":"Projects for ML/CV/AI/Data Science.","_content":"# Projects List\n**For more information about my projects, please follow my [github](https://github.com/lucasxlu).**\n\n* Lagou Job Data Analysis [[Article](https://www.zhihu.com/question/36132174/answer/94392659)] [[Code](https://github.com/lucasxlu/LagouJob.git)] [[Slides](LagouJob.pdf)]\n    > A repository for web data crawling, data storage, and data analysis of [lagou.com](https://www.lagou.com) based on Python3.\n\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]\n    > An AI Cloud Platform with RESTful APIs.\n\n* JiaYuan User Profile [[Article](https://zhuanlan.zhihu.com/p/24515034)] [[Code](https://github.com/lucasxlu/JiaYuan.git)] [[Slides](JiaYuan.pdf)]\n    > A repository for web data crawling, user profile of [jiayuan.com](http://www.jiayuan.com/).\n\n* XiaoLu AI [[Article-Face Beauty Recognition](https://zhuanlan.zhihu.com/p/29399781)] [[Article-Mate Face Recognition](https://zhuanlan.zhihu.com/p/35135539)] [[Code](https://github.com/lucasxlu/XiaoLuAI.git)]\n    > A repository in computer vision and NLP.\n\n* Zhihu Live Analysis [[Article-1](https://zhuanlan.zhihu.com/p/30514792)] [[Article-2](https://zhuanlan.zhihu.com/p/31651544)] [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]\n    > A repository for data crawling, data analysis, data mining and data visualization.\n\n* Image Guard [[Article](https://zhuanlan.zhihu.com/p/29016317)] [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder)]\n    > A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.\n\n* Douban Sentiment Analysis [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)]\n    > A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.\n\n* Web Data Mining [[Article](https://zhuanlan.zhihu.com/p/28954770)] [[Code](https://github.com/lucasxlu/DataHouse.git)]\n    > A repository for web data crawling, analysis and mining.\n\n* Ctrip Web Crawler [[Code](https://github.com/lucasxlu/CtripPro.git)]\n    > A repository for multi-thread web crawler of Ctrip based on Java.\n\n* MateFace (WeChat Mini Program) [[Code](https://github.com/lucasxlu/mateface.git)]\n    > A WeChat Mini Program for mate face comparision.\n\n* Industry Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./IndustryReport.pdf)]\n    > A repository for industry analysis and report.\n\n* AI Career Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./MLJob.pdf)]\n    > A repository and report for career analysis in AI/ML.\n\n* Government Report [[Slides](./GovReport.pdf)]\n    > A repository for government report analysis and report.","source":"projects/index.md","raw":"---\nlayout: \"projects\"\ntitle: \"Projects\"\nheader-img: \"img/header_img/archive-bg.png\"\ncomments: false\ndate: 2018-07-18 12:28:56\ndescription: \"Projects for ML/CV/AI/Data Science.\"\n---\n# Projects List\n**For more information about my projects, please follow my [github](https://github.com/lucasxlu).**\n\n* Lagou Job Data Analysis [[Article](https://www.zhihu.com/question/36132174/answer/94392659)] [[Code](https://github.com/lucasxlu/LagouJob.git)] [[Slides](LagouJob.pdf)]\n    > A repository for web data crawling, data storage, and data analysis of [lagou.com](https://www.lagou.com) based on Python3.\n\n* XCloud [[Java](https://github.com/lucasxlu/CVLH.git)] [[Python](https://github.com/lucasxlu/XCloud.git)]\n    > An AI Cloud Platform with RESTful APIs.\n\n* JiaYuan User Profile [[Article](https://zhuanlan.zhihu.com/p/24515034)] [[Code](https://github.com/lucasxlu/JiaYuan.git)] [[Slides](JiaYuan.pdf)]\n    > A repository for web data crawling, user profile of [jiayuan.com](http://www.jiayuan.com/).\n\n* XiaoLu AI [[Article-Face Beauty Recognition](https://zhuanlan.zhihu.com/p/29399781)] [[Article-Mate Face Recognition](https://zhuanlan.zhihu.com/p/35135539)] [[Code](https://github.com/lucasxlu/XiaoLuAI.git)]\n    > A repository in computer vision and NLP.\n\n* Zhihu Live Analysis [[Article-1](https://zhuanlan.zhihu.com/p/30514792)] [[Article-2](https://zhuanlan.zhihu.com/p/31651544)] [[Code](https://github.com/lucasxlu/ZhihuDataDriven.git)]\n    > A repository for data crawling, data analysis, data mining and data visualization.\n\n* Image Guard [[Article](https://zhuanlan.zhihu.com/p/29016317)] [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder)]\n    > A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.\n\n* Douban Sentiment Analysis [[Code](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)]\n    > A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.\n\n* Web Data Mining [[Article](https://zhuanlan.zhihu.com/p/28954770)] [[Code](https://github.com/lucasxlu/DataHouse.git)]\n    > A repository for web data crawling, analysis and mining.\n\n* Ctrip Web Crawler [[Code](https://github.com/lucasxlu/CtripPro.git)]\n    > A repository for multi-thread web crawler of Ctrip based on Java.\n\n* MateFace (WeChat Mini Program) [[Code](https://github.com/lucasxlu/mateface.git)]\n    > A WeChat Mini Program for mate face comparision.\n\n* Industry Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./IndustryReport.pdf)]\n    > A repository for industry analysis and report.\n\n* AI Career Analysis [[Code](https://github.com/lucasxlu/DataHouse.git)] [[Slides](./MLJob.pdf)]\n    > A repository and report for career analysis in AI/ML.\n\n* Government Report [[Slides](./GovReport.pdf)]\n    > A repository for government report analysis and report.","updated":"2018-11-18T01:47:07.514Z","path":"projects/index.html","_id":"cjopy03fr004s608wnng9al6e","content":"<h1 id=\"Projects-List\"><a href=\"#Projects-List\" class=\"headerlink\" title=\"Projects List\"></a>Projects List</h1><p><strong>For more information about my projects, please follow my <a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">github</a>.</strong></p>\n<ul>\n<li><p>Lagou Job Data Analysis [<a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"LagouJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, data storage, and data analysis of <a href=\"https://www.lagou.com\" target=\"_blank\" rel=\"noopener\">lagou.com</a> based on Python3.</p>\n</blockquote>\n</li>\n<li><p>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]</p>\n<blockquote>\n<p>An AI Cloud Platform with RESTful APIs.</p>\n</blockquote>\n</li>\n<li><p>JiaYuan User Profile [<a href=\"https://zhuanlan.zhihu.com/p/24515034\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/JiaYuan.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"JiaYuan.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, user profile of <a href=\"http://www.jiayuan.com/\" target=\"_blank\" rel=\"noopener\">jiayuan.com</a>.</p>\n</blockquote>\n</li>\n<li><p>XiaoLu AI [<a href=\"https://zhuanlan.zhihu.com/p/29399781\" target=\"_blank\" rel=\"noopener\">Article-Face Beauty Recognition</a>] [<a href=\"https://zhuanlan.zhihu.com/p/35135539\" target=\"_blank\" rel=\"noopener\">Article-Mate Face Recognition</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository in computer vision and NLP.</p>\n</blockquote>\n</li>\n<li><p>Zhihu Live Analysis [<a href=\"https://zhuanlan.zhihu.com/p/30514792\" target=\"_blank\" rel=\"noopener\">Article-1</a>] [<a href=\"https://zhuanlan.zhihu.com/p/31651544\" target=\"_blank\" rel=\"noopener\">Article-2</a>] [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for data crawling, data analysis, data mining and data visualization.</p>\n</blockquote>\n</li>\n<li><p>Image Guard [<a href=\"https://zhuanlan.zhihu.com/p/29016317\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.</p>\n</blockquote>\n</li>\n<li><p>Douban Sentiment Analysis [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.</p>\n</blockquote>\n</li>\n<li><p>Web Data Mining [<a href=\"https://zhuanlan.zhihu.com/p/28954770\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, analysis and mining.</p>\n</blockquote>\n</li>\n<li><p>Ctrip Web Crawler [<a href=\"https://github.com/lucasxlu/CtripPro.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for multi-thread web crawler of Ctrip based on Java.</p>\n</blockquote>\n</li>\n<li><p>MateFace (WeChat Mini Program) [<a href=\"https://github.com/lucasxlu/mateface.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A WeChat Mini Program for mate face comparision.</p>\n</blockquote>\n</li>\n<li><p>Industry Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./IndustryReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for industry analysis and report.</p>\n</blockquote>\n</li>\n<li><p>AI Career Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./MLJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository and report for career analysis in AI/ML.</p>\n</blockquote>\n</li>\n<li><p>Government Report [<a href=\"./GovReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for government report analysis and report.</p>\n</blockquote>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Projects-List\"><a href=\"#Projects-List\" class=\"headerlink\" title=\"Projects List\"></a>Projects List</h1><p><strong>For more information about my projects, please follow my <a href=\"https://github.com/lucasxlu\" target=\"_blank\" rel=\"noopener\">github</a>.</strong></p>\n<ul>\n<li><p>Lagou Job Data Analysis [<a href=\"https://www.zhihu.com/question/36132174/answer/94392659\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/LagouJob.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"LagouJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, data storage, and data analysis of <a href=\"https://www.lagou.com\" target=\"_blank\" rel=\"noopener\">lagou.com</a> based on Python3.</p>\n</blockquote>\n</li>\n<li><p>XCloud [<a href=\"https://github.com/lucasxlu/CVLH.git\" target=\"_blank\" rel=\"noopener\">Java</a>] [<a href=\"https://github.com/lucasxlu/XCloud.git\" target=\"_blank\" rel=\"noopener\">Python</a>]</p>\n<blockquote>\n<p>An AI Cloud Platform with RESTful APIs.</p>\n</blockquote>\n</li>\n<li><p>JiaYuan User Profile [<a href=\"https://zhuanlan.zhihu.com/p/24515034\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/JiaYuan.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"JiaYuan.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, user profile of <a href=\"http://www.jiayuan.com/\" target=\"_blank\" rel=\"noopener\">jiayuan.com</a>.</p>\n</blockquote>\n</li>\n<li><p>XiaoLu AI [<a href=\"https://zhuanlan.zhihu.com/p/29399781\" target=\"_blank\" rel=\"noopener\">Article-Face Beauty Recognition</a>] [<a href=\"https://zhuanlan.zhihu.com/p/35135539\" target=\"_blank\" rel=\"noopener\">Article-Mate Face Recognition</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository in computer vision and NLP.</p>\n</blockquote>\n</li>\n<li><p>Zhihu Live Analysis [<a href=\"https://zhuanlan.zhihu.com/p/30514792\" target=\"_blank\" rel=\"noopener\">Article-1</a>] [<a href=\"https://zhuanlan.zhihu.com/p/31651544\" target=\"_blank\" rel=\"noopener\">Article-2</a>] [<a href=\"https://github.com/lucasxlu/ZhihuDataDriven.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for data crawling, data analysis, data mining and data visualization.</p>\n</blockquote>\n</li>\n<li><p>Image Guard [<a href=\"https://zhuanlan.zhihu.com/p/29016317\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/imgguarder\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for pornography image recognition based on deep learning, and random forests with hand-crafted features.</p>\n</blockquote>\n</li>\n<li><p>Douban Sentiment Analysis [<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for sentiment analysis in douban comments based on TF-IDF weighted word2vec, deep AutoEncoder and SVM.</p>\n</blockquote>\n</li>\n<li><p>Web Data Mining [<a href=\"https://zhuanlan.zhihu.com/p/28954770\" target=\"_blank\" rel=\"noopener\">Article</a>] [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for web data crawling, analysis and mining.</p>\n</blockquote>\n</li>\n<li><p>Ctrip Web Crawler [<a href=\"https://github.com/lucasxlu/CtripPro.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A repository for multi-thread web crawler of Ctrip based on Java.</p>\n</blockquote>\n</li>\n<li><p>MateFace (WeChat Mini Program) [<a href=\"https://github.com/lucasxlu/mateface.git\" target=\"_blank\" rel=\"noopener\">Code</a>]</p>\n<blockquote>\n<p>A WeChat Mini Program for mate face comparision.</p>\n</blockquote>\n</li>\n<li><p>Industry Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./IndustryReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for industry analysis and report.</p>\n</blockquote>\n</li>\n<li><p>AI Career Analysis [<a href=\"https://github.com/lucasxlu/DataHouse.git\" target=\"_blank\" rel=\"noopener\">Code</a>] [<a href=\"./MLJob.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository and report for career analysis in AI/ML.</p>\n</blockquote>\n</li>\n<li><p>Government Report [<a href=\"./GovReport.pdf\">Slides</a>]</p>\n<blockquote>\n<p>A repository for government report analysis and report.</p>\n</blockquote>\n</li>\n</ul>\n"},{"layout":"tags","title":"Tags","description":"You can find articles by tags.","header-img":null,"_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\ndescription: \"You can find articles by tags.\"\nheader-img: \n---\n","date":"2018-10-01T04:40:10.462Z","updated":"2018-10-01T04:40:10.462Z","path":"tags/index.html","comments":1,"_id":"cjopy03fu004t608wopdyyyl8","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"[Algorithm] Sort","date":"2018-08-02T03:37:29.000Z","mathjax":true,"catagories":["Algorithm","Data Structure","Graph"],"_content":"## 初级排序算法\n### 选择排序\n首先找到数组中最小的那个元素，其次，将它和数组中的第一个元素交换位置（如果第一个元素就是最小元素就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组中第二个元素交换位置。如此往复，直至将整个数组排序。这叫做 __选择排序__，因为它总是在不断选择剩余元素中最小的元素。\n\n> 对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较与$N$次交换。\n\n```java\npublic int[] selectSort (int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        int min = i;\n        for (int j = i + 1; j < input.length; j++) {\n            if (input[j] < input[min]) {\n                min = j;\n            }\n        }\n        if (input[min] != input[i]) {\n            int tmp = input[min];\n            input[min] = input[i];\n            input[i] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n* 选择排序的运行时间和输入无关，为了找到最小元素而扫描一遍数组并不能为下一遍扫描提供什么信息。\n* 选择排序的数据移动是最少的，每次交换都会改变两个数组元素的值，因此选择排序用了$N$次交换——交换次数和数组大小是线性关系。\n\n### 插入排序\n为了给要插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位。与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的位置腾出空间，它们可能会被移动。但是当索引到达数组最右端，数组排序就完成了。__与选择排序不同的是，插入排序所需的时间取决于输入中元素的初始顺序__。\n\n> 对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\sim N^2/4$次比较以及$\\sim N^2/4$次交换。最坏情况下需要$\\sim N^2/2$比较和$\\sim N^2/2$次交换。最好情况下需要$N-1$次比较和0次交换。\n\n```java\npublic int[] insertSort(int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        for (int j = i; j > 0 && input[j] < input[j - 1]; j--) {\n            int tmp = input[j];\n            input[j] = input[j - 1];\n            input[j - 1] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n> 插入排序对部分有序的数组十分高效，也很适合小规模数组。\n\n> 对于随机排序的无重复主键的数组，插入排序和选择排序的运行时间是$O(N^2)$级别的。\n\n### 希尔排序\n希尔排序是一种基于插入排序的快速排序算法，对于大规模乱序数组，插入排序很慢，因为它只会交换相邻元素，因此元素只能一点点从数组一端移动到另一端。因此，希尔排序为了加速简单地改进了插入排序，__交换不相邻的元素以对数组的局部进行排序，并最终用插入排序将局部有序的数组排序__。\n\n希尔排序的思想是使数组中任意间隔为$h$的元素都是有序的，这样的数组被称为$h$有序数组。对于每一个$h$，用插入排序将$h$个子数组独立地排序，但因为子数组是相互独立的，一个更简单的方法是在$h-$子数组中将每个元素交换到比它大的元素之前去。只需要在插入排序的代码中将移动元素的距离由1改为$h$即可。这样，希尔排序的实现就转换为一个类似于插入排序但使用不同增量的过程。\n\n希尔排序更高效的原因是它权衡了子数组的规模和有序性，排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序。\n\n```java\npublic int[] shellSort(int[] input) {\n    int h = 1;\n    while (h < input.length / 3)\n        h = 3 * h + 1;\n\n    while (h >= 1) {\n        for (int i = h; i < input.length; i++) {\n            for (int j = i; j >= h && input[j] < input[j - h]; j -= h) {\n                int tmp = input[j];\n                input[j] = input[j - h];\n                input[j - h] = tmp;\n            }\n\n            h /= 3;\n        }\n    }\n\n    return input;\n}\n```\n\n> 最坏情况下，希尔排序的比较次数和$N^{3/2}$成正比。\n\n### 归并排序\n要将一个数组排序，可以先递归地将它分成两半分别排序，然后将结果归并起来。归并排序能够保证任意长度为$N$的数组排序所需的时间$NlogN$成正比；它的主要缺点是它所需要的额外空间和$N$成正比。\n\n* 原地归并\n```java\nprivate static void merge(Comparable[] input, int low, int mid, int high) {\n    int i = low, j = mid + 1;\n    for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n        aux[k] = input[k];\n    }\n\n    for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n        if (i > mid) input[k] = aux[j++];\n        else if (j > high) input[k] = aux[i++];\n        else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n        else input[k] = aux[i++];\n    }\n}\n```\n\n* 自顶向下归并\n```java\npublic class Merge {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        sort(a, 0, a.length - 1);\n    }\n\n    private static void merge(Comparable[] input, int low, int mid, int high) {\n        int i = low, j = mid + 1;\n        for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n            aux[k] = input[k];\n        }\n\n        for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n            if (i > mid) input[k] = aux[j++];\n            else if (j > high) input[k] = aux[i++];\n            else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n            else input[k] = aux[i++];\n        }\n    }\n\n    private static void sort(Comparable[] a, int lo, int hi) {\n        // sort array a[lo...hi]\n        if (hi <= lo) return;\n        int mid = lo + (hi - lo) / 2;\n        sort(a, lo, mid);\n        sort(a, mid + 1, hi);\n        merge(a, lo, mid, hi);\n    }\n}\n```\n\n> 对于长度为$N$的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$。\n\n* 自底向上归并  \n先归并那些微型数组，然后再成对归并得到的子数组，直到我们将整个数组归并在一起。\n```java\npublic class MergeBU {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        for (int sz = 1; sz < a.length; sz=sz+sz) {\n            for (int lo = 0; lo < a.length - sz; lo += sz + sz) {\n                merge(a, lo, lo + sz - 1, Math.min(lo + 2 * sz - 1, a.length - 1));\n            }\n\n        }\n    }\n}\n```\n\n### 快速排序\n快排的特点：它是原地排序；将长度为$N$的数组排序所需时间和$NlgN$成正比。其缺点是非常脆弱，其最坏情况下的性能只有平方级别。\n\n快排是一种分治算法。它将一个数组分成两个子数组，将两部分独立地排序，快排和归并是互补的：归并排序将将数组分成两个子数组分别排序，并将有序的子数组归并以将整个数组排序；而快排将数组排序的方式则是当两个子数组都有序时整个数组也就自然有序了。\n\n一般策略是：先随意地取 a[lo] 作为切分元素，即那个将会被排定的元素，然后我们从数组的左端开始向右扫描，直到找到一个大于等于它的元素，然后再从数组的右端向左开始扫描，直到找到一个小于等于它的元素。交换它们的位置。如此继续，我们就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，我们只需要将切分元素 a[lo]和左子数组最右侧的元素（a[j]）交换然后返回 j 即可。\n\n> 将长度为$N$的无重复数组排序，快排平均需要$\\sim 2NlgN$次比较。最多需要$\\frac{N^2}{2}$次比较，但随机打乱能预防这种情况。\n\n","source":"_posts/algo-sort.md","raw":"---\ntitle: \"[Algorithm] Sort\"\ndate: 2018-08-02 11:37:29\nmathjax: true\ntags:\n- Algorithm\n- Data Structure\n- Graph\ncatagories:\n- Algorithm\n- Data Structure\n- Graph\n---\n## 初级排序算法\n### 选择排序\n首先找到数组中最小的那个元素，其次，将它和数组中的第一个元素交换位置（如果第一个元素就是最小元素就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组中第二个元素交换位置。如此往复，直至将整个数组排序。这叫做 __选择排序__，因为它总是在不断选择剩余元素中最小的元素。\n\n> 对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较与$N$次交换。\n\n```java\npublic int[] selectSort (int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        int min = i;\n        for (int j = i + 1; j < input.length; j++) {\n            if (input[j] < input[min]) {\n                min = j;\n            }\n        }\n        if (input[min] != input[i]) {\n            int tmp = input[min];\n            input[min] = input[i];\n            input[i] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n* 选择排序的运行时间和输入无关，为了找到最小元素而扫描一遍数组并不能为下一遍扫描提供什么信息。\n* 选择排序的数据移动是最少的，每次交换都会改变两个数组元素的值，因此选择排序用了$N$次交换——交换次数和数组大小是线性关系。\n\n### 插入排序\n为了给要插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位。与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的位置腾出空间，它们可能会被移动。但是当索引到达数组最右端，数组排序就完成了。__与选择排序不同的是，插入排序所需的时间取决于输入中元素的初始顺序__。\n\n> 对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\sim N^2/4$次比较以及$\\sim N^2/4$次交换。最坏情况下需要$\\sim N^2/2$比较和$\\sim N^2/2$次交换。最好情况下需要$N-1$次比较和0次交换。\n\n```java\npublic int[] insertSort(int[] input) {\n    for (int i = 0; i < input.length; i++) {\n        for (int j = i; j > 0 && input[j] < input[j - 1]; j--) {\n            int tmp = input[j];\n            input[j] = input[j - 1];\n            input[j - 1] = tmp;\n        }\n    }\n\n    return input;\n}\n```\n> 插入排序对部分有序的数组十分高效，也很适合小规模数组。\n\n> 对于随机排序的无重复主键的数组，插入排序和选择排序的运行时间是$O(N^2)$级别的。\n\n### 希尔排序\n希尔排序是一种基于插入排序的快速排序算法，对于大规模乱序数组，插入排序很慢，因为它只会交换相邻元素，因此元素只能一点点从数组一端移动到另一端。因此，希尔排序为了加速简单地改进了插入排序，__交换不相邻的元素以对数组的局部进行排序，并最终用插入排序将局部有序的数组排序__。\n\n希尔排序的思想是使数组中任意间隔为$h$的元素都是有序的，这样的数组被称为$h$有序数组。对于每一个$h$，用插入排序将$h$个子数组独立地排序，但因为子数组是相互独立的，一个更简单的方法是在$h-$子数组中将每个元素交换到比它大的元素之前去。只需要在插入排序的代码中将移动元素的距离由1改为$h$即可。这样，希尔排序的实现就转换为一个类似于插入排序但使用不同增量的过程。\n\n希尔排序更高效的原因是它权衡了子数组的规模和有序性，排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序。\n\n```java\npublic int[] shellSort(int[] input) {\n    int h = 1;\n    while (h < input.length / 3)\n        h = 3 * h + 1;\n\n    while (h >= 1) {\n        for (int i = h; i < input.length; i++) {\n            for (int j = i; j >= h && input[j] < input[j - h]; j -= h) {\n                int tmp = input[j];\n                input[j] = input[j - h];\n                input[j - h] = tmp;\n            }\n\n            h /= 3;\n        }\n    }\n\n    return input;\n}\n```\n\n> 最坏情况下，希尔排序的比较次数和$N^{3/2}$成正比。\n\n### 归并排序\n要将一个数组排序，可以先递归地将它分成两半分别排序，然后将结果归并起来。归并排序能够保证任意长度为$N$的数组排序所需的时间$NlogN$成正比；它的主要缺点是它所需要的额外空间和$N$成正比。\n\n* 原地归并\n```java\nprivate static void merge(Comparable[] input, int low, int mid, int high) {\n    int i = low, j = mid + 1;\n    for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n        aux[k] = input[k];\n    }\n\n    for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n        if (i > mid) input[k] = aux[j++];\n        else if (j > high) input[k] = aux[i++];\n        else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n        else input[k] = aux[i++];\n    }\n}\n```\n\n* 自顶向下归并\n```java\npublic class Merge {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        sort(a, 0, a.length - 1);\n    }\n\n    private static void merge(Comparable[] input, int low, int mid, int high) {\n        int i = low, j = mid + 1;\n        for (int k = low; k <= high; k++) {  // copy input[low...high] to aux[low...high]\n            aux[k] = input[k];\n        }\n\n        for (int k = low; k <= high; k++) {  // merge back to input[low...high]\n            if (i > mid) input[k] = aux[j++];\n            else if (j > high) input[k] = aux[i++];\n            else if (aux[j].compareTo(aux[i]) < 0) input[k] = aux[j++];\n            else input[k] = aux[i++];\n        }\n    }\n\n    private static void sort(Comparable[] a, int lo, int hi) {\n        // sort array a[lo...hi]\n        if (hi <= lo) return;\n        int mid = lo + (hi - lo) / 2;\n        sort(a, lo, mid);\n        sort(a, mid + 1, hi);\n        merge(a, lo, mid, hi);\n    }\n}\n```\n\n> 对于长度为$N$的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$。\n\n* 自底向上归并  \n先归并那些微型数组，然后再成对归并得到的子数组，直到我们将整个数组归并在一起。\n```java\npublic class MergeBU {\n    private static Comparable[] aux;\n\n    public static void sort(Comparable[] a) {\n        aux = new Comparable[a.length];\n        for (int sz = 1; sz < a.length; sz=sz+sz) {\n            for (int lo = 0; lo < a.length - sz; lo += sz + sz) {\n                merge(a, lo, lo + sz - 1, Math.min(lo + 2 * sz - 1, a.length - 1));\n            }\n\n        }\n    }\n}\n```\n\n### 快速排序\n快排的特点：它是原地排序；将长度为$N$的数组排序所需时间和$NlgN$成正比。其缺点是非常脆弱，其最坏情况下的性能只有平方级别。\n\n快排是一种分治算法。它将一个数组分成两个子数组，将两部分独立地排序，快排和归并是互补的：归并排序将将数组分成两个子数组分别排序，并将有序的子数组归并以将整个数组排序；而快排将数组排序的方式则是当两个子数组都有序时整个数组也就自然有序了。\n\n一般策略是：先随意地取 a[lo] 作为切分元素，即那个将会被排定的元素，然后我们从数组的左端开始向右扫描，直到找到一个大于等于它的元素，然后再从数组的右端向左开始扫描，直到找到一个小于等于它的元素。交换它们的位置。如此继续，我们就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，我们只需要将切分元素 a[lo]和左子数组最右侧的元素（a[j]）交换然后返回 j 即可。\n\n> 将长度为$N$的无重复数组排序，快排平均需要$\\sim 2NlgN$次比较。最多需要$\\frac{N^2}{2}$次比较，但随机打乱能预防这种情况。\n\n","slug":"algo-sort","published":1,"updated":"2018-10-01T04:40:08.403Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bc0000608wv8lsgxk0","content":"<h2 id=\"初级排序算法\"><a href=\"#初级排序算法\" class=\"headerlink\" title=\"初级排序算法\"></a>初级排序算法</h2><h3 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a>选择排序</h3><p>首先找到数组中最小的那个元素，其次，将它和数组中的第一个元素交换位置（如果第一个元素就是最小元素就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组中第二个元素交换位置。如此往复，直至将整个数组排序。这叫做 <strong>选择排序</strong>，因为它总是在不断选择剩余元素中最小的元素。</p>\n<blockquote>\n<p>对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较与$N$次交换。</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] selectSort (<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> min = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i + <span class=\"number\">1</span>; j &lt; input.length; j++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (input[j] &lt; input[min]) &#123;</span><br><span class=\"line\">                min = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (input[min] != input[i]) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[min];</span><br><span class=\"line\">            input[min] = input[i];</span><br><span class=\"line\">            input[i] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>选择排序的运行时间和输入无关，为了找到最小元素而扫描一遍数组并不能为下一遍扫描提供什么信息。</li>\n<li>选择排序的数据移动是最少的，每次交换都会改变两个数组元素的值，因此选择排序用了$N$次交换——交换次数和数组大小是线性关系。</li>\n</ul>\n<h3 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a>插入排序</h3><p>为了给要插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位。与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的位置腾出空间，它们可能会被移动。但是当索引到达数组最右端，数组排序就完成了。<strong>与选择排序不同的是，插入排序所需的时间取决于输入中元素的初始顺序</strong>。</p>\n<blockquote>\n<p>对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\sim N^2/4$次比较以及$\\sim N^2/4$次交换。最坏情况下需要$\\sim N^2/2$比较和$\\sim N^2/2$次交换。最好情况下需要$N-1$次比较和0次交换。</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] insertSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt; <span class=\"number\">0</span> &amp;&amp; input[j] &lt; input[j - <span class=\"number\">1</span>]; j--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">            input[j] = input[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            input[j - <span class=\"number\">1</span>] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>插入排序对部分有序的数组十分高效，也很适合小规模数组。</p>\n</blockquote>\n<blockquote>\n<p>对于随机排序的无重复主键的数组，插入排序和选择排序的运行时间是$O(N^2)$级别的。</p>\n</blockquote>\n<h3 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h3><p>希尔排序是一种基于插入排序的快速排序算法，对于大规模乱序数组，插入排序很慢，因为它只会交换相邻元素，因此元素只能一点点从数组一端移动到另一端。因此，希尔排序为了加速简单地改进了插入排序，<strong>交换不相邻的元素以对数组的局部进行排序，并最终用插入排序将局部有序的数组排序</strong>。</p>\n<p>希尔排序的思想是使数组中任意间隔为$h$的元素都是有序的，这样的数组被称为$h$有序数组。对于每一个$h$，用插入排序将$h$个子数组独立地排序，但因为子数组是相互独立的，一个更简单的方法是在$h-$子数组中将每个元素交换到比它大的元素之前去。只需要在插入排序的代码中将移动元素的距离由1改为$h$即可。这样，希尔排序的实现就转换为一个类似于插入排序但使用不同增量的过程。</p>\n<p>希尔排序更高效的原因是它权衡了子数组的规模和有序性，排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] shellSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> h = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &lt; input.length / <span class=\"number\">3</span>)</span><br><span class=\"line\">        h = <span class=\"number\">3</span> * h + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &gt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = h; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt;= h &amp;&amp; input[j] &lt; input[j - h]; j -= h) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">                input[j] = input[j - h];</span><br><span class=\"line\">                input[j - h] = tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            h /= <span class=\"number\">3</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>最坏情况下，希尔排序的比较次数和$N^{3/2}$成正比。</p>\n</blockquote>\n<h3 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h3><p>要将一个数组排序，可以先递归地将它分成两半分别排序，然后将结果归并起来。归并排序能够保证任意长度为$N$的数组排序所需的时间$NlogN$成正比；它的主要缺点是它所需要的额外空间和$N$成正比。</p>\n<ul>\n<li><p>原地归并</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">        aux[k] = input[k];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>自顶向下归并</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Merge</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        sort(a, <span class=\"number\">0</span>, a.length - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">            aux[k] = input[k];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a, <span class=\"keyword\">int</span> lo, <span class=\"keyword\">int</span> hi)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// sort array a[lo...hi]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hi &lt;= lo) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mid = lo + (hi - lo) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        sort(a, lo, mid);</span><br><span class=\"line\">        sort(a, mid + <span class=\"number\">1</span>, hi);</span><br><span class=\"line\">        merge(a, lo, mid, hi);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<blockquote>\n<p>对于长度为$N$的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$。</p>\n</blockquote>\n<ul>\n<li>自底向上归并<br>先归并那些微型数组，然后再成对归并得到的子数组，直到我们将整个数组归并在一起。<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MergeBU</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = <span class=\"number\">1</span>; sz &lt; a.length; sz=sz+sz) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> lo = <span class=\"number\">0</span>; lo &lt; a.length - sz; lo += sz + sz) &#123;</span><br><span class=\"line\">                merge(a, lo, lo + sz - <span class=\"number\">1</span>, Math.min(lo + <span class=\"number\">2</span> * sz - <span class=\"number\">1</span>, a.length - <span class=\"number\">1</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h3><p>快排的特点：它是原地排序；将长度为$N$的数组排序所需时间和$NlgN$成正比。其缺点是非常脆弱，其最坏情况下的性能只有平方级别。</p>\n<p>快排是一种分治算法。它将一个数组分成两个子数组，将两部分独立地排序，快排和归并是互补的：归并排序将将数组分成两个子数组分别排序，并将有序的子数组归并以将整个数组排序；而快排将数组排序的方式则是当两个子数组都有序时整个数组也就自然有序了。</p>\n<p>一般策略是：先随意地取 a[lo] 作为切分元素，即那个将会被排定的元素，然后我们从数组的左端开始向右扫描，直到找到一个大于等于它的元素，然后再从数组的右端向左开始扫描，直到找到一个小于等于它的元素。交换它们的位置。如此继续，我们就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，我们只需要将切分元素 a[lo]和左子数组最右侧的元素（a[j]）交换然后返回 j 即可。</p>\n<blockquote>\n<p>将长度为$N$的无重复数组排序，快排平均需要$\\sim 2NlgN$次比较。最多需要$\\frac{N^2}{2}$次比较，但随机打乱能预防这种情况。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"初级排序算法\"><a href=\"#初级排序算法\" class=\"headerlink\" title=\"初级排序算法\"></a>初级排序算法</h2><h3 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a>选择排序</h3><p>首先找到数组中最小的那个元素，其次，将它和数组中的第一个元素交换位置（如果第一个元素就是最小元素就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组中第二个元素交换位置。如此往复，直至将整个数组排序。这叫做 <strong>选择排序</strong>，因为它总是在不断选择剩余元素中最小的元素。</p>\n<blockquote>\n<p>对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较与$N$次交换。</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] selectSort (<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> min = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i + <span class=\"number\">1</span>; j &lt; input.length; j++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (input[j] &lt; input[min]) &#123;</span><br><span class=\"line\">                min = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (input[min] != input[i]) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[min];</span><br><span class=\"line\">            input[min] = input[i];</span><br><span class=\"line\">            input[i] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>选择排序的运行时间和输入无关，为了找到最小元素而扫描一遍数组并不能为下一遍扫描提供什么信息。</li>\n<li>选择排序的数据移动是最少的，每次交换都会改变两个数组元素的值，因此选择排序用了$N$次交换——交换次数和数组大小是线性关系。</li>\n</ul>\n<h3 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a>插入排序</h3><p>为了给要插入的元素腾出空间，我们需要将其余所有元素在插入之前都向右移动一位。与选择排序一样，当前索引左边的所有元素都是有序的，但它们的最终位置还不确定，为了给更小的位置腾出空间，它们可能会被移动。但是当索引到达数组最右端，数组排序就完成了。<strong>与选择排序不同的是，插入排序所需的时间取决于输入中元素的初始顺序</strong>。</p>\n<blockquote>\n<p>对于随机排列的长度为$N$且主键不重复的数组，平均情况下插入排序需要$\\sim N^2/4$次比较以及$\\sim N^2/4$次交换。最坏情况下需要$\\sim N^2/2$比较和$\\sim N^2/2$次交换。最好情况下需要$N-1$次比较和0次交换。</p>\n</blockquote>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] insertSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt; <span class=\"number\">0</span> &amp;&amp; input[j] &lt; input[j - <span class=\"number\">1</span>]; j--) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">            input[j] = input[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            input[j - <span class=\"number\">1</span>] = tmp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>插入排序对部分有序的数组十分高效，也很适合小规模数组。</p>\n</blockquote>\n<blockquote>\n<p>对于随机排序的无重复主键的数组，插入排序和选择排序的运行时间是$O(N^2)$级别的。</p>\n</blockquote>\n<h3 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h3><p>希尔排序是一种基于插入排序的快速排序算法，对于大规模乱序数组，插入排序很慢，因为它只会交换相邻元素，因此元素只能一点点从数组一端移动到另一端。因此，希尔排序为了加速简单地改进了插入排序，<strong>交换不相邻的元素以对数组的局部进行排序，并最终用插入排序将局部有序的数组排序</strong>。</p>\n<p>希尔排序的思想是使数组中任意间隔为$h$的元素都是有序的，这样的数组被称为$h$有序数组。对于每一个$h$，用插入排序将$h$个子数组独立地排序，但因为子数组是相互独立的，一个更简单的方法是在$h-$子数组中将每个元素交换到比它大的元素之前去。只需要在插入排序的代码中将移动元素的距离由1改为$h$即可。这样，希尔排序的实现就转换为一个类似于插入排序但使用不同增量的过程。</p>\n<p>希尔排序更高效的原因是它权衡了子数组的规模和有序性，排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span>[] shellSort(<span class=\"keyword\">int</span>[] input) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> h = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &lt; input.length / <span class=\"number\">3</span>)</span><br><span class=\"line\">        h = <span class=\"number\">3</span> * h + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (h &gt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = h; i &lt; input.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = i; j &gt;= h &amp;&amp; input[j] &lt; input[j - h]; j -= h) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">int</span> tmp = input[j];</span><br><span class=\"line\">                input[j] = input[j - h];</span><br><span class=\"line\">                input[j - h] = tmp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            h /= <span class=\"number\">3</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> input;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>最坏情况下，希尔排序的比较次数和$N^{3/2}$成正比。</p>\n</blockquote>\n<h3 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h3><p>要将一个数组排序，可以先递归地将它分成两半分别排序，然后将结果归并起来。归并排序能够保证任意长度为$N$的数组排序所需的时间$NlogN$成正比；它的主要缺点是它所需要的额外空间和$N$成正比。</p>\n<ul>\n<li><p>原地归并</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">        aux[k] = input[k];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">        <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>自顶向下归并</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Merge</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        sort(a, <span class=\"number\">0</span>, a.length - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">merge</span><span class=\"params\">(Comparable[] input, <span class=\"keyword\">int</span> low, <span class=\"keyword\">int</span> mid, <span class=\"keyword\">int</span> high)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = low, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// copy input[low...high] to aux[low...high]</span></span><br><span class=\"line\">            aux[k] = input[k];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> k = low; k &lt;= high; k++) &#123;  <span class=\"comment\">// merge back to input[low...high]</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt; mid) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (j &gt; high) input[k] = aux[i++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (aux[j].compareTo(aux[i]) &lt; <span class=\"number\">0</span>) input[k] = aux[j++];</span><br><span class=\"line\">            <span class=\"keyword\">else</span> input[k] = aux[i++];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a, <span class=\"keyword\">int</span> lo, <span class=\"keyword\">int</span> hi)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// sort array a[lo...hi]</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hi &lt;= lo) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> mid = lo + (hi - lo) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        sort(a, lo, mid);</span><br><span class=\"line\">        sort(a, mid + <span class=\"number\">1</span>, hi);</span><br><span class=\"line\">        merge(a, lo, mid, hi);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<blockquote>\n<p>对于长度为$N$的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$。</p>\n</blockquote>\n<ul>\n<li>自底向上归并<br>先归并那些微型数组，然后再成对归并得到的子数组，直到我们将整个数组归并在一起。<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MergeBU</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Comparable[] aux;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">sort</span><span class=\"params\">(Comparable[] a)</span> </span>&#123;</span><br><span class=\"line\">        aux = <span class=\"keyword\">new</span> Comparable[a.length];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> sz = <span class=\"number\">1</span>; sz &lt; a.length; sz=sz+sz) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> lo = <span class=\"number\">0</span>; lo &lt; a.length - sz; lo += sz + sz) &#123;</span><br><span class=\"line\">                merge(a, lo, lo + sz - <span class=\"number\">1</span>, Math.min(lo + <span class=\"number\">2</span> * sz - <span class=\"number\">1</span>, a.length - <span class=\"number\">1</span>));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h3><p>快排的特点：它是原地排序；将长度为$N$的数组排序所需时间和$NlgN$成正比。其缺点是非常脆弱，其最坏情况下的性能只有平方级别。</p>\n<p>快排是一种分治算法。它将一个数组分成两个子数组，将两部分独立地排序，快排和归并是互补的：归并排序将将数组分成两个子数组分别排序，并将有序的子数组归并以将整个数组排序；而快排将数组排序的方式则是当两个子数组都有序时整个数组也就自然有序了。</p>\n<p>一般策略是：先随意地取 a[lo] 作为切分元素，即那个将会被排定的元素，然后我们从数组的左端开始向右扫描，直到找到一个大于等于它的元素，然后再从数组的右端向左开始扫描，直到找到一个小于等于它的元素。交换它们的位置。如此继续，我们就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，我们只需要将切分元素 a[lo]和左子数组最右侧的元素（a[j]）交换然后返回 j 即可。</p>\n<blockquote>\n<p>将长度为$N$的无重复数组排序，快排平均需要$\\sim 2NlgN$次比较。最多需要$\\frac{N^2}{2}$次比较，但随机打乱能预防这种情况。</p>\n</blockquote>\n"},{"title":"[Book] Storytelling With Data","date":"2018-08-11T14:54:59.000Z","catagories":["Data Science","Data Visualization","PPT","Presentation"],"_content":"## Introduction\n数据可视化(Data Visualization)是Data Science领域一个非常非常核心的内容，很多时候，我们往往会花很多力气去建模分析数据，然而最终给你的老板汇报，或者是编写分析报告的时候，通常会以图形化的方式展现。这个时候，若你能够 __利用数据讲故事__ ，那么你的汇报就会十分精彩。本文内容来自一本我个人非常喜欢的书，作者是Google工作多年、数据可视化领域的专家。若你也对数据可视化感兴趣，欢迎去阅读原著：《[Storytelling With Data](http://www.storytellingwithdata.com/book/)》。\n\n## 选择有效的图表\n### 简单文本\n当只有一两项数据需要分享时，简单文本是绝佳的沟通方法。考虑只用数字本身(尽可能突出)和一些辅助性文字来清晰地阐述观点。例如：\n\n![Original Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png)\n> 上图用了相当多的文字和空间衬托仅仅两项数据。图表本身对数据的解读并没有多少帮助。\n\n修改后：  \n![Revised Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png)\n\n### 表格\n使用表格时需要记住一点：让设计融入背景。__让数据占据核心地位__。不要让厚重的边框和阴影与数据争夺受众的注意力。要用窄边框或者空白来区分表格的元素。\n![Tables](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png)\n\n也可以使用 __热力图__ 辅助表格，这会使得极值更容易被观众捕捉：  \n![Heatmap](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png)\n\n### 需要避开的陷阱\n* 不要使用饼图\n* 不要使用3D效果图\n* 不要使用双y轴的图，而是应该将它们分开成两个单独的图  \n![Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png)  \n![Revised Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png)\n\n## 去除杂乱\n* 巧用留白\n* 用对比(颜色、字号)突出要表达的元素\n* 对齐使得页面更整洁\n* Less is More:  \n![Less is More](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png)\n\n## 聚焦观众视线\n### 文字中的前注意属性\n![Preattentive Attributes](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png)\n\n### 图表中的前注意属性\n![Without Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png)\n\n加入强调之后：  \n![With Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png)\n\n### 颜色\n使用少量的颜色。通常选择灰色做背景，再挑选一个大胆的颜色（例如蓝色）来吸引注意。\n\n![Color](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png)\n\n### 避免\"意大利面\"式的图表策略\n下图的折线图太过于杂乱，观众无法从中获取有用的信息。\n![Spaghetti Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png)\n\n对这类图表的改进可利用 __前注意属性一次强调一根线条__。然后从空间上隔离这些线条的图表：\n#### 一次只强调一根线 (颜色 + 线条粗细)\n![One Line Highlighted](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png)\n\n#### 空间隔离\n![Vertically Apart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png)\n\n#### 混合方法 (空间上分离 + 一次只强调一根线条)\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png)\n\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png)\n\n### 饼图的替代方案\n未处理前的饼图：  \n![Pie Chart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png)\n\n#### 方案1: 直观展示数字\n![Show Numbers Directly](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png)\n\n#### 方案2: 简单条形图\n![Simple Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png)\n\n#### 方案3: 水平堆叠条形图\n![Stacked Horizontal Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png)\n\n#### 方案4: 斜率图\n![Slope Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png)\n> 通过线条的斜率很容易看出项目前后每个类别百分比的 __变化__，易于 __对比来突出项目效果__。\n","source":"_posts/book-storytelling-with-data.md","raw":"---\ntitle: \"[Book] Storytelling With Data\"\ndate: 2018-08-11 22:54:59\ntags:\n- Data Visualization\n- Data Science\ncatagories:\n- Data Science\n- Data Visualization\n- PPT\n- Presentation\n---\n## Introduction\n数据可视化(Data Visualization)是Data Science领域一个非常非常核心的内容，很多时候，我们往往会花很多力气去建模分析数据，然而最终给你的老板汇报，或者是编写分析报告的时候，通常会以图形化的方式展现。这个时候，若你能够 __利用数据讲故事__ ，那么你的汇报就会十分精彩。本文内容来自一本我个人非常喜欢的书，作者是Google工作多年、数据可视化领域的专家。若你也对数据可视化感兴趣，欢迎去阅读原著：《[Storytelling With Data](http://www.storytellingwithdata.com/book/)》。\n\n## 选择有效的图表\n### 简单文本\n当只有一两项数据需要分享时，简单文本是绝佳的沟通方法。考虑只用数字本身(尽可能突出)和一些辅助性文字来清晰地阐述观点。例如：\n\n![Original Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png)\n> 上图用了相当多的文字和空间衬托仅仅两项数据。图表本身对数据的解读并没有多少帮助。\n\n修改后：  \n![Revised Report](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png)\n\n### 表格\n使用表格时需要记住一点：让设计融入背景。__让数据占据核心地位__。不要让厚重的边框和阴影与数据争夺受众的注意力。要用窄边框或者空白来区分表格的元素。\n![Tables](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png)\n\n也可以使用 __热力图__ 辅助表格，这会使得极值更容易被观众捕捉：  \n![Heatmap](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png)\n\n### 需要避开的陷阱\n* 不要使用饼图\n* 不要使用3D效果图\n* 不要使用双y轴的图，而是应该将它们分开成两个单独的图  \n![Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png)  \n![Revised Double Y-axis](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png)\n\n## 去除杂乱\n* 巧用留白\n* 用对比(颜色、字号)突出要表达的元素\n* 对齐使得页面更整洁\n* Less is More:  \n![Less is More](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png)\n\n## 聚焦观众视线\n### 文字中的前注意属性\n![Preattentive Attributes](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png)\n\n### 图表中的前注意属性\n![Without Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png)\n\n加入强调之后：  \n![With Preattentive Attributes in Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png)\n\n### 颜色\n使用少量的颜色。通常选择灰色做背景，再挑选一个大胆的颜色（例如蓝色）来吸引注意。\n\n![Color](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png)\n\n### 避免\"意大利面\"式的图表策略\n下图的折线图太过于杂乱，观众无法从中获取有用的信息。\n![Spaghetti Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png)\n\n对这类图表的改进可利用 __前注意属性一次强调一根线条__。然后从空间上隔离这些线条的图表：\n#### 一次只强调一根线 (颜色 + 线条粗细)\n![One Line Highlighted](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png)\n\n#### 空间隔离\n![Vertically Apart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png)\n\n#### 混合方法 (空间上分离 + 一次只强调一根线条)\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png)\n\n![Combined Approach](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png)\n\n### 饼图的替代方案\n未处理前的饼图：  \n![Pie Chart](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png)\n\n#### 方案1: 直观展示数字\n![Show Numbers Directly](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png)\n\n#### 方案2: 简单条形图\n![Simple Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png)\n\n#### 方案3: 水平堆叠条形图\n![Stacked Horizontal Bar Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png)\n\n#### 方案4: 斜率图\n![Slope Graph](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png)\n> 通过线条的斜率很容易看出项目前后每个类别百分比的 __变化__，易于 __对比来突出项目效果__。\n","slug":"book-storytelling-with-data","published":1,"updated":"2018-10-01T04:40:08.404Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bi0002608wgrt7lxch","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>数据可视化(Data Visualization)是Data Science领域一个非常非常核心的内容，很多时候，我们往往会花很多力气去建模分析数据，然而最终给你的老板汇报，或者是编写分析报告的时候，通常会以图形化的方式展现。这个时候，若你能够 <strong>利用数据讲故事</strong> ，那么你的汇报就会十分精彩。本文内容来自一本我个人非常喜欢的书，作者是Google工作多年、数据可视化领域的专家。若你也对数据可视化感兴趣，欢迎去阅读原著：《<a href=\"http://www.storytellingwithdata.com/book/\" target=\"_blank\" rel=\"noopener\">Storytelling With Data</a>》。</p>\n<h2 id=\"选择有效的图表\"><a href=\"#选择有效的图表\" class=\"headerlink\" title=\"选择有效的图表\"></a>选择有效的图表</h2><h3 id=\"简单文本\"><a href=\"#简单文本\" class=\"headerlink\" title=\"简单文本\"></a>简单文本</h3><p>当只有一两项数据需要分享时，简单文本是绝佳的沟通方法。考虑只用数字本身(尽可能突出)和一些辅助性文字来清晰地阐述观点。例如：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png\" alt=\"Original Report\"></p>\n<blockquote>\n<p>上图用了相当多的文字和空间衬托仅仅两项数据。图表本身对数据的解读并没有多少帮助。</p>\n</blockquote>\n<p>修改后：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png\" alt=\"Revised Report\"></p>\n<h3 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h3><p>使用表格时需要记住一点：让设计融入背景。<strong>让数据占据核心地位</strong>。不要让厚重的边框和阴影与数据争夺受众的注意力。要用窄边框或者空白来区分表格的元素。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png\" alt=\"Tables\"></p>\n<p>也可以使用 <strong>热力图</strong> 辅助表格，这会使得极值更容易被观众捕捉：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png\" alt=\"Heatmap\"></p>\n<h3 id=\"需要避开的陷阱\"><a href=\"#需要避开的陷阱\" class=\"headerlink\" title=\"需要避开的陷阱\"></a>需要避开的陷阱</h3><ul>\n<li>不要使用饼图</li>\n<li>不要使用3D效果图</li>\n<li>不要使用双y轴的图，而是应该将它们分开成两个单独的图<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png\" alt=\"Double Y-axis\"><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png\" alt=\"Revised Double Y-axis\"></li>\n</ul>\n<h2 id=\"去除杂乱\"><a href=\"#去除杂乱\" class=\"headerlink\" title=\"去除杂乱\"></a>去除杂乱</h2><ul>\n<li>巧用留白</li>\n<li>用对比(颜色、字号)突出要表达的元素</li>\n<li>对齐使得页面更整洁</li>\n<li>Less is More:<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png\" alt=\"Less is More\"></li>\n</ul>\n<h2 id=\"聚焦观众视线\"><a href=\"#聚焦观众视线\" class=\"headerlink\" title=\"聚焦观众视线\"></a>聚焦观众视线</h2><h3 id=\"文字中的前注意属性\"><a href=\"#文字中的前注意属性\" class=\"headerlink\" title=\"文字中的前注意属性\"></a>文字中的前注意属性</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png\" alt=\"Preattentive Attributes\"></p>\n<h3 id=\"图表中的前注意属性\"><a href=\"#图表中的前注意属性\" class=\"headerlink\" title=\"图表中的前注意属性\"></a>图表中的前注意属性</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png\" alt=\"Without Preattentive Attributes in Graph\"></p>\n<p>加入强调之后：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png\" alt=\"With Preattentive Attributes in Graph\"></p>\n<h3 id=\"颜色\"><a href=\"#颜色\" class=\"headerlink\" title=\"颜色\"></a>颜色</h3><p>使用少量的颜色。通常选择灰色做背景，再挑选一个大胆的颜色（例如蓝色）来吸引注意。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png\" alt=\"Color\"></p>\n<h3 id=\"避免”意大利面”式的图表策略\"><a href=\"#避免”意大利面”式的图表策略\" class=\"headerlink\" title=\"避免”意大利面”式的图表策略\"></a>避免”意大利面”式的图表策略</h3><p>下图的折线图太过于杂乱，观众无法从中获取有用的信息。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png\" alt=\"Spaghetti Graph\"></p>\n<p>对这类图表的改进可利用 <strong>前注意属性一次强调一根线条</strong>。然后从空间上隔离这些线条的图表：</p>\n<h4 id=\"一次只强调一根线-颜色-线条粗细\"><a href=\"#一次只强调一根线-颜色-线条粗细\" class=\"headerlink\" title=\"一次只强调一根线 (颜色 + 线条粗细)\"></a>一次只强调一根线 (颜色 + 线条粗细)</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png\" alt=\"One Line Highlighted\"></p>\n<h4 id=\"空间隔离\"><a href=\"#空间隔离\" class=\"headerlink\" title=\"空间隔离\"></a>空间隔离</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png\" alt=\"Vertically Apart\"></p>\n<h4 id=\"混合方法-空间上分离-一次只强调一根线条\"><a href=\"#混合方法-空间上分离-一次只强调一根线条\" class=\"headerlink\" title=\"混合方法 (空间上分离 + 一次只强调一根线条)\"></a>混合方法 (空间上分离 + 一次只强调一根线条)</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png\" alt=\"Combined Approach\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png\" alt=\"Combined Approach\"></p>\n<h3 id=\"饼图的替代方案\"><a href=\"#饼图的替代方案\" class=\"headerlink\" title=\"饼图的替代方案\"></a>饼图的替代方案</h3><p>未处理前的饼图：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png\" alt=\"Pie Chart\"></p>\n<h4 id=\"方案1-直观展示数字\"><a href=\"#方案1-直观展示数字\" class=\"headerlink\" title=\"方案1: 直观展示数字\"></a>方案1: 直观展示数字</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png\" alt=\"Show Numbers Directly\"></p>\n<h4 id=\"方案2-简单条形图\"><a href=\"#方案2-简单条形图\" class=\"headerlink\" title=\"方案2: 简单条形图\"></a>方案2: 简单条形图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png\" alt=\"Simple Bar Graph\"></p>\n<h4 id=\"方案3-水平堆叠条形图\"><a href=\"#方案3-水平堆叠条形图\" class=\"headerlink\" title=\"方案3: 水平堆叠条形图\"></a>方案3: 水平堆叠条形图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png\" alt=\"Stacked Horizontal Bar Graph\"></p>\n<h4 id=\"方案4-斜率图\"><a href=\"#方案4-斜率图\" class=\"headerlink\" title=\"方案4: 斜率图\"></a>方案4: 斜率图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png\" alt=\"Slope Graph\"></p>\n<blockquote>\n<p>通过线条的斜率很容易看出项目前后每个类别百分比的 <strong>变化</strong>，易于 <strong>对比来突出项目效果</strong>。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>数据可视化(Data Visualization)是Data Science领域一个非常非常核心的内容，很多时候，我们往往会花很多力气去建模分析数据，然而最终给你的老板汇报，或者是编写分析报告的时候，通常会以图形化的方式展现。这个时候，若你能够 <strong>利用数据讲故事</strong> ，那么你的汇报就会十分精彩。本文内容来自一本我个人非常喜欢的书，作者是Google工作多年、数据可视化领域的专家。若你也对数据可视化感兴趣，欢迎去阅读原著：《<a href=\"http://www.storytellingwithdata.com/book/\" target=\"_blank\" rel=\"noopener\">Storytelling With Data</a>》。</p>\n<h2 id=\"选择有效的图表\"><a href=\"#选择有效的图表\" class=\"headerlink\" title=\"选择有效的图表\"></a>选择有效的图表</h2><h3 id=\"简单文本\"><a href=\"#简单文本\" class=\"headerlink\" title=\"简单文本\"></a>简单文本</h3><p>当只有一两项数据需要分享时，简单文本是绝佳的沟通方法。考虑只用数字本身(尽可能突出)和一些辅助性文字来清晰地阐述观点。例如：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_original.png\" alt=\"Original Report\"></p>\n<blockquote>\n<p>上图用了相当多的文字和空间衬托仅仅两项数据。图表本身对数据的解读并没有多少帮助。</p>\n</blockquote>\n<p>修改后：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/report_revised.png\" alt=\"Revised Report\"></p>\n<h3 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h3><p>使用表格时需要记住一点：让设计融入背景。<strong>让数据占据核心地位</strong>。不要让厚重的边框和阴影与数据争夺受众的注意力。要用窄边框或者空白来区分表格的元素。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/tables.png\" alt=\"Tables\"></p>\n<p>也可以使用 <strong>热力图</strong> 辅助表格，这会使得极值更容易被观众捕捉：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/heatmap.png\" alt=\"Heatmap\"></p>\n<h3 id=\"需要避开的陷阱\"><a href=\"#需要避开的陷阱\" class=\"headerlink\" title=\"需要避开的陷阱\"></a>需要避开的陷阱</h3><ul>\n<li>不要使用饼图</li>\n<li>不要使用3D效果图</li>\n<li>不要使用双y轴的图，而是应该将它们分开成两个单独的图<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-before.png\" alt=\"Double Y-axis\"><br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/double-y-revised.png\" alt=\"Revised Double Y-axis\"></li>\n</ul>\n<h2 id=\"去除杂乱\"><a href=\"#去除杂乱\" class=\"headerlink\" title=\"去除杂乱\"></a>去除杂乱</h2><ul>\n<li>巧用留白</li>\n<li>用对比(颜色、字号)突出要表达的元素</li>\n<li>对齐使得页面更整洁</li>\n<li>Less is More:<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/less-is-more.png\" alt=\"Less is More\"></li>\n</ul>\n<h2 id=\"聚焦观众视线\"><a href=\"#聚焦观众视线\" class=\"headerlink\" title=\"聚焦观众视线\"></a>聚焦观众视线</h2><h3 id=\"文字中的前注意属性\"><a href=\"#文字中的前注意属性\" class=\"headerlink\" title=\"文字中的前注意属性\"></a>文字中的前注意属性</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/preatten-attr.png\" alt=\"Preattentive Attributes\"></p>\n<h3 id=\"图表中的前注意属性\"><a href=\"#图表中的前注意属性\" class=\"headerlink\" title=\"图表中的前注意属性\"></a>图表中的前注意属性</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/no-attr.png\" alt=\"Without Preattentive Attributes in Graph\"></p>\n<p>加入强调之后：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/with-attr.png\" alt=\"With Preattentive Attributes in Graph\"></p>\n<h3 id=\"颜色\"><a href=\"#颜色\" class=\"headerlink\" title=\"颜色\"></a>颜色</h3><p>使用少量的颜色。通常选择灰色做背景，再挑选一个大胆的颜色（例如蓝色）来吸引注意。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/color.png\" alt=\"Color\"></p>\n<h3 id=\"避免”意大利面”式的图表策略\"><a href=\"#避免”意大利面”式的图表策略\" class=\"headerlink\" title=\"避免”意大利面”式的图表策略\"></a>避免”意大利面”式的图表策略</h3><p>下图的折线图太过于杂乱，观众无法从中获取有用的信息。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/spaghetti-graph.png\" alt=\"Spaghetti Graph\"></p>\n<p>对这类图表的改进可利用 <strong>前注意属性一次强调一根线条</strong>。然后从空间上隔离这些线条的图表：</p>\n<h4 id=\"一次只强调一根线-颜色-线条粗细\"><a href=\"#一次只强调一根线-颜色-线条粗细\" class=\"headerlink\" title=\"一次只强调一根线 (颜色 + 线条粗细)\"></a>一次只强调一根线 (颜色 + 线条粗细)</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/one-line-highlighted.png\" alt=\"One Line Highlighted\"></p>\n<h4 id=\"空间隔离\"><a href=\"#空间隔离\" class=\"headerlink\" title=\"空间隔离\"></a>空间隔离</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/v-apart.png\" alt=\"Vertically Apart\"></p>\n<h4 id=\"混合方法-空间上分离-一次只强调一根线条\"><a href=\"#混合方法-空间上分离-一次只强调一根线条\" class=\"headerlink\" title=\"混合方法 (空间上分离 + 一次只强调一根线条)\"></a>混合方法 (空间上分离 + 一次只强调一根线条)</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-v.png\" alt=\"Combined Approach\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/combined-approach-h.png\" alt=\"Combined Approach\"></p>\n<h3 id=\"饼图的替代方案\"><a href=\"#饼图的替代方案\" class=\"headerlink\" title=\"饼图的替代方案\"></a>饼图的替代方案</h3><p>未处理前的饼图：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/pie-chart.png\" alt=\"Pie Chart\"></p>\n<h4 id=\"方案1-直观展示数字\"><a href=\"#方案1-直观展示数字\" class=\"headerlink\" title=\"方案1: 直观展示数字\"></a>方案1: 直观展示数字</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/numbers.png\" alt=\"Show Numbers Directly\"></p>\n<h4 id=\"方案2-简单条形图\"><a href=\"#方案2-简单条形图\" class=\"headerlink\" title=\"方案2: 简单条形图\"></a>方案2: 简单条形图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/bar.png\" alt=\"Simple Bar Graph\"></p>\n<h4 id=\"方案3-水平堆叠条形图\"><a href=\"#方案3-水平堆叠条形图\" class=\"headerlink\" title=\"方案3: 水平堆叠条形图\"></a>方案3: 水平堆叠条形图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/h-bar.png\" alt=\"Stacked Horizontal Bar Graph\"></p>\n<h4 id=\"方案4-斜率图\"><a href=\"#方案4-斜率图\" class=\"headerlink\" title=\"方案4: 斜率图\"></a>方案4: 斜率图</h4><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/book-storytelling-with-data/slope.png\" alt=\"Slope Graph\"></p>\n<blockquote>\n<p>通过线条的斜率很容易看出项目前后每个类别百分比的 <strong>变化</strong>，易于 <strong>对比来突出项目效果</strong>。</p>\n</blockquote>\n"},{"title":"[CV] Face Anti-Spoofing","date":"2018-10-29T16:23:26.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Face Anti-Spoofing"],"_content":"## Introduction\nFace Anti-spoofing，即人脸活体检测，随着iPhoneX FaceID的应用，人脸解锁得到了越来越多的关注，而anti-spoofing无疑是整个人脸解锁环节中非常重要的一环。试想一下，如果连真假脸都区分不出，那安全性无疑是会大打折扣。Face Anti-spoofing在近些年的顶会上也有相关的文献发表。但和众多research benchmark存在的问题一样，目前得dataset capacity太小了，往往很多时候各种model都是在相关数据集上overfitting，更无从谈起实际应用场景了。工业界，因数据量级很大，很多时候也是将其视为一个传统的Binary Classification问题，而根据我们组的模型上线情况反馈来看，Precision和Recall一般都可以到达99.9%+，所以工业界很多时候都是一个数据问题，而非模型和Loss问题。\n\n目前主流的Attack方式有以下3种：\n* print attack: 即打印人脸照片攻击\n* replay attack: 即播放视频攻击\n* 3D mask attack: 即带上3D面具进行攻击\n\n目前主流的Anti-spoofing方法主要有以下几种：\n* Image Quality Analysis: 这个很容易理解，因为大多数攻击照片都是拍摄屏幕获得，所以往往会存在一些颜色失真、反光、模糊、形变(recapture时不同角度造成的)、moire pattern (可以由LBP descriptor表示)、边框等，所以这些pattern是很容易被deep models学到，且泛化能力也都不错。有Paper[1]表明，从R Channel中提取的特征比G、B、GrayScale表示能力要更强。\n* Command Motion: 就是根据系统发出的指令，用户根据指令进行“眨眼”、“点头”、“转向”、“念一段文字”、“做出某个指定表情”等等来验证活体。\n* 3D Depth Information: 真假脸最显著的区别就是活体是立体的，而print/replay attack往往是2D的，所以很容易通过3D深度信息进行区分。此外，在设置Reject Option的时候，也要考虑拍摄距离，不能太远，也不能太近。\n\n\n本文旨在对CVPR/ECCV/TIP/TIFS等顶会/顶刊Paper的idea做一下梳理。\n\n## Reference\n1. Patel K, Han H, Jain A K. [Secure face unlock: Spoof detection on smartphones](http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf)[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.\n2. ","source":"_posts/cv-antispoofing.md","raw":"---\ntitle: \"[CV] Face Anti-Spoofing\"\ndate: 2018-10-30 00:23:26\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Anti-Spoofing\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Anti-Spoofing\n---\n## Introduction\nFace Anti-spoofing，即人脸活体检测，随着iPhoneX FaceID的应用，人脸解锁得到了越来越多的关注，而anti-spoofing无疑是整个人脸解锁环节中非常重要的一环。试想一下，如果连真假脸都区分不出，那安全性无疑是会大打折扣。Face Anti-spoofing在近些年的顶会上也有相关的文献发表。但和众多research benchmark存在的问题一样，目前得dataset capacity太小了，往往很多时候各种model都是在相关数据集上overfitting，更无从谈起实际应用场景了。工业界，因数据量级很大，很多时候也是将其视为一个传统的Binary Classification问题，而根据我们组的模型上线情况反馈来看，Precision和Recall一般都可以到达99.9%+，所以工业界很多时候都是一个数据问题，而非模型和Loss问题。\n\n目前主流的Attack方式有以下3种：\n* print attack: 即打印人脸照片攻击\n* replay attack: 即播放视频攻击\n* 3D mask attack: 即带上3D面具进行攻击\n\n目前主流的Anti-spoofing方法主要有以下几种：\n* Image Quality Analysis: 这个很容易理解，因为大多数攻击照片都是拍摄屏幕获得，所以往往会存在一些颜色失真、反光、模糊、形变(recapture时不同角度造成的)、moire pattern (可以由LBP descriptor表示)、边框等，所以这些pattern是很容易被deep models学到，且泛化能力也都不错。有Paper[1]表明，从R Channel中提取的特征比G、B、GrayScale表示能力要更强。\n* Command Motion: 就是根据系统发出的指令，用户根据指令进行“眨眼”、“点头”、“转向”、“念一段文字”、“做出某个指定表情”等等来验证活体。\n* 3D Depth Information: 真假脸最显著的区别就是活体是立体的，而print/replay attack往往是2D的，所以很容易通过3D深度信息进行区分。此外，在设置Reject Option的时候，也要考虑拍摄距离，不能太远，也不能太近。\n\n\n本文旨在对CVPR/ECCV/TIP/TIFS等顶会/顶刊Paper的idea做一下梳理。\n\n## Reference\n1. Patel K, Han H, Jain A K. [Secure face unlock: Spoof detection on smartphones](http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf)[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.\n2. ","slug":"cv-antispoofing","published":1,"updated":"2018-10-29T16:25:25.350Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bo0005608wvuyyafyt","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Face Anti-spoofing，即人脸活体检测，随着iPhoneX FaceID的应用，人脸解锁得到了越来越多的关注，而anti-spoofing无疑是整个人脸解锁环节中非常重要的一环。试想一下，如果连真假脸都区分不出，那安全性无疑是会大打折扣。Face Anti-spoofing在近些年的顶会上也有相关的文献发表。但和众多research benchmark存在的问题一样，目前得dataset capacity太小了，往往很多时候各种model都是在相关数据集上overfitting，更无从谈起实际应用场景了。工业界，因数据量级很大，很多时候也是将其视为一个传统的Binary Classification问题，而根据我们组的模型上线情况反馈来看，Precision和Recall一般都可以到达99.9%+，所以工业界很多时候都是一个数据问题，而非模型和Loss问题。</p>\n<p>目前主流的Attack方式有以下3种：</p>\n<ul>\n<li>print attack: 即打印人脸照片攻击</li>\n<li>replay attack: 即播放视频攻击</li>\n<li>3D mask attack: 即带上3D面具进行攻击</li>\n</ul>\n<p>目前主流的Anti-spoofing方法主要有以下几种：</p>\n<ul>\n<li>Image Quality Analysis: 这个很容易理解，因为大多数攻击照片都是拍摄屏幕获得，所以往往会存在一些颜色失真、反光、模糊、形变(recapture时不同角度造成的)、moire pattern (可以由LBP descriptor表示)、边框等，所以这些pattern是很容易被deep models学到，且泛化能力也都不错。有Paper[1]表明，从R Channel中提取的特征比G、B、GrayScale表示能力要更强。</li>\n<li>Command Motion: 就是根据系统发出的指令，用户根据指令进行“眨眼”、“点头”、“转向”、“念一段文字”、“做出某个指定表情”等等来验证活体。</li>\n<li>3D Depth Information: 真假脸最显著的区别就是活体是立体的，而print/replay attack往往是2D的，所以很容易通过3D深度信息进行区分。此外，在设置Reject Option的时候，也要考虑拍摄距离，不能太远，也不能太近。</li>\n</ul>\n<p>本文旨在对CVPR/ECCV/TIP/TIFS等顶会/顶刊Paper的idea做一下梳理。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Patel K, Han H, Jain A K. <a href=\"http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf\" target=\"_blank\" rel=\"noopener\">Secure face unlock: Spoof detection on smartphones</a>[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.</li>\n<li></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Face Anti-spoofing，即人脸活体检测，随着iPhoneX FaceID的应用，人脸解锁得到了越来越多的关注，而anti-spoofing无疑是整个人脸解锁环节中非常重要的一环。试想一下，如果连真假脸都区分不出，那安全性无疑是会大打折扣。Face Anti-spoofing在近些年的顶会上也有相关的文献发表。但和众多research benchmark存在的问题一样，目前得dataset capacity太小了，往往很多时候各种model都是在相关数据集上overfitting，更无从谈起实际应用场景了。工业界，因数据量级很大，很多时候也是将其视为一个传统的Binary Classification问题，而根据我们组的模型上线情况反馈来看，Precision和Recall一般都可以到达99.9%+，所以工业界很多时候都是一个数据问题，而非模型和Loss问题。</p>\n<p>目前主流的Attack方式有以下3种：</p>\n<ul>\n<li>print attack: 即打印人脸照片攻击</li>\n<li>replay attack: 即播放视频攻击</li>\n<li>3D mask attack: 即带上3D面具进行攻击</li>\n</ul>\n<p>目前主流的Anti-spoofing方法主要有以下几种：</p>\n<ul>\n<li>Image Quality Analysis: 这个很容易理解，因为大多数攻击照片都是拍摄屏幕获得，所以往往会存在一些颜色失真、反光、模糊、形变(recapture时不同角度造成的)、moire pattern (可以由LBP descriptor表示)、边框等，所以这些pattern是很容易被deep models学到，且泛化能力也都不错。有Paper[1]表明，从R Channel中提取的特征比G、B、GrayScale表示能力要更强。</li>\n<li>Command Motion: 就是根据系统发出的指令，用户根据指令进行“眨眼”、“点头”、“转向”、“念一段文字”、“做出某个指定表情”等等来验证活体。</li>\n<li>3D Depth Information: 真假脸最显著的区别就是活体是立体的，而print/replay attack往往是2D的，所以很容易通过3D深度信息进行区分。此外，在设置Reject Option的时候，也要考虑拍摄距离，不能太远，也不能太近。</li>\n</ul>\n<p>本文旨在对CVPR/ECCV/TIP/TIFS等顶会/顶刊Paper的idea做一下梳理。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Patel K, Han H, Jain A K. <a href=\"http://www.jdl.ac.cn/doc/2011/201711222512198092_hanhu-journal.pdf\" target=\"_blank\" rel=\"noopener\">Secure face unlock: Spoof detection on smartphones</a>[J]. IEEE transactions on information forensics and security, 2016, 11(10): 2268-2283.</li>\n<li></li>\n</ol>\n"},{"title":"[CV] Object Detection","date":"2018-11-11T13:07:05.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Object Detection"],"_content":"## Introduction\nObject Detection是Computer Vision领域一个非常火热的研究方向。并且在工业界也有着十分广泛的应用(例如人脸检测、无人驾驶的行人/车辆检测等等)。本质旨在梳理RCNN--SPPNet--Fast RCNN--Faster RCNN--FCN--Mask RCNN，YOLO v1/2/3, SSD等Object Detection这些非常经典的工作。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n## RCNN (Region-based CNN)\n> Paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)\n\n### What is RCNN?\n这篇Paper可以看做是Deep Learning在Object Detection大获成功的开端，对Machine Learning/Pattern Recognition熟悉的读者应该都知道，**Feature Matters in approximately every task!** 而RCNN性能提升最大的因素之一便是很好地利用了CNN提取的Feature，而不是像先前的detector那样使用手工设计的feature(例如SIFT/LBP/HOG等)。\n\nRCNN可以认为是Regions with CNN features，即(1)先利用[Selective Search算法](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)生成大约2000个Region Proposal，(2)Pretrained CNN从这些Region Proposal中提取deep feature(from pool5)，(3)然后再利用linear SVM进行one-VS-rest分类。从而将Object Detection问题转化为一个Classification问题，对于Selective Search框选不准的bbox，后面使用<font color=\"orange\">Bounding Box Regression</font>(下面会详细介绍)进行校准。这便是RCNN的主要idea。\n\n![RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png)\n\n### Details of RCNN\n#### Pretraining and Fine-tuning\nRCNN先将Base Network在ImageNet上train一个1000 class的classifier，然后将output neuron设置为21(20个Foreground + 1个Background)在target datasets上fine-tune。其中，将由Selective Search生成的Region Proposal与groundtruth Bbox的$IOU\\geq 0.5$的定义为positive sample，其他的定义为negative sample。\n\n作者在实验中发现，pool5 features learned from ImageNet足够general，并且在domain-specific datasets上学习non-linear classifier可以获得非常大的性能提升。\n\n#### Bounding Box Regression\n输入是$N$个training pairs，$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$代表$P^i$像素点的$(x,y)$坐标点、width和height。$G=(G_x,G_y,G_w,G_h)$代表groundtruth bbox。BBox Regression的目的就是为了学习一种mapping使得proposed box $P$ 映射到 groundtruth box $G$。\n\n将$x,y$的transformation设为$d_x(P),d_y(P)$，属于<font color=\"red\">scale-invariant translation。$w,h$是log-space translation</font>。学习完成后，可将input proposal转换为predicted groundtruth box $\\hat{G}$:\n$$\n\\hat{G}_x=P_w d_x(P)+P_x\n$$\n\n$$\n\\hat{G}_y=P_h d_x(P)+P_y\n$$\n\n$$\n\\hat{G}_w=P_w exp(d_w(P))\n$$\n\n$$\n\\hat{G}_h=P_h exp(d_h(P))\n$$\n\n每个$d_{\\star}(P)$都用一个线性函数来进行建模，使用$pool_5$ feature，权重的学习则使用OLS优化即可：\n$$\nw_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2\n$$\n\nThe regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:\n$$\nt_x=\\frac{G_x-P_x}{P_w}\n$$\n\n$$\nt_y=\\frac{G_y-P_y}{P_h}\n$$\n\n$$\nt_w=log(\\frac{G_w}{P_w})\n$$\n\n$$\nt_h=log(\\frac{G_h}{P_h})\n$$\n\n在选取Proposed Bbox的时候，我们只选取离Groundtruth Bbox比较近的($IOU\\geq 0.6$)来做Bounding Box Regression。\n\n以上就是Deep Learning在Object Detection领域一个开创性的工作--RCNN。若有疑问，欢迎给我留言！\n\n\n## SPPNet\n> Paper: [Spatial pyramid pooling in deep convolutional networks for visual recognition.](https://arxiv.org/pdf/1406.4729v4.pdf)\n\n### What is SPPNet?\nSPPNet(Spatial Pyramid Pooling)是基于RCNN进行改进的一个Object Detection算法。介绍SPPNet之前，我们不妨先来看一下RCNN有什么问题？RCNN，即Region-based CNN，它需要CNN作为base network去做特征提取，而传统CNN需要固定的squared input，而为了满足这个条件，就需要手工地对原图进行裁剪、变形等操作，而这样势必会丢失信息。作者意识到这种现象的原因不在于卷积层，而在于FC Layers需要固定的输入尺寸，因此通过在feature map的SSPlayer可以满足对多尺度的feature map裁剪，从而concatenate得到固定尺寸的特征输入。取得了很好的效果，在detection任务上，region proposal直接在feature map上生成，而不是在原图上生成，因此可以仅仅通过一次特征提取，而不需要像RCNN那样提取2000次(2000个 Region Proposal)，这大大加速了检测效率。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：现如今的Deep Architecture比较多采用Fully Convolutional Architecture(全卷积结构)，而不含Fully Connected Layers，在最后做分类或回归任务时，采用Global Average Pooling即可。\n\n![Crop/Warp VS SPP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg)\n\n\n### Why SPPNet?\nSPPNet究竟有什么过人之处得到了Kaiming He大神的赏识呢？\n1. SPP可以在不顾input size的情况下获取fixed size output，这是sliding window做不到的。\n2. SPP uses multi-level spatial bins，而sliding window仅仅使用single window size。<font color=\"red\">multi-level pooling对object deformation则十分地robust</font>。\n3. SPP can pool features extracted at variable scales thanks to the flexibility of input scales.\n4. Training with variable-size images increases scale-invariance and reduces over-fitting.\n\n### Details of SPPNet\n#### SPP Layer\nSPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.\n\n![SPP Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg)\n\n这样，通过不同spatial bin pooling得到的k个 M-dimensional feature concatenation，我们就可以得到fixed length的feature vector了，接下来是不是就可以愉快地用FC Layers/SVM等ML算法train了？\n\n#### SPP for Detection\nRCNN需要从2K个Region Proposal中feedforwad Pretrained CNN去提取特征，这显然是非常低效的。SPPNet直接将整张image(possible multi-scale))作为输入，这样就可以只feedforwad一次CNN。然后在<font color=\"red\">feature map层面</font>获取candidate window，SPP Layer pool到fixed-length feature representation of the window。\n\n![Pooling](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg)\n\nRegion Proposal生成阶段和RCNN比较相似，依然是[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)生成2000个bbox candidate，然后将原始image resize使得$min(w, h)=s$，文中采用 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the features。对于每个window，该Pooling操作得到一个12800-Dimensional (256×50) 的向量。这个向量作为FC Layers的输入，然后和RCNN一样训练linear SVM去做分类。\n\n训练SPP Detector时，正负样本的采样是基于groundtruth bbox为基准，$IOU\\geq 0.3$为positive sample，反之为negative sample。\n\n\n## Fast RCNN\n> Paper: [Fast RCNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)\n\nFast RCNN是Object Detection领域一个非常经典的算法。它的novelty在于引入了两个branch来做multi-task learning(category classification和bbox regression)。\n\n### Why Fast RCNN?\n按照惯例，我们不妨先来看一看之前的算法(RCNN/SPPNet)有什么缺点？\n1. 它们(RCNN/SPP)的训练都属于multi-stage pipeline，即先要利用Selective Search生成2K个Region Proposal，然后用log loss去fine-tune一个deep CNN，用Deep CNN抽取的feature去拟合linear SVM，最后再去做Bounding Box Regression。\n2. 训练很费时，CNN需要从每一个Region Proposal抽取deep feature来拟合linear SVM。\n3. testing的时候慢啊，还是太慢了。因为需要将Deep CNN抽取的feature先缓存到磁盘，再读取feature来拟合linear SVM，你说麻烦不麻烦。\n\n那我们再来看看Fast RCNN为什么优秀？\n1. 设计了一个multi-task loss，来同时优化object classification和bounding box regression。\n2. Training is single stage.\n3. Higher performance than RCNN and SPPNet.\n\n### Details of Fast RCNN\n![Fast RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg)\n\nFast RCNN pipeline如上图所示：它将whole image with several object proposals作为输入，CNN抽取feature，对于每一个object proposal，<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>，然后将走过RoI Pooling Layer的feature vector输送到随后的multi-branch，一同做classification和bbox regression。\n\n可以看到，Fast RCNN模型里面一个非常重要的组件叫做<font color=\"red\">RoI Pooling</font>，那么接下来我们就来细细分析一下RoI Pooling究竟是何方神圣。\n\n#### The RoI pooling layer\nFast RCNN原文里是这样说的：\n> RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.\n> \n> In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.\n>\n> RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].\n\n什么意思呢？就是在任何valid region proposal里面，把某层的feature map划分成多个小方块，每个小方块做max pooling，这样就得到了尺寸更小的feature map。\n\n#### Fine-tuning for detection\n##### Multi-Task Loss\n之前也说过，Fast RCNN同时做了$K+1$ (K个object class + 1个background) 类的classification($p=(p_0,p_1,\\cdots,p_K)$)和bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)。\n\nWe use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(对linear regression熟悉的读者不妨思考一下为什么要对width和height做log). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:\n$$\nL(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)\n$$\n$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.\n\n我们再来看看Loss Function的第二部分(即regression loss)，$[u\\geq 1]$代表只有满足$u\\geq 1$时这个式子才为1，否则为0。在我们的setting中，background的$[u\\geq 1]$自然而然就设为0啦。我们接着分析regression loss，既然是regression，惯常的手法是使用MSE Loss对不对？但是MSE Loss属于Cost-sensitive Loss啊，对outliers非常的敏感，因此Ross大神使用了更加柔和的$Smooth L_1 Loss$。\n$$\nL_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)\n$$\n\nSmooth L1 Loss写得详细一点呢，就是这样的：\n$$\nsmooth_{L_1}(x)=\n\\begin{cases}\n0.5x^2 & if |x|<1\\\\\n|x|-0.5 & otherwise\n\\end{cases}\n$$\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：想详细了解Machine Learning中的Loss，请参考我的[另一篇文章](https://lucasxlu.github.io/blog/2018/07/24/ml-loss/)。\n\n##### Mini-batch sampling\n在Fine-tuning阶段，每个mini-batch随机采样自$N=2$类image，每一类都是64个sample，与groundtruth bbox $IOU\\geq 0.5$的设为foreground samples[$u=1$]，反之为background samples[$u=0$]。\n\n### Fast R-CNN detection\n因为Fully Connected Layers的计算太费时，而FC Layers的计算显然就是大矩阵相乘，因此很容易联想到用truncated SVD来进行加速。\n\nIn this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:\n$$\nW\\approx U\\Sigma_t V^T\n$$\n$U$是由$W$前$t$个left-singular vectors组成的$u\\times t$矩阵，$\\Sigma_t$是包含$W$矩阵前$t$个singular value的$t\\times t$对角矩阵，$V$是由$W$前$t$个right-singular vectors组成的$v\\times t$矩阵。Truncated SVD可以将参数从$uv$降到$t(u+v)$。\n\n这里值得一提的有两点：\n1. $conv_1$ layers feature map通常都足够地general，并且task-independent。因此可以直接用来抽feature就行。\n2. Region Proposal的数量对Detection的性能并没有什么太大影响 (关键还是看feature啊！以及sampling mini-batch的时候正负样本的不均衡问题，详情请参阅kaiming He大神的[Focal Loss](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf))。\n\n\n## Faster RCNN\n> Paper: [Faster r-cnn: Towards real-time object detection with region proposal networks](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n\nFaster RCNN也是Object Detection领域里一个非常具有代表性的工作，一个最大的改进就是Region Proposal Network(RPN)，RPN究竟神奇在什么地方呢？我们先来回顾一下RCNN--SPP--Fast RCNN，这些都属于two-stage detector，什么意思呢？就是先利用<font color=\"red\">Selective Search</font>生成2000个Region Proposal，然后再将其转化为一个机器学习中的分类问题。而<font color=\"red\">Selective Search</font>实际上是非常低效的，RPN则很好地完善了这一点，即直接从整个Network Architecture里生成Region Proposal。RPN是一种全卷积网络，它可以同时预测object bounds，以及objectness score。因为RPN的Feature是和Detection Network共享的，所以整个Region Proposal的生成几乎是cost-free的。所以，这也就是Faster RCNN中**Faster**一词的由来。\n\n\n### What is Faster RCNN?\n$$\nFaster RCNN = Fast RCNN + RPN\n$$\n按照惯例，一个算法的提出显然是为了解决之前算法的不足。那之前的算法都有什么问题呢？\n如果对之前的detector熟悉的话，shared features between proposals已经被解决，但是<font color=\"red\">Region Proposal的生成变成了最大的计算瓶颈</font>。这便是RPN产生的缘由。\n\n作者注意到，conv feature maps used by region-based detectors也可以被用于生成region proposals。在这些conv features顶端，<font color=\"red\">通过添加两个额外的卷积层来构造RPN：一个conv layer用于encode每个conv feature map position到一个低维向量(256-d)；另一个conv layer在每一个conv feature map position中输出k个region proposal with various scales and aspect ratios的objectness score和regression bounds。</font>下面重点介绍一下RPN。\n\n### Region Proposal Network\nRPN是一个全卷积网络，可以接受任意尺寸的image作为输入，并且输出一系列object proposals以及其对应的objectness score。那么RPN是如何生成region proposals的呢？\n\n首先，在最后一个shared conv feature map上slide一个小网络，这个小网络全连接到input conv feature map上$n\\times n$的spatial window。而每一个sliding window映射到一个256-d的feature vector，这个feature vector输入到两个fully connected layers--一个做box regression，另一个做box classification。值得注意的是，因为这个小网络是以sliding window的方式操作的，所以fully-connected layers在所有的spatial locations都是共享的。该结构由一个$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers组成。\n\n#### Translation-Invariant Anchors\n对于每个sliding window location，同时预测$k$个region proposals，所以regression layer有$4k$个encoding了$k$个bbox坐标的outputs。classification layer有$2k$个scores(每个region proposal估计object/non-object的概率)。\n\n> The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n\n文章中采用3个scales和3个aspect ratios，所以每个sliding position一共得到$k=9$个anchors。所以对于每个$W\\times H$的conv feature map一共产生$WHk$个anchor。这种方法一个很重要的性质就是**它是translation invariant**。\n\n#### A Loss Function for Learning Region Proposals\n训练RPN时，我们为每一个anchor分配binary class(即是不是一个object)。我们为以下这两类anchors分配positive label:\n1. 与groundtruth bbox有最高IoU的anchor\n2. 与任意groundtruth bbox $IoU\\geq 0.7$的anchor\n\n同时，可能出现一个groundtruth bbox分配到多个anchor的情况，我们将与所有groundtruth bbox $IoU\\leq 0.3$的anchor设为negative anchor。而那些既不是positive又不是negative的anchor则对training过程没有用处。然后，Faster RCNN的Loss Function就变成：\n$$\nL(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})\n$$\n$p_i$是anchor $i$ 被预测为是一个object的概率，若anchor为positive，则groundtruth label $p_i^{{\\star}}$为1；若anchor为negative则为0；$t_i$是包含4个预测bbox坐标点的向量，$t_i^{\\star}$是groundtruth positive anchor坐标点的向量。$L_{cls}$是二分类的Log Loss(object VS non-object)。对于regression loss，文章使用$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$，其中$R$是Smooth L1 Loss(和Fast RCNN中一样)。$p_i^{\\star} L_{reg}$表示<font color=\"red\">仅仅在positive anchor ($p_i^{\\star}=1$)时才被激活，否则($p_i^{\\star}=0$)不激活</font>。\n\nBounding Box Regression依旧是采用之前的pipeline：\n$$\nt_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})\n$$\n\n$$\nt_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})\n$$\n\n> In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. **To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights**. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.\n\n#### Sharing Convolutional Features for Region Proposal and Object Detection\n1. Fine-tune在ImageNet上Pretrain的RPN来完成region proposal task。\n2. 利用RPN生成的region proposal来train Fast RCNN。注意在这一步骤中RPN和Faster RCNN没有共享卷积层。\n3. 利用detector network来初始化RPN训练，但是我们fix shared conv layers，仅仅fine-tune单独属于RPN的层。注意在这一步骤中RPN和Fast RCNN共享了卷积层。\n4. Fix所有shared conv layers，fine-tune Fast RCNN的fc layers。至此，RPN和Fast RCNN共享卷积层，并且形成了一个unified network。\n\n\n## SSD\n> Paper: [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325v5.pdf)\n\nSSD是one-stage detector里一个非常著名的算法，那什么叫做one-stage和two-stage呢？回想一下，从DL Detector发展到现在，我们之前介绍的RCNN/SSP/Fast RCNN/Faster RCNN等，都是属于two-stage detectors，意思就是说**第一步需要生成region proposals，第二步再将整个detection转化为对这些region proposals的classification问题来做**。那所谓的one-stage detection就自然是不需要生成region proposals了，而是直接输出bbox了。Faster RCNN里面作者已经分析了，two-stage detection为啥慢？很大原因就是因为region proposal generation太慢了(例如Selective Search算法)，所以提出了RPN来辅助生成region proposals。\n\n### What is SSD?\nSSD最主要的改进就是**使用了一个小的Convolution Filter来预测object category和bbox offset**。那如何处理多尺度问题呢？SSD采取的策略是将这些conv filter应用到多个feature map上，来使得整个模型对Scale Invariant。\n\n![SSD Framework](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png)\n\n### Details of SSD\nSSD主要部件如下：一个DCNN用来提取feature，产生fixed-size的bbox以及这些bbox中每个类的presence score，然后NMS用来输出最后的检测结果。Feature Extraction部分和普通的分类DCNN没啥太大的区别，作者在后面新添加了新的结构：\n1. **Multi-scale feature maps for detection**: 在base feature extraction network之后额外添加新的conv layers(所以得到了multi-scale的feature maps)，来使得模型可以处理multi-scale的detection。\n2. **Convolutional predictors for detection**: 每一个新添加的feature layer可以基于```small conv filters```产生fixed-size detection predictions。\n3. **Default boxes and aspect ratios**: 对于每个feature map cell，算法给出cell中default box的relative offset，以及class-score(表示在每个box中一个class instance出现的概率)。具体的，对于每个given location的$k$个box，产生4个bbox offset和$c$个class score，这样就对每个```feature map location```上产生了$(c+4)k$个filters，那么对于一个$m\\times n$的```feature map```，则产生$(c+4)kmn$个output。这个做法和Faster RCNN中的anchor box有点类似，但是**SSD中将它用到了多个不同resolution的feature map上，因此多个feature map的不同default box shape使得我们可以很高效地给出output box shape**。\n\n![SSD and YOLO](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png)\n\n#### Training of SSD\n前面已经提到了，SSD会在default box周围生成一系列varied location/ratio/scale的boxes，那么到底哪一个box才是和groundtruth真正匹配的呢？作者采用了这样的一个Matching Strategy: 对于每一个groundtruth box，我们从default boxes中选择不同location/ratio/scale的boxes，然后计算它们和任意一个groundtruth box的Jaccard Overlap，并挑选出超过阈值的boxes。那么SSD最终的Loss就可以写成：\n$$\nL(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))\n$$\n其中$N$代表matched default boxes的数量，Localisation Loss和Faster RCNN一样也选择了Smooth L1。$x_{ij}^p=\\{0,1\\}$为第$i$个default box是否和第$j$个groundtruth box匹配的indicator。\n$$\nL_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)\n$$\n\n$$\n\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w\n$$\n\n$$\n\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h\n$$\n\n$$\n\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})\n$$\n\n$$\n\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})\n$$\n\nConfidence Loss采用Softmax Loss:\n$$\nL_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)\n$$\n其中，$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$\n\nSSD在检测large object时效果很好，但是在检测small object时则效果比较差，这是因为在higher layers，feature map包含的small object信息太少，可通过将input size由$300\\times 300$改为$512\\times 512$，**Zoom Data Augmentation**(即采用zoom in来生成large objects, zoom out来生成small objects)来进行一定程度的缓解。\n\n\n## Light-head RCNN\n> Paper: [Light-Head R-CNN: In Defense of Two-Stage Object Detector](https://arxiv.org/pdf/1711.07264v2.pdf)\n\n### Introduction\n在介绍Light-head RCNN之前，我们先来回顾一下常见的two-stage detector为什么是heavy-head？作者发现two-stage detector之所以慢，就是因为two-stage detector在RoI Warp前/后 会进行非常密集的计算，例如Faster RCNN包含2个fully connected layers做nRoI Recognition，RFCN会生成很大的score maps。所以无论你的backbone network使用了多么精巧的小网络结构，但是总体速度还是提升不上去。所以针对这个问题，作者在本文提出了```light-head``` RCNN。所谓的```light-head```，其实说白了就是```使用thin feature map + cheap RCNN subnet (pooling和单层fully connected layer)```。\n\n大家都知道，two-stage detector，其实是将detection问题转化为一个classification问题来完成的。也就是说，在第一个stage，模型会生成很多region proposal (此为```body```)，然后在第二个stage对这些region proposal进行分类 (此为```head```)。通常，two-stage detector的accuracy要比one-stage detector高的，所以为了accuracy，head往往会设计得非常heavy。Light-head RCNN是这么做的：\n> In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce \"thin\" feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.\n\n### Delve Into Light-Head RCNN\n#### RCNN Subnet\n> Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, **R-FCN first produces a set of score maps for each region, whose channel number will be $classes\\_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation**.\n\nFaster RCNN虽然在RoI Classification上表现得很好，但是它需要global average pooling来减小第一个fully connected layer的计算量，```而GAP会影响spatial localization```。此外，Faster RCNN对每一个RoI都要feedforward一遍RCNN subnet，所以在当proposal的数量很大时，效率就非常低了。\n\n#### Thin Feature Maps for RoI Warping\n在feed region proposal到RCNN subnet之前，用RoI warping来使得得到fixed shape的feature maps。本文提出的light-head产生了一系列```thin feature maps```，然后再接RoI Pooling层。在实验中，作者发现```RoI warping on thin feature maps```不仅仅提高了精度，而且节省了training和inference的时间。而且，如果直接应用RoI pooling到thin feature maps上，一方面模型可以减少计算量，另一方面可以去掉GAP来保留spatial information。\n\n![Light Head RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg)\n\n### Experiments\n作者在实验中发现，regression loss比classification loss要小很多，所以```将regression loss的权重进行double来balance multi-task training```。\n\n\n## YOLO v1\n> Paper: [You Only Look Once: Unified, Real-Time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n\n## Reference\n1. Girshick, Ross, et al. [\"Rich feature hierarchies for accurate object detection and semantic segmentation.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. He, Kaiming, et al. [\"Spatial pyramid pooling in deep convolutional networks for visual recognition.\"](https://arxiv.org/pdf/1406.4729v4.pdf) European conference on computer vision. Springer, Cham, 2014.\n3. Girshick, Ross. [\"Fast r-cnn.\"](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) Proceedings of the IEEE international conference on computer vision. 2015.\n4. Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollár. [\"Focal Loss for Dense Object Detection.\"](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)\n5. Ren, Shaoqing, et al. [\"Faster r-cnn: Towards real-time object detection with region proposal networks.\"](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) Advances in neural information processing systems. 2015.\n6. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016, October). [Ssd: Single shot multibox detector](https://arxiv.org/pdf/1512.02325v5.pdf). In European conference on computer vision (pp. 21-37). Springer, Cham.\n7. Redmon, Joseph, et al. [\"You only look once: Unified, real-time object detection.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n8. Li Z, Peng C, Yu G, et al. [Light-head r-cnn: In defense of two-stage object detector](https://arxiv.org/pdf/1711.07264v2.pdf)[J]. arXiv preprint arXiv:1711.07264, 2017.","source":"_posts/cv-detection.md","raw":"---\ntitle: \"[CV] Object Detection\"\ndate: 2018-11-11 21:07:05\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Object Detection\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Object Detection\n---\n## Introduction\nObject Detection是Computer Vision领域一个非常火热的研究方向。并且在工业界也有着十分广泛的应用(例如人脸检测、无人驾驶的行人/车辆检测等等)。本质旨在梳理RCNN--SPPNet--Fast RCNN--Faster RCNN--FCN--Mask RCNN，YOLO v1/2/3, SSD等Object Detection这些非常经典的工作。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n## RCNN (Region-based CNN)\n> Paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)\n\n### What is RCNN?\n这篇Paper可以看做是Deep Learning在Object Detection大获成功的开端，对Machine Learning/Pattern Recognition熟悉的读者应该都知道，**Feature Matters in approximately every task!** 而RCNN性能提升最大的因素之一便是很好地利用了CNN提取的Feature，而不是像先前的detector那样使用手工设计的feature(例如SIFT/LBP/HOG等)。\n\nRCNN可以认为是Regions with CNN features，即(1)先利用[Selective Search算法](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)生成大约2000个Region Proposal，(2)Pretrained CNN从这些Region Proposal中提取deep feature(from pool5)，(3)然后再利用linear SVM进行one-VS-rest分类。从而将Object Detection问题转化为一个Classification问题，对于Selective Search框选不准的bbox，后面使用<font color=\"orange\">Bounding Box Regression</font>(下面会详细介绍)进行校准。这便是RCNN的主要idea。\n\n![RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png)\n\n### Details of RCNN\n#### Pretraining and Fine-tuning\nRCNN先将Base Network在ImageNet上train一个1000 class的classifier，然后将output neuron设置为21(20个Foreground + 1个Background)在target datasets上fine-tune。其中，将由Selective Search生成的Region Proposal与groundtruth Bbox的$IOU\\geq 0.5$的定义为positive sample，其他的定义为negative sample。\n\n作者在实验中发现，pool5 features learned from ImageNet足够general，并且在domain-specific datasets上学习non-linear classifier可以获得非常大的性能提升。\n\n#### Bounding Box Regression\n输入是$N$个training pairs，$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$代表$P^i$像素点的$(x,y)$坐标点、width和height。$G=(G_x,G_y,G_w,G_h)$代表groundtruth bbox。BBox Regression的目的就是为了学习一种mapping使得proposed box $P$ 映射到 groundtruth box $G$。\n\n将$x,y$的transformation设为$d_x(P),d_y(P)$，属于<font color=\"red\">scale-invariant translation。$w,h$是log-space translation</font>。学习完成后，可将input proposal转换为predicted groundtruth box $\\hat{G}$:\n$$\n\\hat{G}_x=P_w d_x(P)+P_x\n$$\n\n$$\n\\hat{G}_y=P_h d_x(P)+P_y\n$$\n\n$$\n\\hat{G}_w=P_w exp(d_w(P))\n$$\n\n$$\n\\hat{G}_h=P_h exp(d_h(P))\n$$\n\n每个$d_{\\star}(P)$都用一个线性函数来进行建模，使用$pool_5$ feature，权重的学习则使用OLS优化即可：\n$$\nw_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2\n$$\n\nThe regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:\n$$\nt_x=\\frac{G_x-P_x}{P_w}\n$$\n\n$$\nt_y=\\frac{G_y-P_y}{P_h}\n$$\n\n$$\nt_w=log(\\frac{G_w}{P_w})\n$$\n\n$$\nt_h=log(\\frac{G_h}{P_h})\n$$\n\n在选取Proposed Bbox的时候，我们只选取离Groundtruth Bbox比较近的($IOU\\geq 0.6$)来做Bounding Box Regression。\n\n以上就是Deep Learning在Object Detection领域一个开创性的工作--RCNN。若有疑问，欢迎给我留言！\n\n\n## SPPNet\n> Paper: [Spatial pyramid pooling in deep convolutional networks for visual recognition.](https://arxiv.org/pdf/1406.4729v4.pdf)\n\n### What is SPPNet?\nSPPNet(Spatial Pyramid Pooling)是基于RCNN进行改进的一个Object Detection算法。介绍SPPNet之前，我们不妨先来看一下RCNN有什么问题？RCNN，即Region-based CNN，它需要CNN作为base network去做特征提取，而传统CNN需要固定的squared input，而为了满足这个条件，就需要手工地对原图进行裁剪、变形等操作，而这样势必会丢失信息。作者意识到这种现象的原因不在于卷积层，而在于FC Layers需要固定的输入尺寸，因此通过在feature map的SSPlayer可以满足对多尺度的feature map裁剪，从而concatenate得到固定尺寸的特征输入。取得了很好的效果，在detection任务上，region proposal直接在feature map上生成，而不是在原图上生成，因此可以仅仅通过一次特征提取，而不需要像RCNN那样提取2000次(2000个 Region Proposal)，这大大加速了检测效率。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：现如今的Deep Architecture比较多采用Fully Convolutional Architecture(全卷积结构)，而不含Fully Connected Layers，在最后做分类或回归任务时，采用Global Average Pooling即可。\n\n![Crop/Warp VS SPP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg)\n\n\n### Why SPPNet?\nSPPNet究竟有什么过人之处得到了Kaiming He大神的赏识呢？\n1. SPP可以在不顾input size的情况下获取fixed size output，这是sliding window做不到的。\n2. SPP uses multi-level spatial bins，而sliding window仅仅使用single window size。<font color=\"red\">multi-level pooling对object deformation则十分地robust</font>。\n3. SPP can pool features extracted at variable scales thanks to the flexibility of input scales.\n4. Training with variable-size images increases scale-invariance and reduces over-fitting.\n\n### Details of SPPNet\n#### SPP Layer\nSPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.\n\n![SPP Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg)\n\n这样，通过不同spatial bin pooling得到的k个 M-dimensional feature concatenation，我们就可以得到fixed length的feature vector了，接下来是不是就可以愉快地用FC Layers/SVM等ML算法train了？\n\n#### SPP for Detection\nRCNN需要从2K个Region Proposal中feedforwad Pretrained CNN去提取特征，这显然是非常低效的。SPPNet直接将整张image(possible multi-scale))作为输入，这样就可以只feedforwad一次CNN。然后在<font color=\"red\">feature map层面</font>获取candidate window，SPP Layer pool到fixed-length feature representation of the window。\n\n![Pooling](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg)\n\nRegion Proposal生成阶段和RCNN比较相似，依然是[Selective Search](https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf)生成2000个bbox candidate，然后将原始image resize使得$min(w, h)=s$，文中采用 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the features。对于每个window，该Pooling操作得到一个12800-Dimensional (256×50) 的向量。这个向量作为FC Layers的输入，然后和RCNN一样训练linear SVM去做分类。\n\n训练SPP Detector时，正负样本的采样是基于groundtruth bbox为基准，$IOU\\geq 0.3$为positive sample，反之为negative sample。\n\n\n## Fast RCNN\n> Paper: [Fast RCNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)\n\nFast RCNN是Object Detection领域一个非常经典的算法。它的novelty在于引入了两个branch来做multi-task learning(category classification和bbox regression)。\n\n### Why Fast RCNN?\n按照惯例，我们不妨先来看一看之前的算法(RCNN/SPPNet)有什么缺点？\n1. 它们(RCNN/SPP)的训练都属于multi-stage pipeline，即先要利用Selective Search生成2K个Region Proposal，然后用log loss去fine-tune一个deep CNN，用Deep CNN抽取的feature去拟合linear SVM，最后再去做Bounding Box Regression。\n2. 训练很费时，CNN需要从每一个Region Proposal抽取deep feature来拟合linear SVM。\n3. testing的时候慢啊，还是太慢了。因为需要将Deep CNN抽取的feature先缓存到磁盘，再读取feature来拟合linear SVM，你说麻烦不麻烦。\n\n那我们再来看看Fast RCNN为什么优秀？\n1. 设计了一个multi-task loss，来同时优化object classification和bounding box regression。\n2. Training is single stage.\n3. Higher performance than RCNN and SPPNet.\n\n### Details of Fast RCNN\n![Fast RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg)\n\nFast RCNN pipeline如上图所示：它将whole image with several object proposals作为输入，CNN抽取feature，对于每一个object proposal，<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>，然后将走过RoI Pooling Layer的feature vector输送到随后的multi-branch，一同做classification和bbox regression。\n\n可以看到，Fast RCNN模型里面一个非常重要的组件叫做<font color=\"red\">RoI Pooling</font>，那么接下来我们就来细细分析一下RoI Pooling究竟是何方神圣。\n\n#### The RoI pooling layer\nFast RCNN原文里是这样说的：\n> RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.\n> \n> In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.\n>\n> RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].\n\n什么意思呢？就是在任何valid region proposal里面，把某层的feature map划分成多个小方块，每个小方块做max pooling，这样就得到了尺寸更小的feature map。\n\n#### Fine-tuning for detection\n##### Multi-Task Loss\n之前也说过，Fast RCNN同时做了$K+1$ (K个object class + 1个background) 类的classification($p=(p_0,p_1,\\cdots,p_K)$)和bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)。\n\nWe use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(对linear regression熟悉的读者不妨思考一下为什么要对width和height做log). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:\n$$\nL(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)\n$$\n$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.\n\n我们再来看看Loss Function的第二部分(即regression loss)，$[u\\geq 1]$代表只有满足$u\\geq 1$时这个式子才为1，否则为0。在我们的setting中，background的$[u\\geq 1]$自然而然就设为0啦。我们接着分析regression loss，既然是regression，惯常的手法是使用MSE Loss对不对？但是MSE Loss属于Cost-sensitive Loss啊，对outliers非常的敏感，因此Ross大神使用了更加柔和的$Smooth L_1 Loss$。\n$$\nL_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)\n$$\n\nSmooth L1 Loss写得详细一点呢，就是这样的：\n$$\nsmooth_{L_1}(x)=\n\\begin{cases}\n0.5x^2 & if |x|<1\\\\\n|x|-0.5 & otherwise\n\\end{cases}\n$$\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：想详细了解Machine Learning中的Loss，请参考我的[另一篇文章](https://lucasxlu.github.io/blog/2018/07/24/ml-loss/)。\n\n##### Mini-batch sampling\n在Fine-tuning阶段，每个mini-batch随机采样自$N=2$类image，每一类都是64个sample，与groundtruth bbox $IOU\\geq 0.5$的设为foreground samples[$u=1$]，反之为background samples[$u=0$]。\n\n### Fast R-CNN detection\n因为Fully Connected Layers的计算太费时，而FC Layers的计算显然就是大矩阵相乘，因此很容易联想到用truncated SVD来进行加速。\n\nIn this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:\n$$\nW\\approx U\\Sigma_t V^T\n$$\n$U$是由$W$前$t$个left-singular vectors组成的$u\\times t$矩阵，$\\Sigma_t$是包含$W$矩阵前$t$个singular value的$t\\times t$对角矩阵，$V$是由$W$前$t$个right-singular vectors组成的$v\\times t$矩阵。Truncated SVD可以将参数从$uv$降到$t(u+v)$。\n\n这里值得一提的有两点：\n1. $conv_1$ layers feature map通常都足够地general，并且task-independent。因此可以直接用来抽feature就行。\n2. Region Proposal的数量对Detection的性能并没有什么太大影响 (关键还是看feature啊！以及sampling mini-batch的时候正负样本的不均衡问题，详情请参阅kaiming He大神的[Focal Loss](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf))。\n\n\n## Faster RCNN\n> Paper: [Faster r-cnn: Towards real-time object detection with region proposal networks](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n\nFaster RCNN也是Object Detection领域里一个非常具有代表性的工作，一个最大的改进就是Region Proposal Network(RPN)，RPN究竟神奇在什么地方呢？我们先来回顾一下RCNN--SPP--Fast RCNN，这些都属于two-stage detector，什么意思呢？就是先利用<font color=\"red\">Selective Search</font>生成2000个Region Proposal，然后再将其转化为一个机器学习中的分类问题。而<font color=\"red\">Selective Search</font>实际上是非常低效的，RPN则很好地完善了这一点，即直接从整个Network Architecture里生成Region Proposal。RPN是一种全卷积网络，它可以同时预测object bounds，以及objectness score。因为RPN的Feature是和Detection Network共享的，所以整个Region Proposal的生成几乎是cost-free的。所以，这也就是Faster RCNN中**Faster**一词的由来。\n\n\n### What is Faster RCNN?\n$$\nFaster RCNN = Fast RCNN + RPN\n$$\n按照惯例，一个算法的提出显然是为了解决之前算法的不足。那之前的算法都有什么问题呢？\n如果对之前的detector熟悉的话，shared features between proposals已经被解决，但是<font color=\"red\">Region Proposal的生成变成了最大的计算瓶颈</font>。这便是RPN产生的缘由。\n\n作者注意到，conv feature maps used by region-based detectors也可以被用于生成region proposals。在这些conv features顶端，<font color=\"red\">通过添加两个额外的卷积层来构造RPN：一个conv layer用于encode每个conv feature map position到一个低维向量(256-d)；另一个conv layer在每一个conv feature map position中输出k个region proposal with various scales and aspect ratios的objectness score和regression bounds。</font>下面重点介绍一下RPN。\n\n### Region Proposal Network\nRPN是一个全卷积网络，可以接受任意尺寸的image作为输入，并且输出一系列object proposals以及其对应的objectness score。那么RPN是如何生成region proposals的呢？\n\n首先，在最后一个shared conv feature map上slide一个小网络，这个小网络全连接到input conv feature map上$n\\times n$的spatial window。而每一个sliding window映射到一个256-d的feature vector，这个feature vector输入到两个fully connected layers--一个做box regression，另一个做box classification。值得注意的是，因为这个小网络是以sliding window的方式操作的，所以fully-connected layers在所有的spatial locations都是共享的。该结构由一个$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers组成。\n\n#### Translation-Invariant Anchors\n对于每个sliding window location，同时预测$k$个region proposals，所以regression layer有$4k$个encoding了$k$个bbox坐标的outputs。classification layer有$2k$个scores(每个region proposal估计object/non-object的概率)。\n\n> The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.\n\n文章中采用3个scales和3个aspect ratios，所以每个sliding position一共得到$k=9$个anchors。所以对于每个$W\\times H$的conv feature map一共产生$WHk$个anchor。这种方法一个很重要的性质就是**它是translation invariant**。\n\n#### A Loss Function for Learning Region Proposals\n训练RPN时，我们为每一个anchor分配binary class(即是不是一个object)。我们为以下这两类anchors分配positive label:\n1. 与groundtruth bbox有最高IoU的anchor\n2. 与任意groundtruth bbox $IoU\\geq 0.7$的anchor\n\n同时，可能出现一个groundtruth bbox分配到多个anchor的情况，我们将与所有groundtruth bbox $IoU\\leq 0.3$的anchor设为negative anchor。而那些既不是positive又不是negative的anchor则对training过程没有用处。然后，Faster RCNN的Loss Function就变成：\n$$\nL(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})\n$$\n$p_i$是anchor $i$ 被预测为是一个object的概率，若anchor为positive，则groundtruth label $p_i^{{\\star}}$为1；若anchor为negative则为0；$t_i$是包含4个预测bbox坐标点的向量，$t_i^{\\star}$是groundtruth positive anchor坐标点的向量。$L_{cls}$是二分类的Log Loss(object VS non-object)。对于regression loss，文章使用$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$，其中$R$是Smooth L1 Loss(和Fast RCNN中一样)。$p_i^{\\star} L_{reg}$表示<font color=\"red\">仅仅在positive anchor ($p_i^{\\star}=1$)时才被激活，否则($p_i^{\\star}=0$)不激活</font>。\n\nBounding Box Regression依旧是采用之前的pipeline：\n$$\nt_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})\n$$\n\n$$\nt_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})\n$$\n\n> In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. **To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights**. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.\n\n#### Sharing Convolutional Features for Region Proposal and Object Detection\n1. Fine-tune在ImageNet上Pretrain的RPN来完成region proposal task。\n2. 利用RPN生成的region proposal来train Fast RCNN。注意在这一步骤中RPN和Faster RCNN没有共享卷积层。\n3. 利用detector network来初始化RPN训练，但是我们fix shared conv layers，仅仅fine-tune单独属于RPN的层。注意在这一步骤中RPN和Fast RCNN共享了卷积层。\n4. Fix所有shared conv layers，fine-tune Fast RCNN的fc layers。至此，RPN和Fast RCNN共享卷积层，并且形成了一个unified network。\n\n\n## SSD\n> Paper: [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325v5.pdf)\n\nSSD是one-stage detector里一个非常著名的算法，那什么叫做one-stage和two-stage呢？回想一下，从DL Detector发展到现在，我们之前介绍的RCNN/SSP/Fast RCNN/Faster RCNN等，都是属于two-stage detectors，意思就是说**第一步需要生成region proposals，第二步再将整个detection转化为对这些region proposals的classification问题来做**。那所谓的one-stage detection就自然是不需要生成region proposals了，而是直接输出bbox了。Faster RCNN里面作者已经分析了，two-stage detection为啥慢？很大原因就是因为region proposal generation太慢了(例如Selective Search算法)，所以提出了RPN来辅助生成region proposals。\n\n### What is SSD?\nSSD最主要的改进就是**使用了一个小的Convolution Filter来预测object category和bbox offset**。那如何处理多尺度问题呢？SSD采取的策略是将这些conv filter应用到多个feature map上，来使得整个模型对Scale Invariant。\n\n![SSD Framework](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png)\n\n### Details of SSD\nSSD主要部件如下：一个DCNN用来提取feature，产生fixed-size的bbox以及这些bbox中每个类的presence score，然后NMS用来输出最后的检测结果。Feature Extraction部分和普通的分类DCNN没啥太大的区别，作者在后面新添加了新的结构：\n1. **Multi-scale feature maps for detection**: 在base feature extraction network之后额外添加新的conv layers(所以得到了multi-scale的feature maps)，来使得模型可以处理multi-scale的detection。\n2. **Convolutional predictors for detection**: 每一个新添加的feature layer可以基于```small conv filters```产生fixed-size detection predictions。\n3. **Default boxes and aspect ratios**: 对于每个feature map cell，算法给出cell中default box的relative offset，以及class-score(表示在每个box中一个class instance出现的概率)。具体的，对于每个given location的$k$个box，产生4个bbox offset和$c$个class score，这样就对每个```feature map location```上产生了$(c+4)k$个filters，那么对于一个$m\\times n$的```feature map```，则产生$(c+4)kmn$个output。这个做法和Faster RCNN中的anchor box有点类似，但是**SSD中将它用到了多个不同resolution的feature map上，因此多个feature map的不同default box shape使得我们可以很高效地给出output box shape**。\n\n![SSD and YOLO](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png)\n\n#### Training of SSD\n前面已经提到了，SSD会在default box周围生成一系列varied location/ratio/scale的boxes，那么到底哪一个box才是和groundtruth真正匹配的呢？作者采用了这样的一个Matching Strategy: 对于每一个groundtruth box，我们从default boxes中选择不同location/ratio/scale的boxes，然后计算它们和任意一个groundtruth box的Jaccard Overlap，并挑选出超过阈值的boxes。那么SSD最终的Loss就可以写成：\n$$\nL(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))\n$$\n其中$N$代表matched default boxes的数量，Localisation Loss和Faster RCNN一样也选择了Smooth L1。$x_{ij}^p=\\{0,1\\}$为第$i$个default box是否和第$j$个groundtruth box匹配的indicator。\n$$\nL_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)\n$$\n\n$$\n\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w\n$$\n\n$$\n\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h\n$$\n\n$$\n\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})\n$$\n\n$$\n\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})\n$$\n\nConfidence Loss采用Softmax Loss:\n$$\nL_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)\n$$\n其中，$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$\n\nSSD在检测large object时效果很好，但是在检测small object时则效果比较差，这是因为在higher layers，feature map包含的small object信息太少，可通过将input size由$300\\times 300$改为$512\\times 512$，**Zoom Data Augmentation**(即采用zoom in来生成large objects, zoom out来生成small objects)来进行一定程度的缓解。\n\n\n## Light-head RCNN\n> Paper: [Light-Head R-CNN: In Defense of Two-Stage Object Detector](https://arxiv.org/pdf/1711.07264v2.pdf)\n\n### Introduction\n在介绍Light-head RCNN之前，我们先来回顾一下常见的two-stage detector为什么是heavy-head？作者发现two-stage detector之所以慢，就是因为two-stage detector在RoI Warp前/后 会进行非常密集的计算，例如Faster RCNN包含2个fully connected layers做nRoI Recognition，RFCN会生成很大的score maps。所以无论你的backbone network使用了多么精巧的小网络结构，但是总体速度还是提升不上去。所以针对这个问题，作者在本文提出了```light-head``` RCNN。所谓的```light-head```，其实说白了就是```使用thin feature map + cheap RCNN subnet (pooling和单层fully connected layer)```。\n\n大家都知道，two-stage detector，其实是将detection问题转化为一个classification问题来完成的。也就是说，在第一个stage，模型会生成很多region proposal (此为```body```)，然后在第二个stage对这些region proposal进行分类 (此为```head```)。通常，two-stage detector的accuracy要比one-stage detector高的，所以为了accuracy，head往往会设计得非常heavy。Light-head RCNN是这么做的：\n> In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce \"thin\" feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.\n\n### Delve Into Light-Head RCNN\n#### RCNN Subnet\n> Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, **R-FCN first produces a set of score maps for each region, whose channel number will be $classes\\_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation**.\n\nFaster RCNN虽然在RoI Classification上表现得很好，但是它需要global average pooling来减小第一个fully connected layer的计算量，```而GAP会影响spatial localization```。此外，Faster RCNN对每一个RoI都要feedforward一遍RCNN subnet，所以在当proposal的数量很大时，效率就非常低了。\n\n#### Thin Feature Maps for RoI Warping\n在feed region proposal到RCNN subnet之前，用RoI warping来使得得到fixed shape的feature maps。本文提出的light-head产生了一系列```thin feature maps```，然后再接RoI Pooling层。在实验中，作者发现```RoI warping on thin feature maps```不仅仅提高了精度，而且节省了training和inference的时间。而且，如果直接应用RoI pooling到thin feature maps上，一方面模型可以减少计算量，另一方面可以去掉GAP来保留spatial information。\n\n![Light Head RCNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg)\n\n### Experiments\n作者在实验中发现，regression loss比classification loss要小很多，所以```将regression loss的权重进行double来balance multi-task training```。\n\n\n## YOLO v1\n> Paper: [You Only Look Once: Unified, Real-Time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n\n\n## Reference\n1. Girshick, Ross, et al. [\"Rich feature hierarchies for accurate object detection and semantic segmentation.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. He, Kaiming, et al. [\"Spatial pyramid pooling in deep convolutional networks for visual recognition.\"](https://arxiv.org/pdf/1406.4729v4.pdf) European conference on computer vision. Springer, Cham, 2014.\n3. Girshick, Ross. [\"Fast r-cnn.\"](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) Proceedings of the IEEE international conference on computer vision. 2015.\n4. Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollár. [\"Focal Loss for Dense Object Detection.\"](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)\n5. Ren, Shaoqing, et al. [\"Faster r-cnn: Towards real-time object detection with region proposal networks.\"](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf) Advances in neural information processing systems. 2015.\n6. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016, October). [Ssd: Single shot multibox detector](https://arxiv.org/pdf/1512.02325v5.pdf). In European conference on computer vision (pp. 21-37). Springer, Cham.\n7. Redmon, Joseph, et al. [\"You only look once: Unified, real-time object detection.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n8. Li Z, Peng C, Yu G, et al. [Light-head r-cnn: In defense of two-stage object detector](https://arxiv.org/pdf/1711.07264v2.pdf)[J]. arXiv preprint arXiv:1711.07264, 2017.","slug":"cv-detection","published":1,"updated":"2018-11-16T11:33:13.192Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bp0006608wtrrk0xw5","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Object Detection是Computer Vision领域一个非常火热的研究方向。并且在工业界也有着十分广泛的应用(例如人脸检测、无人驾驶的行人/车辆检测等等)。本质旨在梳理RCNN–SPPNet–Fast RCNN–Faster RCNN–FCN–Mask RCNN，YOLO v1/2/3, SSD等Object Detection这些非常经典的工作。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"RCNN-Region-based-CNN\"><a href=\"#RCNN-Region-based-CNN\" class=\"headerlink\" title=\"RCNN (Region-based CNN)\"></a>RCNN (Region-based CNN)</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n</blockquote>\n<h3 id=\"What-is-RCNN\"><a href=\"#What-is-RCNN\" class=\"headerlink\" title=\"What is RCNN?\"></a>What is RCNN?</h3><p>这篇Paper可以看做是Deep Learning在Object Detection大获成功的开端，对Machine Learning/Pattern Recognition熟悉的读者应该都知道，<strong>Feature Matters in approximately every task!</strong> 而RCNN性能提升最大的因素之一便是很好地利用了CNN提取的Feature，而不是像先前的detector那样使用手工设计的feature(例如SIFT/LBP/HOG等)。</p>\n<p>RCNN可以认为是Regions with CNN features，即(1)先利用<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search算法</a>生成大约2000个Region Proposal，(2)Pretrained CNN从这些Region Proposal中提取deep feature(from pool5)，(3)然后再利用linear SVM进行one-VS-rest分类。从而将Object Detection问题转化为一个Classification问题，对于Selective Search框选不准的bbox，后面使用<font color=\"orange\">Bounding Box Regression</font>(下面会详细介绍)进行校准。这便是RCNN的主要idea。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png\" alt=\"RCNN\"></p>\n<h3 id=\"Details-of-RCNN\"><a href=\"#Details-of-RCNN\" class=\"headerlink\" title=\"Details of RCNN\"></a>Details of RCNN</h3><h4 id=\"Pretraining-and-Fine-tuning\"><a href=\"#Pretraining-and-Fine-tuning\" class=\"headerlink\" title=\"Pretraining and Fine-tuning\"></a>Pretraining and Fine-tuning</h4><p>RCNN先将Base Network在ImageNet上train一个1000 class的classifier，然后将output neuron设置为21(20个Foreground + 1个Background)在target datasets上fine-tune。其中，将由Selective Search生成的Region Proposal与groundtruth Bbox的$IOU\\geq 0.5$的定义为positive sample，其他的定义为negative sample。</p>\n<p>作者在实验中发现，pool5 features learned from ImageNet足够general，并且在domain-specific datasets上学习non-linear classifier可以获得非常大的性能提升。</p>\n<h4 id=\"Bounding-Box-Regression\"><a href=\"#Bounding-Box-Regression\" class=\"headerlink\" title=\"Bounding Box Regression\"></a>Bounding Box Regression</h4><p>输入是$N$个training pairs，$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$代表$P^i$像素点的$(x,y)$坐标点、width和height。$G=(G_x,G_y,G_w,G_h)$代表groundtruth bbox。BBox Regression的目的就是为了学习一种mapping使得proposed box $P$ 映射到 groundtruth box $G$。</p>\n<p>将$x,y$的transformation设为$d_x(P),d_y(P)$，属于<font color=\"red\">scale-invariant translation。$w,h$是log-space translation</font>。学习完成后，可将input proposal转换为predicted groundtruth box $\\hat{G}$:<br>$$<br>\\hat{G}_x=P_w d_x(P)+P_x<br>$$</p>\n<p>$$<br>\\hat{G}_y=P_h d_x(P)+P_y<br>$$</p>\n<p>$$<br>\\hat{G}_w=P_w exp(d_w(P))<br>$$</p>\n<p>$$<br>\\hat{G}_h=P_h exp(d_h(P))<br>$$</p>\n<p>每个$d_{\\star}(P)$都用一个线性函数来进行建模，使用$pool_5$ feature，权重的学习则使用OLS优化即可：<br>$$<br>w_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2<br>$$</p>\n<p>The regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:<br>$$<br>t_x=\\frac{G_x-P_x}{P_w}<br>$$</p>\n<p>$$<br>t_y=\\frac{G_y-P_y}{P_h}<br>$$</p>\n<p>$$<br>t_w=log(\\frac{G_w}{P_w})<br>$$</p>\n<p>$$<br>t_h=log(\\frac{G_h}{P_h})<br>$$</p>\n<p>在选取Proposed Bbox的时候，我们只选取离Groundtruth Bbox比较近的($IOU\\geq 0.6$)来做Bounding Box Regression。</p>\n<p>以上就是Deep Learning在Object Detection领域一个开创性的工作–RCNN。若有疑问，欢迎给我留言！</p>\n<h2 id=\"SPPNet\"><a href=\"#SPPNet\" class=\"headerlink\" title=\"SPPNet\"></a>SPPNet</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a></p>\n</blockquote>\n<h3 id=\"What-is-SPPNet\"><a href=\"#What-is-SPPNet\" class=\"headerlink\" title=\"What is SPPNet?\"></a>What is SPPNet?</h3><p>SPPNet(Spatial Pyramid Pooling)是基于RCNN进行改进的一个Object Detection算法。介绍SPPNet之前，我们不妨先来看一下RCNN有什么问题？RCNN，即Region-based CNN，它需要CNN作为base network去做特征提取，而传统CNN需要固定的squared input，而为了满足这个条件，就需要手工地对原图进行裁剪、变形等操作，而这样势必会丢失信息。作者意识到这种现象的原因不在于卷积层，而在于FC Layers需要固定的输入尺寸，因此通过在feature map的SSPlayer可以满足对多尺度的feature map裁剪，从而concatenate得到固定尺寸的特征输入。取得了很好的效果，在detection任务上，region proposal直接在feature map上生成，而不是在原图上生成，因此可以仅仅通过一次特征提取，而不需要像RCNN那样提取2000次(2000个 Region Proposal)，这大大加速了检测效率。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：现如今的Deep Architecture比较多采用Fully Convolutional Architecture(全卷积结构)，而不含Fully Connected Layers，在最后做分类或回归任务时，采用Global Average Pooling即可。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg\" alt=\"Crop/Warp VS SPP\"></p>\n<h3 id=\"Why-SPPNet\"><a href=\"#Why-SPPNet\" class=\"headerlink\" title=\"Why SPPNet?\"></a>Why SPPNet?</h3><p>SPPNet究竟有什么过人之处得到了Kaiming He大神的赏识呢？</p>\n<ol>\n<li>SPP可以在不顾input size的情况下获取fixed size output，这是sliding window做不到的。</li>\n<li>SPP uses multi-level spatial bins，而sliding window仅仅使用single window size。<font color=\"red\">multi-level pooling对object deformation则十分地robust</font>。</li>\n<li>SPP can pool features extracted at variable scales thanks to the flexibility of input scales.</li>\n<li>Training with variable-size images increases scale-invariance and reduces over-fitting.</li>\n</ol>\n<h3 id=\"Details-of-SPPNet\"><a href=\"#Details-of-SPPNet\" class=\"headerlink\" title=\"Details of SPPNet\"></a>Details of SPPNet</h3><h4 id=\"SPP-Layer\"><a href=\"#SPP-Layer\" class=\"headerlink\" title=\"SPP Layer\"></a>SPP Layer</h4><p>SPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg\" alt=\"SPP Layer\"></p>\n<p>这样，通过不同spatial bin pooling得到的k个 M-dimensional feature concatenation，我们就可以得到fixed length的feature vector了，接下来是不是就可以愉快地用FC Layers/SVM等ML算法train了？</p>\n<h4 id=\"SPP-for-Detection\"><a href=\"#SPP-for-Detection\" class=\"headerlink\" title=\"SPP for Detection\"></a>SPP for Detection</h4><p>RCNN需要从2K个Region Proposal中feedforwad Pretrained CNN去提取特征，这显然是非常低效的。SPPNet直接将整张image(possible multi-scale))作为输入，这样就可以只feedforwad一次CNN。然后在<font color=\"red\">feature map层面</font>获取candidate window，SPP Layer pool到fixed-length feature representation of the window。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg\" alt=\"Pooling\"></p>\n<p>Region Proposal生成阶段和RCNN比较相似，依然是<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>生成2000个bbox candidate，然后将原始image resize使得$min(w, h)=s$，文中采用 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the features。对于每个window，该Pooling操作得到一个12800-Dimensional (256×50) 的向量。这个向量作为FC Layers的输入，然后和RCNN一样训练linear SVM去做分类。</p>\n<p>训练SPP Detector时，正负样本的采样是基于groundtruth bbox为基准，$IOU\\geq 0.3$为positive sample，反之为negative sample。</p>\n<h2 id=\"Fast-RCNN\"><a href=\"#Fast-RCNN\" class=\"headerlink\" title=\"Fast RCNN\"></a>Fast RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast RCNN</a></p>\n</blockquote>\n<p>Fast RCNN是Object Detection领域一个非常经典的算法。它的novelty在于引入了两个branch来做multi-task learning(category classification和bbox regression)。</p>\n<h3 id=\"Why-Fast-RCNN\"><a href=\"#Why-Fast-RCNN\" class=\"headerlink\" title=\"Why Fast RCNN?\"></a>Why Fast RCNN?</h3><p>按照惯例，我们不妨先来看一看之前的算法(RCNN/SPPNet)有什么缺点？</p>\n<ol>\n<li>它们(RCNN/SPP)的训练都属于multi-stage pipeline，即先要利用Selective Search生成2K个Region Proposal，然后用log loss去fine-tune一个deep CNN，用Deep CNN抽取的feature去拟合linear SVM，最后再去做Bounding Box Regression。</li>\n<li>训练很费时，CNN需要从每一个Region Proposal抽取deep feature来拟合linear SVM。</li>\n<li>testing的时候慢啊，还是太慢了。因为需要将Deep CNN抽取的feature先缓存到磁盘，再读取feature来拟合linear SVM，你说麻烦不麻烦。</li>\n</ol>\n<p>那我们再来看看Fast RCNN为什么优秀？</p>\n<ol>\n<li>设计了一个multi-task loss，来同时优化object classification和bounding box regression。</li>\n<li>Training is single stage.</li>\n<li>Higher performance than RCNN and SPPNet.</li>\n</ol>\n<h3 id=\"Details-of-Fast-RCNN\"><a href=\"#Details-of-Fast-RCNN\" class=\"headerlink\" title=\"Details of Fast RCNN\"></a>Details of Fast RCNN</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg\" alt=\"Fast RCNN\"></p>\n<p>Fast RCNN pipeline如上图所示：它将whole image with several object proposals作为输入，CNN抽取feature，对于每一个object proposal，<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>，然后将走过RoI Pooling Layer的feature vector输送到随后的multi-branch，一同做classification和bbox regression。</p>\n<p>可以看到，Fast RCNN模型里面一个非常重要的组件叫做<font color=\"red\">RoI Pooling</font>，那么接下来我们就来细细分析一下RoI Pooling究竟是何方神圣。</p>\n<h4 id=\"The-RoI-pooling-layer\"><a href=\"#The-RoI-pooling-layer\" class=\"headerlink\" title=\"The RoI pooling layer\"></a>The RoI pooling layer</h4><p>Fast RCNN原文里是这样说的：</p>\n<blockquote>\n<p>RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.</p>\n<p>In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.</p>\n<p>RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].</p>\n</blockquote>\n<p>什么意思呢？就是在任何valid region proposal里面，把某层的feature map划分成多个小方块，每个小方块做max pooling，这样就得到了尺寸更小的feature map。</p>\n<h4 id=\"Fine-tuning-for-detection\"><a href=\"#Fine-tuning-for-detection\" class=\"headerlink\" title=\"Fine-tuning for detection\"></a>Fine-tuning for detection</h4><h5 id=\"Multi-Task-Loss\"><a href=\"#Multi-Task-Loss\" class=\"headerlink\" title=\"Multi-Task Loss\"></a>Multi-Task Loss</h5><p>之前也说过，Fast RCNN同时做了$K+1$ (K个object class + 1个background) 类的classification($p=(p_0,p_1,\\cdots,p_K)$)和bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)。</p>\n<p>We use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(对linear regression熟悉的读者不妨思考一下为什么要对width和height做log). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:<br>$$<br>L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)<br>$$<br>$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.</p>\n<p>我们再来看看Loss Function的第二部分(即regression loss)，$[u\\geq 1]$代表只有满足$u\\geq 1$时这个式子才为1，否则为0。在我们的setting中，background的$[u\\geq 1]$自然而然就设为0啦。我们接着分析regression loss，既然是regression，惯常的手法是使用MSE Loss对不对？但是MSE Loss属于Cost-sensitive Loss啊，对outliers非常的敏感，因此Ross大神使用了更加柔和的$Smooth L_1 Loss$。<br>$$<br>L_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)<br>$$</p>\n<p>Smooth L1 Loss写得详细一点呢，就是这样的：<br>$$<br>smooth_{L_1}(x)=<br>\\begin{cases}<br>0.5x^2 &amp; if |x|&lt;1\\\\<br>|x|-0.5 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：想详细了解Machine Learning中的Loss，请参考我的<a href=\"https://lucasxlu.github.io/blog/2018/07/24/ml-loss/\">另一篇文章</a>。</p>\n</blockquote>\n<h5 id=\"Mini-batch-sampling\"><a href=\"#Mini-batch-sampling\" class=\"headerlink\" title=\"Mini-batch sampling\"></a>Mini-batch sampling</h5><p>在Fine-tuning阶段，每个mini-batch随机采样自$N=2$类image，每一类都是64个sample，与groundtruth bbox $IOU\\geq 0.5$的设为foreground samples[$u=1$]，反之为background samples[$u=0$]。</p>\n<h3 id=\"Fast-R-CNN-detection\"><a href=\"#Fast-R-CNN-detection\" class=\"headerlink\" title=\"Fast R-CNN detection\"></a>Fast R-CNN detection</h3><p>因为Fully Connected Layers的计算太费时，而FC Layers的计算显然就是大矩阵相乘，因此很容易联想到用truncated SVD来进行加速。</p>\n<p>In this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:<br>$$<br>W\\approx U\\Sigma_t V^T<br>$$<br>$U$是由$W$前$t$个left-singular vectors组成的$u\\times t$矩阵，$\\Sigma_t$是包含$W$矩阵前$t$个singular value的$t\\times t$对角矩阵，$V$是由$W$前$t$个right-singular vectors组成的$v\\times t$矩阵。Truncated SVD可以将参数从$uv$降到$t(u+v)$。</p>\n<p>这里值得一提的有两点：</p>\n<ol>\n<li>$conv_1$ layers feature map通常都足够地general，并且task-independent。因此可以直接用来抽feature就行。</li>\n<li>Region Proposal的数量对Detection的性能并没有什么太大影响 (关键还是看feature啊！以及sampling mini-batch的时候正负样本的不均衡问题，详情请参阅kaiming He大神的<a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss</a>)。</li>\n</ol>\n<h2 id=\"Faster-RCNN\"><a href=\"#Faster-RCNN\" class=\"headerlink\" title=\"Faster RCNN\"></a>Faster RCNN</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks</a></p>\n</blockquote>\n<p>Faster RCNN也是Object Detection领域里一个非常具有代表性的工作，一个最大的改进就是Region Proposal Network(RPN)，RPN究竟神奇在什么地方呢？我们先来回顾一下RCNN–SPP–Fast RCNN，这些都属于two-stage detector，什么意思呢？就是先利用<font color=\"red\">Selective Search</font>生成2000个Region Proposal，然后再将其转化为一个机器学习中的分类问题。而<font color=\"red\">Selective Search</font>实际上是非常低效的，RPN则很好地完善了这一点，即直接从整个Network Architecture里生成Region Proposal。RPN是一种全卷积网络，它可以同时预测object bounds，以及objectness score。因为RPN的Feature是和Detection Network共享的，所以整个Region Proposal的生成几乎是cost-free的。所以，这也就是Faster RCNN中<strong>Faster</strong>一词的由来。</p>\n<h3 id=\"What-is-Faster-RCNN\"><a href=\"#What-is-Faster-RCNN\" class=\"headerlink\" title=\"What is Faster RCNN?\"></a>What is Faster RCNN?</h3><p>$$<br>Faster RCNN = Fast RCNN + RPN<br>$$<br>按照惯例，一个算法的提出显然是为了解决之前算法的不足。那之前的算法都有什么问题呢？<br>如果对之前的detector熟悉的话，shared features between proposals已经被解决，但是<font color=\"red\">Region Proposal的生成变成了最大的计算瓶颈</font>。这便是RPN产生的缘由。</p>\n<p>作者注意到，conv feature maps used by region-based detectors也可以被用于生成region proposals。在这些conv features顶端，<font color=\"red\">通过添加两个额外的卷积层来构造RPN：一个conv layer用于encode每个conv feature map position到一个低维向量(256-d)；另一个conv layer在每一个conv feature map position中输出k个region proposal with various scales and aspect ratios的objectness score和regression bounds。</font>下面重点介绍一下RPN。</p>\n<h3 id=\"Region-Proposal-Network\"><a href=\"#Region-Proposal-Network\" class=\"headerlink\" title=\"Region Proposal Network\"></a>Region Proposal Network</h3><p>RPN是一个全卷积网络，可以接受任意尺寸的image作为输入，并且输出一系列object proposals以及其对应的objectness score。那么RPN是如何生成region proposals的呢？</p>\n<p>首先，在最后一个shared conv feature map上slide一个小网络，这个小网络全连接到input conv feature map上$n\\times n$的spatial window。而每一个sliding window映射到一个256-d的feature vector，这个feature vector输入到两个fully connected layers–一个做box regression，另一个做box classification。值得注意的是，因为这个小网络是以sliding window的方式操作的，所以fully-connected layers在所有的spatial locations都是共享的。该结构由一个$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers组成。</p>\n<h4 id=\"Translation-Invariant-Anchors\"><a href=\"#Translation-Invariant-Anchors\" class=\"headerlink\" title=\"Translation-Invariant Anchors\"></a>Translation-Invariant Anchors</h4><p>对于每个sliding window location，同时预测$k$个region proposals，所以regression layer有$4k$个encoding了$k$个bbox坐标的outputs。classification layer有$2k$个scores(每个region proposal估计object/non-object的概率)。</p>\n<blockquote>\n<p>The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.</p>\n</blockquote>\n<p>文章中采用3个scales和3个aspect ratios，所以每个sliding position一共得到$k=9$个anchors。所以对于每个$W\\times H$的conv feature map一共产生$WHk$个anchor。这种方法一个很重要的性质就是<strong>它是translation invariant</strong>。</p>\n<h4 id=\"A-Loss-Function-for-Learning-Region-Proposals\"><a href=\"#A-Loss-Function-for-Learning-Region-Proposals\" class=\"headerlink\" title=\"A Loss Function for Learning Region Proposals\"></a>A Loss Function for Learning Region Proposals</h4><p>训练RPN时，我们为每一个anchor分配binary class(即是不是一个object)。我们为以下这两类anchors分配positive label:</p>\n<ol>\n<li>与groundtruth bbox有最高IoU的anchor</li>\n<li>与任意groundtruth bbox $IoU\\geq 0.7$的anchor</li>\n</ol>\n<p>同时，可能出现一个groundtruth bbox分配到多个anchor的情况，我们将与所有groundtruth bbox $IoU\\leq 0.3$的anchor设为negative anchor。而那些既不是positive又不是negative的anchor则对training过程没有用处。然后，Faster RCNN的Loss Function就变成：<br>$$<br>L(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})<br>$$<br>$p_i$是anchor $i$ 被预测为是一个object的概率，若anchor为positive，则groundtruth label $p_i^$为1；若anchor为negative则为0；$t_i$是包含4个预测bbox坐标点的向量，$t_i^{\\star}$是groundtruth positive anchor坐标点的向量。$L_{cls}$是二分类的Log Loss(object VS non-object)。对于regression loss，文章使用$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$，其中$R$是Smooth L1 Loss(和Fast RCNN中一样)。$p_i^{\\star} L_{reg}$表示<font color=\"red\">仅仅在positive anchor ($p_i^{\\star}=1$)时才被激活，否则($p_i^{\\star}=0$)不激活</font>。</p>\n<p>Bounding Box Regression依旧是采用之前的pipeline：<br>$$<br>t_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})<br>$$</p>\n<p>$$<br>t_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})<br>$$</p>\n<blockquote>\n<p>In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. <strong>To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights</strong>. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.</p>\n</blockquote>\n<h4 id=\"Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\"><a href=\"#Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\" class=\"headerlink\" title=\"Sharing Convolutional Features for Region Proposal and Object Detection\"></a>Sharing Convolutional Features for Region Proposal and Object Detection</h4><ol>\n<li>Fine-tune在ImageNet上Pretrain的RPN来完成region proposal task。</li>\n<li>利用RPN生成的region proposal来train Fast RCNN。注意在这一步骤中RPN和Faster RCNN没有共享卷积层。</li>\n<li>利用detector network来初始化RPN训练，但是我们fix shared conv layers，仅仅fine-tune单独属于RPN的层。注意在这一步骤中RPN和Fast RCNN共享了卷积层。</li>\n<li>Fix所有shared conv layers，fine-tune Fast RCNN的fc layers。至此，RPN和Fast RCNN共享卷积层，并且形成了一个unified network。</li>\n</ol>\n<h2 id=\"SSD\"><a href=\"#SSD\" class=\"headerlink\" title=\"SSD\"></a>SSD</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">SSD: Single Shot MultiBox Detector</a></p>\n</blockquote>\n<p>SSD是one-stage detector里一个非常著名的算法，那什么叫做one-stage和two-stage呢？回想一下，从DL Detector发展到现在，我们之前介绍的RCNN/SSP/Fast RCNN/Faster RCNN等，都是属于two-stage detectors，意思就是说<strong>第一步需要生成region proposals，第二步再将整个detection转化为对这些region proposals的classification问题来做</strong>。那所谓的one-stage detection就自然是不需要生成region proposals了，而是直接输出bbox了。Faster RCNN里面作者已经分析了，two-stage detection为啥慢？很大原因就是因为region proposal generation太慢了(例如Selective Search算法)，所以提出了RPN来辅助生成region proposals。</p>\n<h3 id=\"What-is-SSD\"><a href=\"#What-is-SSD\" class=\"headerlink\" title=\"What is SSD?\"></a>What is SSD?</h3><p>SSD最主要的改进就是<strong>使用了一个小的Convolution Filter来预测object category和bbox offset</strong>。那如何处理多尺度问题呢？SSD采取的策略是将这些conv filter应用到多个feature map上，来使得整个模型对Scale Invariant。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png\" alt=\"SSD Framework\"></p>\n<h3 id=\"Details-of-SSD\"><a href=\"#Details-of-SSD\" class=\"headerlink\" title=\"Details of SSD\"></a>Details of SSD</h3><p>SSD主要部件如下：一个DCNN用来提取feature，产生fixed-size的bbox以及这些bbox中每个类的presence score，然后NMS用来输出最后的检测结果。Feature Extraction部分和普通的分类DCNN没啥太大的区别，作者在后面新添加了新的结构：</p>\n<ol>\n<li><strong>Multi-scale feature maps for detection</strong>: 在base feature extraction network之后额外添加新的conv layers(所以得到了multi-scale的feature maps)，来使得模型可以处理multi-scale的detection。</li>\n<li><strong>Convolutional predictors for detection</strong>: 每一个新添加的feature layer可以基于<code>small conv filters</code>产生fixed-size detection predictions。</li>\n<li><strong>Default boxes and aspect ratios</strong>: 对于每个feature map cell，算法给出cell中default box的relative offset，以及class-score(表示在每个box中一个class instance出现的概率)。具体的，对于每个given location的$k$个box，产生4个bbox offset和$c$个class score，这样就对每个<code>feature map location</code>上产生了$(c+4)k$个filters，那么对于一个$m\\times n$的<code>feature map</code>，则产生$(c+4)kmn$个output。这个做法和Faster RCNN中的anchor box有点类似，但是<strong>SSD中将它用到了多个不同resolution的feature map上，因此多个feature map的不同default box shape使得我们可以很高效地给出output box shape</strong>。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png\" alt=\"SSD and YOLO\"></p>\n<h4 id=\"Training-of-SSD\"><a href=\"#Training-of-SSD\" class=\"headerlink\" title=\"Training of SSD\"></a>Training of SSD</h4><p>前面已经提到了，SSD会在default box周围生成一系列varied location/ratio/scale的boxes，那么到底哪一个box才是和groundtruth真正匹配的呢？作者采用了这样的一个Matching Strategy: 对于每一个groundtruth box，我们从default boxes中选择不同location/ratio/scale的boxes，然后计算它们和任意一个groundtruth box的Jaccard Overlap，并挑选出超过阈值的boxes。那么SSD最终的Loss就可以写成：<br>$$<br>L(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))<br>$$<br>其中$N$代表matched default boxes的数量，Localisation Loss和Faster RCNN一样也选择了Smooth L1。$x_{ij}^p=\\{0,1\\}$为第$i$个default box是否和第$j$个groundtruth box匹配的indicator。<br>$$<br>L_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h<br>$$</p>\n<p>$$<br>\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})<br>$$</p>\n<p>$$<br>\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})<br>$$</p>\n<p>Confidence Loss采用Softmax Loss:<br>$$<br>L_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)<br>$$<br>其中，$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$</p>\n<p>SSD在检测large object时效果很好，但是在检测small object时则效果比较差，这是因为在higher layers，feature map包含的small object信息太少，可通过将input size由$300\\times 300$改为$512\\times 512$，<strong>Zoom Data Augmentation</strong>(即采用zoom in来生成large objects, zoom out来生成small objects)来进行一定程度的缓解。</p>\n<h2 id=\"Light-head-RCNN\"><a href=\"#Light-head-RCNN\" class=\"headerlink\" title=\"Light-head RCNN\"></a>Light-head RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>在介绍Light-head RCNN之前，我们先来回顾一下常见的two-stage detector为什么是heavy-head？作者发现two-stage detector之所以慢，就是因为two-stage detector在RoI Warp前/后 会进行非常密集的计算，例如Faster RCNN包含2个fully connected layers做nRoI Recognition，RFCN会生成很大的score maps。所以无论你的backbone network使用了多么精巧的小网络结构，但是总体速度还是提升不上去。所以针对这个问题，作者在本文提出了<code>light-head</code> RCNN。所谓的<code>light-head</code>，其实说白了就是<code>使用thin feature map + cheap RCNN subnet (pooling和单层fully connected layer)</code>。</p>\n<p>大家都知道，two-stage detector，其实是将detection问题转化为一个classification问题来完成的。也就是说，在第一个stage，模型会生成很多region proposal (此为<code>body</code>)，然后在第二个stage对这些region proposal进行分类 (此为<code>head</code>)。通常，two-stage detector的accuracy要比one-stage detector高的，所以为了accuracy，head往往会设计得非常heavy。Light-head RCNN是这么做的：</p>\n<blockquote>\n<p>In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce “thin” feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.</p>\n</blockquote>\n<h3 id=\"Delve-Into-Light-Head-RCNN\"><a href=\"#Delve-Into-Light-Head-RCNN\" class=\"headerlink\" title=\"Delve Into Light-Head RCNN\"></a>Delve Into Light-Head RCNN</h3><h4 id=\"RCNN-Subnet\"><a href=\"#RCNN-Subnet\" class=\"headerlink\" title=\"RCNN Subnet\"></a>RCNN Subnet</h4><blockquote>\n<p>Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, <strong>R-FCN first produces a set of score maps for each region, whose channel number will be $classes_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation</strong>.</p>\n</blockquote>\n<p>Faster RCNN虽然在RoI Classification上表现得很好，但是它需要global average pooling来减小第一个fully connected layer的计算量，<code>而GAP会影响spatial localization</code>。此外，Faster RCNN对每一个RoI都要feedforward一遍RCNN subnet，所以在当proposal的数量很大时，效率就非常低了。</p>\n<h4 id=\"Thin-Feature-Maps-for-RoI-Warping\"><a href=\"#Thin-Feature-Maps-for-RoI-Warping\" class=\"headerlink\" title=\"Thin Feature Maps for RoI Warping\"></a>Thin Feature Maps for RoI Warping</h4><p>在feed region proposal到RCNN subnet之前，用RoI warping来使得得到fixed shape的feature maps。本文提出的light-head产生了一系列<code>thin feature maps</code>，然后再接RoI Pooling层。在实验中，作者发现<code>RoI warping on thin feature maps</code>不仅仅提高了精度，而且节省了training和inference的时间。而且，如果直接应用RoI pooling到thin feature maps上，一方面模型可以减少计算量，另一方面可以去掉GAP来保留spatial information。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg\" alt=\"Light Head RCNN\"></p>\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>作者在实验中发现，regression loss比classification loss要小很多，所以<code>将regression loss的权重进行double来balance multi-task training</code>。</p>\n<h2 id=\"YOLO-v1\"><a href=\"#YOLO-v1\" class=\"headerlink\" title=\"YOLO v1\"></a>YOLO v1</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You Only Look Once: Unified, Real-Time Object Detection</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Girshick, Ross, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Rich feature hierarchies for accurate object detection and semantic segmentation.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>He, Kaiming, et al. <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">“Spatial pyramid pooling in deep convolutional networks for visual recognition.”</a> European conference on computer vision. Springer, Cham, 2014.</li>\n<li>Girshick, Ross. <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Fast r-cnn.”</a> Proceedings of the IEEE international conference on computer vision. 2015.</li>\n<li>Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollár. <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Focal Loss for Dense Object Detection.”</a></li>\n<li>Ren, Shaoqing, et al. <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">“Faster r-cnn: Towards real-time object detection with region proposal networks.”</a> Advances in neural information processing systems. 2015.</li>\n<li>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., &amp; Berg, A. C. (2016, October). <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">Ssd: Single shot multibox detector</a>. In European conference on computer vision (pp. 21-37). Springer, Cham.</li>\n<li>Redmon, Joseph, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">“You only look once: Unified, real-time object detection.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</li>\n<li>Li Z, Peng C, Yu G, et al. <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-head r-cnn: In defense of two-stage object detector</a>[J]. arXiv preprint arXiv:1711.07264, 2017.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Object Detection是Computer Vision领域一个非常火热的研究方向。并且在工业界也有着十分广泛的应用(例如人脸检测、无人驾驶的行人/车辆检测等等)。本质旨在梳理RCNN–SPPNet–Fast RCNN–Faster RCNN–FCN–Mask RCNN，YOLO v1/2/3, SSD等Object Detection这些非常经典的工作。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"RCNN-Region-based-CNN\"><a href=\"#RCNN-Region-based-CNN\" class=\"headerlink\" title=\"RCNN (Region-based CNN)\"></a>RCNN (Region-based CNN)</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>\n</blockquote>\n<h3 id=\"What-is-RCNN\"><a href=\"#What-is-RCNN\" class=\"headerlink\" title=\"What is RCNN?\"></a>What is RCNN?</h3><p>这篇Paper可以看做是Deep Learning在Object Detection大获成功的开端，对Machine Learning/Pattern Recognition熟悉的读者应该都知道，<strong>Feature Matters in approximately every task!</strong> 而RCNN性能提升最大的因素之一便是很好地利用了CNN提取的Feature，而不是像先前的detector那样使用手工设计的feature(例如SIFT/LBP/HOG等)。</p>\n<p>RCNN可以认为是Regions with CNN features，即(1)先利用<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search算法</a>生成大约2000个Region Proposal，(2)Pretrained CNN从这些Region Proposal中提取deep feature(from pool5)，(3)然后再利用linear SVM进行one-VS-rest分类。从而将Object Detection问题转化为一个Classification问题，对于Selective Search框选不准的bbox，后面使用<font color=\"orange\">Bounding Box Regression</font>(下面会详细介绍)进行校准。这便是RCNN的主要idea。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/rcnn.png\" alt=\"RCNN\"></p>\n<h3 id=\"Details-of-RCNN\"><a href=\"#Details-of-RCNN\" class=\"headerlink\" title=\"Details of RCNN\"></a>Details of RCNN</h3><h4 id=\"Pretraining-and-Fine-tuning\"><a href=\"#Pretraining-and-Fine-tuning\" class=\"headerlink\" title=\"Pretraining and Fine-tuning\"></a>Pretraining and Fine-tuning</h4><p>RCNN先将Base Network在ImageNet上train一个1000 class的classifier，然后将output neuron设置为21(20个Foreground + 1个Background)在target datasets上fine-tune。其中，将由Selective Search生成的Region Proposal与groundtruth Bbox的$IOU\\geq 0.5$的定义为positive sample，其他的定义为negative sample。</p>\n<p>作者在实验中发现，pool5 features learned from ImageNet足够general，并且在domain-specific datasets上学习non-linear classifier可以获得非常大的性能提升。</p>\n<h4 id=\"Bounding-Box-Regression\"><a href=\"#Bounding-Box-Regression\" class=\"headerlink\" title=\"Bounding Box Regression\"></a>Bounding Box Regression</h4><p>输入是$N$个training pairs，$\\{(P^i,G^i)\\}_{i=1,2,\\cdots,N}, P^i=(P_x^i,P_y^i,P_w^i,P_h^i)$代表$P^i$像素点的$(x,y)$坐标点、width和height。$G=(G_x,G_y,G_w,G_h)$代表groundtruth bbox。BBox Regression的目的就是为了学习一种mapping使得proposed box $P$ 映射到 groundtruth box $G$。</p>\n<p>将$x,y$的transformation设为$d_x(P),d_y(P)$，属于<font color=\"red\">scale-invariant translation。$w,h$是log-space translation</font>。学习完成后，可将input proposal转换为predicted groundtruth box $\\hat{G}$:<br>$$<br>\\hat{G}_x=P_w d_x(P)+P_x<br>$$</p>\n<p>$$<br>\\hat{G}_y=P_h d_x(P)+P_y<br>$$</p>\n<p>$$<br>\\hat{G}_w=P_w exp(d_w(P))<br>$$</p>\n<p>$$<br>\\hat{G}_h=P_h exp(d_h(P))<br>$$</p>\n<p>每个$d_{\\star}(P)$都用一个线性函数来进行建模，使用$pool_5$ feature，权重的学习则使用OLS优化即可：<br>$$<br>w_{\\star}=\\mathop{argmin} \\limits_{\\hat{w}_{\\star}} \\sum_i^N (t_{\\star}^i-\\hat{w}_{\\star}^T \\phi_5 (P^i))^2 + \\lambda||\\hat{w}_{\\star}||^2<br>$$</p>\n<p>The regression targets $t_{\\star}$ for the training pair $(P, G)$ are defined as:<br>$$<br>t_x=\\frac{G_x-P_x}{P_w}<br>$$</p>\n<p>$$<br>t_y=\\frac{G_y-P_y}{P_h}<br>$$</p>\n<p>$$<br>t_w=log(\\frac{G_w}{P_w})<br>$$</p>\n<p>$$<br>t_h=log(\\frac{G_h}{P_h})<br>$$</p>\n<p>在选取Proposed Bbox的时候，我们只选取离Groundtruth Bbox比较近的($IOU\\geq 0.6$)来做Bounding Box Regression。</p>\n<p>以上就是Deep Learning在Object Detection领域一个开创性的工作–RCNN。若有疑问，欢迎给我留言！</p>\n<h2 id=\"SPPNet\"><a href=\"#SPPNet\" class=\"headerlink\" title=\"SPPNet\"></a>SPPNet</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">Spatial pyramid pooling in deep convolutional networks for visual recognition.</a></p>\n</blockquote>\n<h3 id=\"What-is-SPPNet\"><a href=\"#What-is-SPPNet\" class=\"headerlink\" title=\"What is SPPNet?\"></a>What is SPPNet?</h3><p>SPPNet(Spatial Pyramid Pooling)是基于RCNN进行改进的一个Object Detection算法。介绍SPPNet之前，我们不妨先来看一下RCNN有什么问题？RCNN，即Region-based CNN，它需要CNN作为base network去做特征提取，而传统CNN需要固定的squared input，而为了满足这个条件，就需要手工地对原图进行裁剪、变形等操作，而这样势必会丢失信息。作者意识到这种现象的原因不在于卷积层，而在于FC Layers需要固定的输入尺寸，因此通过在feature map的SSPlayer可以满足对多尺度的feature map裁剪，从而concatenate得到固定尺寸的特征输入。取得了很好的效果，在detection任务上，region proposal直接在feature map上生成，而不是在原图上生成，因此可以仅仅通过一次特征提取，而不需要像RCNN那样提取2000次(2000个 Region Proposal)，这大大加速了检测效率。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：现如今的Deep Architecture比较多采用Fully Convolutional Architecture(全卷积结构)，而不含Fully Connected Layers，在最后做分类或回归任务时，采用Global Average Pooling即可。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/cw_vs_spp.jpg\" alt=\"Crop/Warp VS SPP\"></p>\n<h3 id=\"Why-SPPNet\"><a href=\"#Why-SPPNet\" class=\"headerlink\" title=\"Why SPPNet?\"></a>Why SPPNet?</h3><p>SPPNet究竟有什么过人之处得到了Kaiming He大神的赏识呢？</p>\n<ol>\n<li>SPP可以在不顾input size的情况下获取fixed size output，这是sliding window做不到的。</li>\n<li>SPP uses multi-level spatial bins，而sliding window仅仅使用single window size。<font color=\"red\">multi-level pooling对object deformation则十分地robust</font>。</li>\n<li>SPP can pool features extracted at variable scales thanks to the flexibility of input scales.</li>\n<li>Training with variable-size images increases scale-invariance and reduces over-fitting.</li>\n</ol>\n<h3 id=\"Details-of-SPPNet\"><a href=\"#Details-of-SPPNet\" class=\"headerlink\" title=\"Details of SPPNet\"></a>Details of SPPNet</h3><h4 id=\"SPP-Layer\"><a href=\"#SPP-Layer\" class=\"headerlink\" title=\"SPP Layer\"></a>SPP Layer</h4><p>SPP Layer can maintain spatial information by pooling in local spatial bins. <font color=\"red\">These spatial bins have sizes proportional to the image size, so the number of bins is fixed regardless of the image size.</font> This is in contrast to the sliding window pooling of the previous deep networks,where the number of sliding windows depends on the input size.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/spp_layer.jpg\" alt=\"SPP Layer\"></p>\n<p>这样，通过不同spatial bin pooling得到的k个 M-dimensional feature concatenation，我们就可以得到fixed length的feature vector了，接下来是不是就可以愉快地用FC Layers/SVM等ML算法train了？</p>\n<h4 id=\"SPP-for-Detection\"><a href=\"#SPP-for-Detection\" class=\"headerlink\" title=\"SPP for Detection\"></a>SPP for Detection</h4><p>RCNN需要从2K个Region Proposal中feedforwad Pretrained CNN去提取特征，这显然是非常低效的。SPPNet直接将整张image(possible multi-scale))作为输入，这样就可以只feedforwad一次CNN。然后在<font color=\"red\">feature map层面</font>获取candidate window，SPP Layer pool到fixed-length feature representation of the window。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/pooling.jpg\" alt=\"Pooling\"></p>\n<p>Region Proposal生成阶段和RCNN比较相似，依然是<a href=\"https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf\" target=\"_blank\" rel=\"noopener\">Selective Search</a>生成2000个bbox candidate，然后将原始image resize使得$min(w, h)=s$，文中采用 4-level spatial pyramid ($1\\times 1, 2\\times 2, 3\\times 3,6\\times 6$, totally 50 bins) to pool the features。对于每个window，该Pooling操作得到一个12800-Dimensional (256×50) 的向量。这个向量作为FC Layers的输入，然后和RCNN一样训练linear SVM去做分类。</p>\n<p>训练SPP Detector时，正负样本的采样是基于groundtruth bbox为基准，$IOU\\geq 0.3$为positive sample，反之为negative sample。</p>\n<h2 id=\"Fast-RCNN\"><a href=\"#Fast-RCNN\" class=\"headerlink\" title=\"Fast RCNN\"></a>Fast RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">Fast RCNN</a></p>\n</blockquote>\n<p>Fast RCNN是Object Detection领域一个非常经典的算法。它的novelty在于引入了两个branch来做multi-task learning(category classification和bbox regression)。</p>\n<h3 id=\"Why-Fast-RCNN\"><a href=\"#Why-Fast-RCNN\" class=\"headerlink\" title=\"Why Fast RCNN?\"></a>Why Fast RCNN?</h3><p>按照惯例，我们不妨先来看一看之前的算法(RCNN/SPPNet)有什么缺点？</p>\n<ol>\n<li>它们(RCNN/SPP)的训练都属于multi-stage pipeline，即先要利用Selective Search生成2K个Region Proposal，然后用log loss去fine-tune一个deep CNN，用Deep CNN抽取的feature去拟合linear SVM，最后再去做Bounding Box Regression。</li>\n<li>训练很费时，CNN需要从每一个Region Proposal抽取deep feature来拟合linear SVM。</li>\n<li>testing的时候慢啊，还是太慢了。因为需要将Deep CNN抽取的feature先缓存到磁盘，再读取feature来拟合linear SVM，你说麻烦不麻烦。</li>\n</ol>\n<p>那我们再来看看Fast RCNN为什么优秀？</p>\n<ol>\n<li>设计了一个multi-task loss，来同时优化object classification和bounding box regression。</li>\n<li>Training is single stage.</li>\n<li>Higher performance than RCNN and SPPNet.</li>\n</ol>\n<h3 id=\"Details-of-Fast-RCNN\"><a href=\"#Details-of-Fast-RCNN\" class=\"headerlink\" title=\"Details of Fast RCNN\"></a>Details of Fast RCNN</h3><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/fastrcnn.jpg\" alt=\"Fast RCNN\"></p>\n<p>Fast RCNN pipeline如上图所示：它将whole image with several object proposals作为输入，CNN抽取feature，对于每一个object proposal，<font color=\"red\">region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map</font>，然后将走过RoI Pooling Layer的feature vector输送到随后的multi-branch，一同做classification和bbox regression。</p>\n<p>可以看到，Fast RCNN模型里面一个非常重要的组件叫做<font color=\"red\">RoI Pooling</font>，那么接下来我们就来细细分析一下RoI Pooling究竟是何方神圣。</p>\n<h4 id=\"The-RoI-pooling-layer\"><a href=\"#The-RoI-pooling-layer\" class=\"headerlink\" title=\"The RoI pooling layer\"></a>The RoI pooling layer</h4><p>Fast RCNN原文里是这样说的：</p>\n<blockquote>\n<p>RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of $H\\times W$ (e.g., $7\\times 7$), where $H$ and $W$ are layer hyper-parameters that are independent of any particular RoI.</p>\n<p>In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple $(r, c, h, w)$ that specifies its top-left corner $(r, c)$ and its height and width $(h, w)$.</p>\n<p>RoI max pooling works by dividing the $h\\times w$ RoI window into an $H\\times W$ grid of sub-windows of approximate size $h/H \\times w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].</p>\n</blockquote>\n<p>什么意思呢？就是在任何valid region proposal里面，把某层的feature map划分成多个小方块，每个小方块做max pooling，这样就得到了尺寸更小的feature map。</p>\n<h4 id=\"Fine-tuning-for-detection\"><a href=\"#Fine-tuning-for-detection\" class=\"headerlink\" title=\"Fine-tuning for detection\"></a>Fine-tuning for detection</h4><h5 id=\"Multi-Task-Loss\"><a href=\"#Multi-Task-Loss\" class=\"headerlink\" title=\"Multi-Task Loss\"></a>Multi-Task Loss</h5><p>之前也说过，Fast RCNN同时做了$K+1$ (K个object class + 1个background) 类的classification($p=(p_0,p_1,\\cdots,p_K)$)和bbox regression($t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$)。</p>\n<p>We use the parameterization for $t^k$ given in [9], in which <font color=\"red\">$t^k$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal</font>(对linear regression熟悉的读者不妨思考一下为什么要对width和height做log). Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$. We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression:<br>$$<br>L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda [u\\geq1]L_{loc}(t^u,v)<br>$$<br>$L_{cls}(p,u)=-logp_u$ is log loss for true class $u$.</p>\n<p>我们再来看看Loss Function的第二部分(即regression loss)，$[u\\geq 1]$代表只有满足$u\\geq 1$时这个式子才为1，否则为0。在我们的setting中，background的$[u\\geq 1]$自然而然就设为0啦。我们接着分析regression loss，既然是regression，惯常的手法是使用MSE Loss对不对？但是MSE Loss属于Cost-sensitive Loss啊，对outliers非常的敏感，因此Ross大神使用了更加柔和的$Smooth L_1 Loss$。<br>$$<br>L_{loc}(t^u,v)\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_1}(t_i^u-v_i)<br>$$</p>\n<p>Smooth L1 Loss写得详细一点呢，就是这样的：<br>$$<br>smooth_{L_1}(x)=<br>\\begin{cases}<br>0.5x^2 &amp; if |x|&lt;1\\\\<br>|x|-0.5 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：想详细了解Machine Learning中的Loss，请参考我的<a href=\"https://lucasxlu.github.io/blog/2018/07/24/ml-loss/\">另一篇文章</a>。</p>\n</blockquote>\n<h5 id=\"Mini-batch-sampling\"><a href=\"#Mini-batch-sampling\" class=\"headerlink\" title=\"Mini-batch sampling\"></a>Mini-batch sampling</h5><p>在Fine-tuning阶段，每个mini-batch随机采样自$N=2$类image，每一类都是64个sample，与groundtruth bbox $IOU\\geq 0.5$的设为foreground samples[$u=1$]，反之为background samples[$u=0$]。</p>\n<h3 id=\"Fast-R-CNN-detection\"><a href=\"#Fast-R-CNN-detection\" class=\"headerlink\" title=\"Fast R-CNN detection\"></a>Fast R-CNN detection</h3><p>因为Fully Connected Layers的计算太费时，而FC Layers的计算显然就是大矩阵相乘，因此很容易联想到用truncated SVD来进行加速。</p>\n<p>In this technique, a layer parameterized by the $u\\times v$ weight matrix $W$ is approximately factorized as:<br>$$<br>W\\approx U\\Sigma_t V^T<br>$$<br>$U$是由$W$前$t$个left-singular vectors组成的$u\\times t$矩阵，$\\Sigma_t$是包含$W$矩阵前$t$个singular value的$t\\times t$对角矩阵，$V$是由$W$前$t$个right-singular vectors组成的$v\\times t$矩阵。Truncated SVD可以将参数从$uv$降到$t(u+v)$。</p>\n<p>这里值得一提的有两点：</p>\n<ol>\n<li>$conv_1$ layers feature map通常都足够地general，并且task-independent。因此可以直接用来抽feature就行。</li>\n<li>Region Proposal的数量对Detection的性能并没有什么太大影响 (关键还是看feature啊！以及sampling mini-batch的时候正负样本的不均衡问题，详情请参阅kaiming He大神的<a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Focal Loss</a>)。</li>\n</ol>\n<h2 id=\"Faster-RCNN\"><a href=\"#Faster-RCNN\" class=\"headerlink\" title=\"Faster RCNN\"></a>Faster RCNN</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">Faster r-cnn: Towards real-time object detection with region proposal networks</a></p>\n</blockquote>\n<p>Faster RCNN也是Object Detection领域里一个非常具有代表性的工作，一个最大的改进就是Region Proposal Network(RPN)，RPN究竟神奇在什么地方呢？我们先来回顾一下RCNN–SPP–Fast RCNN，这些都属于two-stage detector，什么意思呢？就是先利用<font color=\"red\">Selective Search</font>生成2000个Region Proposal，然后再将其转化为一个机器学习中的分类问题。而<font color=\"red\">Selective Search</font>实际上是非常低效的，RPN则很好地完善了这一点，即直接从整个Network Architecture里生成Region Proposal。RPN是一种全卷积网络，它可以同时预测object bounds，以及objectness score。因为RPN的Feature是和Detection Network共享的，所以整个Region Proposal的生成几乎是cost-free的。所以，这也就是Faster RCNN中<strong>Faster</strong>一词的由来。</p>\n<h3 id=\"What-is-Faster-RCNN\"><a href=\"#What-is-Faster-RCNN\" class=\"headerlink\" title=\"What is Faster RCNN?\"></a>What is Faster RCNN?</h3><p>$$<br>Faster RCNN = Fast RCNN + RPN<br>$$<br>按照惯例，一个算法的提出显然是为了解决之前算法的不足。那之前的算法都有什么问题呢？<br>如果对之前的detector熟悉的话，shared features between proposals已经被解决，但是<font color=\"red\">Region Proposal的生成变成了最大的计算瓶颈</font>。这便是RPN产生的缘由。</p>\n<p>作者注意到，conv feature maps used by region-based detectors也可以被用于生成region proposals。在这些conv features顶端，<font color=\"red\">通过添加两个额外的卷积层来构造RPN：一个conv layer用于encode每个conv feature map position到一个低维向量(256-d)；另一个conv layer在每一个conv feature map position中输出k个region proposal with various scales and aspect ratios的objectness score和regression bounds。</font>下面重点介绍一下RPN。</p>\n<h3 id=\"Region-Proposal-Network\"><a href=\"#Region-Proposal-Network\" class=\"headerlink\" title=\"Region Proposal Network\"></a>Region Proposal Network</h3><p>RPN是一个全卷积网络，可以接受任意尺寸的image作为输入，并且输出一系列object proposals以及其对应的objectness score。那么RPN是如何生成region proposals的呢？</p>\n<p>首先，在最后一个shared conv feature map上slide一个小网络，这个小网络全连接到input conv feature map上$n\\times n$的spatial window。而每一个sliding window映射到一个256-d的feature vector，这个feature vector输入到两个fully connected layers–一个做box regression，另一个做box classification。值得注意的是，因为这个小网络是以sliding window的方式操作的，所以fully-connected layers在所有的spatial locations都是共享的。该结构由一个$n\\times n$ conv layer followed by two sibling $1\\times 1$ conv layers组成。</p>\n<h4 id=\"Translation-Invariant-Anchors\"><a href=\"#Translation-Invariant-Anchors\" class=\"headerlink\" title=\"Translation-Invariant Anchors\"></a>Translation-Invariant Anchors</h4><p>对于每个sliding window location，同时预测$k$个region proposals，所以regression layer有$4k$个encoding了$k$个bbox坐标的outputs。classification layer有$2k$个scores(每个region proposal估计object/non-object的概率)。</p>\n<blockquote>\n<p>The $k$ proposals are parameterized relative to $k$ reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio.</p>\n</blockquote>\n<p>文章中采用3个scales和3个aspect ratios，所以每个sliding position一共得到$k=9$个anchors。所以对于每个$W\\times H$的conv feature map一共产生$WHk$个anchor。这种方法一个很重要的性质就是<strong>它是translation invariant</strong>。</p>\n<h4 id=\"A-Loss-Function-for-Learning-Region-Proposals\"><a href=\"#A-Loss-Function-for-Learning-Region-Proposals\" class=\"headerlink\" title=\"A Loss Function for Learning Region Proposals\"></a>A Loss Function for Learning Region Proposals</h4><p>训练RPN时，我们为每一个anchor分配binary class(即是不是一个object)。我们为以下这两类anchors分配positive label:</p>\n<ol>\n<li>与groundtruth bbox有最高IoU的anchor</li>\n<li>与任意groundtruth bbox $IoU\\geq 0.7$的anchor</li>\n</ol>\n<p>同时，可能出现一个groundtruth bbox分配到多个anchor的情况，我们将与所有groundtruth bbox $IoU\\leq 0.3$的anchor设为negative anchor。而那些既不是positive又不是negative的anchor则对training过程没有用处。然后，Faster RCNN的Loss Function就变成：<br>$$<br>L(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^{\\star}) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^{\\star} L_{reg}(t_i,t_i^{\\star})<br>$$<br>$p_i$是anchor $i$ 被预测为是一个object的概率，若anchor为positive，则groundtruth label $p_i^$为1；若anchor为negative则为0；$t_i$是包含4个预测bbox坐标点的向量，$t_i^{\\star}$是groundtruth positive anchor坐标点的向量。$L_{cls}$是二分类的Log Loss(object VS non-object)。对于regression loss，文章使用$L_{reg}(t_i,t_i^{\\star})=R(t_i-t_i^{\\star})$，其中$R$是Smooth L1 Loss(和Fast RCNN中一样)。$p_i^{\\star} L_{reg}$表示<font color=\"red\">仅仅在positive anchor ($p_i^{\\star}=1$)时才被激活，否则($p_i^{\\star}=0$)不激活</font>。</p>\n<p>Bounding Box Regression依旧是采用之前的pipeline：<br>$$<br>t_x=\\frac{x-x_a}{w_a},t_y=\\frac{y-y_a}{h_a},t_w=log(\\frac{w}{w_a}),t_h=log(\\frac{h}{h_a})<br>$$</p>\n<p>$$<br>t_x^{\\star}=\\frac{x^{\\star}-x_a}{w_a},t_y^{\\star}=\\frac{y^{\\star}-y_a}{h_a},t_w^{\\star}=log(\\frac{w^{\\star}}{w_a}),t_h^{\\star}=log(\\frac{h^{\\star}}{h_a})<br>$$</p>\n<blockquote>\n<p>In our formulation, the features used for regression are of the same spatial size $(n\\times n)$ on the feature maps. <strong>To account for varying sizes, a set of $k$ bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the $k$ regressors do not share weights</strong>. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.</p>\n</blockquote>\n<h4 id=\"Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\"><a href=\"#Sharing-Convolutional-Features-for-Region-Proposal-and-Object-Detection\" class=\"headerlink\" title=\"Sharing Convolutional Features for Region Proposal and Object Detection\"></a>Sharing Convolutional Features for Region Proposal and Object Detection</h4><ol>\n<li>Fine-tune在ImageNet上Pretrain的RPN来完成region proposal task。</li>\n<li>利用RPN生成的region proposal来train Fast RCNN。注意在这一步骤中RPN和Faster RCNN没有共享卷积层。</li>\n<li>利用detector network来初始化RPN训练，但是我们fix shared conv layers，仅仅fine-tune单独属于RPN的层。注意在这一步骤中RPN和Fast RCNN共享了卷积层。</li>\n<li>Fix所有shared conv layers，fine-tune Fast RCNN的fc layers。至此，RPN和Fast RCNN共享卷积层，并且形成了一个unified network。</li>\n</ol>\n<h2 id=\"SSD\"><a href=\"#SSD\" class=\"headerlink\" title=\"SSD\"></a>SSD</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">SSD: Single Shot MultiBox Detector</a></p>\n</blockquote>\n<p>SSD是one-stage detector里一个非常著名的算法，那什么叫做one-stage和two-stage呢？回想一下，从DL Detector发展到现在，我们之前介绍的RCNN/SSP/Fast RCNN/Faster RCNN等，都是属于two-stage detectors，意思就是说<strong>第一步需要生成region proposals，第二步再将整个detection转化为对这些region proposals的classification问题来做</strong>。那所谓的one-stage detection就自然是不需要生成region proposals了，而是直接输出bbox了。Faster RCNN里面作者已经分析了，two-stage detection为啥慢？很大原因就是因为region proposal generation太慢了(例如Selective Search算法)，所以提出了RPN来辅助生成region proposals。</p>\n<h3 id=\"What-is-SSD\"><a href=\"#What-is-SSD\" class=\"headerlink\" title=\"What is SSD?\"></a>What is SSD?</h3><p>SSD最主要的改进就是<strong>使用了一个小的Convolution Filter来预测object category和bbox offset</strong>。那如何处理多尺度问题呢？SSD采取的策略是将这些conv filter应用到多个feature map上，来使得整个模型对Scale Invariant。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD.png\" alt=\"SSD Framework\"></p>\n<h3 id=\"Details-of-SSD\"><a href=\"#Details-of-SSD\" class=\"headerlink\" title=\"Details of SSD\"></a>Details of SSD</h3><p>SSD主要部件如下：一个DCNN用来提取feature，产生fixed-size的bbox以及这些bbox中每个类的presence score，然后NMS用来输出最后的检测结果。Feature Extraction部分和普通的分类DCNN没啥太大的区别，作者在后面新添加了新的结构：</p>\n<ol>\n<li><strong>Multi-scale feature maps for detection</strong>: 在base feature extraction network之后额外添加新的conv layers(所以得到了multi-scale的feature maps)，来使得模型可以处理multi-scale的detection。</li>\n<li><strong>Convolutional predictors for detection</strong>: 每一个新添加的feature layer可以基于<code>small conv filters</code>产生fixed-size detection predictions。</li>\n<li><strong>Default boxes and aspect ratios</strong>: 对于每个feature map cell，算法给出cell中default box的relative offset，以及class-score(表示在每个box中一个class instance出现的概率)。具体的，对于每个given location的$k$个box，产生4个bbox offset和$c$个class score，这样就对每个<code>feature map location</code>上产生了$(c+4)k$个filters，那么对于一个$m\\times n$的<code>feature map</code>，则产生$(c+4)kmn$个output。这个做法和Faster RCNN中的anchor box有点类似，但是<strong>SSD中将它用到了多个不同resolution的feature map上，因此多个feature map的不同default box shape使得我们可以很高效地给出output box shape</strong>。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/SSD_YOLO.png\" alt=\"SSD and YOLO\"></p>\n<h4 id=\"Training-of-SSD\"><a href=\"#Training-of-SSD\" class=\"headerlink\" title=\"Training of SSD\"></a>Training of SSD</h4><p>前面已经提到了，SSD会在default box周围生成一系列varied location/ratio/scale的boxes，那么到底哪一个box才是和groundtruth真正匹配的呢？作者采用了这样的一个Matching Strategy: 对于每一个groundtruth box，我们从default boxes中选择不同location/ratio/scale的boxes，然后计算它们和任意一个groundtruth box的Jaccard Overlap，并挑选出超过阈值的boxes。那么SSD最终的Loss就可以写成：<br>$$<br>L(x,c,l,g)=\\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))<br>$$<br>其中$N$代表matched default boxes的数量，Localisation Loss和Faster RCNN一样也选择了Smooth L1。$x_{ij}^p=\\{0,1\\}$为第$i$个default box是否和第$j$个groundtruth box匹配的indicator。<br>$$<br>L_{loc}(x,l,g)=\\sum_{i\\in Pos} \\sum_{m\\in \\{cx,cy,w,h\\}}x_{ij}^k smoothL_1(l_i^m-\\hat{g}_j^m)<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w<br>$$</p>\n<p>$$<br>\\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h<br>$$</p>\n<p>$$<br>\\hat{g}_j^w=log(\\frac{g_j^w}{d_i^w})<br>$$</p>\n<p>$$<br>\\hat{g}_j^h=log(\\frac{g_j^h}{d_i^h})<br>$$</p>\n<p>Confidence Loss采用Softmax Loss:<br>$$<br>L_{conf}(x, c)=-\\sum_{i\\in Pos}^N x_{ij}^p log(\\hat{c}_i^0)-\\sum_{\\in Neg}log(\\hat{c}_i^0)<br>$$<br>其中，$\\hat{c}_i^p=\\frac{exp(c_i^p)}{\\sum_p exp(c_i^p)}$</p>\n<p>SSD在检测large object时效果很好，但是在检测small object时则效果比较差，这是因为在higher layers，feature map包含的small object信息太少，可通过将input size由$300\\times 300$改为$512\\times 512$，<strong>Zoom Data Augmentation</strong>(即采用zoom in来生成large objects, zoom out来生成small objects)来进行一定程度的缓解。</p>\n<h2 id=\"Light-head-RCNN\"><a href=\"#Light-head-RCNN\" class=\"headerlink\" title=\"Light-head RCNN\"></a>Light-head RCNN</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>在介绍Light-head RCNN之前，我们先来回顾一下常见的two-stage detector为什么是heavy-head？作者发现two-stage detector之所以慢，就是因为two-stage detector在RoI Warp前/后 会进行非常密集的计算，例如Faster RCNN包含2个fully connected layers做nRoI Recognition，RFCN会生成很大的score maps。所以无论你的backbone network使用了多么精巧的小网络结构，但是总体速度还是提升不上去。所以针对这个问题，作者在本文提出了<code>light-head</code> RCNN。所谓的<code>light-head</code>，其实说白了就是<code>使用thin feature map + cheap RCNN subnet (pooling和单层fully connected layer)</code>。</p>\n<p>大家都知道，two-stage detector，其实是将detection问题转化为一个classification问题来完成的。也就是说，在第一个stage，模型会生成很多region proposal (此为<code>body</code>)，然后在第二个stage对这些region proposal进行分类 (此为<code>head</code>)。通常，two-stage detector的accuracy要比one-stage detector高的，所以为了accuracy，head往往会设计得非常heavy。Light-head RCNN是这么做的：</p>\n<blockquote>\n<p>In this paper, we propose a light-head design to build an efficient yet accurate two-stage detector. Specifically, we apply a large-kernel separable convolution to produce “thin” feature maps with small channel number ($\\alpha \\times p\\times p$ is used in our experiments and $\\alpha\\leq 10$). This design greatly reduces the computation of following RoI-wise subnetwork and makes the detection system memory-friendly. A cheap single fully-connected layer is attached to the pooling layer, which well exploits the feature representation for classification and regression.</p>\n</blockquote>\n<h3 id=\"Delve-Into-Light-Head-RCNN\"><a href=\"#Delve-Into-Light-Head-RCNN\" class=\"headerlink\" title=\"Delve Into Light-Head RCNN\"></a>Delve Into Light-Head RCNN</h3><h4 id=\"RCNN-Subnet\"><a href=\"#RCNN-Subnet\" class=\"headerlink\" title=\"RCNN Subnet\"></a>RCNN Subnet</h4><blockquote>\n<p>Faster R-CNN adopts a powerful R-CNN which utilizes two large fully connected layers or whole Resnet stage 5 [28, 29] as a second stage classifier, which is beneficial to the detection performance. Therefore Faster R-CNN and its extensions perform leading accuracy in the most challenging benchmarks like COCO. However, the computation could be intensive especially when the number of object proposals is large. To speed up RoI-wise subnet, <strong>R-FCN first produces a set of score maps for each region, whose channel number will be $classes_num\\times p \\times p$ ($p$ is the followed pooling size), and then pool along each RoI and average vote the final prediction. Using a computation-free R-CNN subnet, R-FCN gets comparable results by involving more computation on RoI shared score maps generation</strong>.</p>\n</blockquote>\n<p>Faster RCNN虽然在RoI Classification上表现得很好，但是它需要global average pooling来减小第一个fully connected layer的计算量，<code>而GAP会影响spatial localization</code>。此外，Faster RCNN对每一个RoI都要feedforward一遍RCNN subnet，所以在当proposal的数量很大时，效率就非常低了。</p>\n<h4 id=\"Thin-Feature-Maps-for-RoI-Warping\"><a href=\"#Thin-Feature-Maps-for-RoI-Warping\" class=\"headerlink\" title=\"Thin Feature Maps for RoI Warping\"></a>Thin Feature Maps for RoI Warping</h4><p>在feed region proposal到RCNN subnet之前，用RoI warping来使得得到fixed shape的feature maps。本文提出的light-head产生了一系列<code>thin feature maps</code>，然后再接RoI Pooling层。在实验中，作者发现<code>RoI warping on thin feature maps</code>不仅仅提高了精度，而且节省了training和inference的时间。而且，如果直接应用RoI pooling到thin feature maps上，一方面模型可以减少计算量，另一方面可以去掉GAP来保留spatial information。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-detection/light_head_rcnn.jpg\" alt=\"Light Head RCNN\"></p>\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>作者在实验中发现，regression loss比classification loss要小很多，所以<code>将regression loss的权重进行double来balance multi-task training</code>。</p>\n<h2 id=\"YOLO-v1\"><a href=\"#YOLO-v1\" class=\"headerlink\" title=\"YOLO v1\"></a>YOLO v1</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">You Only Look Once: Unified, Real-Time Object Detection</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Girshick, Ross, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Rich feature hierarchies for accurate object detection and semantic segmentation.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>He, Kaiming, et al. <a href=\"https://arxiv.org/pdf/1406.4729v4.pdf\" target=\"_blank\" rel=\"noopener\">“Spatial pyramid pooling in deep convolutional networks for visual recognition.”</a> European conference on computer vision. Springer, Cham, 2014.</li>\n<li>Girshick, Ross. <a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Fast r-cnn.”</a> Proceedings of the IEEE international conference on computer vision. 2015.</li>\n<li>Ross, Tsung-Yi Lin Priya Goyal, and Girshick Kaiming He Piotr Dollár. <a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Focal Loss for Dense Object Detection.”</a></li>\n<li>Ren, Shaoqing, et al. <a href=\"http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf\" target=\"_blank\" rel=\"noopener\">“Faster r-cnn: Towards real-time object detection with region proposal networks.”</a> Advances in neural information processing systems. 2015.</li>\n<li>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., &amp; Berg, A. C. (2016, October). <a href=\"https://arxiv.org/pdf/1512.02325v5.pdf\" target=\"_blank\" rel=\"noopener\">Ssd: Single shot multibox detector</a>. In European conference on computer vision (pp. 21-37). Springer, Cham.</li>\n<li>Redmon, Joseph, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">“You only look once: Unified, real-time object detection.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</li>\n<li>Li Z, Peng C, Yu G, et al. <a href=\"https://arxiv.org/pdf/1711.07264v2.pdf\" target=\"_blank\" rel=\"noopener\">Light-head r-cnn: In defense of two-stage object detector</a>[J]. arXiv preprint arXiv:1711.07264, 2017.</li>\n</ol>\n"},{"title":"[CV] Face Recognition","date":"2018-09-03T10:08:11.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Face Recognition"],"_content":"## Introduction\n人脸识别(Face Recognition)是工业界和学术界都非常火热的一个方向，并且已经催生出许多成功的应用落地场景，比如刷脸支付、安检等。而Face Recognition最大的突破也是由Deep Learning Architecture + 一系列精巧的Loss Function带来的。本文旨在对Face Recognition领域里的一些经典Paper进行梳理，详情请参阅Reference部分的Paper原文。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n\n## Face Recognition as N-Categories Classification Problems\n在Metric Learning里的一系列优秀的Loss还未被引入Face Recognition之前，Face Verification/Identification一个非常直观的想法就是直接train 一个 n-categories classifier。然后将最后一层的输出作为input image的特征，再选取合适的distance metric来决定这两张脸是否属于同一个人。这种做法的一些经典工作就是[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)。这篇Paper发表在CVPR2014上面，属于非常古董的模型了，鉴于近年来已经几乎不这么做了，所以本文仅仅象征性地回顾一下这几篇具有代表性的Paper。我们会把讨论重心放在Metric Learning的一系列Loss上。\n\n> Paper: [Deep learning face representation from predicting 10,000 classes](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n\n这篇Paper其实idea非常简单，就是把Face Recognition问题转换为一个$N$-类Classification问题，其中$N$代表dataset中identity的数量。为了增强feature representation能力，作者也将各个facial region的特征做concatenation。[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)的Architecture如下：\n![DeepID](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg)\n\n注意到DeepID的输入部分，除了后一个conv layer的feature map之外，还有前一个max-pooling的输出，这样做的好处在于Network能够获取multi-scale的input，也可以视为一种skipping layer(将lower level feature和higher level feature做feature fusion)。那么最后一个hidden layer的输入就是这样子的：\n$$\ny_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)\n$$\n\n另外，作者在实验中意识到，<font color=\"red\">随着identity 数量的增加，整个网络的feature representation learning和performance都会随之增加</font>。[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)在LFW上达到了97.45%的精度。\n\n\n## FaceNet\nGoogle的[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)是我个人认为在Face Recognition领域里一篇非常insightful的Paper，通过引入triplets并直接在**Euclidean Space**作为feature vector度量，[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)在LFW上达到了99.63%的效果。\n\n> Paper: [Facenet: A unified embedding for face recognition and clustering.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)\n\n### What is FaceNet?\n[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)的Idea其实也比较简单。简而言之呢，就是通过DNN学习一种**Euclidean Embedding**，来使得inter-class更加compact，inter-class更加地separable，这就是本文的核心角色——[Triplet Loss](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)。\n\n![FaceNet](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg)\n\n### Details of Triplet Loss\nTriplet Embedding是通过一个Network将输入$x$映射到$d$-维输出$f(x)\\in \\mathbb{R}^d$，文章中将其做了一个Normalization，即$||f(x)||_2=1$。Triplet Loss的目的就是为了找到一个person的anchor $x_i^a$，使得它与相同identity的positive images $x_i^p$更加地close，而与不同identity的negative images更加地separate。写成公式就是：\n$$\n||x_i^a-x_i^p||_2^2 + \\alpha < ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}\n$$\n$\\alpha$就代表margin，那么Minimization of Triplet Loss就是：\n$$\n\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+\n$$\nTriplet Loss确定了，那么下一步就是如何选择合适的Triplets。\n\n### Triplet Selection\n<font color=\"red\">为了保证快速收敛，我们需要violate triplet的constraint，即挑选anchor $x_i^a$，来挑选hard positive $x_i^p$来满足$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$，以及hard negative $x_i^n$来满足$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>。\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：读者仔细体会一下这里和triplet loss definition的区别，为啥是相反的？这里可视为一种[hard negative mining](http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf)。\n\n在整个training set上计算$argmax$和$argmin$是不太现实的，文中采取了两个做法：\n* 训练每$n$步离线来生成triplets，使用most recent network checkpoint和dataset的子集来计算$argmax$和$argmin$。\n* 在线生成triplets，这种做法可视为在一个mini-batch选择hard positive/negative exemplars。\n\nSelecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:\n$$\n||f(x_i^a)-f(x_i^p)||_2^2 < ||f(x_i^a)-f(x_i^n)||_2^2\n$$\n<font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n### Experiments\n对于Face Verification Task，判断两张图是否为一个人，我们仅需比较这两个特征向量的squared $L_2$ distance $D(x_i,x_j)$是否超过了某个阈值即可。\n* True Accepts代表face pairs $(i,j)被正确分类到同一个identity$:\n  $TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n* False Accepts代表face pairs $(i,j)被错误分类到同一个identity$:\n  $FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n\n\n## Center Loss\n> Paper: [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf)\n\nFace Recognition领域，除了设计更加优秀的Network Architecture，也有另一个方向的工作是在设计更加优秀的Loss。[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)就是其中之一。和FaceNet中使用[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)的目的一样，[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)依然是为了使得intra-class more compact and inter-class more separate。本文就来简要介绍一下[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)。\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)通过学习每一个类的中心向量，来同时更新这个center，以及最小化deep features和其对应class的centers之间的距离。CNN的Loss为Softmax Loss与[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)的加权。Softmax Loss仅仅会让不同的class分开，但[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)还会使得相同class的deep features更加靠近类的centers。通过这种joint supervision(Softmax + Center Loss)，不仅仅inter-class的difference被加大了，而且intra-class的variantions也被减小了。因此便可以学得更加discriminative的feature representation。这便是[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)的大致idea。\n\n### What is Center Loss?\nSoftmax Loss是这样的：\n$$\n\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}\n$$\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)则是这样的：\n$$\n\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n为了更新center vector $c_{y_i}$，文中采取的做法是在每一个mini-batch中进行更新(而不是在整个training set中更新)，然后center vector $c_{y_i}$的计算为相关class feature的平均值。此外，为了避免mislabeled samples，我们使用$\\alpha$来控制center vector的learning rate。Center Loss的梯度求导可以表示为：\n$$\n\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}\n$$\n\n$$\n\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}\n$$\n\nwhere $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.\n$$\n\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n\n整个学习算法如下：\n![Learning of Center Loss](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg)\n\n网络结构如下：\n![Center Loss Architecture](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg)\n\nCenter Loss的好处在于：\n* Joint supervision of Softmax Loss and Center Loss能够大大加强DCNN的feature learning能力。\n* 其他Metric Learning的Loss例如[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf), Contractive Loss等pairs selection是非常麻烦的一件事情，但是Center Loss则不需要复杂的triplet pairs selection。\n\n网络学习完成，在做Face Verification/Identification时，<font color=\"red\">第一个 FC Layers的feature被当作特征，同时，我们也将水平翻转图片的feature进行concatenation，作为最终的face feature，PCA降维之后，Cosine Distance, Nearest Neighbor and Threshold comparison用来作为判断是否为同一个人的依据</font>。\n\n\n## NormFace\n> Paper: [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf)\n\n\n\n\n## Reference\n1. Sun, Yi, Xiaogang Wang, and Xiaoou Tang. [\"Deep learning face representation from predicting 10,000 classes.\"](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [\"Facenet: A unified embedding for face recognition and clustering.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n3. Wen Y, Zhang K, Li Z, Qiao Y. [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf). In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.\n4. Wang F, Xiang X, Cheng J, Yuille AL. [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf). InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.\n","source":"_posts/cv-face-rec.md","raw":"---\ntitle: \"[CV] Face Recognition\"\ndate: 2018-09-03 18:08:11\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Recognition\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Face Recognition\n---\n## Introduction\n人脸识别(Face Recognition)是工业界和学术界都非常火热的一个方向，并且已经催生出许多成功的应用落地场景，比如刷脸支付、安检等。而Face Recognition最大的突破也是由Deep Learning Architecture + 一系列精巧的Loss Function带来的。本文旨在对Face Recognition领域里的一些经典Paper进行梳理，详情请参阅Reference部分的Paper原文。\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n\n## Face Recognition as N-Categories Classification Problems\n在Metric Learning里的一系列优秀的Loss还未被引入Face Recognition之前，Face Verification/Identification一个非常直观的想法就是直接train 一个 n-categories classifier。然后将最后一层的输出作为input image的特征，再选取合适的distance metric来决定这两张脸是否属于同一个人。这种做法的一些经典工作就是[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)。这篇Paper发表在CVPR2014上面，属于非常古董的模型了，鉴于近年来已经几乎不这么做了，所以本文仅仅象征性地回顾一下这几篇具有代表性的Paper。我们会把讨论重心放在Metric Learning的一系列Loss上。\n\n> Paper: [Deep learning face representation from predicting 10,000 classes](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n\n这篇Paper其实idea非常简单，就是把Face Recognition问题转换为一个$N$-类Classification问题，其中$N$代表dataset中identity的数量。为了增强feature representation能力，作者也将各个facial region的特征做concatenation。[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)的Architecture如下：\n![DeepID](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg)\n\n注意到DeepID的输入部分，除了后一个conv layer的feature map之外，还有前一个max-pooling的输出，这样做的好处在于Network能够获取multi-scale的input，也可以视为一种skipping layer(将lower level feature和higher level feature做feature fusion)。那么最后一个hidden layer的输入就是这样子的：\n$$\ny_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)\n$$\n\n另外，作者在实验中意识到，<font color=\"red\">随着identity 数量的增加，整个网络的feature representation learning和performance都会随之增加</font>。[DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)在LFW上达到了97.45%的精度。\n\n\n## FaceNet\nGoogle的[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)是我个人认为在Face Recognition领域里一篇非常insightful的Paper，通过引入triplets并直接在**Euclidean Space**作为feature vector度量，[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)在LFW上达到了99.63%的效果。\n\n> Paper: [Facenet: A unified embedding for face recognition and clustering.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)\n\n### What is FaceNet?\n[FaceNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)的Idea其实也比较简单。简而言之呢，就是通过DNN学习一种**Euclidean Embedding**，来使得inter-class更加compact，inter-class更加地separable，这就是本文的核心角色——[Triplet Loss](https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)。\n\n![FaceNet](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg)\n\n### Details of Triplet Loss\nTriplet Embedding是通过一个Network将输入$x$映射到$d$-维输出$f(x)\\in \\mathbb{R}^d$，文章中将其做了一个Normalization，即$||f(x)||_2=1$。Triplet Loss的目的就是为了找到一个person的anchor $x_i^a$，使得它与相同identity的positive images $x_i^p$更加地close，而与不同identity的negative images更加地separate。写成公式就是：\n$$\n||x_i^a-x_i^p||_2^2 + \\alpha < ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}\n$$\n$\\alpha$就代表margin，那么Minimization of Triplet Loss就是：\n$$\n\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+\n$$\nTriplet Loss确定了，那么下一步就是如何选择合适的Triplets。\n\n### Triplet Selection\n<font color=\"red\">为了保证快速收敛，我们需要violate triplet的constraint，即挑选anchor $x_i^a$，来挑选hard positive $x_i^p$来满足$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$，以及hard negative $x_i^n$来满足$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>。\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：读者仔细体会一下这里和triplet loss definition的区别，为啥是相反的？这里可视为一种[hard negative mining](http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf)。\n\n在整个training set上计算$argmax$和$argmin$是不太现实的，文中采取了两个做法：\n* 训练每$n$步离线来生成triplets，使用most recent network checkpoint和dataset的子集来计算$argmax$和$argmin$。\n* 在线生成triplets，这种做法可视为在一个mini-batch选择hard positive/negative exemplars。\n\nSelecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:\n$$\n||f(x_i^a)-f(x_i^p)||_2^2 < ||f(x_i^a)-f(x_i^n)||_2^2\n$$\n<font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n### Experiments\n对于Face Verification Task，判断两张图是否为一个人，我们仅需比较这两个特征向量的squared $L_2$ distance $D(x_i,x_j)$是否超过了某个阈值即可。\n* True Accepts代表face pairs $(i,j)被正确分类到同一个identity$:\n  $TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n* False Accepts代表face pairs $(i,j)被错误分类到同一个identity$:\n  $FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$\n\n\n## Center Loss\n> Paper: [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf)\n\nFace Recognition领域，除了设计更加优秀的Network Architecture，也有另一个方向的工作是在设计更加优秀的Loss。[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)就是其中之一。和FaceNet中使用[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf)的目的一样，[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)依然是为了使得intra-class more compact and inter-class more separate。本文就来简要介绍一下[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)。\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)通过学习每一个类的中心向量，来同时更新这个center，以及最小化deep features和其对应class的centers之间的距离。CNN的Loss为Softmax Loss与[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)的加权。Softmax Loss仅仅会让不同的class分开，但[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)还会使得相同class的deep features更加靠近类的centers。通过这种joint supervision(Softmax + Center Loss)，不仅仅inter-class的difference被加大了，而且intra-class的variantions也被减小了。因此便可以学得更加discriminative的feature representation。这便是[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)的大致idea。\n\n### What is Center Loss?\nSoftmax Loss是这样的：\n$$\n\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}\n$$\n\n[Center Loss](https://ydwen.github.io/papers/WenECCV16.pdf)则是这样的：\n$$\n\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n为了更新center vector $c_{y_i}$，文中采取的做法是在每一个mini-batch中进行更新(而不是在整个training set中更新)，然后center vector $c_{y_i}$的计算为相关class feature的平均值。此外，为了避免mislabeled samples，我们使用$\\alpha$来控制center vector的learning rate。Center Loss的梯度求导可以表示为：\n$$\n\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}\n$$\n\n$$\n\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}\n$$\n\nwhere $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.\n$$\n\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2\n$$\n\n整个学习算法如下：\n![Learning of Center Loss](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg)\n\n网络结构如下：\n![Center Loss Architecture](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg)\n\nCenter Loss的好处在于：\n* Joint supervision of Softmax Loss and Center Loss能够大大加强DCNN的feature learning能力。\n* 其他Metric Learning的Loss例如[Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf), Contractive Loss等pairs selection是非常麻烦的一件事情，但是Center Loss则不需要复杂的triplet pairs selection。\n\n网络学习完成，在做Face Verification/Identification时，<font color=\"red\">第一个 FC Layers的feature被当作特征，同时，我们也将水平翻转图片的feature进行concatenation，作为最终的face feature，PCA降维之后，Cosine Distance, Nearest Neighbor and Threshold comparison用来作为判断是否为同一个人的依据</font>。\n\n\n## NormFace\n> Paper: [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf)\n\n\n\n\n## Reference\n1. Sun, Yi, Xiaogang Wang, and Xiaoou Tang. [\"Deep learning face representation from predicting 10,000 classes.\"](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.\n2. Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [\"Facenet: A unified embedding for face recognition and clustering.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n3. Wen Y, Zhang K, Li Z, Qiao Y. [A discriminative feature learning approach for deep face recognition](https://ydwen.github.io/papers/WenECCV16.pdf). In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.\n4. Wang F, Xiang X, Cheng J, Yuille AL. [Normface: L2 hypersphere embedding for face verification](https://arxiv.org/pdf/1704.06369v4.pdf). InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.\n","slug":"cv-face-rec","published":1,"updated":"2018-10-02T04:19:42.047Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03br0007608w4uz8pp8x","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>人脸识别(Face Recognition)是工业界和学术界都非常火热的一个方向，并且已经催生出许多成功的应用落地场景，比如刷脸支付、安检等。而Face Recognition最大的突破也是由Deep Learning Architecture + 一系列精巧的Loss Function带来的。本文旨在对Face Recognition领域里的一些经典Paper进行梳理，详情请参阅Reference部分的Paper原文。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"Face-Recognition-as-N-Categories-Classification-Problems\"><a href=\"#Face-Recognition-as-N-Categories-Classification-Problems\" class=\"headerlink\" title=\"Face Recognition as N-Categories Classification Problems\"></a>Face Recognition as N-Categories Classification Problems</h2><p>在Metric Learning里的一系列优秀的Loss还未被引入Face Recognition之前，Face Verification/Identification一个非常直观的想法就是直接train 一个 n-categories classifier。然后将最后一层的输出作为input image的特征，再选取合适的distance metric来决定这两张脸是否属于同一个人。这种做法的一些经典工作就是<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>。这篇Paper发表在CVPR2014上面，属于非常古董的模型了，鉴于近年来已经几乎不这么做了，所以本文仅仅象征性地回顾一下这几篇具有代表性的Paper。我们会把讨论重心放在Metric Learning的一系列Loss上。</p>\n<blockquote>\n<p>Paper: <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes</a></p>\n</blockquote>\n<p>这篇Paper其实idea非常简单，就是把Face Recognition问题转换为一个$N$-类Classification问题，其中$N$代表dataset中identity的数量。为了增强feature representation能力，作者也将各个facial region的特征做concatenation。<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>的Architecture如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg\" alt=\"DeepID\"></p>\n<p>注意到DeepID的输入部分，除了后一个conv layer的feature map之外，还有前一个max-pooling的输出，这样做的好处在于Network能够获取multi-scale的input，也可以视为一种skipping layer(将lower level feature和higher level feature做feature fusion)。那么最后一个hidden layer的输入就是这样子的：<br>$$<br>y_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)<br>$$</p>\n<p>另外，作者在实验中意识到，<font color=\"red\">随着identity 数量的增加，整个网络的feature representation learning和performance都会随之增加</font>。<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>在LFW上达到了97.45%的精度。</p>\n<h2 id=\"FaceNet\"><a href=\"#FaceNet\" class=\"headerlink\" title=\"FaceNet\"></a>FaceNet</h2><p>Google的<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>是我个人认为在Face Recognition领域里一篇非常insightful的Paper，通过引入triplets并直接在<strong>Euclidean Space</strong>作为feature vector度量，<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>在LFW上达到了99.63%的效果。</p>\n<blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a></p>\n</blockquote>\n<h3 id=\"What-is-FaceNet\"><a href=\"#What-is-FaceNet\" class=\"headerlink\" title=\"What is FaceNet?\"></a>What is FaceNet?</h3><p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>的Idea其实也比较简单。简而言之呢，就是通过DNN学习一种<strong>Euclidean Embedding</strong>，来使得inter-class更加compact，inter-class更加地separable，这就是本文的核心角色——<a href=\"https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg\" alt=\"FaceNet\"></p>\n<h3 id=\"Details-of-Triplet-Loss\"><a href=\"#Details-of-Triplet-Loss\" class=\"headerlink\" title=\"Details of Triplet Loss\"></a>Details of Triplet Loss</h3><p>Triplet Embedding是通过一个Network将输入$x$映射到$d$-维输出$f(x)\\in \\mathbb{R}^d$，文章中将其做了一个Normalization，即$||f(x)||_2=1$。Triplet Loss的目的就是为了找到一个person的anchor $x_i^a$，使得它与相同identity的positive images $x_i^p$更加地close，而与不同identity的negative images更加地separate。写成公式就是：<br>$$<br>||x_i^a-x_i^p||_2^2 + \\alpha &lt; ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}<br>$$<br>$\\alpha$就代表margin，那么Minimization of Triplet Loss就是：<br>$$<br>\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+<br>$$<br>Triplet Loss确定了，那么下一步就是如何选择合适的Triplets。</p>\n<h3 id=\"Triplet-Selection\"><a href=\"#Triplet-Selection\" class=\"headerlink\" title=\"Triplet Selection\"></a>Triplet Selection</h3><font color=\"red\">为了保证快速收敛，我们需要violate triplet的constraint，即挑选anchor $x_i^a$，来挑选hard positive $x_i^p$来满足$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$，以及hard negative $x_i^n$来满足$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>。<br>&gt; <a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：读者仔细体会一下这里和triplet loss definition的区别，为啥是相反的？这里可视为一种<a href=\"http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf\" target=\"_blank\" rel=\"noopener\">hard negative mining</a>。<br><br>在整个training set上计算$argmax$和$argmin$是不太现实的，文中采取了两个做法：<br><em> 训练每$n$步离线来生成triplets，使用most recent network checkpoint和dataset的子集来计算$argmax$和$argmin$。\n</em> 在线生成triplets，这种做法可视为在一个mini-batch选择hard positive/negative exemplars。<br><br>Selecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:<br>$$<br>||f(x_i^a)-f(x_i^p)||_2^2 &lt; ||f(x_i^a)-f(x_i^n)||_2^2<br>$$<br><font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>对于Face Verification Task，判断两张图是否为一个人，我们仅需比较这两个特征向量的squared $L_2$ distance $D(x_i,x_j)$是否超过了某个阈值即可。</p>\n<ul>\n<li>True Accepts代表face pairs $(i,j)被正确分类到同一个identity$:<br>$TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n<li>False Accepts代表face pairs $(i,j)被错误分类到同一个identity$:<br>$FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n</ul>\n<h2 id=\"Center-Loss\"><a href=\"#Center-Loss\" class=\"headerlink\" title=\"Center Loss\"></a>Center Loss</h2><blockquote>\n<p>Paper: <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a></p>\n</blockquote>\n<p>Face Recognition领域，除了设计更加优秀的Network Architecture，也有另一个方向的工作是在设计更加优秀的Loss。<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>就是其中之一。和FaceNet中使用<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>的目的一样，<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>依然是为了使得intra-class more compact and inter-class more separate。本文就来简要介绍一下<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>。</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>通过学习每一个类的中心向量，来同时更新这个center，以及最小化deep features和其对应class的centers之间的距离。CNN的Loss为Softmax Loss与<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>的加权。Softmax Loss仅仅会让不同的class分开，但<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>还会使得相同class的deep features更加靠近类的centers。通过这种joint supervision(Softmax + Center Loss)，不仅仅inter-class的difference被加大了，而且intra-class的variantions也被减小了。因此便可以学得更加discriminative的feature representation。这便是<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>的大致idea。</p>\n<h3 id=\"What-is-Center-Loss\"><a href=\"#What-is-Center-Loss\" class=\"headerlink\" title=\"What is Center Loss?\"></a>What is Center Loss?</h3><p>Softmax Loss是这样的：<br>$$<br>\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}<br>$$</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>则是这样的：<br>$$<br>\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$<br>为了更新center vector $c_{y_i}$，文中采取的做法是在每一个mini-batch中进行更新(而不是在整个training set中更新)，然后center vector $c_{y_i}$的计算为相关class feature的平均值。此外，为了避免mislabeled samples，我们使用$\\alpha$来控制center vector的learning rate。Center Loss的梯度求导可以表示为：<br>$$<br>\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}<br>$$</p>\n<p>$$<br>\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}<br>$$</p>\n<p>where $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.<br>$$<br>\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$</p>\n<p>整个学习算法如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg\" alt=\"Learning of Center Loss\"></p>\n<p>网络结构如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg\" alt=\"Center Loss Architecture\"></p>\n<p>Center Loss的好处在于：</p>\n<ul>\n<li>Joint supervision of Softmax Loss and Center Loss能够大大加强DCNN的feature learning能力。</li>\n<li>其他Metric Learning的Loss例如<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>, Contractive Loss等pairs selection是非常麻烦的一件事情，但是Center Loss则不需要复杂的triplet pairs selection。</li>\n</ul>\n<p>网络学习完成，在做Face Verification/Identification时，<font color=\"red\">第一个 FC Layers的feature被当作特征，同时，我们也将水平翻转图片的feature进行concatenation，作为最终的face feature，PCA降维之后，Cosine Distance, Nearest Neighbor and Threshold comparison用来作为判断是否为同一个人的依据</font>。</p>\n<h2 id=\"NormFace\"><a href=\"#NormFace\" class=\"headerlink\" title=\"NormFace\"></a>NormFace</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Sun, Yi, Xiaogang Wang, and Xiaoou Tang. <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">“Deep learning face representation from predicting 10,000 classes.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>Schroff, Florian, Dmitry Kalenichenko, and James Philbin. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Facenet: A unified embedding for face recognition and clustering.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n<li>Wen Y, Zhang K, Li Z, Qiao Y. <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a>. In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.</li>\n<li>Wang F, Xiang X, Cheng J, Yuille AL. <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a>. InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>人脸识别(Face Recognition)是工业界和学术界都非常火热的一个方向，并且已经催生出许多成功的应用落地场景，比如刷脸支付、安检等。而Face Recognition最大的突破也是由Deep Learning Architecture + 一系列精巧的Loss Function带来的。本文旨在对Face Recognition领域里的一些经典Paper进行梳理，详情请参阅Reference部分的Paper原文。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"Face-Recognition-as-N-Categories-Classification-Problems\"><a href=\"#Face-Recognition-as-N-Categories-Classification-Problems\" class=\"headerlink\" title=\"Face Recognition as N-Categories Classification Problems\"></a>Face Recognition as N-Categories Classification Problems</h2><p>在Metric Learning里的一系列优秀的Loss还未被引入Face Recognition之前，Face Verification/Identification一个非常直观的想法就是直接train 一个 n-categories classifier。然后将最后一层的输出作为input image的特征，再选取合适的distance metric来决定这两张脸是否属于同一个人。这种做法的一些经典工作就是<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>。这篇Paper发表在CVPR2014上面，属于非常古董的模型了，鉴于近年来已经几乎不这么做了，所以本文仅仅象征性地回顾一下这几篇具有代表性的Paper。我们会把讨论重心放在Metric Learning的一系列Loss上。</p>\n<blockquote>\n<p>Paper: <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">Deep learning face representation from predicting 10,000 classes</a></p>\n</blockquote>\n<p>这篇Paper其实idea非常简单，就是把Face Recognition问题转换为一个$N$-类Classification问题，其中$N$代表dataset中identity的数量。为了增强feature representation能力，作者也将各个facial region的特征做concatenation。<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>的Architecture如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/deepid.jpg\" alt=\"DeepID\"></p>\n<p>注意到DeepID的输入部分，除了后一个conv layer的feature map之外，还有前一个max-pooling的输出，这样做的好处在于Network能够获取multi-scale的input，也可以视为一种skipping layer(将lower level feature和higher level feature做feature fusion)。那么最后一个hidden layer的输入就是这样子的：<br>$$<br>y_j=max(0, \\sum_i x_i^1\\cdot w_{i,j}^1 + \\sum_i x_i^2\\cdot w_{i,j}^2 + b_j)<br>$$</p>\n<p>另外，作者在实验中意识到，<font color=\"red\">随着identity 数量的增加，整个网络的feature representation learning和performance都会随之增加</font>。<a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">DeepID</a>在LFW上达到了97.45%的精度。</p>\n<h2 id=\"FaceNet\"><a href=\"#FaceNet\" class=\"headerlink\" title=\"FaceNet\"></a>FaceNet</h2><p>Google的<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>是我个人认为在Face Recognition领域里一篇非常insightful的Paper，通过引入triplets并直接在<strong>Euclidean Space</strong>作为feature vector度量，<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>在LFW上达到了99.63%的效果。</p>\n<blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Facenet: A unified embedding for face recognition and clustering.</a></p>\n</blockquote>\n<h3 id=\"What-is-FaceNet\"><a href=\"#What-is-FaceNet\" class=\"headerlink\" title=\"What is FaceNet?\"></a>What is FaceNet?</h3><p><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">FaceNet</a>的Idea其实也比较简单。简而言之呢，就是通过DNN学习一种<strong>Euclidean Embedding</strong>，来使得inter-class更加compact，inter-class更加地separable，这就是本文的核心角色——<a href=\"https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/facenet.jpg\" alt=\"FaceNet\"></p>\n<h3 id=\"Details-of-Triplet-Loss\"><a href=\"#Details-of-Triplet-Loss\" class=\"headerlink\" title=\"Details of Triplet Loss\"></a>Details of Triplet Loss</h3><p>Triplet Embedding是通过一个Network将输入$x$映射到$d$-维输出$f(x)\\in \\mathbb{R}^d$，文章中将其做了一个Normalization，即$||f(x)||_2=1$。Triplet Loss的目的就是为了找到一个person的anchor $x_i^a$，使得它与相同identity的positive images $x_i^p$更加地close，而与不同identity的negative images更加地separate。写成公式就是：<br>$$<br>||x_i^a-x_i^p||_2^2 + \\alpha &lt; ||x_i^a-x_i^n||_2^2 \\quad \\forall (x_i^a,x_i^p,x_i^n)\\in \\mathcal{T}<br>$$<br>$\\alpha$就代表margin，那么Minimization of Triplet Loss就是：<br>$$<br>\\sum_{i}^N [||f(x_i^a)-f(x_i^p)||_2^2 - ||f(x_i^a)-f(x_i^n)||_2^2 + \\alpha ]_+<br>$$<br>Triplet Loss确定了，那么下一步就是如何选择合适的Triplets。</p>\n<h3 id=\"Triplet-Selection\"><a href=\"#Triplet-Selection\" class=\"headerlink\" title=\"Triplet Selection\"></a>Triplet Selection</h3><font color=\"red\">为了保证快速收敛，我们需要violate triplet的constraint，即挑选anchor $x_i^a$，来挑选hard positive $x_i^p$来满足$\\mathop{argmax} \\limits_{x_i^p}||f(x_i^a)-f(x_i^p)||_2^2$，以及hard negative $x_i^n$来满足$\\mathop{argmin} \\limits_{x_i^p}||f(x_i^a)-f(x_i^n)||_2^2$</font>。<br>&gt; <a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：读者仔细体会一下这里和triplet loss definition的区别，为啥是相反的？这里可视为一种<a href=\"http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf\" target=\"_blank\" rel=\"noopener\">hard negative mining</a>。<br><br>在整个training set上计算$argmax$和$argmin$是不太现实的，文中采取了两个做法：<br><em> 训练每$n$步离线来生成triplets，使用most recent network checkpoint和dataset的子集来计算$argmax$和$argmin$。\n</em> 在线生成triplets，这种做法可视为在一个mini-batch选择hard positive/negative exemplars。<br><br>Selecting the hardest negatives can in practice lead to bad local minima early on in training,specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_i$ such that:<br>$$<br>||f(x_i^a)-f(x_i^p)||_2^2 &lt; ||f(x_i^a)-f(x_i^n)||_2^2<br>$$<br><font color=\"red\">We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin $\\alpha$.</font>\n\n<h3 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h3><p>对于Face Verification Task，判断两张图是否为一个人，我们仅需比较这两个特征向量的squared $L_2$ distance $D(x_i,x_j)$是否超过了某个阈值即可。</p>\n<ul>\n<li>True Accepts代表face pairs $(i,j)被正确分类到同一个identity$:<br>$TA(d)=\\{(i,j)\\in \\mathcal{P}_{same},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n<li>False Accepts代表face pairs $(i,j)被错误分类到同一个identity$:<br>$FA(d)=\\{(i,j)\\in \\mathcal{P}_{diff},\\quad with \\quad D(x_i,x_j)\\leq d\\}$</li>\n</ul>\n<h2 id=\"Center-Loss\"><a href=\"#Center-Loss\" class=\"headerlink\" title=\"Center Loss\"></a>Center Loss</h2><blockquote>\n<p>Paper: <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a></p>\n</blockquote>\n<p>Face Recognition领域，除了设计更加优秀的Network Architecture，也有另一个方向的工作是在设计更加优秀的Loss。<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>就是其中之一。和FaceNet中使用<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>的目的一样，<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>依然是为了使得intra-class more compact and inter-class more separate。本文就来简要介绍一下<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>。</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>通过学习每一个类的中心向量，来同时更新这个center，以及最小化deep features和其对应class的centers之间的距离。CNN的Loss为Softmax Loss与<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>的加权。Softmax Loss仅仅会让不同的class分开，但<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>还会使得相同class的deep features更加靠近类的centers。通过这种joint supervision(Softmax + Center Loss)，不仅仅inter-class的difference被加大了，而且intra-class的variantions也被减小了。因此便可以学得更加discriminative的feature representation。这便是<a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>的大致idea。</p>\n<h3 id=\"What-is-Center-Loss\"><a href=\"#What-is-Center-Loss\" class=\"headerlink\" title=\"What is Center Loss?\"></a>What is Center Loss?</h3><p>Softmax Loss是这样的：<br>$$<br>\\mathcal{L}_S=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i+b_j}}<br>$$</p>\n<p><a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">Center Loss</a>则是这样的：<br>$$<br>\\mathcal{L}_C=\\frac{1}{2}\\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$<br>为了更新center vector $c_{y_i}$，文中采取的做法是在每一个mini-batch中进行更新(而不是在整个training set中更新)，然后center vector $c_{y_i}$的计算为相关class feature的平均值。此外，为了避免mislabeled samples，我们使用$\\alpha$来控制center vector的learning rate。Center Loss的梯度求导可以表示为：<br>$$<br>\\frac{\\partial \\mathcal{L}_C}{\\partial x_i}=x_i - c_{y_i}<br>$$</p>\n<p>$$<br>\\Delta c_j=\\frac{\\sum_{i=1}^m \\delta(y_i=j)\\cdot (c_j-x_i)}{1+\\sum_{i=1}^m\\delta(y_i=j)}<br>$$</p>\n<p>where $\\delta(condition) = 1$ if the condition is satisfied, and $\\delta(condition) = 0$ if not. $\\alpha$ is restricted in $[0, 1]$. We adopt the joint supervision of softmax loss and center loss to train the CNNs for discriminative feature learning. The formulation is given in Eq. 5.<br>$$<br>\\mathcal{L}=\\mathcal{L}_S+\\lambda \\mathcal{L}_C=-\\sum_{i=1}^m log\\frac{e^{W_{y_i}^Tx_i + b_{y_i}}}{\\sum_{j=1}^n e^{W_j^Tx_i + b_j}} + \\frac{\\lambda}{2} \\sum_{i=1}^m ||x_i-c_{y_i}||_2^2<br>$$</p>\n<p>整个学习算法如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_update.jpg\" alt=\"Learning of Center Loss\"></p>\n<p>网络结构如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/cv-face-rec/centerloss_nn.jpg\" alt=\"Center Loss Architecture\"></p>\n<p>Center Loss的好处在于：</p>\n<ul>\n<li>Joint supervision of Softmax Loss and Center Loss能够大大加强DCNN的feature learning能力。</li>\n<li>其他Metric Learning的Loss例如<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Triplet Loss</a>, Contractive Loss等pairs selection是非常麻烦的一件事情，但是Center Loss则不需要复杂的triplet pairs selection。</li>\n</ul>\n<p>网络学习完成，在做Face Verification/Identification时，<font color=\"red\">第一个 FC Layers的feature被当作特征，同时，我们也将水平翻转图片的feature进行concatenation，作为最终的face feature，PCA降维之后，Cosine Distance, Nearest Neighbor and Threshold comparison用来作为判断是否为同一个人的依据</font>。</p>\n<h2 id=\"NormFace\"><a href=\"#NormFace\" class=\"headerlink\" title=\"NormFace\"></a>NormFace</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a></p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Sun, Yi, Xiaogang Wang, and Xiaoou Tang. <a href=\"http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf\" target=\"_blank\" rel=\"noopener\">“Deep learning face representation from predicting 10,000 classes.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</li>\n<li>Schroff, Florian, Dmitry Kalenichenko, and James Philbin. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Facenet: A unified embedding for face recognition and clustering.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n<li>Wen Y, Zhang K, Li Z, Qiao Y. <a href=\"https://ydwen.github.io/papers/WenECCV16.pdf\" target=\"_blank\" rel=\"noopener\">A discriminative feature learning approach for deep face recognition</a>. In European Conference on Computer Vision 2016 Oct 8 (pp. 499-515). Springer, Cham.</li>\n<li>Wang F, Xiang X, Cheng J, Yuille AL. <a href=\"https://arxiv.org/pdf/1704.06369v4.pdf\" target=\"_blank\" rel=\"noopener\">Normface: L2 hypersphere embedding for face verification</a>. InProceedings of the 2017 ACM on Multimedia Conference 2017 Oct 23 (pp. 1041-1049). ACM.</li>\n</ol>\n"},{"title":"[CV] Image Quality Assessment","date":"2018-11-04T08:32:11.000Z","mathjax":true,"catagories":["Computer Vision","Machine Learning","Deep Learning","Digital Image Processing"],"_content":"## Introduction\nImage Quality Assessment (IQA) 是计算机视觉领域一个非常重要的研究方向，并且在许多方向也有着非常好的落地场景(例如我在滴滴出行实习时，就需要设计算法来实现对网约车司机上传的证件照进行图像质量分析，若存在大规模的反光(reflection)、模糊(blur)等，就需要予以拒绝)；此外，IQA也常常被用于[Face Anti-Spoofing](https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/)，因为有时候print/replay attack的图片/视频 和活体相比，其图像质量往往会比较差(例如颜色失真、反光、模糊、变形等)，因此也是一个非常显著的特征。\n\nIQA主要分为3种：(1) 将distorted image和original image进行质量比较的，称为*full reference*。(2) 当reference image不可获取时，称为*no-reference*。(3) 当reference image只有部分可以获取时，称为*reduced reference*。\n\nIQA主要的Metric是*MSE*, *PSNR (Peak Signal-to-Noise Ratio)* 和 *SSMI (structural similarity)*。\n\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n\n## Reference\n1. Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, [\"Image quality assessment: From error visibility to structural similarity,\"](http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf) IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n2. Talebi, Hossein, and Peyman Milanfar. [\"Nima: Neural image assessment.\"](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8352823) IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.","source":"_posts/cv-iqa.md","raw":"---\ntitle: \"[CV] Image Quality Assessment\"\ndate: 2018-11-04 16:32:11\nmathjax: true\ntags:\n- Computer Vision\n- Machine Learning\n- Deep Learning\n- Digital Image Processing\ncatagories:\n- Computer Vision\n- Machine Learning\n- Deep Learning\n- Digital Image Processing\n---\n## Introduction\nImage Quality Assessment (IQA) 是计算机视觉领域一个非常重要的研究方向，并且在许多方向也有着非常好的落地场景(例如我在滴滴出行实习时，就需要设计算法来实现对网约车司机上传的证件照进行图像质量分析，若存在大规模的反光(reflection)、模糊(blur)等，就需要予以拒绝)；此外，IQA也常常被用于[Face Anti-Spoofing](https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/)，因为有时候print/replay attack的图片/视频 和活体相比，其图像质量往往会比较差(例如颜色失真、反光、模糊、变形等)，因此也是一个非常显著的特征。\n\nIQA主要分为3种：(1) 将distorted image和original image进行质量比较的，称为*full reference*。(2) 当reference image不可获取时，称为*no-reference*。(3) 当reference image只有部分可以获取时，称为*reduced reference*。\n\nIQA主要的Metric是*MSE*, *PSNR (Peak Signal-to-Noise Ratio)* 和 *SSMI (structural similarity)*。\n\n\n> [@LucasX](https://www.zhihu.com/people/xulu-0620/activities)注：本文长期更新。\n\n\n## Reference\n1. Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, [\"Image quality assessment: From error visibility to structural similarity,\"](http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf) IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.\n2. Talebi, Hossein, and Peyman Milanfar. [\"Nima: Neural image assessment.\"](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8352823) IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.","slug":"cv-iqa","published":1,"updated":"2018-11-04T14:13:47.400Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bt0009608whetyugdt","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Image Quality Assessment (IQA) 是计算机视觉领域一个非常重要的研究方向，并且在许多方向也有着非常好的落地场景(例如我在滴滴出行实习时，就需要设计算法来实现对网约车司机上传的证件照进行图像质量分析，若存在大规模的反光(reflection)、模糊(blur)等，就需要予以拒绝)；此外，IQA也常常被用于<a href=\"https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/\">Face Anti-Spoofing</a>，因为有时候print/replay attack的图片/视频 和活体相比，其图像质量往往会比较差(例如颜色失真、反光、模糊、变形等)，因此也是一个非常显著的特征。</p>\n<p>IQA主要分为3种：(1) 将distorted image和original image进行质量比较的，称为<em>full reference</em>。(2) 当reference image不可获取时，称为<em>no-reference</em>。(3) 当reference image只有部分可以获取时，称为<em>reduced reference</em>。</p>\n<p>IQA主要的Metric是<em>MSE</em>, <em>PSNR (Peak Signal-to-Noise Ratio)</em> 和 <em>SSMI (structural similarity)</em>。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, <a href=\"http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\" target=\"_blank\" rel=\"noopener\">“Image quality assessment: From error visibility to structural similarity,”</a> IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</li>\n<li>Talebi, Hossein, and Peyman Milanfar. <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8352823\" target=\"_blank\" rel=\"noopener\">“Nima: Neural image assessment.”</a> IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Image Quality Assessment (IQA) 是计算机视觉领域一个非常重要的研究方向，并且在许多方向也有着非常好的落地场景(例如我在滴滴出行实习时，就需要设计算法来实现对网约车司机上传的证件照进行图像质量分析，若存在大规模的反光(reflection)、模糊(blur)等，就需要予以拒绝)；此外，IQA也常常被用于<a href=\"https://lucasxlu.github.io/blog/2018/10/30/cv-antispoofing/\">Face Anti-Spoofing</a>，因为有时候print/replay attack的图片/视频 和活体相比，其图像质量往往会比较差(例如颜色失真、反光、模糊、变形等)，因此也是一个非常显著的特征。</p>\n<p>IQA主要分为3种：(1) 将distorted image和original image进行质量比较的，称为<em>full reference</em>。(2) 当reference image不可获取时，称为<em>no-reference</em>。(3) 当reference image只有部分可以获取时，称为<em>reduced reference</em>。</p>\n<p>IQA主要的Metric是<em>MSE</em>, <em>PSNR (Peak Signal-to-Noise Ratio)</em> 和 <em>SSMI (structural similarity)</em>。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620/activities\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：本文长期更新。</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, <a href=\"http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\" target=\"_blank\" rel=\"noopener\">“Image quality assessment: From error visibility to structural similarity,”</a> IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.</li>\n<li>Talebi, Hossein, and Peyman Milanfar. <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8352823\" target=\"_blank\" rel=\"noopener\">“Nima: Neural image assessment.”</a> IEEE Transactions on Image Processing 27.8 (2018): 3998-4011.</li>\n</ol>\n"},{"title":"[DIP] Image Feature","date":"2018-08-22T07:53:01.000Z","mathjax":true,"catagories":["Digital Image Processing","Computer Vision"],"_content":"## Introduction\nComputer Vision已进入Deep Learning时代，但传统图像特征提取方法依然在很多方向有着不少的应用。毕竟DNN计算复杂度太高，且过于依赖Large Scale Labeled Dataset，所以Deep Learning也并非万能的。本文就传统图像特征提取算子做一下简单的归纳。\n\n> 本文内容主要来源于TPAMI的一篇文章《[A Performance Evaluation of Local Descriptors](https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf)》，详情请阅读原文！\n\n## Image Descriptors\n### Image Pixels\n简单，但是高维向量带来的复杂计算量。可通过下采样、PCA等方式进行降维。\n\n### Distribution-Based Descriptors\nA simple descriptor is the distribution of the pixel intensities represented by a histogram. \n\n#### SIFT\nThe descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. **The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection**. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods **compute a histogram describing the edge distribution in a region**. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.\n\n## EXPERIMENTAL SETUP\n### Support Regions\nLindeberg [23] has developed a scaleinvariant \"blob\" detector, where **a \"blob\" is defined by a maximum of the normalized Laplacian in scale-space**. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Ga°rding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.\n\n#### Region Detectors\n**Harris points** [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.\n\n**Harris-Laplace regions** [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.\n\n**Hessian-Laplace regions** [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.\n\n**Harris-Affine regions** [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. \n\n**Hessian-Affine regions** [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.\n\n**Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries**. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.\n\n### Descriptors\n#### SIFT \nSIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.\n\nEach orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.\n\n#### Gradient location-orientation histogram (GLOH)\nGradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.\n\n#### Shape context\nShape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.\n\n#### PCA-SIFT\nPCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.\n\n#### Spin image\nSpin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.\n\n#### Cross correlation\nCross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.\n\n## DISCUSSION AND CONCLUSIONS\nIn most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.\n\n## Appendix\n### 颜色直方图\n颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。颜色直方图所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。计算颜色直方图需要将颜色空间划分成若干个小的颜色区间，每个小区间成为直方图的一个 Bin。这个过程称为颜色量化（Color Quantization）。然后，通过计算颜色落在每个小区间内的像素数量可以得到颜色直方图。颜色量化有许多方法，例如向量量化、聚类方法或者神经网络方法。最为常用的做法是将颜色空间的各个分量（维度）均匀地进行划分。选择合适的颜色小区间（即直方图的 Bin）数目和颜色量化方法与具体应用的性能和效率要求有关。一般来说，颜色小区间的数目越多，直方图对颜色的分辨能力就越强。然而，Bin 的数目很大的颜色直方图不但会增加计算负担，也不利于在大型图像库中建立索引。而且对于某些应用来说，使用非常精细的颜色空间划分方法不一定能够提高检索效果，特别是对于不能容忍对相关图像错漏的那些应用。另一种有效减少直方图 Bin 的数目的办法是只选用那些数值最大（即像素数目最多）的 Bin 来构造图像特征，因为这些表示主要颜色的 Bin 能够表达图像中大部分像素的颜色。实验证明这种方法并不会降低颜色直方图的检索效果。事实上，由于忽略了那些数值较小的 Bin，颜色直方图对噪声的敏感程度降低了，检索效果也更好。\n\n### 灰度直方图\n灰度直方图是灰度级的函数，它表示图像中具有每种灰度级的象素的个数，反映图像中每种灰度出现的频率。由于灰度直方图维数最多只能有 256 维，描述能力有限，所以利用灰度直方图表示图像特征的研究比较少。灰度直方图的横坐标是灰度级，纵坐标是该灰度级出现的频率，是图像的最基本的统计特征。\n\n### LBP\n**LBP(Local  Binary  Pattern，缩写为 LBP)是一种用来描述图像局部纹理特征的算子**；显然，它的作用是进行特征提取，而且，提取的特征是图像的纹理特征，并且，是局部的纹理特征。LBP 算子定义为 **在 $3\\times 3$ 的窗口内，以窗口中心像素为阈值，将相邻的 8 个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为 1，否则为 0。这样，3*3 领域内的 8 个点可产生 8 bit 的无符号数，即得到该窗口的 LBP值，并用这个值来反映该区域的纹理信息**。\n虽然 LBP 直方图和灰度直方图一样，只有 256 维，但由于灰度直方图是对图像的整体统计，而 LBP 是对图像的局部的特征描述，因此比灰度直方图有更好的特征表达能力。\n\n### SIFT\nSIFT 是一种提取局部特征的算法，在尺度空间寻找极值点，提取位置，尺度，旋转不变量。SIFT 特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 SIFT 算法步骤：检测尺度空间极值点，精确定位极值点，为每个关键点指定方向参数，关键点描述子的生成。\n\n**SIFT检测到的每个关键点有三个信息：位置、所处尺度、方向**。\n\n### HOG\n梯度方向直方图（Histogram of Oriented Gradients，简称 HOG） 描述子是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。\n\nHOG 描述器最重要的思想是：在一副图像中，局部目标的表象和形状（Appearance and Shape）能够被梯度或边缘的方向密度分布很好地描述。具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或 Block）进行对比度归一化（Contrast-Normalized），所采用的方法是：先计算各直方图在这个区间（Block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。与其他的特征描述方法相比，HOG 描述器后很多优点: **由于 HOG 方法是在图像的局部细胞单元上操作，所以它对图像几何的（Geometric）和光学的（Photometric）形变都能保持很好的不变性**。\n","source":"_posts/dip-image-feature.md","raw":"---\ntitle: \"[DIP] Image Feature\"\ndate: 2018-08-22 15:53:01\nmathjax: true\ntags:\n- Digital Image Processing\n- Computer Vision\ncatagories:\n- Digital Image Processing\n- Computer Vision\n---\n## Introduction\nComputer Vision已进入Deep Learning时代，但传统图像特征提取方法依然在很多方向有着不少的应用。毕竟DNN计算复杂度太高，且过于依赖Large Scale Labeled Dataset，所以Deep Learning也并非万能的。本文就传统图像特征提取算子做一下简单的归纳。\n\n> 本文内容主要来源于TPAMI的一篇文章《[A Performance Evaluation of Local Descriptors](https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf)》，详情请阅读原文！\n\n## Image Descriptors\n### Image Pixels\n简单，但是高维向量带来的复杂计算量。可通过下采样、PCA等方式进行降维。\n\n### Distribution-Based Descriptors\nA simple descriptor is the distribution of the pixel intensities represented by a histogram. \n\n#### SIFT\nThe descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. **The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection**. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods **compute a histogram describing the edge distribution in a region**. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.\n\n## EXPERIMENTAL SETUP\n### Support Regions\nLindeberg [23] has developed a scaleinvariant \"blob\" detector, where **a \"blob\" is defined by a maximum of the normalized Laplacian in scale-space**. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Ga°rding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.\n\n#### Region Detectors\n**Harris points** [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.\n\n**Harris-Laplace regions** [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.\n\n**Hessian-Laplace regions** [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.\n\n**Harris-Affine regions** [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. \n\n**Hessian-Affine regions** [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.\n\n**Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries**. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.\n\n### Descriptors\n#### SIFT \nSIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.\n\nEach orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.\n\n#### Gradient location-orientation histogram (GLOH)\nGradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.\n\n#### Shape context\nShape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.\n\n#### PCA-SIFT\nPCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.\n\n#### Spin image\nSpin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.\n\n#### Cross correlation\nCross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.\n\n## DISCUSSION AND CONCLUSIONS\nIn most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.\n\n## Appendix\n### 颜色直方图\n颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。颜色直方图所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。计算颜色直方图需要将颜色空间划分成若干个小的颜色区间，每个小区间成为直方图的一个 Bin。这个过程称为颜色量化（Color Quantization）。然后，通过计算颜色落在每个小区间内的像素数量可以得到颜色直方图。颜色量化有许多方法，例如向量量化、聚类方法或者神经网络方法。最为常用的做法是将颜色空间的各个分量（维度）均匀地进行划分。选择合适的颜色小区间（即直方图的 Bin）数目和颜色量化方法与具体应用的性能和效率要求有关。一般来说，颜色小区间的数目越多，直方图对颜色的分辨能力就越强。然而，Bin 的数目很大的颜色直方图不但会增加计算负担，也不利于在大型图像库中建立索引。而且对于某些应用来说，使用非常精细的颜色空间划分方法不一定能够提高检索效果，特别是对于不能容忍对相关图像错漏的那些应用。另一种有效减少直方图 Bin 的数目的办法是只选用那些数值最大（即像素数目最多）的 Bin 来构造图像特征，因为这些表示主要颜色的 Bin 能够表达图像中大部分像素的颜色。实验证明这种方法并不会降低颜色直方图的检索效果。事实上，由于忽略了那些数值较小的 Bin，颜色直方图对噪声的敏感程度降低了，检索效果也更好。\n\n### 灰度直方图\n灰度直方图是灰度级的函数，它表示图像中具有每种灰度级的象素的个数，反映图像中每种灰度出现的频率。由于灰度直方图维数最多只能有 256 维，描述能力有限，所以利用灰度直方图表示图像特征的研究比较少。灰度直方图的横坐标是灰度级，纵坐标是该灰度级出现的频率，是图像的最基本的统计特征。\n\n### LBP\n**LBP(Local  Binary  Pattern，缩写为 LBP)是一种用来描述图像局部纹理特征的算子**；显然，它的作用是进行特征提取，而且，提取的特征是图像的纹理特征，并且，是局部的纹理特征。LBP 算子定义为 **在 $3\\times 3$ 的窗口内，以窗口中心像素为阈值，将相邻的 8 个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为 1，否则为 0。这样，3*3 领域内的 8 个点可产生 8 bit 的无符号数，即得到该窗口的 LBP值，并用这个值来反映该区域的纹理信息**。\n虽然 LBP 直方图和灰度直方图一样，只有 256 维，但由于灰度直方图是对图像的整体统计，而 LBP 是对图像的局部的特征描述，因此比灰度直方图有更好的特征表达能力。\n\n### SIFT\nSIFT 是一种提取局部特征的算法，在尺度空间寻找极值点，提取位置，尺度，旋转不变量。SIFT 特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 SIFT 算法步骤：检测尺度空间极值点，精确定位极值点，为每个关键点指定方向参数，关键点描述子的生成。\n\n**SIFT检测到的每个关键点有三个信息：位置、所处尺度、方向**。\n\n### HOG\n梯度方向直方图（Histogram of Oriented Gradients，简称 HOG） 描述子是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。\n\nHOG 描述器最重要的思想是：在一副图像中，局部目标的表象和形状（Appearance and Shape）能够被梯度或边缘的方向密度分布很好地描述。具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或 Block）进行对比度归一化（Contrast-Normalized），所采用的方法是：先计算各直方图在这个区间（Block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。与其他的特征描述方法相比，HOG 描述器后很多优点: **由于 HOG 方法是在图像的局部细胞单元上操作，所以它对图像几何的（Geometric）和光学的（Photometric）形变都能保持很好的不变性**。\n","slug":"dip-image-feature","published":1,"updated":"2018-10-01T04:40:08.551Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bv000a608w1fey66vl","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Computer Vision已进入Deep Learning时代，但传统图像特征提取方法依然在很多方向有着不少的应用。毕竟DNN计算复杂度太高，且过于依赖Large Scale Labeled Dataset，所以Deep Learning也并非万能的。本文就传统图像特征提取算子做一下简单的归纳。</p>\n<blockquote>\n<p>本文内容主要来源于TPAMI的一篇文章《<a href=\"https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf\" target=\"_blank\" rel=\"noopener\">A Performance Evaluation of Local Descriptors</a>》，详情请阅读原文！</p>\n</blockquote>\n<h2 id=\"Image-Descriptors\"><a href=\"#Image-Descriptors\" class=\"headerlink\" title=\"Image Descriptors\"></a>Image Descriptors</h2><h3 id=\"Image-Pixels\"><a href=\"#Image-Pixels\" class=\"headerlink\" title=\"Image Pixels\"></a>Image Pixels</h3><p>简单，但是高维向量带来的复杂计算量。可通过下采样、PCA等方式进行降维。</p>\n<h3 id=\"Distribution-Based-Descriptors\"><a href=\"#Distribution-Based-Descriptors\" class=\"headerlink\" title=\"Distribution-Based Descriptors\"></a>Distribution-Based Descriptors</h3><p>A simple descriptor is the distribution of the pixel intensities represented by a histogram. </p>\n<h4 id=\"SIFT\"><a href=\"#SIFT\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>The descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. <strong>The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection</strong>. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods <strong>compute a histogram describing the edge distribution in a region</strong>. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.</p>\n<h2 id=\"EXPERIMENTAL-SETUP\"><a href=\"#EXPERIMENTAL-SETUP\" class=\"headerlink\" title=\"EXPERIMENTAL SETUP\"></a>EXPERIMENTAL SETUP</h2><h3 id=\"Support-Regions\"><a href=\"#Support-Regions\" class=\"headerlink\" title=\"Support Regions\"></a>Support Regions</h3><p>Lindeberg [23] has developed a scaleinvariant “blob” detector, where <strong>a “blob” is defined by a maximum of the normalized Laplacian in scale-space</strong>. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Ga°rding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.</p>\n<h4 id=\"Region-Detectors\"><a href=\"#Region-Detectors\" class=\"headerlink\" title=\"Region Detectors\"></a>Region Detectors</h4><p><strong>Harris points</strong> [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.</p>\n<p><strong>Harris-Laplace regions</strong> [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.</p>\n<p><strong>Hessian-Laplace regions</strong> [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.</p>\n<p><strong>Harris-Affine regions</strong> [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. </p>\n<p><strong>Hessian-Affine regions</strong> [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.</p>\n<p><strong>Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries</strong>. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.</p>\n<h3 id=\"Descriptors\"><a href=\"#Descriptors\" class=\"headerlink\" title=\"Descriptors\"></a>Descriptors</h3><h4 id=\"SIFT-1\"><a href=\"#SIFT-1\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>SIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.</p>\n<p>Each orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.</p>\n<h4 id=\"Gradient-location-orientation-histogram-GLOH\"><a href=\"#Gradient-location-orientation-histogram-GLOH\" class=\"headerlink\" title=\"Gradient location-orientation histogram (GLOH)\"></a>Gradient location-orientation histogram (GLOH)</h4><p>Gradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.</p>\n<h4 id=\"Shape-context\"><a href=\"#Shape-context\" class=\"headerlink\" title=\"Shape context\"></a>Shape context</h4><p>Shape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.</p>\n<h4 id=\"PCA-SIFT\"><a href=\"#PCA-SIFT\" class=\"headerlink\" title=\"PCA-SIFT\"></a>PCA-SIFT</h4><p>PCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.</p>\n<h4 id=\"Spin-image\"><a href=\"#Spin-image\" class=\"headerlink\" title=\"Spin image\"></a>Spin image</h4><p>Spin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.</p>\n<h4 id=\"Cross-correlation\"><a href=\"#Cross-correlation\" class=\"headerlink\" title=\"Cross correlation\"></a>Cross correlation</h4><p>Cross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.</p>\n<h2 id=\"DISCUSSION-AND-CONCLUSIONS\"><a href=\"#DISCUSSION-AND-CONCLUSIONS\" class=\"headerlink\" title=\"DISCUSSION AND CONCLUSIONS\"></a>DISCUSSION AND CONCLUSIONS</h2><p>In most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><h3 id=\"颜色直方图\"><a href=\"#颜色直方图\" class=\"headerlink\" title=\"颜色直方图\"></a>颜色直方图</h3><p>颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。颜色直方图所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。计算颜色直方图需要将颜色空间划分成若干个小的颜色区间，每个小区间成为直方图的一个 Bin。这个过程称为颜色量化（Color Quantization）。然后，通过计算颜色落在每个小区间内的像素数量可以得到颜色直方图。颜色量化有许多方法，例如向量量化、聚类方法或者神经网络方法。最为常用的做法是将颜色空间的各个分量（维度）均匀地进行划分。选择合适的颜色小区间（即直方图的 Bin）数目和颜色量化方法与具体应用的性能和效率要求有关。一般来说，颜色小区间的数目越多，直方图对颜色的分辨能力就越强。然而，Bin 的数目很大的颜色直方图不但会增加计算负担，也不利于在大型图像库中建立索引。而且对于某些应用来说，使用非常精细的颜色空间划分方法不一定能够提高检索效果，特别是对于不能容忍对相关图像错漏的那些应用。另一种有效减少直方图 Bin 的数目的办法是只选用那些数值最大（即像素数目最多）的 Bin 来构造图像特征，因为这些表示主要颜色的 Bin 能够表达图像中大部分像素的颜色。实验证明这种方法并不会降低颜色直方图的检索效果。事实上，由于忽略了那些数值较小的 Bin，颜色直方图对噪声的敏感程度降低了，检索效果也更好。</p>\n<h3 id=\"灰度直方图\"><a href=\"#灰度直方图\" class=\"headerlink\" title=\"灰度直方图\"></a>灰度直方图</h3><p>灰度直方图是灰度级的函数，它表示图像中具有每种灰度级的象素的个数，反映图像中每种灰度出现的频率。由于灰度直方图维数最多只能有 256 维，描述能力有限，所以利用灰度直方图表示图像特征的研究比较少。灰度直方图的横坐标是灰度级，纵坐标是该灰度级出现的频率，是图像的最基本的统计特征。</p>\n<h3 id=\"LBP\"><a href=\"#LBP\" class=\"headerlink\" title=\"LBP\"></a>LBP</h3><p><strong>LBP(Local  Binary  Pattern，缩写为 LBP)是一种用来描述图像局部纹理特征的算子</strong>；显然，它的作用是进行特征提取，而且，提取的特征是图像的纹理特征，并且，是局部的纹理特征。LBP 算子定义为 <strong>在 $3\\times 3$ 的窗口内，以窗口中心像素为阈值，将相邻的 8 个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为 1，否则为 0。这样，3*3 领域内的 8 个点可产生 8 bit 的无符号数，即得到该窗口的 LBP值，并用这个值来反映该区域的纹理信息</strong>。<br>虽然 LBP 直方图和灰度直方图一样，只有 256 维，但由于灰度直方图是对图像的整体统计，而 LBP 是对图像的局部的特征描述，因此比灰度直方图有更好的特征表达能力。</p>\n<h3 id=\"SIFT-2\"><a href=\"#SIFT-2\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h3><p>SIFT 是一种提取局部特征的算法，在尺度空间寻找极值点，提取位置，尺度，旋转不变量。SIFT 特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 SIFT 算法步骤：检测尺度空间极值点，精确定位极值点，为每个关键点指定方向参数，关键点描述子的生成。</p>\n<p><strong>SIFT检测到的每个关键点有三个信息：位置、所处尺度、方向</strong>。</p>\n<h3 id=\"HOG\"><a href=\"#HOG\" class=\"headerlink\" title=\"HOG\"></a>HOG</h3><p>梯度方向直方图（Histogram of Oriented Gradients，简称 HOG） 描述子是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。</p>\n<p>HOG 描述器最重要的思想是：在一副图像中，局部目标的表象和形状（Appearance and Shape）能够被梯度或边缘的方向密度分布很好地描述。具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或 Block）进行对比度归一化（Contrast-Normalized），所采用的方法是：先计算各直方图在这个区间（Block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。与其他的特征描述方法相比，HOG 描述器后很多优点: <strong>由于 HOG 方法是在图像的局部细胞单元上操作，所以它对图像几何的（Geometric）和光学的（Photometric）形变都能保持很好的不变性</strong>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Computer Vision已进入Deep Learning时代，但传统图像特征提取方法依然在很多方向有着不少的应用。毕竟DNN计算复杂度太高，且过于依赖Large Scale Labeled Dataset，所以Deep Learning也并非万能的。本文就传统图像特征提取算子做一下简单的归纳。</p>\n<blockquote>\n<p>本文内容主要来源于TPAMI的一篇文章《<a href=\"https://github.com/lucasxlu/blog/raw/master/source/_posts/dip-image-feature/TPAMI-A%20performance%20evaluation%20of%20local%20descriptors.pdf\" target=\"_blank\" rel=\"noopener\">A Performance Evaluation of Local Descriptors</a>》，详情请阅读原文！</p>\n</blockquote>\n<h2 id=\"Image-Descriptors\"><a href=\"#Image-Descriptors\" class=\"headerlink\" title=\"Image Descriptors\"></a>Image Descriptors</h2><h3 id=\"Image-Pixels\"><a href=\"#Image-Pixels\" class=\"headerlink\" title=\"Image Pixels\"></a>Image Pixels</h3><p>简单，但是高维向量带来的复杂计算量。可通过下采样、PCA等方式进行降维。</p>\n<h3 id=\"Distribution-Based-Descriptors\"><a href=\"#Distribution-Based-Descriptors\" class=\"headerlink\" title=\"Distribution-Based Descriptors\"></a>Distribution-Based Descriptors</h3><p>A simple descriptor is the distribution of the pixel intensities represented by a histogram. </p>\n<h4 id=\"SIFT\"><a href=\"#SIFT\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>The descriptor is represented by a 3D histogram of gradient locations and orientations; see Fig. 1 for an illustration. The contribution to the locationand orientation bins is weighted by the gradient magnitude. <strong>The quantization of gradient locations and orientations makes the descriptor robust to small geometric distortions and small errors in the region detection</strong>. Geometric histogram [1] and shape context [3] implement the same idea and are very similar to the SIFT descriptor. Both methods <strong>compute a histogram describing the edge distribution in a region</strong>. These descriptors were successfully used, for example, for shape recognition of drawings for which edges are reliable features.</p>\n<h2 id=\"EXPERIMENTAL-SETUP\"><a href=\"#EXPERIMENTAL-SETUP\" class=\"headerlink\" title=\"EXPERIMENTAL SETUP\"></a>EXPERIMENTAL SETUP</h2><h3 id=\"Support-Regions\"><a href=\"#Support-Regions\" class=\"headerlink\" title=\"Support Regions\"></a>Support Regions</h3><p>Lindeberg [23] has developed a scaleinvariant “blob” detector, where <strong>a “blob” is defined by a maximum of the normalized Laplacian in scale-space</strong>. Lowe [25] approximates the Laplacian with difference-of-Gaussian (DoG) filters and also detects local extrema in scalespace. Lindeberg and Ga°rding [24] make the blob detector affine-invariant using an affine adaptation process based on the second moment matrix.</p>\n<h4 id=\"Region-Detectors\"><a href=\"#Region-Detectors\" class=\"headerlink\" title=\"Region Detectors\"></a>Region Detectors</h4><p><strong>Harris points</strong> [15] are invariant to rotation. The support region is a fixed size neighborhood of $41\\times 41$ pixels centered at the interest point.</p>\n<p><strong>Harris-Laplace regions</strong> [29] are invariant to rotation and scale changes. The points are detected by the scale-adapted Harris function and selected in scale-space by the Laplacian- of-Gaussian operator. Harris-Laplace detects cornerlike structures.</p>\n<p><strong>Hessian-Laplace regions</strong> [25], [32] are invariant to rotation and scale changes. Points are localized in space at the local maxima of the Hessian determinant and in scale at the local maxima of the Laplacian-of-Gaussian. This detector is similar to the DoG approach [26], which localizes points at local scale-space maxima of the difference-of-Gaussian. Both approaches detect similar blob-like structures. However, Hessian-Laplace obtains a higher localization accuracy in scale-space, as DoG also responds to edges and detection is unstable in this case. The scale selection accuracy is also higher than in the case of the Harris-Laplace detector. Laplacian scale selection acts as a matched filter and works better on blob-like structures than on corners since the shape of the Laplacian kernel fits to the blobs. The accuracy of the detectors affects the descriptor performance.</p>\n<p><strong>Harris-Affine regions</strong> [32] are invariant to affine image transformations. Localization and scale are estimated by the Harris-Laplace detector. The affine neighborhood is determined by the affine adaptation process based on the second moment matrix. </p>\n<p><strong>Hessian-Affine regions</strong> [33] are invariant to affine image transformations. Localization and scale are estimated by the Hessian-Laplace detector and the affine neighborhood is determined by the affine adaptation process.</p>\n<p><strong>Hessian-Affine and Hessian-Laplace detect mainly blob-like structures for which the signal variations lie on the blob boundaries</strong>. To include these signal changes into the description, the measurement region is three times larger than the detected region. This factor is used for all scale and affine detectors. All the regions are mapped to a circular region of constant radius to obtain scale and affine invariance. The size of the normalized region should not be too small in order to represent the local structure at a sufficient resolution. In all experiments, this size is arbitrarily set to 41 pixels.</p>\n<h3 id=\"Descriptors\"><a href=\"#Descriptors\" class=\"headerlink\" title=\"Descriptors\"></a>Descriptors</h3><h4 id=\"SIFT-1\"><a href=\"#SIFT-1\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h4><p>SIFT descriptors are computed for normalized image patches with the code provided by Lowe [25]. A descriptor is a 3D histogram of gradient location and orientation, where location is quantized into a $4\\times 4$ location grid and the gradient angle is quantized into eight orientations. The resulting descriptor is of dimension 128.</p>\n<p>Each orientation plane represents the gradient magnitude corresponding to a given orientation. To obtain illumination invariance, the descriptor is normalized by the square root of the sum of squared components.</p>\n<h4 id=\"Gradient-location-orientation-histogram-GLOH\"><a href=\"#Gradient-location-orientation-histogram-GLOH\" class=\"headerlink\" title=\"Gradient location-orientation histogram (GLOH)\"></a>Gradient location-orientation histogram (GLOH)</h4><p>Gradient location-orientation histogram (GLOH) is an extension of the SIFT descriptor designed to increase its robustness and distinctiveness. We compute the SIFT descriptor for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. Note that the central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins. This gives a 272 bin histogram. The size of this descriptor is reduced with PCA. The covariance matrix for PCA is estimated on 47,000 image patches collected from various images (see Section 3.3.1). The 128 largest eigenvectors are used for description.</p>\n<h4 id=\"Shape-context\"><a href=\"#Shape-context\" class=\"headerlink\" title=\"Shape context\"></a>Shape context</h4><p>Shape context is similar to the SIFT descriptor, but is based on edges. Shape context is a 3D histogram of edge point locations and orientations. Edges are extracted by the Canny [5] detector. Location is quantized into nine bins of a log-polar coordinate system as displayed in Fig. 1e with the radius set to 6, 11, and 15 and orientation quantized into four bins (horizontal, vertical, and two diagonals). We therefore obtain a 36 dimensional descriptor.</p>\n<h4 id=\"PCA-SIFT\"><a href=\"#PCA-SIFT\" class=\"headerlink\" title=\"PCA-SIFT\"></a>PCA-SIFT</h4><p>PCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at $39\\times 39$ locations, therefore, the vector is of dimension 3,042. The dimension is reduced to 36 with PCA.</p>\n<h4 id=\"Spin-image\"><a href=\"#Spin-image\" class=\"headerlink\" title=\"Spin image\"></a>Spin image</h4><p>Spin image is a histogram of quantized pixel locations and intensity values. The intensity of a normalized patch is quantized into 10 bins. A 10 bin normalized histogram is computed for each of five rings centered on the region. The dimension of the spin descriptor is 50.</p>\n<h4 id=\"Cross-correlation\"><a href=\"#Cross-correlation\" class=\"headerlink\" title=\"Cross correlation\"></a>Cross correlation</h4><p>Cross correlation. To obtain this descriptor, the region is smoothed and uniformly sampled. To limit the descriptor dimension, we sample at $9\\times 9$ pixel locations. The similarity between two descriptors is measured with cross-correlation.</p>\n<h2 id=\"DISCUSSION-AND-CONCLUSIONS\"><a href=\"#DISCUSSION-AND-CONCLUSIONS\" class=\"headerlink\" title=\"DISCUSSION AND CONCLUSIONS\"></a>DISCUSSION AND CONCLUSIONS</h2><p>In most of the tests, GLOH obtains the best results, closely followed by SIFT. This shows the robustness and the distinctive character of the region-based SIFT descriptor. Shape context also shows a high performance. However, for textured scenes or when edges are not reliable, its score is lower. The best low-dimensional descriptors are gradientmoments and steerable filters. They can be considered as an alternative when the high dimensionality of the histogram-based descriptors is an issue. Differential invariants give significantly worse results than steerable filters, which is surprising as they are based on the same basic components (Gaussian derivatives). The multiplication of derivatives necessary to obtain rotation invariance increases the instability. Cross correlation gives unstable results. The performance depends on the accuracy of interest point and region detection, which decreases for significant geometric transformations. Cross correlation is more sensitive to these errors than other high dimensional descriptors. Regions detected by Hessian-Laplace and Hessian-Affine are mainly blob-like structures. There are no significant signal changes in the center of the blob therefore descriptors perform better on larger neighborhoods. The results are slightly but systematically better on Hessian regions than on Harris regions due to their higher accuracy. The ranking of the descriptors is similar for different matching strategies. We can observe that SIFT gives relatively better results if nearest neighbor distance ratio is used for thresholding. Note that the precision is higher for nearest neighbor based matching than for threshold based matching.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><h3 id=\"颜色直方图\"><a href=\"#颜色直方图\" class=\"headerlink\" title=\"颜色直方图\"></a>颜色直方图</h3><p>颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。颜色直方图所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。计算颜色直方图需要将颜色空间划分成若干个小的颜色区间，每个小区间成为直方图的一个 Bin。这个过程称为颜色量化（Color Quantization）。然后，通过计算颜色落在每个小区间内的像素数量可以得到颜色直方图。颜色量化有许多方法，例如向量量化、聚类方法或者神经网络方法。最为常用的做法是将颜色空间的各个分量（维度）均匀地进行划分。选择合适的颜色小区间（即直方图的 Bin）数目和颜色量化方法与具体应用的性能和效率要求有关。一般来说，颜色小区间的数目越多，直方图对颜色的分辨能力就越强。然而，Bin 的数目很大的颜色直方图不但会增加计算负担，也不利于在大型图像库中建立索引。而且对于某些应用来说，使用非常精细的颜色空间划分方法不一定能够提高检索效果，特别是对于不能容忍对相关图像错漏的那些应用。另一种有效减少直方图 Bin 的数目的办法是只选用那些数值最大（即像素数目最多）的 Bin 来构造图像特征，因为这些表示主要颜色的 Bin 能够表达图像中大部分像素的颜色。实验证明这种方法并不会降低颜色直方图的检索效果。事实上，由于忽略了那些数值较小的 Bin，颜色直方图对噪声的敏感程度降低了，检索效果也更好。</p>\n<h3 id=\"灰度直方图\"><a href=\"#灰度直方图\" class=\"headerlink\" title=\"灰度直方图\"></a>灰度直方图</h3><p>灰度直方图是灰度级的函数，它表示图像中具有每种灰度级的象素的个数，反映图像中每种灰度出现的频率。由于灰度直方图维数最多只能有 256 维，描述能力有限，所以利用灰度直方图表示图像特征的研究比较少。灰度直方图的横坐标是灰度级，纵坐标是该灰度级出现的频率，是图像的最基本的统计特征。</p>\n<h3 id=\"LBP\"><a href=\"#LBP\" class=\"headerlink\" title=\"LBP\"></a>LBP</h3><p><strong>LBP(Local  Binary  Pattern，缩写为 LBP)是一种用来描述图像局部纹理特征的算子</strong>；显然，它的作用是进行特征提取，而且，提取的特征是图像的纹理特征，并且，是局部的纹理特征。LBP 算子定义为 <strong>在 $3\\times 3$ 的窗口内，以窗口中心像素为阈值，将相邻的 8 个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为 1，否则为 0。这样，3*3 领域内的 8 个点可产生 8 bit 的无符号数，即得到该窗口的 LBP值，并用这个值来反映该区域的纹理信息</strong>。<br>虽然 LBP 直方图和灰度直方图一样，只有 256 维，但由于灰度直方图是对图像的整体统计，而 LBP 是对图像的局部的特征描述，因此比灰度直方图有更好的特征表达能力。</p>\n<h3 id=\"SIFT-2\"><a href=\"#SIFT-2\" class=\"headerlink\" title=\"SIFT\"></a>SIFT</h3><p>SIFT 是一种提取局部特征的算法，在尺度空间寻找极值点，提取位置，尺度，旋转不变量。SIFT 特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 SIFT 算法步骤：检测尺度空间极值点，精确定位极值点，为每个关键点指定方向参数，关键点描述子的生成。</p>\n<p><strong>SIFT检测到的每个关键点有三个信息：位置、所处尺度、方向</strong>。</p>\n<h3 id=\"HOG\"><a href=\"#HOG\" class=\"headerlink\" title=\"HOG\"></a>HOG</h3><p>梯度方向直方图（Histogram of Oriented Gradients，简称 HOG） 描述子是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。</p>\n<p>HOG 描述器最重要的思想是：在一副图像中，局部目标的表象和形状（Appearance and Shape）能够被梯度或边缘的方向密度分布很好地描述。具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或 Block）进行对比度归一化（Contrast-Normalized），所采用的方法是：先计算各直方图在这个区间（Block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。与其他的特征描述方法相比，HOG 描述器后很多优点: <strong>由于 HOG 方法是在图像的局部细胞单元上操作，所以它对图像几何的（Geometric）和光学的（Photometric）形变都能保持很好的不变性</strong>。</p>\n"},{"title":"[DL] Auto Encoder","date":"2018-08-22T11:17:56.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Auto Encoder"],"_content":"## Introduction\nAuto Encoder是深度学习里一个用途非常广的无监督学习模型，常常用来降维或者特征学习(例如我在[豆瓣评论情感挖掘这个repository](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)里就使用了Deep AutoEncoder来学习300维word2vec里更discriminative的特征表达)。近年来随着GAN的火热，AutoEncoder也常常站在了generative model的前沿。\n\n## Undercomplete AutoEncoder\n从AutoEncoder里获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的AutoEncoder称为Undercomplete AutoEncoder。学习undercomplete representation将强制AutoEncoder捕捉training data中最显著的特征。\n\n当Decoder是线性的且Loss是MSE，Undercomplete AutoEncoder会学习出与PCA相同的生成子空间。这种情况下，AutoEncoder在训练来执行复制任务的同时学到了training data的主元子空间。因此，拥有non-linear Encoder和non-linear Decoder的AutoEncoder能够学习出更强大的PCA非线性推广。\n\n若Encoder和Decoder被赋予过大的容量，AutoEncoder会执行复制任务而捕捉不到任何有关数据分布的有用信息。\n\n## Regularized AutoEncoder\n若隐藏编码的维度允许与输入相等，或隐藏编码维数大于输入的overcomplete情况下，即使是linear Encoder和linear Decoder也可以学会将输入复制到输出，而学不到任何有关数据分布的有用信息。\n\nRegularized AutoEncoder使用的Loss Function鼓励模型学习其他特征(除了将输入复制到输出)，而不必限制使用浅层的Encoder和Decoder以及小的编码维数来限制模型容量。\n\nSparse AutoEncoder简单地在训练时结合Encoder层的稀疏惩罚$\\Omega(h)$和重构误差：\n$$\nL(x,g(f(x)))+\\Omega(h)\n$$\nSparse AutoEncoder一般用来学习特征。\n\n除了向Cost Function增加一个Regularization，我们也可以通过改变重构误差项来获得一个能学到有用信息的AutoEncoder。\n\nDenoising AutoEncoder最小化：\n$$\nL(x,g(f(\\tilde{x})))\n$$\n其中$\\tilde{x}$是被某种噪声损坏的副本，因此DAE必须撤销这些损坏，而不是简单地复制输入。\n\n另一个Regularized AutoEncoder的策略是使用一个类似Sparse AutoEncoder中的惩罚项$\\Omega$，\n$$\nL(x,g(f(x)))+\\Omega(h,x)\n$$\n但$\\Omega$的形式不同：\n$$\n\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2\n$$\n这迫使模型学习一个在$x$变化很小时目标也没有太大变化的函数。因为这个惩罚只对training data适用，它迫使AutoEncoder学习可以反映training data distribution information的特征。这样的正则化AutoEncoder称为Contractive AutoEncoder(CAE)。\n\n## Details of Denosing AutoEncoder\nDAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的AutoEncoder。DAE的训练过程如下：我们引入一个损坏过程$C(\\tilde{x}|x)$，这个条件分布代表给定数据样本$x$产生损坏样本$\\tilde{x}$的概率。自编码器则根据以下过程，从训练数据对$(x,\\tilde{x})$中学习重构分布$p_{reconstruct}(x|\\tilde{x})$:\n1. 从training data中采一个训练样本$x$\n2. 从$C(\\tilde{x}|x=x)$采一个损坏样本$\\tilde{x}$\n3. 将$(x,\\tilde{x})$作为训练样本来估计AutoEncoder的重构分布$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$，其中$h$是Encoder$f(\\tilde{x})$的输出，$p_{decoder}$根据解码函数$g(h)$定义。\n\n\n","source":"_posts/dl-ae.md","raw":"---\ntitle: \"[DL] Auto Encoder\"\ndate: 2018-08-22 19:17:56\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Data Science\n- Auto Encoder\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Auto Encoder\n---\n## Introduction\nAuto Encoder是深度学习里一个用途非常广的无监督学习模型，常常用来降维或者特征学习(例如我在[豆瓣评论情感挖掘这个repository](https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp)里就使用了Deep AutoEncoder来学习300维word2vec里更discriminative的特征表达)。近年来随着GAN的火热，AutoEncoder也常常站在了generative model的前沿。\n\n## Undercomplete AutoEncoder\n从AutoEncoder里获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的AutoEncoder称为Undercomplete AutoEncoder。学习undercomplete representation将强制AutoEncoder捕捉training data中最显著的特征。\n\n当Decoder是线性的且Loss是MSE，Undercomplete AutoEncoder会学习出与PCA相同的生成子空间。这种情况下，AutoEncoder在训练来执行复制任务的同时学到了training data的主元子空间。因此，拥有non-linear Encoder和non-linear Decoder的AutoEncoder能够学习出更强大的PCA非线性推广。\n\n若Encoder和Decoder被赋予过大的容量，AutoEncoder会执行复制任务而捕捉不到任何有关数据分布的有用信息。\n\n## Regularized AutoEncoder\n若隐藏编码的维度允许与输入相等，或隐藏编码维数大于输入的overcomplete情况下，即使是linear Encoder和linear Decoder也可以学会将输入复制到输出，而学不到任何有关数据分布的有用信息。\n\nRegularized AutoEncoder使用的Loss Function鼓励模型学习其他特征(除了将输入复制到输出)，而不必限制使用浅层的Encoder和Decoder以及小的编码维数来限制模型容量。\n\nSparse AutoEncoder简单地在训练时结合Encoder层的稀疏惩罚$\\Omega(h)$和重构误差：\n$$\nL(x,g(f(x)))+\\Omega(h)\n$$\nSparse AutoEncoder一般用来学习特征。\n\n除了向Cost Function增加一个Regularization，我们也可以通过改变重构误差项来获得一个能学到有用信息的AutoEncoder。\n\nDenoising AutoEncoder最小化：\n$$\nL(x,g(f(\\tilde{x})))\n$$\n其中$\\tilde{x}$是被某种噪声损坏的副本，因此DAE必须撤销这些损坏，而不是简单地复制输入。\n\n另一个Regularized AutoEncoder的策略是使用一个类似Sparse AutoEncoder中的惩罚项$\\Omega$，\n$$\nL(x,g(f(x)))+\\Omega(h,x)\n$$\n但$\\Omega$的形式不同：\n$$\n\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2\n$$\n这迫使模型学习一个在$x$变化很小时目标也没有太大变化的函数。因为这个惩罚只对training data适用，它迫使AutoEncoder学习可以反映training data distribution information的特征。这样的正则化AutoEncoder称为Contractive AutoEncoder(CAE)。\n\n## Details of Denosing AutoEncoder\nDAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的AutoEncoder。DAE的训练过程如下：我们引入一个损坏过程$C(\\tilde{x}|x)$，这个条件分布代表给定数据样本$x$产生损坏样本$\\tilde{x}$的概率。自编码器则根据以下过程，从训练数据对$(x,\\tilde{x})$中学习重构分布$p_{reconstruct}(x|\\tilde{x})$:\n1. 从training data中采一个训练样本$x$\n2. 从$C(\\tilde{x}|x=x)$采一个损坏样本$\\tilde{x}$\n3. 将$(x,\\tilde{x})$作为训练样本来估计AutoEncoder的重构分布$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$，其中$h$是Encoder$f(\\tilde{x})$的输出，$p_{decoder}$根据解码函数$g(h)$定义。\n\n\n","slug":"dl-ae","published":1,"updated":"2018-10-01T04:40:08.655Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bx000c608w18wlry9o","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Auto Encoder是深度学习里一个用途非常广的无监督学习模型，常常用来降维或者特征学习(例如我在<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">豆瓣评论情感挖掘这个repository</a>里就使用了Deep AutoEncoder来学习300维word2vec里更discriminative的特征表达)。近年来随着GAN的火热，AutoEncoder也常常站在了generative model的前沿。</p>\n<h2 id=\"Undercomplete-AutoEncoder\"><a href=\"#Undercomplete-AutoEncoder\" class=\"headerlink\" title=\"Undercomplete AutoEncoder\"></a>Undercomplete AutoEncoder</h2><p>从AutoEncoder里获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的AutoEncoder称为Undercomplete AutoEncoder。学习undercomplete representation将强制AutoEncoder捕捉training data中最显著的特征。</p>\n<p>当Decoder是线性的且Loss是MSE，Undercomplete AutoEncoder会学习出与PCA相同的生成子空间。这种情况下，AutoEncoder在训练来执行复制任务的同时学到了training data的主元子空间。因此，拥有non-linear Encoder和non-linear Decoder的AutoEncoder能够学习出更强大的PCA非线性推广。</p>\n<p>若Encoder和Decoder被赋予过大的容量，AutoEncoder会执行复制任务而捕捉不到任何有关数据分布的有用信息。</p>\n<h2 id=\"Regularized-AutoEncoder\"><a href=\"#Regularized-AutoEncoder\" class=\"headerlink\" title=\"Regularized AutoEncoder\"></a>Regularized AutoEncoder</h2><p>若隐藏编码的维度允许与输入相等，或隐藏编码维数大于输入的overcomplete情况下，即使是linear Encoder和linear Decoder也可以学会将输入复制到输出，而学不到任何有关数据分布的有用信息。</p>\n<p>Regularized AutoEncoder使用的Loss Function鼓励模型学习其他特征(除了将输入复制到输出)，而不必限制使用浅层的Encoder和Decoder以及小的编码维数来限制模型容量。</p>\n<p>Sparse AutoEncoder简单地在训练时结合Encoder层的稀疏惩罚$\\Omega(h)$和重构误差：<br>$$<br>L(x,g(f(x)))+\\Omega(h)<br>$$<br>Sparse AutoEncoder一般用来学习特征。</p>\n<p>除了向Cost Function增加一个Regularization，我们也可以通过改变重构误差项来获得一个能学到有用信息的AutoEncoder。</p>\n<p>Denoising AutoEncoder最小化：<br>$$<br>L(x,g(f(\\tilde{x})))<br>$$<br>其中$\\tilde{x}$是被某种噪声损坏的副本，因此DAE必须撤销这些损坏，而不是简单地复制输入。</p>\n<p>另一个Regularized AutoEncoder的策略是使用一个类似Sparse AutoEncoder中的惩罚项$\\Omega$，<br>$$<br>L(x,g(f(x)))+\\Omega(h,x)<br>$$<br>但$\\Omega$的形式不同：<br>$$<br>\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2<br>$$<br>这迫使模型学习一个在$x$变化很小时目标也没有太大变化的函数。因为这个惩罚只对training data适用，它迫使AutoEncoder学习可以反映training data distribution information的特征。这样的正则化AutoEncoder称为Contractive AutoEncoder(CAE)。</p>\n<h2 id=\"Details-of-Denosing-AutoEncoder\"><a href=\"#Details-of-Denosing-AutoEncoder\" class=\"headerlink\" title=\"Details of Denosing AutoEncoder\"></a>Details of Denosing AutoEncoder</h2><p>DAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的AutoEncoder。DAE的训练过程如下：我们引入一个损坏过程$C(\\tilde{x}|x)$，这个条件分布代表给定数据样本$x$产生损坏样本$\\tilde{x}$的概率。自编码器则根据以下过程，从训练数据对$(x,\\tilde{x})$中学习重构分布$p_{reconstruct}(x|\\tilde{x})$:</p>\n<ol>\n<li>从training data中采一个训练样本$x$</li>\n<li>从$C(\\tilde{x}|x=x)$采一个损坏样本$\\tilde{x}$</li>\n<li>将$(x,\\tilde{x})$作为训练样本来估计AutoEncoder的重构分布$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$，其中$h$是Encoder$f(\\tilde{x})$的输出，$p_{decoder}$根据解码函数$g(h)$定义。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Auto Encoder是深度学习里一个用途非常广的无监督学习模型，常常用来降维或者特征学习(例如我在<a href=\"https://github.com/lucasxlu/XiaoLuAI/tree/master/nlp\" target=\"_blank\" rel=\"noopener\">豆瓣评论情感挖掘这个repository</a>里就使用了Deep AutoEncoder来学习300维word2vec里更discriminative的特征表达)。近年来随着GAN的火热，AutoEncoder也常常站在了generative model的前沿。</p>\n<h2 id=\"Undercomplete-AutoEncoder\"><a href=\"#Undercomplete-AutoEncoder\" class=\"headerlink\" title=\"Undercomplete AutoEncoder\"></a>Undercomplete AutoEncoder</h2><p>从AutoEncoder里获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的AutoEncoder称为Undercomplete AutoEncoder。学习undercomplete representation将强制AutoEncoder捕捉training data中最显著的特征。</p>\n<p>当Decoder是线性的且Loss是MSE，Undercomplete AutoEncoder会学习出与PCA相同的生成子空间。这种情况下，AutoEncoder在训练来执行复制任务的同时学到了training data的主元子空间。因此，拥有non-linear Encoder和non-linear Decoder的AutoEncoder能够学习出更强大的PCA非线性推广。</p>\n<p>若Encoder和Decoder被赋予过大的容量，AutoEncoder会执行复制任务而捕捉不到任何有关数据分布的有用信息。</p>\n<h2 id=\"Regularized-AutoEncoder\"><a href=\"#Regularized-AutoEncoder\" class=\"headerlink\" title=\"Regularized AutoEncoder\"></a>Regularized AutoEncoder</h2><p>若隐藏编码的维度允许与输入相等，或隐藏编码维数大于输入的overcomplete情况下，即使是linear Encoder和linear Decoder也可以学会将输入复制到输出，而学不到任何有关数据分布的有用信息。</p>\n<p>Regularized AutoEncoder使用的Loss Function鼓励模型学习其他特征(除了将输入复制到输出)，而不必限制使用浅层的Encoder和Decoder以及小的编码维数来限制模型容量。</p>\n<p>Sparse AutoEncoder简单地在训练时结合Encoder层的稀疏惩罚$\\Omega(h)$和重构误差：<br>$$<br>L(x,g(f(x)))+\\Omega(h)<br>$$<br>Sparse AutoEncoder一般用来学习特征。</p>\n<p>除了向Cost Function增加一个Regularization，我们也可以通过改变重构误差项来获得一个能学到有用信息的AutoEncoder。</p>\n<p>Denoising AutoEncoder最小化：<br>$$<br>L(x,g(f(\\tilde{x})))<br>$$<br>其中$\\tilde{x}$是被某种噪声损坏的副本，因此DAE必须撤销这些损坏，而不是简单地复制输入。</p>\n<p>另一个Regularized AutoEncoder的策略是使用一个类似Sparse AutoEncoder中的惩罚项$\\Omega$，<br>$$<br>L(x,g(f(x)))+\\Omega(h,x)<br>$$<br>但$\\Omega$的形式不同：<br>$$<br>\\Omega(h,x)=\\lambda \\sum_i ||\\triangledown_xh_i||^2<br>$$<br>这迫使模型学习一个在$x$变化很小时目标也没有太大变化的函数。因为这个惩罚只对training data适用，它迫使AutoEncoder学习可以反映training data distribution information的特征。这样的正则化AutoEncoder称为Contractive AutoEncoder(CAE)。</p>\n<h2 id=\"Details-of-Denosing-AutoEncoder\"><a href=\"#Details-of-Denosing-AutoEncoder\" class=\"headerlink\" title=\"Details of Denosing AutoEncoder\"></a>Details of Denosing AutoEncoder</h2><p>DAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的AutoEncoder。DAE的训练过程如下：我们引入一个损坏过程$C(\\tilde{x}|x)$，这个条件分布代表给定数据样本$x$产生损坏样本$\\tilde{x}$的概率。自编码器则根据以下过程，从训练数据对$(x,\\tilde{x})$中学习重构分布$p_{reconstruct}(x|\\tilde{x})$:</p>\n<ol>\n<li>从training data中采一个训练样本$x$</li>\n<li>从$C(\\tilde{x}|x=x)$采一个损坏样本$\\tilde{x}$</li>\n<li>将$(x,\\tilde{x})$作为训练样本来估计AutoEncoder的重构分布$p_{reconstruct}(x|\\tilde{x})=p_{decoder}(x|h)$，其中$h$是Encoder$f(\\tilde{x})$的输出，$p_{decoder}$根据解码函数$g(h)$定义。</li>\n</ol>\n"},{"title":"[DL] Architecture","date":"2018-11-18T14:29:40.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Computer Vision","Image Classification","Network Architecture"],"_content":"## Introduction\nDeep Learning有三宝：Network Architecture，Loss Function and Optimization。对于大多数人而言，Optimization门槛还是很高的（需要非常深厚的数学功底），所以绝大多数的Paper偏向还是设计更好的Network Architecture或者堆更加精巧的Loss Function。Ian Goodfellow大佬也曾说过：现如今Deep Learning的繁荣，网络结构探究的贡献度远远高于优化算法的贡献度。所以本文旨在梳理从AlexNet到CliqueNet这些经典的work。\n> [@LucasX](https://www.zhihu.com/people/xulu-0620)注：对于优化算法，可参考我的[这一篇文章](https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/)。\n\n## AlexNet\n> Paper: [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\nAlexNet可以看作是Deep Learning在Large Scale Image Classification Task上第一次大放异彩。也是从AlexNet起，越来越多的Computer Vision Researcher开始将重心由设计更好的hand-crafted features转为设计更加精巧的网络结构。因此AlexNet是具备划时代意义的经典work。\n\nAlexNet整体结构其实非常非常简单，5层conv + 3层FC + Softmax。AlexNet使用了ReLU来代替Sigmoid作为non-linearity transformation，并且使用双GPU训练，以及一系列的Data Augmentation操作，Dropout，对于今天的工作仍然具备很深远的影响。\n\n## VGG\n> Paper: [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)\n\nVGG也是一篇非常经典的工作，并且在今天的很多任务上依旧可以看到VGG的影子。不同于AlexNet，VGG使用了非常小的Filter($3\\times 3$)，以及类似于<font color=\"red\">basic block</font>的结构(读者不妨回想一下GoogLeNet、ResNet、ResNeXt、DenseNet是不是也是由一系列block堆积而成的)。\n\n不妨思考一下为啥要用$3\\times 3$的卷积核呢？\n1. 两个堆叠的$3\\times 3$卷积核对应$5\\times 5$的receptive field。而三个$3\\times 3$卷积核对应$7\\times 7$的receptive field。那为啥不直接用$7\\times 7$卷积呢？原因就在于通过堆叠的3个$3\\times 3$卷积核，<font color=\"red\">我们引入了更多的non-linearity transformation，这有助于我们的网络学习更加discriminative的特征表达</font>。\n2. 减少了参数：3个channel为$C$的$3\\times 3$卷积的参数为: $3(3^2C^2)=27C^2$。而一个channel为$C$的$7\\times 7$卷积的参数为: $7^2C^2=49C^2$。\n    > This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)\n\n\n## GoogLeNet\n> Paper: [Going deeper with convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n\n因DCNN在一系列CV任务上均取得了非常好的效果，所以大家开始将精力由hand-crafted features转换到network architecture上来了。GoogLeNet也是经典网络中一个非常值得关注的模型，其中值得关注的设计就是**Multi-branch + Feature Concatenation**，这是今天很多深度学习算法也依旧在使用的方法。GoogLeNet中，作者大量使用了$1\\times 1$ conv (注：$1\\times 1$ conv最先来自[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))，这样有以下好处：\n* 作为dimension reduction来remove computational bottlenecks\n* 既然computational bottlenecks减少了，那么在相同FLOPs下，我们可以设计更加deep的网络结构，从而辅助更好的representation learning\n\nInception Module的基础结构如下图所示：\n![Inception Module](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg)\n> 在走每一个$3\\times 3$和$5\\times 5$ conv之前，先过一遍$1\\times 1$ conv，一方面可以起到**dimension reduction**的作用；另一方面也引入了更多的**non-linearity transformation**，而这对于整个网络的representation learning ability是非常重要的(这个套路基本和[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)一样，感兴趣的读者可以去阅读[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)原文)。\n\nGoogLeNet就是通过一系列的Inception Module堆叠而成(读者不妨再仔细思考一下，VGG/ResNet/ResNeXt等等网络是不是也是由一系列小block堆叠而成？)。此外，因GoogLeNet是Multi-branch的结构，所以作者在中间层也添加了classification layer作为supervision来辅助gradient flow(读者不妨回忆一下，经典的人脸识别算法DeepID是不是也是这么做的？)。\n\n\n## ResNet\n> Paper: [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n作者认为，ResNet可以称得上是自AlexNet以来，Deep Learning发展最insightful的idea，ResNet的主角shortcut至今也被广泛应用与Deep Architecture的设计中(如DenseNet, CliqueNet, Deep Layer Aggregation等)。\n此前的网络设计趋势是“越来越深”，但神经网络的设计真的就如同段子所言“一层一层往上堆叠就好了吗？”显然不是的，ResNet作者Kaiming He大神在Paper中做了一些实验，验证了当Network越来越深时，Accuracy就饱和了，然后迅速下降，值得一提的是<font color=\"red\">这种性能下降并不是由于参数过多随之而来的overfitting造成的</font>。\n\n### What is Residual Network?\n设想DNN的目的是为了学习某种function $\\mathcal{H}(x)$，作者并没有直接设计DNN Architecture去学习这种function，而是先学习另一种function $\\mathcal{F}(x):=\\mathcal{H}(x) - x$，那么原来的$\\mathcal{H}(x)$是不是就可以表示成<font color=\"red\">$\\mathcal{F}(x)+x$</font>。作者假设这种结构比原先的$\\mathcal{H}(x)$更容易优化。\n> 例如，若某种identity mapping是最优的，那么，将残差push到0要比通过一系列non-linearity transformation来学习identity mapping更为高效。\n \nShortcut可以表示成如下结构：\n$$\ny=\\mathcal{F}(x, \\{W_i\\}) + x\n$$\n$\\mathcal{F}(x, \\{W_i\\})$可以表示多个conv layers，两个feature map通过channel by channel element-wise 叠加。\n\n网络结构的设计方面，依旧是参考了著名的[VGG](https://arxiv.org/pdf/1409.1556v6.pdf)，即：使用大量$3\\times 3$ filters并且遵循这两条原则：\n1. 对于输出相同feature map size的层使用相同数量的filter\n2. 若feature map size减半，则filter的数量则翻倍，来维持每一层的time complexity\n\n对于feature map dimension相同的情况，则只需要element-wise addition即可；若feature map dimension double了，可以采取zero padding来增加dimension，或者采用$1\\times 1$ conv来进行升维。\n\nResNet到这里基本就介绍完了，实验部分当然是在classification/detection/segmentation task上吊打了当前所有的state-of-the-art。ResNet很简单的idea对不对？不得不佩服一下Kaiming大神，他的东西总是简单而有效！\n\n\n## ShuffleNet\n在Computer Vision领域，除了像AlexNet、VGG、GoogLeNet、ResNet、DenseNet、CliqueNet等一系列比较“重量级”的网络结构之外，也有一些非常轻量级的模型，而轻量级模型对于移动设备而言无疑是非常重要的。这里就介绍一下轻量级模型的代表作之一：[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)。\n> Paper: [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n\n### What is ShuffleNet?\n[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)结构最重要的两个部分就是**Pointwise Group Convolution**和**Channel Shuffle**。\n\n![Channel Shuffle](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg)\n\n![ShuffleNet Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg)\n\n> Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.\n\n#### Advantages of Point-Wise Convolution\nNote that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.\n\n#### Channel Shuffle vs. No Shuffle\nThe purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.\n\n\n## MobileNet V1\n> Paper: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861v1.pdf)\n\nCNN驱动了许多视觉任务的飞速发展，然而传统结构例如ResNet、Inception、VGG等FLOP非常大，这使得对于移动端和嵌入式设备的训练与部署变得非常困难。所以近些年来，轻量级网络的设计也成为了一个非常热门的研究方向，[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)和[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)就是其中的代表。前面我们已经介绍了[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)，本篇我们就大致回顾一下[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)。\n\n### Depth-wise Separable Convolution\nMobileNet最主要的结构就是**Depth-wise Separable Convolution**。DW Conv为什么能减少model size呢？我们不妨先来细致分析一下传统的卷积需要多少参数:\n假设传统卷积层接受一个$D_F\\times D_F\\times M$的feature map作为输入，然后输出$D_F\\times D_F\\times N$的feature map，所以卷积核的size是$D_K\\times D_K\\times M\\times N$，所以需要的计算量为：$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$，所以Computational Cost依赖于input channel $M$，output channel $N$，卷积核尺寸$D_K\\times D_K$和feature map的尺寸$D_F\\times D_F$。\n\n但是MobileNet应用Depth-wise Conv来对Kernel Size和Output Channel进行了解耦。传统Conv Operation通过filters来对features进行filter，然后重组(Combinations)以形成新的representations。Filtering和Combinations可通过DW Separable Conv来分成两步进行。\n\n**Depth-wise Separable Convolution由两层组成：Depth-wise Conv + Point-wise Conv**。DW Conv对于每个channel应用单个filter，PW Conv (a simple $1\\times 1$ Conv)用来建立DW layer输出的linear combination。**DW Conv的Computational Cost为: $D_K\\times D_K\\times M\\times D_F\\times D_F$**，与传统Conv相比低了$N$倍。\n\nDW Conv虽然高效，然而它仅仅filter了input channel，__却没有对其进行combination__，所以需要额外的layer来对DW Conv后的feature进行linear combination来产生新的representation，这就是基于$1\\times 1$ Conv的PW Conv。\n\nThe combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.\n\nDW Separable Conv的Computational Cost为：\n$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$，前者为DW Conv的cost，后者为PW Conv的cost。\n\nBy expressing convolution as a two step process of filtering and combining we get a reduction in computation of:\n$$\n\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}\n$$\n可以看到，Depth-wise Separable Conv的计算量仅仅为传统Conv的$\\frac{1}{N} + \\frac{1}{D_K^2}$。\n![DW Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png)\n\n### Width Multiplier: Thinner Models\nIn order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:\n\n$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$\n\nwhere $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha < 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.\n\n### Resolution Multiplier: Reduced Representation\nThe second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:\n \n$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$\n\nwhere $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and ρ < 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.\n\n\n## MobileNet V2\n> Paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)\n\n[MobileNet V2](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)是发表在[CVPR2018](http://openaccess.thecvf.com/CVPR2018.py)上的Paper，在移动端网络的设计方面又向前走了一步。MobileNet V2最大的contribution如下: \n\n> Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.\n\n### Preliminaries, discussion and intuition\n#### Depthwise Separable Convolutions\n说起Light-weighted Architecture呢，DW Conv是必不可少的组件了，同理，MobileNet V2也是基于DW Conv做的改进。\n\n> The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.\n\n传统Conv接受一个$h_i\\times w_i\\times d_i$输入，应用卷积核$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$来产生$h_i\\times w_i\\times d_j$的输出。所以传统Conv的Computational Cost为：$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$。而DW Separable Conv的Computational Cost仅仅为：$h_i\\times w_i\\times d_i(k^2+d_j)$，__减少了将近$k^2$倍的计算量__。\n\n#### Linear Bottlenecks\n> It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.\n\nDCNN的架构大致是这样的：Conv + ReLU + (Pool) + (FC) + Softmax。DNN之所以拟合能力超强，原因就在于non-linearity transformation，而由于gradient vanishing/exploding的原因，Sigmoid已经淡出了历史舞台，取而代之的是ReLU。我们来分析分析ReLU有什么缺点：\n\n> It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. **In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.**\n\n> On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.\n\n简而言之，ReLu有以下两种性质：\n1. 若Manifold of Interest在ReLU之后非零，那么它就相当于是一个线性变换。\n2. ReLU能够保存input manifold完整的信息，**但是当且仅当input manifold位于input space的低维子空间中时**。\n\n所以，也就不难理解为什么MobileNet V2要先将high-dimensional hidden representations先做一次low-dimensional embedding，然后再变换回到high-dimensional了。\n\n![Evolution of Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png)\n\n#### Inverted residuals\n> The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.\n\n回想一下，[MobileNet V1](https://arxiv.org/pdf/1704.04861v1.pdf)的结构就是一个普通的feedforwad network，而shortcut在[ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)里已经被证明是非常effective的了，所以MobileNet V2自然而然地引入了skip connection了。\n\n#### MobileNet V2 Architecture\n> We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.\n\n![DCNN Architecture Comparison](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png)\n\n\n## ResNeXt\n> Paper: [Aggregated Residual Transformations for Deep Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)\n\n### Introduction\n自AlexNet以来，Deep Learning涌现了一大批设计优良的网络(如VGG，Inception，ResNet等)。ResNeXt则在ResNet的基础上，一定程度上参考了Inception的设计，即**split-transform-merge**。\n\n\n### The Core of ResNeXt\n> Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 1×1 convolutions), transformed by a set of specialized filters (3×3, 5×5, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.\n\n尽管Inception的**split-transform-merge**策略是非常行之有效的，但是该网络结构过于复杂，人工设计的痕迹过重(相比之下VGG和ResNet则是由相同的block stacking而成)，给人的感觉就是专门为了ImageNet去做的优化，所以当你想要迁移到其他的dataset时就会比较麻烦。因此，ResNeXt的设计就是：在VGG/ResNet的stacking block的基础上，融合进了Inception的split-transform-merge策略。这就是ResNeXt的基础idea。作者在实验中发现cardinality (the size of the set of transformations)对performance的影响是最大的，甚至要大于width和depth。\n\n> Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view\nour method as ensembling, because the members to be aggregated\nare trained jointly, not independently.\n\n> The above operation can be recast as a combination of\nsplitting, transforming, and aggregating. \n1. Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. \n2. Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.\n3. Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.\n\n若将$W$更换为更一般的形式，即任意一种function mapping: $\\mathcal{T}(x)$，那么aggregated transformations就变成了:\n$$\n\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n其中$\\mathcal{T}_i$可以将$x$映射到低维空间。$C$是transformation的size，也就是本文主角——**cardinality**。\n\n> In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.\n\n![ResNeXt Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg)\n\n那么在ResNet的identical mapping背景下，就变成了这样一种熟悉的结构：\n$$\ny=x+\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n\n> **Relation to Grouped Convolutions**. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.\n\n![Equivalent Building Blocks of ResNeXt](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg)\n\n\n## DenseNet\n> Paper: [Densely Connected Convolutional Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n\n### What is DenseNet?\nDenseNet是CVPR2017 Best Paper，是继ResNet之后更加优秀的网络。[前面我们已经介绍过](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)，ResNet一定程度上解决了gradient vanishing的问题，通过ResNet中的identical mapping使得网络深度可以到达上千层。那么DenseNet又做了哪些改进呢？本文为你一一解答！\n\n在介绍DenseNet之前，我们先回顾一下ResNet做了什么改动，当shortcuts还未被引入DCNN之前，AlexNet/VGG/GoogLeNet都属于构造比较简单的feedforward network，即信息**一层一层往前传播，在BP时梯度一层一层往后传**，但是这样在网络结构很深的时候，就会存在gradient vanishing的问题。所以Kaiming He创造性地引入了skip connection，来使得信息可以从第$i$层之间做identical mapping传播到第$i+t$层，这样就保证了信息的高效流通。\n> 注: 关于ResNet更详细的介绍，请参考[这里](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)。\n\n![Dense Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg)\n\n而DenseNet，就是把这种skip connection做到了极致。为了保证信息在不同layer之间的流通，DenseNet将skip connection做到了每一层和该层之后的所有层中。和ResNet中采用的DW Summation不同的是，DenseNet直接concatenate不同层的features (因为ResNet中DW Summation会影响信息流动)。\n\n值得一提的是，**DenseNet比ResNet的参数更少**。因为dense connection的结构，使得网络不需要重新学习多余的feature maps。\n此外，every layer都可以从loss层直接获得梯度，从early layer是获得信息流，也产生了一种**deep supervision**。最后，作者还注意到dense connections一定程度上可以视为Regularization，所以可以缓解overfitting。\n\n它有以下优点：\n* 减轻了gradient vanishing问题\n* 加强了梯度传播\n* 更好的feature reuse\n* 极大地减少了参数\n\n### Delve into DenseNet\n#### Dense connectivity\nTo further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:\n$$\nx_l=H_l([x_0,x_1,\\cdots,x_{l-1}])\n$$\nwhere $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l−1$.\n\n#### Pooling layers\nThe concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.\n\n#### Growth rate\nIf each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l−1)$ input\nfeature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.\n\n**One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's \"collective knowledge\". One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer**.\n\n#### Bottleneck layers\nAlthough each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.\n\n\n## Identity Mappings in Deep Residual Networks\n> Paper: [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n\n### Introduction\nResNet已经成了很多CV任务的标配，作者Kaiming He在ResNet里引入了shortcut来辅助DCNN的学习与优化，但是对于shortcut为什么能work则没有过多提及。本文是ResNet作者本人发表在ECCV'16上的Paper，主要在于解释identical mapping为何能work，并且对比了identical mapping的一些变体，最后提出了pre-activation。\n\n### Delve Into ResNet and Identical Mapping\nResNet可以表示为这样：\n$$\ny_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\n\n$$\nx_{l+1} = f(y_l)\n$$\n其中，$\\mathcal{F}$代表residual function，$h(x_l)=x_l$代表identical mapping，$f$代表ReLU函数。\n\n在本文中，作者发现，**若$h(x_l)$和$f(y_l)$都是identical mapping的话，信息就可以直接从一个unit传播到下几层的units，无论是在forward还是backward都是如此**。\n\n![Proposed Residual Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg)\n\n为了构造identical mapping $f(y_l)=y_l$，我们可以将activation function (ReLU and BN)看作weight layers的```pre-activation```。\n\n### Analysis of Deep Residual Networks\nCVPR'15 Best Paper中的原始ResNet Unit是这样的：\n$$\ny_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)\n$$\n\n$$\nx_{l+1}=f(y_l)\n$$\n若$f$也是identical mapping: $x_{l+1}\\equiv y_l$，就可以得到：\n$$\nx_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\nRecursively，我们可以得到：\n$$\nx_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})\n$$\n所以有：\n$$\nx_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\n$$\n所以，对于deep的$L$层和shallow的$l$层，特征$x_L$可以表示成**shallow unit feature $x_l$和residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$的加和**！说明：\n1. **模型是任意units $L$和$l$ 的residual function**\n2. 特征$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$是所有proceeding residual functions输出的summation再加上$x_0$\n\nBP的时候，根据chain rule，就得到如下公式(假设loss function为$\\epsilon$)：\n$$\n\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))\n$$\n所以，梯度$\\frac{\\partial \\epsilon}{\\partial x_l}$可以看作两个部分：\n1. $\\frac{\\partial \\epsilon}{\\partial x_L}$直接从高层流通回来\n2. $\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$流经了其他的weight layers\n\n#### Discussions\nPaper里也对一些identical mapping的变体进行了实验与探讨，反正scaling, gating, $1\\times 1$ convolutions, and dropout都效果不如原来的好。并且，**由于$1\\times 1$ conv**引入了更多的参数，理论上讲representation learning ability是要比原来的ResNet要高的，结果却比原来低，说明这种performance drop不是因为representation ability，而是因为优化问题所致。\n\n> The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations\n(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.\n\n此外，作者还验证了，当使用BN + ReLU作为pre-activation时，模型performance有了显著地改善。这种改善主要由两点带来：\n1. 因为$f$是identical mapping，所以整个模型的optimization更容易了。\n2. 使用BN作为pre-activation增加了模型的regularization，因BN本身具有regularization的效果。在CVPR'15原始版本的ResNet中，尽管BN normalize了信号，但是却立刻被添加进了shortcut，和未被BN normalize的signal一起merge了。而在pre-activation中，所有weight layers的input均被normalize了。\n\n\n## Reference\n1. Krizhevsky A, Sutskever I, Hinton G E. [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[C]//Advances in neural information processing systems. 2012: 1097-1105.\n2. Simonyan K, Zisserman A. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)[J]. arXiv preprint arXiv:1409.1556, 2014.\n3. Lin M, Chen Q, Yan S. [Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[J]. arXiv preprint arXiv:1312.4400, 2013.\n4. He K, Zhang X, Ren S, Sun J. [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).\n5. Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n6. Chollet, Francois. [\"Xception: Deep Learning with Depthwise Separable Convolutions.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.\n7. Howard, Andrew G., et al. [\"Mobilenets: Efficient convolutional neural networks for mobile vision applications.\"](https://arxiv.org/pdf/1704.04861v1.pdf) arXiv preprint arXiv:1704.04861 (2017).\n8. Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. [\"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\"](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n9. Xie, Saining, et al. [\"Aggregated residual transformations for deep neural networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf) Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.\n10. Huang, Gao, et al. [\"Densely Connected Convolutional Networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) CVPR. Vol. 1. No. 2. 2017.\n11. He K, Zhang X, Ren S, et al. [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.\n12. Szegedy, Christian, et al. [\"Going deeper with convolutions.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n","source":"_posts/dl-architecture.md","raw":"---\ntitle: \"[DL] Architecture\"\ndate: 2018-11-18 22:29:40\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Image Classification\n- Network Architecture\ncatagories:\n- Machine Learning\n- Deep Learning\n- Computer Vision\n- Image Classification\n- Network Architecture\n---\n## Introduction\nDeep Learning有三宝：Network Architecture，Loss Function and Optimization。对于大多数人而言，Optimization门槛还是很高的（需要非常深厚的数学功底），所以绝大多数的Paper偏向还是设计更好的Network Architecture或者堆更加精巧的Loss Function。Ian Goodfellow大佬也曾说过：现如今Deep Learning的繁荣，网络结构探究的贡献度远远高于优化算法的贡献度。所以本文旨在梳理从AlexNet到CliqueNet这些经典的work。\n> [@LucasX](https://www.zhihu.com/people/xulu-0620)注：对于优化算法，可参考我的[这一篇文章](https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/)。\n\n## AlexNet\n> Paper: [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\nAlexNet可以看作是Deep Learning在Large Scale Image Classification Task上第一次大放异彩。也是从AlexNet起，越来越多的Computer Vision Researcher开始将重心由设计更好的hand-crafted features转为设计更加精巧的网络结构。因此AlexNet是具备划时代意义的经典work。\n\nAlexNet整体结构其实非常非常简单，5层conv + 3层FC + Softmax。AlexNet使用了ReLU来代替Sigmoid作为non-linearity transformation，并且使用双GPU训练，以及一系列的Data Augmentation操作，Dropout，对于今天的工作仍然具备很深远的影响。\n\n## VGG\n> Paper: [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)\n\nVGG也是一篇非常经典的工作，并且在今天的很多任务上依旧可以看到VGG的影子。不同于AlexNet，VGG使用了非常小的Filter($3\\times 3$)，以及类似于<font color=\"red\">basic block</font>的结构(读者不妨回想一下GoogLeNet、ResNet、ResNeXt、DenseNet是不是也是由一系列block堆积而成的)。\n\n不妨思考一下为啥要用$3\\times 3$的卷积核呢？\n1. 两个堆叠的$3\\times 3$卷积核对应$5\\times 5$的receptive field。而三个$3\\times 3$卷积核对应$7\\times 7$的receptive field。那为啥不直接用$7\\times 7$卷积呢？原因就在于通过堆叠的3个$3\\times 3$卷积核，<font color=\"red\">我们引入了更多的non-linearity transformation，这有助于我们的网络学习更加discriminative的特征表达</font>。\n2. 减少了参数：3个channel为$C$的$3\\times 3$卷积的参数为: $3(3^2C^2)=27C^2$。而一个channel为$C$的$7\\times 7$卷积的参数为: $7^2C^2=49C^2$。\n    > This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)\n\n\n## GoogLeNet\n> Paper: [Going deeper with convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n\n因DCNN在一系列CV任务上均取得了非常好的效果，所以大家开始将精力由hand-crafted features转换到network architecture上来了。GoogLeNet也是经典网络中一个非常值得关注的模型，其中值得关注的设计就是**Multi-branch + Feature Concatenation**，这是今天很多深度学习算法也依旧在使用的方法。GoogLeNet中，作者大量使用了$1\\times 1$ conv (注：$1\\times 1$ conv最先来自[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf))，这样有以下好处：\n* 作为dimension reduction来remove computational bottlenecks\n* 既然computational bottlenecks减少了，那么在相同FLOPs下，我们可以设计更加deep的网络结构，从而辅助更好的representation learning\n\nInception Module的基础结构如下图所示：\n![Inception Module](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg)\n> 在走每一个$3\\times 3$和$5\\times 5$ conv之前，先过一遍$1\\times 1$ conv，一方面可以起到**dimension reduction**的作用；另一方面也引入了更多的**non-linearity transformation**，而这对于整个网络的representation learning ability是非常重要的(这个套路基本和[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)一样，感兴趣的读者可以去阅读[Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)原文)。\n\nGoogLeNet就是通过一系列的Inception Module堆叠而成(读者不妨再仔细思考一下，VGG/ResNet/ResNeXt等等网络是不是也是由一系列小block堆叠而成？)。此外，因GoogLeNet是Multi-branch的结构，所以作者在中间层也添加了classification layer作为supervision来辅助gradient flow(读者不妨回忆一下，经典的人脸识别算法DeepID是不是也是这么做的？)。\n\n\n## ResNet\n> Paper: [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n作者认为，ResNet可以称得上是自AlexNet以来，Deep Learning发展最insightful的idea，ResNet的主角shortcut至今也被广泛应用与Deep Architecture的设计中(如DenseNet, CliqueNet, Deep Layer Aggregation等)。\n此前的网络设计趋势是“越来越深”，但神经网络的设计真的就如同段子所言“一层一层往上堆叠就好了吗？”显然不是的，ResNet作者Kaiming He大神在Paper中做了一些实验，验证了当Network越来越深时，Accuracy就饱和了，然后迅速下降，值得一提的是<font color=\"red\">这种性能下降并不是由于参数过多随之而来的overfitting造成的</font>。\n\n### What is Residual Network?\n设想DNN的目的是为了学习某种function $\\mathcal{H}(x)$，作者并没有直接设计DNN Architecture去学习这种function，而是先学习另一种function $\\mathcal{F}(x):=\\mathcal{H}(x) - x$，那么原来的$\\mathcal{H}(x)$是不是就可以表示成<font color=\"red\">$\\mathcal{F}(x)+x$</font>。作者假设这种结构比原先的$\\mathcal{H}(x)$更容易优化。\n> 例如，若某种identity mapping是最优的，那么，将残差push到0要比通过一系列non-linearity transformation来学习identity mapping更为高效。\n \nShortcut可以表示成如下结构：\n$$\ny=\\mathcal{F}(x, \\{W_i\\}) + x\n$$\n$\\mathcal{F}(x, \\{W_i\\})$可以表示多个conv layers，两个feature map通过channel by channel element-wise 叠加。\n\n网络结构的设计方面，依旧是参考了著名的[VGG](https://arxiv.org/pdf/1409.1556v6.pdf)，即：使用大量$3\\times 3$ filters并且遵循这两条原则：\n1. 对于输出相同feature map size的层使用相同数量的filter\n2. 若feature map size减半，则filter的数量则翻倍，来维持每一层的time complexity\n\n对于feature map dimension相同的情况，则只需要element-wise addition即可；若feature map dimension double了，可以采取zero padding来增加dimension，或者采用$1\\times 1$ conv来进行升维。\n\nResNet到这里基本就介绍完了，实验部分当然是在classification/detection/segmentation task上吊打了当前所有的state-of-the-art。ResNet很简单的idea对不对？不得不佩服一下Kaiming大神，他的东西总是简单而有效！\n\n\n## ShuffleNet\n在Computer Vision领域，除了像AlexNet、VGG、GoogLeNet、ResNet、DenseNet、CliqueNet等一系列比较“重量级”的网络结构之外，也有一些非常轻量级的模型，而轻量级模型对于移动设备而言无疑是非常重要的。这里就介绍一下轻量级模型的代表作之一：[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)。\n> Paper: [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)\n\n### What is ShuffleNet?\n[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)结构最重要的两个部分就是**Pointwise Group Convolution**和**Channel Shuffle**。\n\n![Channel Shuffle](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg)\n\n![ShuffleNet Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg)\n\n> Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.\n\n#### Advantages of Point-Wise Convolution\nNote that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.\n\n#### Channel Shuffle vs. No Shuffle\nThe purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.\n\n\n## MobileNet V1\n> Paper: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861v1.pdf)\n\nCNN驱动了许多视觉任务的飞速发展，然而传统结构例如ResNet、Inception、VGG等FLOP非常大，这使得对于移动端和嵌入式设备的训练与部署变得非常困难。所以近些年来，轻量级网络的设计也成为了一个非常热门的研究方向，[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)和[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)就是其中的代表。前面我们已经介绍了[ShuffleNet](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)，本篇我们就大致回顾一下[MobileNet](https://arxiv.org/pdf/1704.04861v1.pdf)。\n\n### Depth-wise Separable Convolution\nMobileNet最主要的结构就是**Depth-wise Separable Convolution**。DW Conv为什么能减少model size呢？我们不妨先来细致分析一下传统的卷积需要多少参数:\n假设传统卷积层接受一个$D_F\\times D_F\\times M$的feature map作为输入，然后输出$D_F\\times D_F\\times N$的feature map，所以卷积核的size是$D_K\\times D_K\\times M\\times N$，所以需要的计算量为：$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$，所以Computational Cost依赖于input channel $M$，output channel $N$，卷积核尺寸$D_K\\times D_K$和feature map的尺寸$D_F\\times D_F$。\n\n但是MobileNet应用Depth-wise Conv来对Kernel Size和Output Channel进行了解耦。传统Conv Operation通过filters来对features进行filter，然后重组(Combinations)以形成新的representations。Filtering和Combinations可通过DW Separable Conv来分成两步进行。\n\n**Depth-wise Separable Convolution由两层组成：Depth-wise Conv + Point-wise Conv**。DW Conv对于每个channel应用单个filter，PW Conv (a simple $1\\times 1$ Conv)用来建立DW layer输出的linear combination。**DW Conv的Computational Cost为: $D_K\\times D_K\\times M\\times D_F\\times D_F$**，与传统Conv相比低了$N$倍。\n\nDW Conv虽然高效，然而它仅仅filter了input channel，__却没有对其进行combination__，所以需要额外的layer来对DW Conv后的feature进行linear combination来产生新的representation，这就是基于$1\\times 1$ Conv的PW Conv。\n\nThe combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.\n\nDW Separable Conv的Computational Cost为：\n$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$，前者为DW Conv的cost，后者为PW Conv的cost。\n\nBy expressing convolution as a two step process of filtering and combining we get a reduction in computation of:\n$$\n\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}\n$$\n可以看到，Depth-wise Separable Conv的计算量仅仅为传统Conv的$\\frac{1}{N} + \\frac{1}{D_K^2}$。\n![DW Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png)\n\n### Width Multiplier: Thinner Models\nIn order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:\n\n$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$\n\nwhere $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha < 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.\n\n### Resolution Multiplier: Reduced Representation\nThe second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:\n \n$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$\n\nwhere $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and ρ < 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.\n\n\n## MobileNet V2\n> Paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)\n\n[MobileNet V2](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)是发表在[CVPR2018](http://openaccess.thecvf.com/CVPR2018.py)上的Paper，在移动端网络的设计方面又向前走了一步。MobileNet V2最大的contribution如下: \n\n> Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.\n\n### Preliminaries, discussion and intuition\n#### Depthwise Separable Convolutions\n说起Light-weighted Architecture呢，DW Conv是必不可少的组件了，同理，MobileNet V2也是基于DW Conv做的改进。\n\n> The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.\n\n传统Conv接受一个$h_i\\times w_i\\times d_i$输入，应用卷积核$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$来产生$h_i\\times w_i\\times d_j$的输出。所以传统Conv的Computational Cost为：$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$。而DW Separable Conv的Computational Cost仅仅为：$h_i\\times w_i\\times d_i(k^2+d_j)$，__减少了将近$k^2$倍的计算量__。\n\n#### Linear Bottlenecks\n> It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.\n\nDCNN的架构大致是这样的：Conv + ReLU + (Pool) + (FC) + Softmax。DNN之所以拟合能力超强，原因就在于non-linearity transformation，而由于gradient vanishing/exploding的原因，Sigmoid已经淡出了历史舞台，取而代之的是ReLU。我们来分析分析ReLU有什么缺点：\n\n> It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. **In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.**\n\n> On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.\n\n简而言之，ReLu有以下两种性质：\n1. 若Manifold of Interest在ReLU之后非零，那么它就相当于是一个线性变换。\n2. ReLU能够保存input manifold完整的信息，**但是当且仅当input manifold位于input space的低维子空间中时**。\n\n所以，也就不难理解为什么MobileNet V2要先将high-dimensional hidden representations先做一次low-dimensional embedding，然后再变换回到high-dimensional了。\n\n![Evolution of Separable Conv](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png)\n\n#### Inverted residuals\n> The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.\n\n回想一下，[MobileNet V1](https://arxiv.org/pdf/1704.04861v1.pdf)的结构就是一个普通的feedforwad network，而shortcut在[ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)里已经被证明是非常effective的了，所以MobileNet V2自然而然地引入了skip connection了。\n\n#### MobileNet V2 Architecture\n> We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.\n\n![DCNN Architecture Comparison](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png)\n\n\n## ResNeXt\n> Paper: [Aggregated Residual Transformations for Deep Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)\n\n### Introduction\n自AlexNet以来，Deep Learning涌现了一大批设计优良的网络(如VGG，Inception，ResNet等)。ResNeXt则在ResNet的基础上，一定程度上参考了Inception的设计，即**split-transform-merge**。\n\n\n### The Core of ResNeXt\n> Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 1×1 convolutions), transformed by a set of specialized filters (3×3, 5×5, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.\n\n尽管Inception的**split-transform-merge**策略是非常行之有效的，但是该网络结构过于复杂，人工设计的痕迹过重(相比之下VGG和ResNet则是由相同的block stacking而成)，给人的感觉就是专门为了ImageNet去做的优化，所以当你想要迁移到其他的dataset时就会比较麻烦。因此，ResNeXt的设计就是：在VGG/ResNet的stacking block的基础上，融合进了Inception的split-transform-merge策略。这就是ResNeXt的基础idea。作者在实验中发现cardinality (the size of the set of transformations)对performance的影响是最大的，甚至要大于width和depth。\n\n> Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view\nour method as ensembling, because the members to be aggregated\nare trained jointly, not independently.\n\n> The above operation can be recast as a combination of\nsplitting, transforming, and aggregating. \n1. Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. \n2. Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.\n3. Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.\n\n若将$W$更换为更一般的形式，即任意一种function mapping: $\\mathcal{T}(x)$，那么aggregated transformations就变成了:\n$$\n\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n其中$\\mathcal{T}_i$可以将$x$映射到低维空间。$C$是transformation的size，也就是本文主角——**cardinality**。\n\n> In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.\n\n![ResNeXt Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg)\n\n那么在ResNet的identical mapping背景下，就变成了这样一种熟悉的结构：\n$$\ny=x+\\sum_{i=1}^C \\mathcal{T}_i(x)\n$$\n\n> **Relation to Grouped Convolutions**. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.\n\n![Equivalent Building Blocks of ResNeXt](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg)\n\n\n## DenseNet\n> Paper: [Densely Connected Convolutional Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n\n### What is DenseNet?\nDenseNet是CVPR2017 Best Paper，是继ResNet之后更加优秀的网络。[前面我们已经介绍过](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)，ResNet一定程度上解决了gradient vanishing的问题，通过ResNet中的identical mapping使得网络深度可以到达上千层。那么DenseNet又做了哪些改进呢？本文为你一一解答！\n\n在介绍DenseNet之前，我们先回顾一下ResNet做了什么改动，当shortcuts还未被引入DCNN之前，AlexNet/VGG/GoogLeNet都属于构造比较简单的feedforward network，即信息**一层一层往前传播，在BP时梯度一层一层往后传**，但是这样在网络结构很深的时候，就会存在gradient vanishing的问题。所以Kaiming He创造性地引入了skip connection，来使得信息可以从第$i$层之间做identical mapping传播到第$i+t$层，这样就保证了信息的高效流通。\n> 注: 关于ResNet更详细的介绍，请参考[这里](https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet)。\n\n![Dense Block](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg)\n\n而DenseNet，就是把这种skip connection做到了极致。为了保证信息在不同layer之间的流通，DenseNet将skip connection做到了每一层和该层之后的所有层中。和ResNet中采用的DW Summation不同的是，DenseNet直接concatenate不同层的features (因为ResNet中DW Summation会影响信息流动)。\n\n值得一提的是，**DenseNet比ResNet的参数更少**。因为dense connection的结构，使得网络不需要重新学习多余的feature maps。\n此外，every layer都可以从loss层直接获得梯度，从early layer是获得信息流，也产生了一种**deep supervision**。最后，作者还注意到dense connections一定程度上可以视为Regularization，所以可以缓解overfitting。\n\n它有以下优点：\n* 减轻了gradient vanishing问题\n* 加强了梯度传播\n* 更好的feature reuse\n* 极大地减少了参数\n\n### Delve into DenseNet\n#### Dense connectivity\nTo further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:\n$$\nx_l=H_l([x_0,x_1,\\cdots,x_{l-1}])\n$$\nwhere $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l−1$.\n\n#### Pooling layers\nThe concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.\n\n#### Growth rate\nIf each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l−1)$ input\nfeature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and\nexisting network architectures is that DenseNet can have\nvery narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.\n\n**One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's \"collective knowledge\". One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer**.\n\n#### Bottleneck layers\nAlthough each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.\n\n\n## Identity Mappings in Deep Residual Networks\n> Paper: [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n\n### Introduction\nResNet已经成了很多CV任务的标配，作者Kaiming He在ResNet里引入了shortcut来辅助DCNN的学习与优化，但是对于shortcut为什么能work则没有过多提及。本文是ResNet作者本人发表在ECCV'16上的Paper，主要在于解释identical mapping为何能work，并且对比了identical mapping的一些变体，最后提出了pre-activation。\n\n### Delve Into ResNet and Identical Mapping\nResNet可以表示为这样：\n$$\ny_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\n\n$$\nx_{l+1} = f(y_l)\n$$\n其中，$\\mathcal{F}$代表residual function，$h(x_l)=x_l$代表identical mapping，$f$代表ReLU函数。\n\n在本文中，作者发现，**若$h(x_l)$和$f(y_l)$都是identical mapping的话，信息就可以直接从一个unit传播到下几层的units，无论是在forward还是backward都是如此**。\n\n![Proposed Residual Unit](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg)\n\n为了构造identical mapping $f(y_l)=y_l$，我们可以将activation function (ReLU and BN)看作weight layers的```pre-activation```。\n\n### Analysis of Deep Residual Networks\nCVPR'15 Best Paper中的原始ResNet Unit是这样的：\n$$\ny_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)\n$$\n\n$$\nx_{l+1}=f(y_l)\n$$\n若$f$也是identical mapping: $x_{l+1}\\equiv y_l$，就可以得到：\n$$\nx_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)\n$$\nRecursively，我们可以得到：\n$$\nx_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})\n$$\n所以有：\n$$\nx_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\n$$\n所以，对于deep的$L$层和shallow的$l$层，特征$x_L$可以表示成**shallow unit feature $x_l$和residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$的加和**！说明：\n1. **模型是任意units $L$和$l$ 的residual function**\n2. 特征$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$是所有proceeding residual functions输出的summation再加上$x_0$\n\nBP的时候，根据chain rule，就得到如下公式(假设loss function为$\\epsilon$)：\n$$\n\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))\n$$\n所以，梯度$\\frac{\\partial \\epsilon}{\\partial x_l}$可以看作两个部分：\n1. $\\frac{\\partial \\epsilon}{\\partial x_L}$直接从高层流通回来\n2. $\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$流经了其他的weight layers\n\n#### Discussions\nPaper里也对一些identical mapping的变体进行了实验与探讨，反正scaling, gating, $1\\times 1$ convolutions, and dropout都效果不如原来的好。并且，**由于$1\\times 1$ conv**引入了更多的参数，理论上讲representation learning ability是要比原来的ResNet要高的，结果却比原来低，说明这种performance drop不是因为representation ability，而是因为优化问题所致。\n\n> The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations\n(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.\n\n此外，作者还验证了，当使用BN + ReLU作为pre-activation时，模型performance有了显著地改善。这种改善主要由两点带来：\n1. 因为$f$是identical mapping，所以整个模型的optimization更容易了。\n2. 使用BN作为pre-activation增加了模型的regularization，因BN本身具有regularization的效果。在CVPR'15原始版本的ResNet中，尽管BN normalize了信号，但是却立刻被添加进了shortcut，和未被BN normalize的signal一起merge了。而在pre-activation中，所有weight layers的input均被normalize了。\n\n\n## Reference\n1. Krizhevsky A, Sutskever I, Hinton G E. [Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[C]//Advances in neural information processing systems. 2012: 1097-1105.\n2. Simonyan K, Zisserman A. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556v6.pdf)[J]. arXiv preprint arXiv:1409.1556, 2014.\n3. Lin M, Chen Q, Yan S. [Network in network](https://arxiv.org/pdf/1312.4400v3.pdf)[J]. arXiv preprint arXiv:1312.4400, 2013.\n4. He K, Zhang X, Ren S, Sun J. [Deep residual learning for image recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).\n5. Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n6. Chollet, Francois. [\"Xception: Deep Learning with Depthwise Separable Convolutions.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.\n7. Howard, Andrew G., et al. [\"Mobilenets: Efficient convolutional neural networks for mobile vision applications.\"](https://arxiv.org/pdf/1704.04861v1.pdf) arXiv preprint arXiv:1704.04861 (2017).\n8. Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. [\"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\"](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018\n9. Xie, Saining, et al. [\"Aggregated residual transformations for deep neural networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf) Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.\n10. Huang, Gao, et al. [\"Densely Connected Convolutional Networks.\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) CVPR. Vol. 1. No. 2. 2017.\n11. He K, Zhang X, Ren S, et al. [Identity mappings in deep residual networks](https://arxiv.org/pdf/1603.05027v3.pdf)[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.\n12. Szegedy, Christian, et al. [\"Going deeper with convolutions.\"](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n","slug":"dl-architecture","published":1,"updated":"2018-11-18T14:29:16.777Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03bz000d608wtw5onf9s","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep Learning有三宝：Network Architecture，Loss Function and Optimization。对于大多数人而言，Optimization门槛还是很高的（需要非常深厚的数学功底），所以绝大多数的Paper偏向还是设计更好的Network Architecture或者堆更加精巧的Loss Function。Ian Goodfellow大佬也曾说过：现如今Deep Learning的繁荣，网络结构探究的贡献度远远高于优化算法的贡献度。所以本文旨在梳理从AlexNet到CliqueNet这些经典的work。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：对于优化算法，可参考我的<a href=\"https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/\">这一篇文章</a>。</p>\n</blockquote>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a></p>\n</blockquote>\n<p>AlexNet可以看作是Deep Learning在Large Scale Image Classification Task上第一次大放异彩。也是从AlexNet起，越来越多的Computer Vision Researcher开始将重心由设计更好的hand-crafted features转为设计更加精巧的网络结构。因此AlexNet是具备划时代意义的经典work。</p>\n<p>AlexNet整体结构其实非常非常简单，5层conv + 3层FC + Softmax。AlexNet使用了ReLU来代替Sigmoid作为non-linearity transformation，并且使用双GPU训练，以及一系列的Data Augmentation操作，Dropout，对于今天的工作仍然具备很深远的影响。</p>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a></p>\n</blockquote>\n<p>VGG也是一篇非常经典的工作，并且在今天的很多任务上依旧可以看到VGG的影子。不同于AlexNet，VGG使用了非常小的Filter($3\\times 3$)，以及类似于<font color=\"red\">basic block</font>的结构(读者不妨回想一下GoogLeNet、ResNet、ResNeXt、DenseNet是不是也是由一系列block堆积而成的)。</p>\n<p>不妨思考一下为啥要用$3\\times 3$的卷积核呢？</p>\n<ol>\n<li>两个堆叠的$3\\times 3$卷积核对应$5\\times 5$的receptive field。而三个$3\\times 3$卷积核对应$7\\times 7$的receptive field。那为啥不直接用$7\\times 7$卷积呢？原因就在于通过堆叠的3个$3\\times 3$卷积核，<font color=\"red\">我们引入了更多的non-linearity transformation，这有助于我们的网络学习更加discriminative的特征表达</font>。</li>\n<li>减少了参数：3个channel为$C$的$3\\times 3$卷积的参数为: $3(3^2C^2)=27C^2$。而一个channel为$C$的$7\\times 7$卷积的参数为: $7^2C^2=49C^2$。<blockquote>\n<p>This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"GoogLeNet\"><a href=\"#GoogLeNet\" class=\"headerlink\" title=\"GoogLeNet\"></a>GoogLeNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions</a></p>\n</blockquote>\n<p>因DCNN在一系列CV任务上均取得了非常好的效果，所以大家开始将精力由hand-crafted features转换到network architecture上来了。GoogLeNet也是经典网络中一个非常值得关注的模型，其中值得关注的设计就是<strong>Multi-branch + Feature Concatenation</strong>，这是今天很多深度学习算法也依旧在使用的方法。GoogLeNet中，作者大量使用了$1\\times 1$ conv (注：$1\\times 1$ conv最先来自<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)，这样有以下好处：</p>\n<ul>\n<li>作为dimension reduction来remove computational bottlenecks</li>\n<li>既然computational bottlenecks减少了，那么在相同FLOPs下，我们可以设计更加deep的网络结构，从而辅助更好的representation learning</li>\n</ul>\n<p>Inception Module的基础结构如下图所示：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg\" alt=\"Inception Module\"></p>\n<blockquote>\n<p>在走每一个$3\\times 3$和$5\\times 5$ conv之前，先过一遍$1\\times 1$ conv，一方面可以起到<strong>dimension reduction</strong>的作用；另一方面也引入了更多的<strong>non-linearity transformation</strong>，而这对于整个网络的representation learning ability是非常重要的(这个套路基本和<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>一样，感兴趣的读者可以去阅读<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>原文)。</p>\n</blockquote>\n<p>GoogLeNet就是通过一系列的Inception Module堆叠而成(读者不妨再仔细思考一下，VGG/ResNet/ResNeXt等等网络是不是也是由一系列小block堆叠而成？)。此外，因GoogLeNet是Multi-branch的结构，所以作者在中间层也添加了classification layer作为supervision来辅助gradient flow(读者不妨回忆一下，经典的人脸识别算法DeepID是不是也是这么做的？)。</p>\n<h2 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep Residual Learning for Image Recognition</a></p>\n</blockquote>\n<p>作者认为，ResNet可以称得上是自AlexNet以来，Deep Learning发展最insightful的idea，ResNet的主角shortcut至今也被广泛应用与Deep Architecture的设计中(如DenseNet, CliqueNet, Deep Layer Aggregation等)。<br>此前的网络设计趋势是“越来越深”，但神经网络的设计真的就如同段子所言“一层一层往上堆叠就好了吗？”显然不是的，ResNet作者Kaiming He大神在Paper中做了一些实验，验证了当Network越来越深时，Accuracy就饱和了，然后迅速下降，值得一提的是<font color=\"red\">这种性能下降并不是由于参数过多随之而来的overfitting造成的</font>。</p>\n<h3 id=\"What-is-Residual-Network\"><a href=\"#What-is-Residual-Network\" class=\"headerlink\" title=\"What is Residual Network?\"></a>What is Residual Network?</h3><p>设想DNN的目的是为了学习某种function $\\mathcal{H}(x)$，作者并没有直接设计DNN Architecture去学习这种function，而是先学习另一种function $\\mathcal{F}(x):=\\mathcal{H}(x) - x$，那么原来的$\\mathcal{H}(x)$是不是就可以表示成<font color=\"red\">$\\mathcal{F}(x)+x$</font>。作者假设这种结构比原先的$\\mathcal{H}(x)$更容易优化。</p>\n<blockquote>\n<p>例如，若某种identity mapping是最优的，那么，将残差push到0要比通过一系列non-linearity transformation来学习identity mapping更为高效。</p>\n</blockquote>\n<p>Shortcut可以表示成如下结构：<br>$$<br>y=\\mathcal{F}(x, \\{W_i\\}) + x<br>$$<br>$\\mathcal{F}(x, \\{W_i\\})$可以表示多个conv layers，两个feature map通过channel by channel element-wise 叠加。</p>\n<p>网络结构的设计方面，依旧是参考了著名的<a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">VGG</a>，即：使用大量$3\\times 3$ filters并且遵循这两条原则：</p>\n<ol>\n<li>对于输出相同feature map size的层使用相同数量的filter</li>\n<li>若feature map size减半，则filter的数量则翻倍，来维持每一层的time complexity</li>\n</ol>\n<p>对于feature map dimension相同的情况，则只需要element-wise addition即可；若feature map dimension double了，可以采取zero padding来增加dimension，或者采用$1\\times 1$ conv来进行升维。</p>\n<p>ResNet到这里基本就介绍完了，实验部分当然是在classification/detection/segmentation task上吊打了当前所有的state-of-the-art。ResNet很简单的idea对不对？不得不佩服一下Kaiming大神，他的东西总是简单而有效！</p>\n<h2 id=\"ShuffleNet\"><a href=\"#ShuffleNet\" class=\"headerlink\" title=\"ShuffleNet\"></a>ShuffleNet</h2><p>在Computer Vision领域，除了像AlexNet、VGG、GoogLeNet、ResNet、DenseNet、CliqueNet等一系列比较“重量级”的网络结构之外，也有一些非常轻量级的模型，而轻量级模型对于移动设备而言无疑是非常重要的。这里就介绍一下轻量级模型的代表作之一：<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>。</p>\n<blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p>\n</blockquote>\n<h3 id=\"What-is-ShuffleNet\"><a href=\"#What-is-ShuffleNet\" class=\"headerlink\" title=\"What is ShuffleNet?\"></a>What is ShuffleNet?</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>结构最重要的两个部分就是<strong>Pointwise Group Convolution</strong>和<strong>Channel Shuffle</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg\" alt=\"Channel Shuffle\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg\" alt=\"ShuffleNet Unit\"></p>\n<blockquote>\n<p>Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.</p>\n</blockquote>\n<h4 id=\"Advantages-of-Point-Wise-Convolution\"><a href=\"#Advantages-of-Point-Wise-Convolution\" class=\"headerlink\" title=\"Advantages of Point-Wise Convolution\"></a>Advantages of Point-Wise Convolution</h4><p>Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.</p>\n<h4 id=\"Channel-Shuffle-vs-No-Shuffle\"><a href=\"#Channel-Shuffle-vs-No-Shuffle\" class=\"headerlink\" title=\"Channel Shuffle vs. No Shuffle\"></a>Channel Shuffle vs. No Shuffle</h4><p>The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.</p>\n<h2 id=\"MobileNet-V1\"><a href=\"#MobileNet-V1\" class=\"headerlink\" title=\"MobileNet V1\"></a>MobileNet V1</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>\n</blockquote>\n<p>CNN驱动了许多视觉任务的飞速发展，然而传统结构例如ResNet、Inception、VGG等FLOP非常大，这使得对于移动端和嵌入式设备的训练与部署变得非常困难。所以近些年来，轻量级网络的设计也成为了一个非常热门的研究方向，<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a>和<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>就是其中的代表。前面我们已经介绍了<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>，本篇我们就大致回顾一下<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a>。</p>\n<h3 id=\"Depth-wise-Separable-Convolution\"><a href=\"#Depth-wise-Separable-Convolution\" class=\"headerlink\" title=\"Depth-wise Separable Convolution\"></a>Depth-wise Separable Convolution</h3><p>MobileNet最主要的结构就是<strong>Depth-wise Separable Convolution</strong>。DW Conv为什么能减少model size呢？我们不妨先来细致分析一下传统的卷积需要多少参数:<br>假设传统卷积层接受一个$D_F\\times D_F\\times M$的feature map作为输入，然后输出$D_F\\times D_F\\times N$的feature map，所以卷积核的size是$D_K\\times D_K\\times M\\times N$，所以需要的计算量为：$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$，所以Computational Cost依赖于input channel $M$，output channel $N$，卷积核尺寸$D_K\\times D_K$和feature map的尺寸$D_F\\times D_F$。</p>\n<p>但是MobileNet应用Depth-wise Conv来对Kernel Size和Output Channel进行了解耦。传统Conv Operation通过filters来对features进行filter，然后重组(Combinations)以形成新的representations。Filtering和Combinations可通过DW Separable Conv来分成两步进行。</p>\n<p><strong>Depth-wise Separable Convolution由两层组成：Depth-wise Conv + Point-wise Conv</strong>。DW Conv对于每个channel应用单个filter，PW Conv (a simple $1\\times 1$ Conv)用来建立DW layer输出的linear combination。<strong>DW Conv的Computational Cost为: $D_K\\times D_K\\times M\\times D_F\\times D_F$</strong>，与传统Conv相比低了$N$倍。</p>\n<p>DW Conv虽然高效，然而它仅仅filter了input channel，<strong>却没有对其进行combination</strong>，所以需要额外的layer来对DW Conv后的feature进行linear combination来产生新的representation，这就是基于$1\\times 1$ Conv的PW Conv。</p>\n<p>The combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.</p>\n<p>DW Separable Conv的Computational Cost为：<br>$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$，前者为DW Conv的cost，后者为PW Conv的cost。</p>\n<p>By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:<br>$$<br>\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}<br>$$<br>可以看到，Depth-wise Separable Conv的计算量仅仅为传统Conv的$\\frac{1}{N} + \\frac{1}{D_K^2}$。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png\" alt=\"DW Separable Conv\"></p>\n<h3 id=\"Width-Multiplier-Thinner-Models\"><a href=\"#Width-Multiplier-Thinner-Models\" class=\"headerlink\" title=\"Width Multiplier: Thinner Models\"></a>Width Multiplier: Thinner Models</h3><p>In order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$</p>\n<p>where $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha &lt; 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.</p>\n<h3 id=\"Resolution-Multiplier-Reduced-Representation\"><a href=\"#Resolution-Multiplier-Reduced-Representation\" class=\"headerlink\" title=\"Resolution Multiplier: Reduced Representation\"></a>Resolution Multiplier: Reduced Representation</h3><p>The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$</p>\n<p>where $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and ρ &lt; 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.</p>\n<h2 id=\"MobileNet-V2\"><a href=\"#MobileNet-V2\" class=\"headerlink\" title=\"MobileNet V2\"></a>MobileNet V2</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>\n</blockquote>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V2</a>是发表在<a href=\"http://openaccess.thecvf.com/CVPR2018.py\" target=\"_blank\" rel=\"noopener\">CVPR2018</a>上的Paper，在移动端网络的设计方面又向前走了一步。MobileNet V2最大的contribution如下: </p>\n<blockquote>\n<p>Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.</p>\n</blockquote>\n<h3 id=\"Preliminaries-discussion-and-intuition\"><a href=\"#Preliminaries-discussion-and-intuition\" class=\"headerlink\" title=\"Preliminaries, discussion and intuition\"></a>Preliminaries, discussion and intuition</h3><h4 id=\"Depthwise-Separable-Convolutions\"><a href=\"#Depthwise-Separable-Convolutions\" class=\"headerlink\" title=\"Depthwise Separable Convolutions\"></a>Depthwise Separable Convolutions</h4><p>说起Light-weighted Architecture呢，DW Conv是必不可少的组件了，同理，MobileNet V2也是基于DW Conv做的改进。</p>\n<blockquote>\n<p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.</p>\n</blockquote>\n<p>传统Conv接受一个$h_i\\times w_i\\times d_i$输入，应用卷积核$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$来产生$h_i\\times w_i\\times d_j$的输出。所以传统Conv的Computational Cost为：$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$。而DW Separable Conv的Computational Cost仅仅为：$h_i\\times w_i\\times d_i(k^2+d_j)$，<strong>减少了将近$k^2$倍的计算量</strong>。</p>\n<h4 id=\"Linear-Bottlenecks\"><a href=\"#Linear-Bottlenecks\" class=\"headerlink\" title=\"Linear Bottlenecks\"></a>Linear Bottlenecks</h4><blockquote>\n<p>It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.</p>\n</blockquote>\n<p>DCNN的架构大致是这样的：Conv + ReLU + (Pool) + (FC) + Softmax。DNN之所以拟合能力超强，原因就在于non-linearity transformation，而由于gradient vanishing/exploding的原因，Sigmoid已经淡出了历史舞台，取而代之的是ReLU。我们来分析分析ReLU有什么缺点：</p>\n<blockquote>\n<p>It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. <strong>In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.</strong></p>\n</blockquote>\n<blockquote>\n<p>On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.</p>\n</blockquote>\n<p>简而言之，ReLu有以下两种性质：</p>\n<ol>\n<li>若Manifold of Interest在ReLU之后非零，那么它就相当于是一个线性变换。</li>\n<li>ReLU能够保存input manifold完整的信息，<strong>但是当且仅当input manifold位于input space的低维子空间中时</strong>。</li>\n</ol>\n<p>所以，也就不难理解为什么MobileNet V2要先将high-dimensional hidden representations先做一次low-dimensional embedding，然后再变换回到high-dimensional了。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png\" alt=\"Evolution of Separable Conv\"></p>\n<h4 id=\"Inverted-residuals\"><a href=\"#Inverted-residuals\" class=\"headerlink\" title=\"Inverted residuals\"></a>Inverted residuals</h4><blockquote>\n<p>The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.</p>\n</blockquote>\n<p>回想一下，<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V1</a>的结构就是一个普通的feedforwad network，而shortcut在<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>里已经被证明是非常effective的了，所以MobileNet V2自然而然地引入了skip connection了。</p>\n<h4 id=\"MobileNet-V2-Architecture\"><a href=\"#MobileNet-V2-Architecture\" class=\"headerlink\" title=\"MobileNet V2 Architecture\"></a>MobileNet V2 Architecture</h4><blockquote>\n<p>We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png\" alt=\"DCNN Architecture Comparison\"></p>\n<h2 id=\"ResNeXt\"><a href=\"#ResNeXt\" class=\"headerlink\" title=\"ResNeXt\"></a>ResNeXt</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated Residual Transformations for Deep Neural Networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>自AlexNet以来，Deep Learning涌现了一大批设计优良的网络(如VGG，Inception，ResNet等)。ResNeXt则在ResNet的基础上，一定程度上参考了Inception的设计，即<strong>split-transform-merge</strong>。</p>\n<h3 id=\"The-Core-of-ResNeXt\"><a href=\"#The-Core-of-ResNeXt\" class=\"headerlink\" title=\"The Core of ResNeXt\"></a>The Core of ResNeXt</h3><blockquote>\n<p>Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 1×1 convolutions), transformed by a set of specialized filters (3×3, 5×5, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.</p>\n</blockquote>\n<p>尽管Inception的<strong>split-transform-merge</strong>策略是非常行之有效的，但是该网络结构过于复杂，人工设计的痕迹过重(相比之下VGG和ResNet则是由相同的block stacking而成)，给人的感觉就是专门为了ImageNet去做的优化，所以当你想要迁移到其他的dataset时就会比较麻烦。因此，ResNeXt的设计就是：在VGG/ResNet的stacking block的基础上，融合进了Inception的split-transform-merge策略。这就是ResNeXt的基础idea。作者在实验中发现cardinality (the size of the set of transformations)对performance的影响是最大的，甚至要大于width和depth。</p>\n<blockquote>\n<p>Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view<br>our method as ensembling, because the members to be aggregated<br>are trained jointly, not independently.</p>\n</blockquote>\n<blockquote>\n<p>The above operation can be recast as a combination of<br>splitting, transforming, and aggregating. </p>\n<ol>\n<li>Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. </li>\n<li>Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.</li>\n<li>Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.</li>\n</ol>\n</blockquote>\n<p>若将$W$更换为更一般的形式，即任意一种function mapping: $\\mathcal{T}(x)$，那么aggregated transformations就变成了:<br>$$<br>\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$<br>其中$\\mathcal{T}_i$可以将$x$映射到低维空间。$C$是transformation的size，也就是本文主角——<strong>cardinality</strong>。</p>\n<blockquote>\n<p>In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg\" alt=\"ResNeXt Block\"></p>\n<p>那么在ResNet的identical mapping背景下，就变成了这样一种熟悉的结构：<br>$$<br>y=x+\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$</p>\n<blockquote>\n<p><strong>Relation to Grouped Convolutions</strong>. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg\" alt=\"Equivalent Building Blocks of ResNeXt\"></p>\n<h2 id=\"DenseNet\"><a href=\"#DenseNet\" class=\"headerlink\" title=\"DenseNet\"></a>DenseNet</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n</blockquote>\n<h3 id=\"What-is-DenseNet\"><a href=\"#What-is-DenseNet\" class=\"headerlink\" title=\"What is DenseNet?\"></a>What is DenseNet?</h3><p>DenseNet是CVPR2017 Best Paper，是继ResNet之后更加优秀的网络。<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\">前面我们已经介绍过</a>，ResNet一定程度上解决了gradient vanishing的问题，通过ResNet中的identical mapping使得网络深度可以到达上千层。那么DenseNet又做了哪些改进呢？本文为你一一解答！</p>\n<p>在介绍DenseNet之前，我们先回顾一下ResNet做了什么改动，当shortcuts还未被引入DCNN之前，AlexNet/VGG/GoogLeNet都属于构造比较简单的feedforward network，即信息<strong>一层一层往前传播，在BP时梯度一层一层往后传</strong>，但是这样在网络结构很深的时候，就会存在gradient vanishing的问题。所以Kaiming He创造性地引入了skip connection，来使得信息可以从第$i$层之间做identical mapping传播到第$i+t$层，这样就保证了信息的高效流通。</p>\n<blockquote>\n<p>注: 关于ResNet更详细的介绍，请参考<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\">这里</a>。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg\" alt=\"Dense Block\"></p>\n<p>而DenseNet，就是把这种skip connection做到了极致。为了保证信息在不同layer之间的流通，DenseNet将skip connection做到了每一层和该层之后的所有层中。和ResNet中采用的DW Summation不同的是，DenseNet直接concatenate不同层的features (因为ResNet中DW Summation会影响信息流动)。</p>\n<p>值得一提的是，<strong>DenseNet比ResNet的参数更少</strong>。因为dense connection的结构，使得网络不需要重新学习多余的feature maps。<br>此外，every layer都可以从loss层直接获得梯度，从early layer是获得信息流，也产生了一种<strong>deep supervision</strong>。最后，作者还注意到dense connections一定程度上可以视为Regularization，所以可以缓解overfitting。</p>\n<p>它有以下优点：</p>\n<ul>\n<li>减轻了gradient vanishing问题</li>\n<li>加强了梯度传播</li>\n<li>更好的feature reuse</li>\n<li>极大地减少了参数</li>\n</ul>\n<h3 id=\"Delve-into-DenseNet\"><a href=\"#Delve-into-DenseNet\" class=\"headerlink\" title=\"Delve into DenseNet\"></a>Delve into DenseNet</h3><h4 id=\"Dense-connectivity\"><a href=\"#Dense-connectivity\" class=\"headerlink\" title=\"Dense connectivity\"></a>Dense connectivity</h4><p>To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:<br>$$<br>x_l=H_l([x_0,x_1,\\cdots,x_{l-1}])<br>$$<br>where $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l−1$.</p>\n<h4 id=\"Pooling-layers\"><a href=\"#Pooling-layers\" class=\"headerlink\" title=\"Pooling layers\"></a>Pooling layers</h4><p>The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.</p>\n<h4 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h4><p>If each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l−1)$ input<br>feature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and<br>existing network architectures is that DenseNet can have<br>very narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.</p>\n<p><strong>One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer</strong>.</p>\n<h4 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h4><p>Although each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.</p>\n<h2 id=\"Identity-Mappings-in-Deep-Residual-Networks\"><a href=\"#Identity-Mappings-in-Deep-Residual-Networks\" class=\"headerlink\" title=\"Identity Mappings in Deep Residual Networks\"></a>Identity Mappings in Deep Residual Networks</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-2\"><a href=\"#Introduction-2\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>ResNet已经成了很多CV任务的标配，作者Kaiming He在ResNet里引入了shortcut来辅助DCNN的学习与优化，但是对于shortcut为什么能work则没有过多提及。本文是ResNet作者本人发表在ECCV’16上的Paper，主要在于解释identical mapping为何能work，并且对比了identical mapping的一些变体，最后提出了pre-activation。</p>\n<h3 id=\"Delve-Into-ResNet-and-Identical-Mapping\"><a href=\"#Delve-Into-ResNet-and-Identical-Mapping\" class=\"headerlink\" title=\"Delve Into ResNet and Identical Mapping\"></a>Delve Into ResNet and Identical Mapping</h3><p>ResNet可以表示为这样：<br>$$<br>y_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1} = f(y_l)<br>$$<br>其中，$\\mathcal{F}$代表residual function，$h(x_l)=x_l$代表identical mapping，$f$代表ReLU函数。</p>\n<p>在本文中，作者发现，<strong>若$h(x_l)$和$f(y_l)$都是identical mapping的话，信息就可以直接从一个unit传播到下几层的units，无论是在forward还是backward都是如此</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg\" alt=\"Proposed Residual Unit\"></p>\n<p>为了构造identical mapping $f(y_l)=y_l$，我们可以将activation function (ReLU and BN)看作weight layers的<code>pre-activation</code>。</p>\n<h3 id=\"Analysis-of-Deep-Residual-Networks\"><a href=\"#Analysis-of-Deep-Residual-Networks\" class=\"headerlink\" title=\"Analysis of Deep Residual Networks\"></a>Analysis of Deep Residual Networks</h3><p>CVPR’15 Best Paper中的原始ResNet Unit是这样的：<br>$$<br>y_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>若$f$也是identical mapping: $x_{l+1}\\equiv y_l$，就可以得到：<br>$$<br>x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$<br>Recursively，我们可以得到：<br>$$<br>x_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})<br>$$<br>所以有：<br>$$<br>x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)<br>$$<br>所以，对于deep的$L$层和shallow的$l$层，特征$x_L$可以表示成<strong>shallow unit feature $x_l$和residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$的加和</strong>！说明：</p>\n<ol>\n<li><strong>模型是任意units $L$和$l$ 的residual function</strong></li>\n<li>特征$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$是所有proceeding residual functions输出的summation再加上$x_0$</li>\n</ol>\n<p>BP的时候，根据chain rule，就得到如下公式(假设loss function为$\\epsilon$)：<br>$$<br>\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))<br>$$<br>所以，梯度$\\frac{\\partial \\epsilon}{\\partial x_l}$可以看作两个部分：</p>\n<ol>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}$直接从高层流通回来</li>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$流经了其他的weight layers</li>\n</ol>\n<h4 id=\"Discussions\"><a href=\"#Discussions\" class=\"headerlink\" title=\"Discussions\"></a>Discussions</h4><p>Paper里也对一些identical mapping的变体进行了实验与探讨，反正scaling, gating, $1\\times 1$ convolutions, and dropout都效果不如原来的好。并且，<strong>由于$1\\times 1$ conv</strong>引入了更多的参数，理论上讲representation learning ability是要比原来的ResNet要高的，结果却比原来低，说明这种performance drop不是因为representation ability，而是因为优化问题所致。</p>\n<blockquote>\n<p>The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations<br>(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.</p>\n</blockquote>\n<p>此外，作者还验证了，当使用BN + ReLU作为pre-activation时，模型performance有了显著地改善。这种改善主要由两点带来：</p>\n<ol>\n<li>因为$f$是identical mapping，所以整个模型的optimization更容易了。</li>\n<li>使用BN作为pre-activation增加了模型的regularization，因BN本身具有regularization的效果。在CVPR’15原始版本的ResNet中，尽管BN normalize了信号，但是却立刻被添加进了shortcut，和未被BN normalize的signal一起merge了。而在pre-activation中，所有weight layers的input均被normalize了。</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Krizhevsky A, Sutskever I, Hinton G E. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a>[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>\n<li>Simonyan K, Zisserman A. <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a>[J]. arXiv preprint arXiv:1409.1556, 2014.</li>\n<li>Lin M, Chen Q, Yan S. <a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>[J]. arXiv preprint arXiv:1312.4400, 2013.</li>\n<li>He K, Zhang X, Ren S, Sun J. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep residual learning for image recognition</a>. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</li>\n<li>Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Chollet, Francois. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Xception: Deep Learning with Depthwise Separable Convolutions.”</a> 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.</li>\n<li>Howard, Andrew G., et al. <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">“Mobilenets: Efficient convolutional neural networks for mobile vision applications.”</a> arXiv preprint arXiv:1704.04861 (2017).</li>\n<li>Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">“MobileNetV2: Inverted Residuals and Linear Bottlenecks.”</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Xie, Saining, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Aggregated residual transformations for deep neural networks.”</a> Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.</li>\n<li>Huang, Gao, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Densely Connected Convolutional Networks.”</a> CVPR. Vol. 1. No. 2. 2017.</li>\n<li>He K, Zhang X, Ren S, et al. <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a>[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.</li>\n<li>Szegedy, Christian, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Going deeper with convolutions.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Deep Learning有三宝：Network Architecture，Loss Function and Optimization。对于大多数人而言，Optimization门槛还是很高的（需要非常深厚的数学功底），所以绝大多数的Paper偏向还是设计更好的Network Architecture或者堆更加精巧的Loss Function。Ian Goodfellow大佬也曾说过：现如今Deep Learning的繁荣，网络结构探究的贡献度远远高于优化算法的贡献度。所以本文旨在梳理从AlexNet到CliqueNet这些经典的work。</p>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/people/xulu-0620\" target=\"_blank\" rel=\"noopener\">@LucasX</a>注：对于优化算法，可参考我的<a href=\"https://lucasxlu.github.io/blog/2018/07/20/dl-optimization/\">这一篇文章</a>。</p>\n</blockquote>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><blockquote>\n<p>Paper: <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a></p>\n</blockquote>\n<p>AlexNet可以看作是Deep Learning在Large Scale Image Classification Task上第一次大放异彩。也是从AlexNet起，越来越多的Computer Vision Researcher开始将重心由设计更好的hand-crafted features转为设计更加精巧的网络结构。因此AlexNet是具备划时代意义的经典work。</p>\n<p>AlexNet整体结构其实非常非常简单，5层conv + 3层FC + Softmax。AlexNet使用了ReLU来代替Sigmoid作为non-linearity transformation，并且使用双GPU训练，以及一系列的Data Augmentation操作，Dropout，对于今天的工作仍然具备很深远的影响。</p>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a></p>\n</blockquote>\n<p>VGG也是一篇非常经典的工作，并且在今天的很多任务上依旧可以看到VGG的影子。不同于AlexNet，VGG使用了非常小的Filter($3\\times 3$)，以及类似于<font color=\"red\">basic block</font>的结构(读者不妨回想一下GoogLeNet、ResNet、ResNeXt、DenseNet是不是也是由一系列block堆积而成的)。</p>\n<p>不妨思考一下为啥要用$3\\times 3$的卷积核呢？</p>\n<ol>\n<li>两个堆叠的$3\\times 3$卷积核对应$5\\times 5$的receptive field。而三个$3\\times 3$卷积核对应$7\\times 7$的receptive field。那为啥不直接用$7\\times 7$卷积呢？原因就在于通过堆叠的3个$3\\times 3$卷积核，<font color=\"red\">我们引入了更多的non-linearity transformation，这有助于我们的网络学习更加discriminative的特征表达</font>。</li>\n<li>减少了参数：3个channel为$C$的$3\\times 3$卷积的参数为: $3(3^2C^2)=27C^2$。而一个channel为$C$的$7\\times 7$卷积的参数为: $7^2C^2=49C^2$。<blockquote>\n<p>This can be seen as imposing a regularisation on the $7\\times 7$ conv. filters, forcing them to have a decomposition through the $3\\times 3$ filters (with non-linearity injected in between)</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"GoogLeNet\"><a href=\"#GoogLeNet\" class=\"headerlink\" title=\"GoogLeNet\"></a>GoogLeNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">Going deeper with convolutions</a></p>\n</blockquote>\n<p>因DCNN在一系列CV任务上均取得了非常好的效果，所以大家开始将精力由hand-crafted features转换到network architecture上来了。GoogLeNet也是经典网络中一个非常值得关注的模型，其中值得关注的设计就是<strong>Multi-branch + Feature Concatenation</strong>，这是今天很多深度学习算法也依旧在使用的方法。GoogLeNet中，作者大量使用了$1\\times 1$ conv (注：$1\\times 1$ conv最先来自<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>)，这样有以下好处：</p>\n<ul>\n<li>作为dimension reduction来remove computational bottlenecks</li>\n<li>既然computational bottlenecks减少了，那么在相同FLOPs下，我们可以设计更加deep的网络结构，从而辅助更好的representation learning</li>\n</ul>\n<p>Inception Module的基础结构如下图所示：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/inception_module.jpg\" alt=\"Inception Module\"></p>\n<blockquote>\n<p>在走每一个$3\\times 3$和$5\\times 5$ conv之前，先过一遍$1\\times 1$ conv，一方面可以起到<strong>dimension reduction</strong>的作用；另一方面也引入了更多的<strong>non-linearity transformation</strong>，而这对于整个网络的representation learning ability是非常重要的(这个套路基本和<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>一样，感兴趣的读者可以去阅读<a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>原文)。</p>\n</blockquote>\n<p>GoogLeNet就是通过一系列的Inception Module堆叠而成(读者不妨再仔细思考一下，VGG/ResNet/ResNeXt等等网络是不是也是由一系列小block堆叠而成？)。此外，因GoogLeNet是Multi-branch的结构，所以作者在中间层也添加了classification layer作为supervision来辅助gradient flow(读者不妨回忆一下，经典的人脸识别算法DeepID是不是也是这么做的？)。</p>\n<h2 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h2><blockquote>\n<p>Paper: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep Residual Learning for Image Recognition</a></p>\n</blockquote>\n<p>作者认为，ResNet可以称得上是自AlexNet以来，Deep Learning发展最insightful的idea，ResNet的主角shortcut至今也被广泛应用与Deep Architecture的设计中(如DenseNet, CliqueNet, Deep Layer Aggregation等)。<br>此前的网络设计趋势是“越来越深”，但神经网络的设计真的就如同段子所言“一层一层往上堆叠就好了吗？”显然不是的，ResNet作者Kaiming He大神在Paper中做了一些实验，验证了当Network越来越深时，Accuracy就饱和了，然后迅速下降，值得一提的是<font color=\"red\">这种性能下降并不是由于参数过多随之而来的overfitting造成的</font>。</p>\n<h3 id=\"What-is-Residual-Network\"><a href=\"#What-is-Residual-Network\" class=\"headerlink\" title=\"What is Residual Network?\"></a>What is Residual Network?</h3><p>设想DNN的目的是为了学习某种function $\\mathcal{H}(x)$，作者并没有直接设计DNN Architecture去学习这种function，而是先学习另一种function $\\mathcal{F}(x):=\\mathcal{H}(x) - x$，那么原来的$\\mathcal{H}(x)$是不是就可以表示成<font color=\"red\">$\\mathcal{F}(x)+x$</font>。作者假设这种结构比原先的$\\mathcal{H}(x)$更容易优化。</p>\n<blockquote>\n<p>例如，若某种identity mapping是最优的，那么，将残差push到0要比通过一系列non-linearity transformation来学习identity mapping更为高效。</p>\n</blockquote>\n<p>Shortcut可以表示成如下结构：<br>$$<br>y=\\mathcal{F}(x, \\{W_i\\}) + x<br>$$<br>$\\mathcal{F}(x, \\{W_i\\})$可以表示多个conv layers，两个feature map通过channel by channel element-wise 叠加。</p>\n<p>网络结构的设计方面，依旧是参考了著名的<a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">VGG</a>，即：使用大量$3\\times 3$ filters并且遵循这两条原则：</p>\n<ol>\n<li>对于输出相同feature map size的层使用相同数量的filter</li>\n<li>若feature map size减半，则filter的数量则翻倍，来维持每一层的time complexity</li>\n</ol>\n<p>对于feature map dimension相同的情况，则只需要element-wise addition即可；若feature map dimension double了，可以采取zero padding来增加dimension，或者采用$1\\times 1$ conv来进行升维。</p>\n<p>ResNet到这里基本就介绍完了，实验部分当然是在classification/detection/segmentation task上吊打了当前所有的state-of-the-art。ResNet很简单的idea对不对？不得不佩服一下Kaiming大神，他的东西总是简单而有效！</p>\n<h2 id=\"ShuffleNet\"><a href=\"#ShuffleNet\" class=\"headerlink\" title=\"ShuffleNet\"></a>ShuffleNet</h2><p>在Computer Vision领域，除了像AlexNet、VGG、GoogLeNet、ResNet、DenseNet、CliqueNet等一系列比较“重量级”的网络结构之外，也有一些非常轻量级的模型，而轻量级模型对于移动设备而言无疑是非常重要的。这里就介绍一下轻量级模型的代表作之一：<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>。</p>\n<blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p>\n</blockquote>\n<h3 id=\"What-is-ShuffleNet\"><a href=\"#What-is-ShuffleNet\" class=\"headerlink\" title=\"What is ShuffleNet?\"></a>What is ShuffleNet?</h3><p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>结构最重要的两个部分就是<strong>Pointwise Group Convolution</strong>和<strong>Channel Shuffle</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/channel_shuffle.jpg\" alt=\"Channel Shuffle\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/shufflenet_unit.jpg\" alt=\"ShuffleNet Unit\"></p>\n<blockquote>\n<p>Given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on lowpower mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations.</p>\n</blockquote>\n<h4 id=\"Advantages-of-Point-Wise-Convolution\"><a href=\"#Advantages-of-Point-Wise-Convolution\" class=\"headerlink\" title=\"Advantages of Point-Wise Convolution\"></a>Advantages of Point-Wise Convolution</h4><p>Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.</p>\n<h4 id=\"Channel-Shuffle-vs-No-Shuffle\"><a href=\"#Channel-Shuffle-vs-No-Shuffle\" class=\"headerlink\" title=\"Channel Shuffle vs. No Shuffle\"></a>Channel Shuffle vs. No Shuffle</h4><p>The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.</p>\n<h2 id=\"MobileNet-V1\"><a href=\"#MobileNet-V1\" class=\"headerlink\" title=\"MobileNet V1\"></a>MobileNet V1</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p>\n</blockquote>\n<p>CNN驱动了许多视觉任务的飞速发展，然而传统结构例如ResNet、Inception、VGG等FLOP非常大，这使得对于移动端和嵌入式设备的训练与部署变得非常困难。所以近些年来，轻量级网络的设计也成为了一个非常热门的研究方向，<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a>和<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>就是其中的代表。前面我们已经介绍了<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet</a>，本篇我们就大致回顾一下<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet</a>。</p>\n<h3 id=\"Depth-wise-Separable-Convolution\"><a href=\"#Depth-wise-Separable-Convolution\" class=\"headerlink\" title=\"Depth-wise Separable Convolution\"></a>Depth-wise Separable Convolution</h3><p>MobileNet最主要的结构就是<strong>Depth-wise Separable Convolution</strong>。DW Conv为什么能减少model size呢？我们不妨先来细致分析一下传统的卷积需要多少参数:<br>假设传统卷积层接受一个$D_F\\times D_F\\times M$的feature map作为输入，然后输出$D_F\\times D_F\\times N$的feature map，所以卷积核的size是$D_K\\times D_K\\times M\\times N$，所以需要的计算量为：$D_K\\times D_K\\times M\\times N\\times D_F\\times D_F$，所以Computational Cost依赖于input channel $M$，output channel $N$，卷积核尺寸$D_K\\times D_K$和feature map的尺寸$D_F\\times D_F$。</p>\n<p>但是MobileNet应用Depth-wise Conv来对Kernel Size和Output Channel进行了解耦。传统Conv Operation通过filters来对features进行filter，然后重组(Combinations)以形成新的representations。Filtering和Combinations可通过DW Separable Conv来分成两步进行。</p>\n<p><strong>Depth-wise Separable Convolution由两层组成：Depth-wise Conv + Point-wise Conv</strong>。DW Conv对于每个channel应用单个filter，PW Conv (a simple $1\\times 1$ Conv)用来建立DW layer输出的linear combination。<strong>DW Conv的Computational Cost为: $D_K\\times D_K\\times M\\times D_F\\times D_F$</strong>，与传统Conv相比低了$N$倍。</p>\n<p>DW Conv虽然高效，然而它仅仅filter了input channel，<strong>却没有对其进行combination</strong>，所以需要额外的layer来对DW Conv后的feature进行linear combination来产生新的representation，这就是基于$1\\times 1$ Conv的PW Conv。</p>\n<p>The combination of depthwise convolution and $1\\times 1$ (pointwise) convolution is called depthwise separable convolution.</p>\n<p>DW Separable Conv的Computational Cost为：<br>$D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F$，前者为DW Conv的cost，后者为PW Conv的cost。</p>\n<p>By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:<br>$$<br>\\frac{D_K\\times D_K \\times M\\times D_F \\times D_F + M\\times N\\times D_F\\times D_F}{D_K\\times D_K \\times M\\times N\\times D_F \\times D_F}=\\frac{1}{N} + \\frac{1}{D_K^2}<br>$$<br>可以看到，Depth-wise Separable Conv的计算量仅仅为传统Conv的$\\frac{1}{N} + \\frac{1}{D_K^2}$。<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dw-sep-conv.png\" alt=\"DW Separable Conv\"></p>\n<h3 id=\"Width-Multiplier-Thinner-Models\"><a href=\"#Width-Multiplier-Thinner-Models\" class=\"headerlink\" title=\"Width Multiplier: Thinner Models\"></a>Width Multiplier: Thinner Models</h3><p>In order to construct these smaller and less computationally expensive models we introduce a very simple parameter $\\alpha$ called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer. The computational cost of a depthwise separable convolution with width multiplier $\\alpha$ is:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times D_F\\times D_F +\\alpha M\\times \\alpha N + D_F\\times D_F$</p>\n<p>where $\\alpha\\in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25. $\\alpha = 1$ is the baseline MobileNet and $\\alpha &lt; 1$ are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly $\\alpha^2$ . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.</p>\n<h3 id=\"Resolution-Multiplier-Reduced-Representation\"><a href=\"#Resolution-Multiplier-Reduced-Representation\" class=\"headerlink\" title=\"Resolution Multiplier: Reduced Representation\"></a>Resolution Multiplier: Reduced Representation</h3><p>The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier $\\rho$. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set $\\rho$ by setting the input resolution. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier $\\alpha$ and resolution multiplier $\\rho$:</p>\n<p>$D_K\\times D_K\\times \\alpha M\\times \\rho D_F\\times \\rho D_F +\\alpha M\\times \\alpha N + \\rho D_F\\times \\rho D_F$</p>\n<p>where $\\rho\\in (0, 1]$ which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. $\\rho = 1$ is the baseline MobileNet and ρ &lt; 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by $\\rho^2$.</p>\n<h2 id=\"MobileNet-V2\"><a href=\"#MobileNet-V2\" class=\"headerlink\" title=\"MobileNet V2\"></a>MobileNet V2</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>\n</blockquote>\n<p><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V2</a>是发表在<a href=\"http://openaccess.thecvf.com/CVPR2018.py\" target=\"_blank\" rel=\"noopener\">CVPR2018</a>上的Paper，在移动端网络的设计方面又向前走了一步。MobileNet V2最大的contribution如下: </p>\n<blockquote>\n<p>Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution.</p>\n</blockquote>\n<h3 id=\"Preliminaries-discussion-and-intuition\"><a href=\"#Preliminaries-discussion-and-intuition\" class=\"headerlink\" title=\"Preliminaries, discussion and intuition\"></a>Preliminaries, discussion and intuition</h3><h4 id=\"Depthwise-Separable-Convolutions\"><a href=\"#Depthwise-Separable-Convolutions\" class=\"headerlink\" title=\"Depthwise Separable Convolutions\"></a>Depthwise Separable Convolutions</h4><p>说起Light-weighted Architecture呢，DW Conv是必不可少的组件了，同理，MobileNet V2也是基于DW Conv做的改进。</p>\n<blockquote>\n<p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1\\times 1$ convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.</p>\n</blockquote>\n<p>传统Conv接受一个$h_i\\times w_i\\times d_i$输入，应用卷积核$K\\in \\mathcal{R}^{k\\times k\\times d_i\\times d_j}$来产生$h_i\\times w_i\\times d_j$的输出。所以传统Conv的Computational Cost为：$h_i\\times w_i\\times d_i \\times d_j\\times k\\times k$。而DW Separable Conv的Computational Cost仅仅为：$h_i\\times w_i\\times d_i(k^2+d_j)$，<strong>减少了将近$k^2$倍的计算量</strong>。</p>\n<h4 id=\"Linear-Bottlenecks\"><a href=\"#Linear-Bottlenecks\" class=\"headerlink\" title=\"Linear Bottlenecks\"></a>Linear Bottlenecks</h4><blockquote>\n<p>It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.</p>\n</blockquote>\n<p>DCNN的架构大致是这样的：Conv + ReLU + (Pool) + (FC) + Softmax。DNN之所以拟合能力超强，原因就在于non-linearity transformation，而由于gradient vanishing/exploding的原因，Sigmoid已经淡出了历史舞台，取而代之的是ReLU。我们来分析分析ReLU有什么缺点：</p>\n<blockquote>\n<p>It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. <strong>In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.</strong></p>\n</blockquote>\n<blockquote>\n<p>On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.</p>\n</blockquote>\n<p>简而言之，ReLu有以下两种性质：</p>\n<ol>\n<li>若Manifold of Interest在ReLU之后非零，那么它就相当于是一个线性变换。</li>\n<li>ReLU能够保存input manifold完整的信息，<strong>但是当且仅当input manifold位于input space的低维子空间中时</strong>。</li>\n</ol>\n<p>所以，也就不难理解为什么MobileNet V2要先将high-dimensional hidden representations先做一次low-dimensional embedding，然后再变换回到high-dimensional了。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/evolution-of-separable-conv.png\" alt=\"Evolution of Separable Conv\"></p>\n<h4 id=\"Inverted-residuals\"><a href=\"#Inverted-residuals\" class=\"headerlink\" title=\"Inverted residuals\"></a>Inverted residuals</h4><blockquote>\n<p>The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.</p>\n</blockquote>\n<p>回想一下，<a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">MobileNet V1</a>的结构就是一个普通的feedforwad network，而shortcut在<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>里已经被证明是非常effective的了，所以MobileNet V2自然而然地引入了skip connection了。</p>\n<h4 id=\"MobileNet-V2-Architecture\"><a href=\"#MobileNet-V2-Architecture\" class=\"headerlink\" title=\"MobileNet V2 Architecture\"></a>MobileNet V2 Architecture</h4><blockquote>\n<p>We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size $3\\times 3$ as is standard for modern networks, and utilize dropout and batch normalization during training.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/mobilenetv2-cnn-comparison.png\" alt=\"DCNN Architecture Comparison\"></p>\n<h2 id=\"ResNeXt\"><a href=\"#ResNeXt\" class=\"headerlink\" title=\"ResNeXt\"></a>ResNeXt</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Aggregated Residual Transformations for Deep Neural Networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-1\"><a href=\"#Introduction-1\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>自AlexNet以来，Deep Learning涌现了一大批设计优良的网络(如VGG，Inception，ResNet等)。ResNeXt则在ResNet的基础上，一定程度上参考了Inception的设计，即<strong>split-transform-merge</strong>。</p>\n<h3 id=\"The-Core-of-ResNeXt\"><a href=\"#The-Core-of-ResNeXt\" class=\"headerlink\" title=\"The Core of ResNeXt\"></a>The Core of ResNeXt</h3><blockquote>\n<p>Unlike VGG-nets, the family of Inception models [38, 17, 39, 37] have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time [38, 39], but an important common property is a split-transform-merge strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by 1×1 convolutions), transformed by a set of specialized filters (3×3, 5×5, etc.), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity.</p>\n</blockquote>\n<p>尽管Inception的<strong>split-transform-merge</strong>策略是非常行之有效的，但是该网络结构过于复杂，人工设计的痕迹过重(相比之下VGG和ResNet则是由相同的block stacking而成)，给人的感觉就是专门为了ImageNet去做的优化，所以当你想要迁移到其他的dataset时就会比较麻烦。因此，ResNeXt的设计就是：在VGG/ResNet的stacking block的基础上，融合进了Inception的split-transform-merge策略。这就是ResNeXt的基础idea。作者在实验中发现cardinality (the size of the set of transformations)对performance的影响是最大的，甚至要大于width和depth。</p>\n<blockquote>\n<p>Our method harnesses additions to aggregate a set of transformations. But we argue that it is imprecise to view<br>our method as ensembling, because the members to be aggregated<br>are trained jointly, not independently.</p>\n</blockquote>\n<blockquote>\n<p>The above operation can be recast as a combination of<br>splitting, transforming, and aggregating. </p>\n<ol>\n<li>Splitting: the vector $x$ is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace $x_i$. </li>\n<li>Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: $w_i x_i$.</li>\n<li>Aggregating: the transformations in all embeddings are aggregated by $\\sum_{i=1}^D$.</li>\n</ol>\n</blockquote>\n<p>若将$W$更换为更一般的形式，即任意一种function mapping: $\\mathcal{T}(x)$，那么aggregated transformations就变成了:<br>$$<br>\\mathcal{F}(x)=\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$<br>其中$\\mathcal{T}_i$可以将$x$映射到低维空间。$C$是transformation的size，也就是本文主角——<strong>cardinality</strong>。</p>\n<blockquote>\n<p>In Eqn.(2), $C$ is the size of the set of transformations to be aggregated. We refer to $C$ as cardinality [2]. In Eqn.(2) $C$ is in a position similar to $D$ in Eqn.(1), but $C$ need not equal $D$ and can be an arbitrary number. While the dimension of width is related to the number of simple transformations (inner product), we argue that the dimension of cardinality controls the number of more complex transformations. We show by experiments that cardinality is an essential dimension and can be more effective than the dimensions of width and depth.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/resnext_block.jpg\" alt=\"ResNeXt Block\"></p>\n<p>那么在ResNet的identical mapping背景下，就变成了这样一种熟悉的结构：<br>$$<br>y=x+\\sum_{i=1}^C \\mathcal{T}_i(x)<br>$$</p>\n<blockquote>\n<p><strong>Relation to Grouped Convolutions</strong>. The above module becomes more succinct using the notation of grouped convolutions [24]. This reformulation is illustrated in Fig. 3(c). All the low-dimensional embeddings (the first $1\\times 1$ layers) can be replaced by a single, wider layer (e.g., $1\\times 1$, 128-d in Fig 3(c)). Splitting is essentially done by the grouped convolutional layer when it divides its input channels into groups. The grouped convolutional layer in Fig. 3(c) performs 32 groups of convolutions whose input and output channels are 4-dimensional. The grouped convolutional layer concatenates them as the outputs of the layer. The block in Fig. 3(c) looks like the original bottleneck residual block in Fig. 1(left), except that Fig. 3(c) is a wider but sparsely connected module.</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/equivalent_building_blocks_of_resnext.jpg\" alt=\"Equivalent Building Blocks of ResNeXt\"></p>\n<h2 id=\"DenseNet\"><a href=\"#DenseNet\" class=\"headerlink\" title=\"DenseNet\"></a>DenseNet</h2><blockquote>\n<p>Paper: <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">Densely Connected Convolutional Networks</a></p>\n</blockquote>\n<h3 id=\"What-is-DenseNet\"><a href=\"#What-is-DenseNet\" class=\"headerlink\" title=\"What is DenseNet?\"></a>What is DenseNet?</h3><p>DenseNet是CVPR2017 Best Paper，是继ResNet之后更加优秀的网络。<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\">前面我们已经介绍过</a>，ResNet一定程度上解决了gradient vanishing的问题，通过ResNet中的identical mapping使得网络深度可以到达上千层。那么DenseNet又做了哪些改进呢？本文为你一一解答！</p>\n<p>在介绍DenseNet之前，我们先回顾一下ResNet做了什么改动，当shortcuts还未被引入DCNN之前，AlexNet/VGG/GoogLeNet都属于构造比较简单的feedforward network，即信息<strong>一层一层往前传播，在BP时梯度一层一层往后传</strong>，但是这样在网络结构很深的时候，就会存在gradient vanishing的问题。所以Kaiming He创造性地引入了skip connection，来使得信息可以从第$i$层之间做identical mapping传播到第$i+t$层，这样就保证了信息的高效流通。</p>\n<blockquote>\n<p>注: 关于ResNet更详细的介绍，请参考<a href=\"https://lucasxlu.github.io/blog/2018/10/23/dl-architecture/#ResNet\">这里</a>。</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/dense_block.jpg\" alt=\"Dense Block\"></p>\n<p>而DenseNet，就是把这种skip connection做到了极致。为了保证信息在不同layer之间的流通，DenseNet将skip connection做到了每一层和该层之后的所有层中。和ResNet中采用的DW Summation不同的是，DenseNet直接concatenate不同层的features (因为ResNet中DW Summation会影响信息流动)。</p>\n<p>值得一提的是，<strong>DenseNet比ResNet的参数更少</strong>。因为dense connection的结构，使得网络不需要重新学习多余的feature maps。<br>此外，every layer都可以从loss层直接获得梯度，从early layer是获得信息流，也产生了一种<strong>deep supervision</strong>。最后，作者还注意到dense connections一定程度上可以视为Regularization，所以可以缓解overfitting。</p>\n<p>它有以下优点：</p>\n<ul>\n<li>减轻了gradient vanishing问题</li>\n<li>加强了梯度传播</li>\n<li>更好的feature reuse</li>\n<li>极大地减少了参数</li>\n</ul>\n<h3 id=\"Delve-into-DenseNet\"><a href=\"#Delve-into-DenseNet\" class=\"headerlink\" title=\"Delve into DenseNet\"></a>Delve into DenseNet</h3><h4 id=\"Dense-connectivity\"><a href=\"#Dense-connectivity\" class=\"headerlink\" title=\"Dense connectivity\"></a>Dense connectivity</h4><p>To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, $x_0, \\cdots, x_{l-1}$, as input:<br>$$<br>x_l=H_l([x_0,x_1,\\cdots,x_{l-1}])<br>$$<br>where $[x_0,x_1,\\cdots,x_{l-1}]$ refers to the concatenation of the feature-maps produced in layers $0, \\cdots, l−1$.</p>\n<h4 id=\"Pooling-layers\"><a href=\"#Pooling-layers\" class=\"headerlink\" title=\"Pooling layers\"></a>Pooling layers</h4><p>The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an $1\\times 1$ convolutional layer followed by a $2\\times 2$ average pooling layer.</p>\n<h4 id=\"Growth-rate\"><a href=\"#Growth-rate\" class=\"headerlink\" title=\"Growth rate\"></a>Growth rate</h4><p>If each function $H_l$ produces $k$ featuremaps, it follows that the $l$-th layer has $k_0 + k\\times (l−1)$ input<br>feature-maps, where $k_0$ is the number of channels in the input layer. An important difference between DenseNet and<br>existing network architectures is that DenseNet can have<br>very narrow layers, e.g., $k = 12$. We refer to the hyperparameter $k$ as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on.</p>\n<p><strong>One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds $k$ feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer</strong>.</p>\n<h4 id=\"Bottleneck-layers\"><a href=\"#Bottleneck-layers\" class=\"headerlink\" title=\"Bottleneck layers\"></a>Bottleneck layers</h4><p>Although each layer only produces $k$ output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\\times 1$ convolution can be introduced as bottleneck layer before each $3\\times 3$ convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv($1\\times 1$)-BN-ReLU-Conv($3\\times 3$) version of $H_l$, as DenseNet-B. In our experiments, we let each $1\\times 1$ convolution produce 4k feature-maps.</p>\n<h2 id=\"Identity-Mappings-in-Deep-Residual-Networks\"><a href=\"#Identity-Mappings-in-Deep-Residual-Networks\" class=\"headerlink\" title=\"Identity Mappings in Deep Residual Networks\"></a>Identity Mappings in Deep Residual Networks</h2><blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a></p>\n</blockquote>\n<h3 id=\"Introduction-2\"><a href=\"#Introduction-2\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>ResNet已经成了很多CV任务的标配，作者Kaiming He在ResNet里引入了shortcut来辅助DCNN的学习与优化，但是对于shortcut为什么能work则没有过多提及。本文是ResNet作者本人发表在ECCV’16上的Paper，主要在于解释identical mapping为何能work，并且对比了identical mapping的一些变体，最后提出了pre-activation。</p>\n<h3 id=\"Delve-Into-ResNet-and-Identical-Mapping\"><a href=\"#Delve-Into-ResNet-and-Identical-Mapping\" class=\"headerlink\" title=\"Delve Into ResNet and Identical Mapping\"></a>Delve Into ResNet and Identical Mapping</h3><p>ResNet可以表示为这样：<br>$$<br>y_l=h(x_l) + \\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1} = f(y_l)<br>$$<br>其中，$\\mathcal{F}$代表residual function，$h(x_l)=x_l$代表identical mapping，$f$代表ReLU函数。</p>\n<p>在本文中，作者发现，<strong>若$h(x_l)$和$f(y_l)$都是identical mapping的话，信息就可以直接从一个unit传播到下几层的units，无论是在forward还是backward都是如此</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-architecture/proposed_residual_unit.jpg\" alt=\"Proposed Residual Unit\"></p>\n<p>为了构造identical mapping $f(y_l)=y_l$，我们可以将activation function (ReLU and BN)看作weight layers的<code>pre-activation</code>。</p>\n<h3 id=\"Analysis-of-Deep-Residual-Networks\"><a href=\"#Analysis-of-Deep-Residual-Networks\" class=\"headerlink\" title=\"Analysis of Deep Residual Networks\"></a>Analysis of Deep Residual Networks</h3><p>CVPR’15 Best Paper中的原始ResNet Unit是这样的：<br>$$<br>y_l=h(x_l)+\\mathcal{F}(x_l, \\mathcal{W}_l)<br>$$</p>\n<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>若$f$也是identical mapping: $x_{l+1}\\equiv y_l$，就可以得到：<br>$$<br>x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)<br>$$<br>Recursively，我们可以得到：<br>$$<br>x_{l+2}=x_{l+1}+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})=x_l+\\mathcal{F}(x_l, \\mathcal{W}_l)+\\mathcal{F}(x_{l+1}, \\mathcal{W}_{l+1})<br>$$<br>所以有：<br>$$<br>x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)<br>$$<br>所以，对于deep的$L$层和shallow的$l$层，特征$x_L$可以表示成<strong>shallow unit feature $x_l$和residual function $\\sum_{i=l}^{L-1}\\mathcal{F}$的加和</strong>！说明：</p>\n<ol>\n<li><strong>模型是任意units $L$和$l$ 的residual function</strong></li>\n<li>特征$x_L=x_0+\\sum_{i=0}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$是所有proceeding residual functions输出的summation再加上$x_0$</li>\n</ol>\n<p>BP的时候，根据chain rule，就得到如下公式(假设loss function为$\\epsilon$)：<br>$$<br>\\frac{\\partial \\epsilon}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}\\frac{\\partial x_L}{\\partial x_l}=\\frac{\\partial \\epsilon}{\\partial x_L}(1+\\frac{\\partial }{\\partial x_l} \\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i))<br>$$<br>所以，梯度$\\frac{\\partial \\epsilon}{\\partial x_l}$可以看作两个部分：</p>\n<ol>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}$直接从高层流通回来</li>\n<li>$\\frac{\\partial \\epsilon}{\\partial x_L}(\\frac{\\partial }{\\partial x_l}\\sum_{i=l}^{L-1}\\mathcal{F})$流经了其他的weight layers</li>\n</ol>\n<h4 id=\"Discussions\"><a href=\"#Discussions\" class=\"headerlink\" title=\"Discussions\"></a>Discussions</h4><p>Paper里也对一些identical mapping的变体进行了实验与探讨，反正scaling, gating, $1\\times 1$ convolutions, and dropout都效果不如原来的好。并且，<strong>由于$1\\times 1$ conv</strong>引入了更多的参数，理论上讲representation learning ability是要比原来的ResNet要高的，结果却比原来低，说明这种performance drop不是因为representation ability，而是因为优化问题所致。</p>\n<blockquote>\n<p>The shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations<br>(scaling, gating, $1\\times 1$ convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and $1\\times 1$ convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and $1\\times 1$ convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts. However, their training error is higher than that of identity shortcuts,indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.</p>\n</blockquote>\n<p>此外，作者还验证了，当使用BN + ReLU作为pre-activation时，模型performance有了显著地改善。这种改善主要由两点带来：</p>\n<ol>\n<li>因为$f$是identical mapping，所以整个模型的optimization更容易了。</li>\n<li>使用BN作为pre-activation增加了模型的regularization，因BN本身具有regularization的效果。在CVPR’15原始版本的ResNet中，尽管BN normalize了信号，但是却立刻被添加进了shortcut，和未被BN normalize的signal一起merge了。而在pre-activation中，所有weight layers的input均被normalize了。</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Krizhevsky A, Sutskever I, Hinton G E. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Imagenet classification with deep convolutional neural networks</a>[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>\n<li>Simonyan K, Zisserman A. <a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" target=\"_blank\" rel=\"noopener\">Very deep convolutional networks for large-scale image recognition</a>[J]. arXiv preprint arXiv:1409.1556, 2014.</li>\n<li>Lin M, Chen Q, Yan S. <a href=\"https://arxiv.org/pdf/1312.4400v3.pdf\" target=\"_blank\" rel=\"noopener\">Network in network</a>[J]. arXiv preprint arXiv:1312.4400, 2013.</li>\n<li>He K, Zhang X, Ren S, Sun J. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">Deep residual learning for image recognition</a>. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</li>\n<li>Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Chollet, Francois. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Xception: Deep Learning with Depthwise Separable Convolutions.”</a> 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.</li>\n<li>Howard, Andrew G., et al. <a href=\"https://arxiv.org/pdf/1704.04861v1.pdf\" target=\"_blank\" rel=\"noopener\">“Mobilenets: Efficient convolutional neural networks for mobile vision applications.”</a> arXiv preprint arXiv:1704.04861 (2017).</li>\n<li>Zhu, Mark Sandler Andrew Howard Menglong, and Andrey Zhmoginov Liang-Chieh Chen. <a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"noopener\">“MobileNetV2: Inverted Residuals and Linear Bottlenecks.”</a>[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</li>\n<li>Xie, Saining, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Aggregated residual transformations for deep neural networks.”</a> Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.</li>\n<li>Huang, Gao, et al. <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Densely Connected Convolutional Networks.”</a> CVPR. Vol. 1. No. 2. 2017.</li>\n<li>He K, Zhang X, Ren S, et al. <a href=\"https://arxiv.org/pdf/1603.05027v3.pdf\" target=\"_blank\" rel=\"noopener\">Identity mappings in deep residual networks</a>[C]//European conference on computer vision. Springer, Cham, 2016: 630-645.</li>\n<li>Szegedy, Christian, et al. <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" target=\"_blank\" rel=\"noopener\">“Going deeper with convolutions.”</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>\n</ol>\n"},{"title":"[DL] Batch Normalization","date":"2018-11-20T15:18:25.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n[Batch Normalization](http://proceedings.mlr.press/v37/ioffe15.pdf)是现如今主流深度学习模型必备组件。笔者认为，这是一个和ResNet里提出的skip connection一样对深度学习发展十分insightful的idea。本文旨在对BatchNorm进行一下系统的梳理与讲解。\n\n> Paper: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n\n训练Deep Models通常是比较麻烦的，以feedforward network为例，每一层输入distribution的改动都会影响其后继的层，若使用了Sigmoid这样的non-linearity transformation，则还会存在saturation的问题(sigmoid两端)。\n\nBatchNorm通过对每一个mini-batch samples做normalization，可以极大地减小这种internal covariate shift，从而让整个网络使用更大的learning rate，取消因防止overfitting对Dropout的依赖，不需要可以关注param initialization等。\n\n假设DNN的optimization object如下：\n$$\n\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)\n$$\n\n使用SGD优化算法的话，会随机sample一个mini-batch，使用mini-batch而非one-by-one的好处在于：\n* 一个batch的gradient是对整个training set中的estimation，若batch size越大显然就会得到更准确的estimation\n* 计算一个mini-batch的gradient显然比one-by-one sample更为高效\n\nSGD虽然有效，但是在feedforward network的训练中会存在一些问题，例如某一层的input会受到其前驱层的影响，所以在DNN中一个小的扰动都会对整个网络的训练带来很大的影响。\n\n若一个learning system的input发生了变化，我们就称该system经历了**covariate shift**。\n\nBatchNorm通过对input进行normalization来减小这种internal covariate shift，此外，BatchNorm通过减少gradient对参数初始化和网络模型参数量的依赖，也会利于gradient flow，在DNN中加入BN层后，允许我们使用更大的learning rate，以及sigmoid activation function (不用担心两端的saturation问题)。此外**BatchNorm还有正则化的作用**(引入了加性和乘性噪声)，从而减少了防止overfitting背景下对Dropout的依赖。\n\n## Normalization via Mini-Batch Statistics\n对于每一层的input：$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$\n其中，$E[x^{(k)}]$和$Var[x^{(k)}]$从整个training set上计算而来。\n\n但是单纯只做normalization会改变layer的表示能力，因此还需要**scale and shift the normalized value**，也就是这样的：\n$$\ny^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}\n$$\n其中，$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$，$\\beta^{(k)}=E[x^{(k)}]$。\n\n![BatchNorm Transform](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg)\n\n每个normalized activation $\\hat{x}^{(k)}$可以视为线性变换$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$后子网络的输入。\n\nBN层的参数更新过程如下：\n![Update of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg)\n\nBN层的训练过程如下：\n![Training of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg)\n\n### BatchNorm in CNN\nBN在CNN中和MLP中有一些差别，CNN中，BatchNorm Transform独立地应用到每一维的channel$x=Wu$上，每一个channel对应的学习参数$\\gamma^{(k)}$和$\\beta^{(k)}$。\n\n### Batch Normalization enables higher learning rates\n通过对activations进行归一化，可以避免在DNN中小的数据扰动对整个网络的影响。此外，BN也使得训练过程对parameters scale更加适应，当learning rate过高，在DNN的BP中，很容易出现gradient explosion和divergence。但是，添加了BN层后，BP就不受parameters scale的影响了。\n例如：\n$$\nBN(Wu)=BN((aW)u)\n$$\nBP时：$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$。此外，$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$，所以larger weights会导致smaller gradients。\n\n\n## Reference\n1. Ioffe S, Szegedy C. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)[C]//International Conference on Machine Learning. 2015: 448-456.","source":"_posts/dl-bn.md","raw":"---\ntitle: \"[DL] Batch Normalization\"\ndate: 2018-11-20 23:18:25\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n[Batch Normalization](http://proceedings.mlr.press/v37/ioffe15.pdf)是现如今主流深度学习模型必备组件。笔者认为，这是一个和ResNet里提出的skip connection一样对深度学习发展十分insightful的idea。本文旨在对BatchNorm进行一下系统的梳理与讲解。\n\n> Paper: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n\n训练Deep Models通常是比较麻烦的，以feedforward network为例，每一层输入distribution的改动都会影响其后继的层，若使用了Sigmoid这样的non-linearity transformation，则还会存在saturation的问题(sigmoid两端)。\n\nBatchNorm通过对每一个mini-batch samples做normalization，可以极大地减小这种internal covariate shift，从而让整个网络使用更大的learning rate，取消因防止overfitting对Dropout的依赖，不需要可以关注param initialization等。\n\n假设DNN的optimization object如下：\n$$\n\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)\n$$\n\n使用SGD优化算法的话，会随机sample一个mini-batch，使用mini-batch而非one-by-one的好处在于：\n* 一个batch的gradient是对整个training set中的estimation，若batch size越大显然就会得到更准确的estimation\n* 计算一个mini-batch的gradient显然比one-by-one sample更为高效\n\nSGD虽然有效，但是在feedforward network的训练中会存在一些问题，例如某一层的input会受到其前驱层的影响，所以在DNN中一个小的扰动都会对整个网络的训练带来很大的影响。\n\n若一个learning system的input发生了变化，我们就称该system经历了**covariate shift**。\n\nBatchNorm通过对input进行normalization来减小这种internal covariate shift，此外，BatchNorm通过减少gradient对参数初始化和网络模型参数量的依赖，也会利于gradient flow，在DNN中加入BN层后，允许我们使用更大的learning rate，以及sigmoid activation function (不用担心两端的saturation问题)。此外**BatchNorm还有正则化的作用**(引入了加性和乘性噪声)，从而减少了防止overfitting背景下对Dropout的依赖。\n\n## Normalization via Mini-Batch Statistics\n对于每一层的input：$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$\n其中，$E[x^{(k)}]$和$Var[x^{(k)}]$从整个training set上计算而来。\n\n但是单纯只做normalization会改变layer的表示能力，因此还需要**scale and shift the normalized value**，也就是这样的：\n$$\ny^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}\n$$\n其中，$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$，$\\beta^{(k)}=E[x^{(k)}]$。\n\n![BatchNorm Transform](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg)\n\n每个normalized activation $\\hat{x}^{(k)}$可以视为线性变换$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$后子网络的输入。\n\nBN层的参数更新过程如下：\n![Update of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg)\n\nBN层的训练过程如下：\n![Training of BN Layer](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg)\n\n### BatchNorm in CNN\nBN在CNN中和MLP中有一些差别，CNN中，BatchNorm Transform独立地应用到每一维的channel$x=Wu$上，每一个channel对应的学习参数$\\gamma^{(k)}$和$\\beta^{(k)}$。\n\n### Batch Normalization enables higher learning rates\n通过对activations进行归一化，可以避免在DNN中小的数据扰动对整个网络的影响。此外，BN也使得训练过程对parameters scale更加适应，当learning rate过高，在DNN的BP中，很容易出现gradient explosion和divergence。但是，添加了BN层后，BP就不受parameters scale的影响了。\n例如：\n$$\nBN(Wu)=BN((aW)u)\n$$\nBP时：$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$。此外，$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$，所以larger weights会导致smaller gradients。\n\n\n## Reference\n1. Ioffe S, Szegedy C. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)[C]//International Conference on Machine Learning. 2015: 448-456.","slug":"dl-bn","published":1,"updated":"2018-11-20T16:17:05.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c1000f608wrmkhhxc2","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p><a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization</a>是现如今主流深度学习模型必备组件。笔者认为，这是一个和ResNet里提出的skip connection一样对深度学习发展十分insightful的idea。本文旨在对BatchNorm进行一下系统的梳理与讲解。</p>\n<blockquote>\n<p>Paper: <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>\n</blockquote>\n<p>训练Deep Models通常是比较麻烦的，以feedforward network为例，每一层输入distribution的改动都会影响其后继的层，若使用了Sigmoid这样的non-linearity transformation，则还会存在saturation的问题(sigmoid两端)。</p>\n<p>BatchNorm通过对每一个mini-batch samples做normalization，可以极大地减小这种internal covariate shift，从而让整个网络使用更大的learning rate，取消因防止overfitting对Dropout的依赖，不需要可以关注param initialization等。</p>\n<p>假设DNN的optimization object如下：<br>$$<br>\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)<br>$$</p>\n<p>使用SGD优化算法的话，会随机sample一个mini-batch，使用mini-batch而非one-by-one的好处在于：</p>\n<ul>\n<li>一个batch的gradient是对整个training set中的estimation，若batch size越大显然就会得到更准确的estimation</li>\n<li>计算一个mini-batch的gradient显然比one-by-one sample更为高效</li>\n</ul>\n<p>SGD虽然有效，但是在feedforward network的训练中会存在一些问题，例如某一层的input会受到其前驱层的影响，所以在DNN中一个小的扰动都会对整个网络的训练带来很大的影响。</p>\n<p>若一个learning system的input发生了变化，我们就称该system经历了<strong>covariate shift</strong>。</p>\n<p>BatchNorm通过对input进行normalization来减小这种internal covariate shift，此外，BatchNorm通过减少gradient对参数初始化和网络模型参数量的依赖，也会利于gradient flow，在DNN中加入BN层后，允许我们使用更大的learning rate，以及sigmoid activation function (不用担心两端的saturation问题)。此外<strong>BatchNorm还有正则化的作用</strong>(引入了加性和乘性噪声)，从而减少了防止overfitting背景下对Dropout的依赖。</p>\n<h2 id=\"Normalization-via-Mini-Batch-Statistics\"><a href=\"#Normalization-via-Mini-Batch-Statistics\" class=\"headerlink\" title=\"Normalization via Mini-Batch Statistics\"></a>Normalization via Mini-Batch Statistics</h2><p>对于每一层的input：$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$<br>其中，$E[x^{(k)}]$和$Var[x^{(k)}]$从整个training set上计算而来。</p>\n<p>但是单纯只做normalization会改变layer的表示能力，因此还需要<strong>scale and shift the normalized value</strong>，也就是这样的：<br>$$<br>y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}<br>$$<br>其中，$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$，$\\beta^{(k)}=E[x^{(k)}]$。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg\" alt=\"BatchNorm Transform\"></p>\n<p>每个normalized activation $\\hat{x}^{(k)}$可以视为线性变换$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$后子网络的输入。</p>\n<p>BN层的参数更新过程如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg\" alt=\"Update of BN Layer\"></p>\n<p>BN层的训练过程如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg\" alt=\"Training of BN Layer\"></p>\n<h3 id=\"BatchNorm-in-CNN\"><a href=\"#BatchNorm-in-CNN\" class=\"headerlink\" title=\"BatchNorm in CNN\"></a>BatchNorm in CNN</h3><p>BN在CNN中和MLP中有一些差别，CNN中，BatchNorm Transform独立地应用到每一维的channel$x=Wu$上，每一个channel对应的学习参数$\\gamma^{(k)}$和$\\beta^{(k)}$。</p>\n<h3 id=\"Batch-Normalization-enables-higher-learning-rates\"><a href=\"#Batch-Normalization-enables-higher-learning-rates\" class=\"headerlink\" title=\"Batch Normalization enables higher learning rates\"></a>Batch Normalization enables higher learning rates</h3><p>通过对activations进行归一化，可以避免在DNN中小的数据扰动对整个网络的影响。此外，BN也使得训练过程对parameters scale更加适应，当learning rate过高，在DNN的BP中，很容易出现gradient explosion和divergence。但是，添加了BN层后，BP就不受parameters scale的影响了。<br>例如：<br>$$<br>BN(Wu)=BN((aW)u)<br>$$<br>BP时：$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$。此外，$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$，所以larger weights会导致smaller gradients。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Ioffe S, Szegedy C. <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>[C]//International Conference on Machine Learning. 2015: 448-456.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p><a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization</a>是现如今主流深度学习模型必备组件。笔者认为，这是一个和ResNet里提出的skip connection一样对深度学习发展十分insightful的idea。本文旨在对BatchNorm进行一下系统的梳理与讲解。</p>\n<blockquote>\n<p>Paper: <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>\n</blockquote>\n<p>训练Deep Models通常是比较麻烦的，以feedforward network为例，每一层输入distribution的改动都会影响其后继的层，若使用了Sigmoid这样的non-linearity transformation，则还会存在saturation的问题(sigmoid两端)。</p>\n<p>BatchNorm通过对每一个mini-batch samples做normalization，可以极大地减小这种internal covariate shift，从而让整个网络使用更大的learning rate，取消因防止overfitting对Dropout的依赖，不需要可以关注param initialization等。</p>\n<p>假设DNN的optimization object如下：<br>$$<br>\\Theta=\\mathop{argmin} \\limits_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i, \\Theta)<br>$$</p>\n<p>使用SGD优化算法的话，会随机sample一个mini-batch，使用mini-batch而非one-by-one的好处在于：</p>\n<ul>\n<li>一个batch的gradient是对整个training set中的estimation，若batch size越大显然就会得到更准确的estimation</li>\n<li>计算一个mini-batch的gradient显然比one-by-one sample更为高效</li>\n</ul>\n<p>SGD虽然有效，但是在feedforward network的训练中会存在一些问题，例如某一层的input会受到其前驱层的影响，所以在DNN中一个小的扰动都会对整个网络的训练带来很大的影响。</p>\n<p>若一个learning system的input发生了变化，我们就称该system经历了<strong>covariate shift</strong>。</p>\n<p>BatchNorm通过对input进行normalization来减小这种internal covariate shift，此外，BatchNorm通过减少gradient对参数初始化和网络模型参数量的依赖，也会利于gradient flow，在DNN中加入BN层后，允许我们使用更大的learning rate，以及sigmoid activation function (不用担心两端的saturation问题)。此外<strong>BatchNorm还有正则化的作用</strong>(引入了加性和乘性噪声)，从而减少了防止overfitting背景下对Dropout的依赖。</p>\n<h2 id=\"Normalization-via-Mini-Batch-Statistics\"><a href=\"#Normalization-via-Mini-Batch-Statistics\" class=\"headerlink\" title=\"Normalization via Mini-Batch Statistics\"></a>Normalization via Mini-Batch Statistics</h2><p>对于每一层的input：$\\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$<br>其中，$E[x^{(k)}]$和$Var[x^{(k)}]$从整个training set上计算而来。</p>\n<p>但是单纯只做normalization会改变layer的表示能力，因此还需要<strong>scale and shift the normalized value</strong>，也就是这样的：<br>$$<br>y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}<br>$$<br>其中，$\\gamma^{(k)}=\\sqrt{Var(x^{(k)})}$，$\\beta^{(k)}=E[x^{(k)}]$。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_transform.jpg\" alt=\"BatchNorm Transform\"></p>\n<p>每个normalized activation $\\hat{x}^{(k)}$可以视为线性变换$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$后子网络的输入。</p>\n<p>BN层的参数更新过程如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_update.jpg\" alt=\"Update of BN Layer\"></p>\n<p>BN层的训练过程如下：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bn/BN_training.jpg\" alt=\"Training of BN Layer\"></p>\n<h3 id=\"BatchNorm-in-CNN\"><a href=\"#BatchNorm-in-CNN\" class=\"headerlink\" title=\"BatchNorm in CNN\"></a>BatchNorm in CNN</h3><p>BN在CNN中和MLP中有一些差别，CNN中，BatchNorm Transform独立地应用到每一维的channel$x=Wu$上，每一个channel对应的学习参数$\\gamma^{(k)}$和$\\beta^{(k)}$。</p>\n<h3 id=\"Batch-Normalization-enables-higher-learning-rates\"><a href=\"#Batch-Normalization-enables-higher-learning-rates\" class=\"headerlink\" title=\"Batch Normalization enables higher learning rates\"></a>Batch Normalization enables higher learning rates</h3><p>通过对activations进行归一化，可以避免在DNN中小的数据扰动对整个网络的影响。此外，BN也使得训练过程对parameters scale更加适应，当learning rate过高，在DNN的BP中，很容易出现gradient explosion和divergence。但是，添加了BN层后，BP就不受parameters scale的影响了。<br>例如：<br>$$<br>BN(Wu)=BN((aW)u)<br>$$<br>BP时：$\\frac{\\partial BN((aW)u)}{\\partial u}=\\frac{\\partial BN(Wu)}{\\partial u}$。此外，$\\frac{\\partial BN((aW)u)}{\\partial aW}=\\frac{1}{a}\\cdot \\frac{\\partial BN(Wu)}{\\partial W}$，所以larger weights会导致smaller gradients。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Ioffe S, Szegedy C. <a href=\"http://proceedings.mlr.press/v37/ioffe15.pdf\" target=\"_blank\" rel=\"noopener\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>[C]//International Conference on Machine Learning. 2015: 448-456.</li>\n</ol>\n"},{"title":"[DL] BackPropogation","date":"2018-07-25T06:33:25.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization"],"_content":"## Introduction\n反向传播是神经网络训练过程中非常重要的步骤。目前许多深度学习框架以（例如Tensorflow）已在定义的computational graph中自行帮开发者完成了反向传播算法的计算。但是作为深度学习领域的研究人员，还是应该了解该算法的本质。本文就对该算法进行深入讲解（素材来自Stanford CS231n Spring,2017）：  \n一个简单的computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)是这样的：  \n![Fig. 1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png)\n\n令$q=x+y$，则$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ \n\n可得$f=qz$，则$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$\n\n![Fig. 2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png)\n\n同时，$\\frac{\\partial f}{\\partial f}=1$\n\n![Fig. 3](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png)\n\n由链式法则可得：\n\n$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$\n\n$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$\n\n![Fig. 4.1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png)\n\n![Fig. 4.2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png)\n\n至此，对于反向传播算法的初步介绍就结束了，下面我们再来看一个更深入的例子：\n\n定义computational graph：\n$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$\n\n同样地，第一步反传，先求$\\frac{\\partial f}{\\partial f}=1$；再求$\\frac{1}{x}$(将分母视为大的变量x)的导数：\n\n![Fig. 5](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png)\n\n![Fig. 6](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png)\n\n接下来求1+x(将分母的指数函数一块视为一个整体变量x)的导数：\n\n![Fig. 7](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png)\n\n现在求分母里大的指数函数的导数：\n\n![Fig. 8](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png)\n\n![Fig. 9](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png)\n\n注意这里的处理方式：将$-(w_0 x_0+w_1 x_1+w_2)$视为一个整体。$e^{-1}$的来源是因为$-(w_0 x_0+w_1 x_1+w_2)=-1$。\n\n接下来再处理$-x$(将$w_0 x_0+w_1 x_1+w_2$视为整体变量$x$)，显然易得：  \n\n![Fig. 10](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png)\n\n接下来处理$w_0 x_0+w_1 x_1+w_2$(公式1)，公式1对$w_2$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。公式1对$X=w_0 x_0+w_1 x_1$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。\n\n![Fig. 11](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png)\n\n接下来处理$w_0 x_0+w_1 x_1$，易得：\n\n![Fig. 12](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n接下来处理$w_0 x_0$和$w_1 x_1$\n\n![Fig. 13](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n例子中的激活函数选用的是Sigmoid函数，利用Sigmoid函数的性质$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$可得：\n\n\n## Reference\n1. http://cs231n.stanford.edu/syllabus.html\n2. http://cs231n.github.io/optimization-2/\n3. http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\n4. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n5. [Derivation of Backpropagation in Convolutional Neural Network (CNN)](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf)\n6. [Backpropagation In Convolutional Neural Networks](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf)","source":"_posts/dl-bp.md","raw":"---\ntitle: \"[DL] BackPropogation\"\ndate: 2018-07-25 14:33:25\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n---\n## Introduction\n反向传播是神经网络训练过程中非常重要的步骤。目前许多深度学习框架以（例如Tensorflow）已在定义的computational graph中自行帮开发者完成了反向传播算法的计算。但是作为深度学习领域的研究人员，还是应该了解该算法的本质。本文就对该算法进行深入讲解（素材来自Stanford CS231n Spring,2017）：  \n一个简单的computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)是这样的：  \n![Fig. 1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png)\n\n令$q=x+y$，则$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ \n\n可得$f=qz$，则$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$\n\n![Fig. 2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png)\n\n同时，$\\frac{\\partial f}{\\partial f}=1$\n\n![Fig. 3](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png)\n\n由链式法则可得：\n\n$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$\n\n$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$\n\n![Fig. 4.1](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png)\n\n![Fig. 4.2](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png)\n\n至此，对于反向传播算法的初步介绍就结束了，下面我们再来看一个更深入的例子：\n\n定义computational graph：\n$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$\n\n同样地，第一步反传，先求$\\frac{\\partial f}{\\partial f}=1$；再求$\\frac{1}{x}$(将分母视为大的变量x)的导数：\n\n![Fig. 5](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png)\n\n![Fig. 6](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png)\n\n接下来求1+x(将分母的指数函数一块视为一个整体变量x)的导数：\n\n![Fig. 7](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png)\n\n现在求分母里大的指数函数的导数：\n\n![Fig. 8](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png)\n\n![Fig. 9](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png)\n\n注意这里的处理方式：将$-(w_0 x_0+w_1 x_1+w_2)$视为一个整体。$e^{-1}$的来源是因为$-(w_0 x_0+w_1 x_1+w_2)=-1$。\n\n接下来再处理$-x$(将$w_0 x_0+w_1 x_1+w_2$视为整体变量$x$)，显然易得：  \n\n![Fig. 10](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png)\n\n接下来处理$w_0 x_0+w_1 x_1+w_2$(公式1)，公式1对$w_2$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。公式1对$X=w_0 x_0+w_1 x_1$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。\n\n![Fig. 11](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png)\n\n接下来处理$w_0 x_0+w_1 x_1$，易得：\n\n![Fig. 12](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n接下来处理$w_0 x_0$和$w_1 x_1$\n\n![Fig. 13](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png)\n\n例子中的激活函数选用的是Sigmoid函数，利用Sigmoid函数的性质$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$可得：\n\n\n## Reference\n1. http://cs231n.stanford.edu/syllabus.html\n2. http://cs231n.github.io/optimization-2/\n3. http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\n4. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n5. [Derivation of Backpropagation in Convolutional Neural Network (CNN)](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf)\n6. [Backpropagation In Convolutional Neural Networks](https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf)","slug":"dl-bp","published":1,"updated":"2018-10-01T04:40:08.673Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c3000h608wvxqe83el","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>反向传播是神经网络训练过程中非常重要的步骤。目前许多深度学习框架以（例如Tensorflow）已在定义的computational graph中自行帮开发者完成了反向传播算法的计算。但是作为深度学习领域的研究人员，还是应该了解该算法的本质。本文就对该算法进行深入讲解（素材来自Stanford CS231n Spring,2017）：<br>一个简单的computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)是这样的：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png\" alt=\"Fig. 1\"></p>\n<p>令$q=x+y$，则$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ </p>\n<p>可得$f=qz$，则$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png\" alt=\"Fig. 2\"></p>\n<p>同时，$\\frac{\\partial f}{\\partial f}=1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png\" alt=\"Fig. 3\"></p>\n<p>由链式法则可得：</p>\n<p>$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$</p>\n<p>$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png\" alt=\"Fig. 4.1\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png\" alt=\"Fig. 4.2\"></p>\n<p>至此，对于反向传播算法的初步介绍就结束了，下面我们再来看一个更深入的例子：</p>\n<p>定义computational graph：<br>$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$</p>\n<p>同样地，第一步反传，先求$\\frac{\\partial f}{\\partial f}=1$；再求$\\frac{1}{x}$(将分母视为大的变量x)的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png\" alt=\"Fig. 5\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png\" alt=\"Fig. 6\"></p>\n<p>接下来求1+x(将分母的指数函数一块视为一个整体变量x)的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png\" alt=\"Fig. 7\"></p>\n<p>现在求分母里大的指数函数的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png\" alt=\"Fig. 8\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png\" alt=\"Fig. 9\"></p>\n<p>注意这里的处理方式：将$-(w_0 x_0+w_1 x_1+w_2)$视为一个整体。$e^{-1}$的来源是因为$-(w_0 x_0+w_1 x_1+w_2)=-1$。</p>\n<p>接下来再处理$-x$(将$w_0 x_0+w_1 x_1+w_2$视为整体变量$x$)，显然易得：  </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png\" alt=\"Fig. 10\"></p>\n<p>接下来处理$w_0 x_0+w_1 x_1+w_2$(公式1)，公式1对$w_2$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。公式1对$X=w_0 x_0+w_1 x_1$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png\" alt=\"Fig. 11\"></p>\n<p>接下来处理$w_0 x_0+w_1 x_1$，易得：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 12\"></p>\n<p>接下来处理$w_0 x_0$和$w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 13\"></p>\n<p>例子中的激活函数选用的是Sigmoid函数，利用Sigmoid函数的性质$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$可得：</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"http://cs231n.stanford.edu/syllabus.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/syllabus.html</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a></li>\n<li><a href=\"http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf</a></li>\n<li><a href=\"https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\" target=\"_blank\" rel=\"noopener\">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf\" target=\"_blank\" rel=\"noopener\">Derivation of Backpropagation in Convolutional Neural Network (CNN)</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf\" target=\"_blank\" rel=\"noopener\">Backpropagation In Convolutional Neural Networks</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>反向传播是神经网络训练过程中非常重要的步骤。目前许多深度学习框架以（例如Tensorflow）已在定义的computational graph中自行帮开发者完成了反向传播算法的计算。但是作为深度学习领域的研究人员，还是应该了解该算法的本质。本文就对该算法进行深入讲解（素材来自Stanford CS231n Spring,2017）：<br>一个简单的computational graph $f(x, y, z)=(x + y)z$ (e.g. $x = -2$, $y = 5$, $z = -4$)是这样的：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig1.png\" alt=\"Fig. 1\"></p>\n<p>令$q=x+y$，则$\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1$ </p>\n<p>可得$f=qz$，则$\\frac{\\partial f}{\\partial q}=z=-4, \\frac{\\partial f}{\\partial z}=q=3$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig2.png\" alt=\"Fig. 2\"></p>\n<p>同时，$\\frac{\\partial f}{\\partial f}=1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig3.png\" alt=\"Fig. 3\"></p>\n<p>由链式法则可得：</p>\n<p>$\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=-4$</p>\n<p>$\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=-4$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-1.png\" alt=\"Fig. 4.1\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig4-2.png\" alt=\"Fig. 4.2\"></p>\n<p>至此，对于反向传播算法的初步介绍就结束了，下面我们再来看一个更深入的例子：</p>\n<p>定义computational graph：<br>$f(w,x)=\\frac{1}{1+e^{-(w_0 x_0+w_1 x_1+w_2)}}$</p>\n<p>同样地，第一步反传，先求$\\frac{\\partial f}{\\partial f}=1$；再求$\\frac{1}{x}$(将分母视为大的变量x)的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig5.png\" alt=\"Fig. 5\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig6.png\" alt=\"Fig. 6\"></p>\n<p>接下来求1+x(将分母的指数函数一块视为一个整体变量x)的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig7.png\" alt=\"Fig. 7\"></p>\n<p>现在求分母里大的指数函数的导数：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig8.png\" alt=\"Fig. 8\"></p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig9.png\" alt=\"Fig. 9\"></p>\n<p>注意这里的处理方式：将$-(w_0 x_0+w_1 x_1+w_2)$视为一个整体。$e^{-1}$的来源是因为$-(w_0 x_0+w_1 x_1+w_2)=-1$。</p>\n<p>接下来再处理$-x$(将$w_0 x_0+w_1 x_1+w_2$视为整体变量$x$)，显然易得：  </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig10.png\" alt=\"Fig. 10\"></p>\n<p>接下来处理$w_0 x_0+w_1 x_1+w_2$(公式1)，公式1对$w_2$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。公式1对$X=w_0 x_0+w_1 x_1$求导结果为1，再乘以之前反向计算的梯度$1×0.2=0.2$。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig11.png\" alt=\"Fig. 11\"></p>\n<p>接下来处理$w_0 x_0+w_1 x_1$，易得：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 12\"></p>\n<p>接下来处理$w_0 x_0$和$w_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-bp/fig12.png\" alt=\"Fig. 13\"></p>\n<p>例子中的激活函数选用的是Sigmoid函数，利用Sigmoid函数的性质$\\frac{d\\sigma_x}{dx}=(1-\\sigma(x))\\sigma(x)$可得：</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"http://cs231n.stanford.edu/syllabus.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/syllabus.html</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a></li>\n<li><a href=\"http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf\" target=\"_blank\" rel=\"noopener\">http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf</a></li>\n<li><a href=\"https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\" target=\"_blank\" rel=\"noopener\">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Derivation-of-CNN.pdf\" target=\"_blank\" rel=\"noopener\">Derivation of Backpropagation in Convolutional Neural Network (CNN)</a></li>\n<li><a href=\"https://github.com/lucasxlu/blog/blob/master/source/_posts/dl-bp/Backpropagation-In-Convolutional-Neural-Networks-DeepGrid.pdf\" target=\"_blank\" rel=\"noopener\">Backpropagation In Convolutional Neural Networks</a></li>\n</ol>\n"},{"title":"[DL] CNN","date":"2018-07-27T08:14:38.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","CNN","Computer Vision"],"_content":"## Introduction\n做Vision的同学们对Convolutional Neural Networks (CNN)一定不会陌生，可以毫不夸张的说，现如今绝大多数的视觉问题，都是由CNN驱动的。从LeNet到如今的ResNet、DenseNet、CliqueNet，CNN的结构发生了很大的变化。本文旨在记录CNN的基础构件。\n\n## Convolution\n1. VALID：无论怎样都不使用zero padding，并且filter只允许访问那些图像中能够完全包含整个核的位置。\n2. SAME：只进行足够的zero padding来保持输出和输入具有相同的大小；然而输入像素中靠近边缘的部分相比于中间部分对于输入像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。\n3. FULL：它进行了足够多的zero padding，使得每个像素在每个方向上恰好被访问了$k$次，最终输出图像的宽度为$m+k-1$。这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。\n\n\n## Pooling\n无论采用什么样的Pooling，当输入做少量变动时，Pooling能够 __帮助输入的表示近似不变__。Shift Invariant指得是当我们对输入进行少量平移时，经过Pooling后的大多数输出并不会发生改变。例如MaxPooling中，Pooling只对周围的最大值比较敏感，而不是对精确的位置。\n\n## Reference\n1. [Deep Learning--CNN](https://www.deeplearningbook.org/contents/convnets.html)","source":"_posts/dl-cnn.md","raw":"---\ntitle: \"[DL] CNN\"\ndate: 2018-07-27 16:14:38\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- CNN\n- Data Science\n- Computer Vision\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- CNN\n- Computer Vision\n---\n## Introduction\n做Vision的同学们对Convolutional Neural Networks (CNN)一定不会陌生，可以毫不夸张的说，现如今绝大多数的视觉问题，都是由CNN驱动的。从LeNet到如今的ResNet、DenseNet、CliqueNet，CNN的结构发生了很大的变化。本文旨在记录CNN的基础构件。\n\n## Convolution\n1. VALID：无论怎样都不使用zero padding，并且filter只允许访问那些图像中能够完全包含整个核的位置。\n2. SAME：只进行足够的zero padding来保持输出和输入具有相同的大小；然而输入像素中靠近边缘的部分相比于中间部分对于输入像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。\n3. FULL：它进行了足够多的zero padding，使得每个像素在每个方向上恰好被访问了$k$次，最终输出图像的宽度为$m+k-1$。这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。\n\n\n## Pooling\n无论采用什么样的Pooling，当输入做少量变动时，Pooling能够 __帮助输入的表示近似不变__。Shift Invariant指得是当我们对输入进行少量平移时，经过Pooling后的大多数输出并不会发生改变。例如MaxPooling中，Pooling只对周围的最大值比较敏感，而不是对精确的位置。\n\n## Reference\n1. [Deep Learning--CNN](https://www.deeplearningbook.org/contents/convnets.html)","slug":"dl-cnn","published":1,"updated":"2018-10-01T04:40:08.836Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c5000k608wl1cri8at","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>做Vision的同学们对Convolutional Neural Networks (CNN)一定不会陌生，可以毫不夸张的说，现如今绝大多数的视觉问题，都是由CNN驱动的。从LeNet到如今的ResNet、DenseNet、CliqueNet，CNN的结构发生了很大的变化。本文旨在记录CNN的基础构件。</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><ol>\n<li>VALID：无论怎样都不使用zero padding，并且filter只允许访问那些图像中能够完全包含整个核的位置。</li>\n<li>SAME：只进行足够的zero padding来保持输出和输入具有相同的大小；然而输入像素中靠近边缘的部分相比于中间部分对于输入像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。</li>\n<li>FULL：它进行了足够多的zero padding，使得每个像素在每个方向上恰好被访问了$k$次，最终输出图像的宽度为$m+k-1$。这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。</li>\n</ol>\n<h2 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h2><p>无论采用什么样的Pooling，当输入做少量变动时，Pooling能够 <strong>帮助输入的表示近似不变</strong>。Shift Invariant指得是当我们对输入进行少量平移时，经过Pooling后的大多数输出并不会发生改变。例如MaxPooling中，Pooling只对周围的最大值比较敏感，而不是对精确的位置。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.deeplearningbook.org/contents/convnets.html\" target=\"_blank\" rel=\"noopener\">Deep Learning–CNN</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>做Vision的同学们对Convolutional Neural Networks (CNN)一定不会陌生，可以毫不夸张的说，现如今绝大多数的视觉问题，都是由CNN驱动的。从LeNet到如今的ResNet、DenseNet、CliqueNet，CNN的结构发生了很大的变化。本文旨在记录CNN的基础构件。</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><ol>\n<li>VALID：无论怎样都不使用zero padding，并且filter只允许访问那些图像中能够完全包含整个核的位置。</li>\n<li>SAME：只进行足够的zero padding来保持输出和输入具有相同的大小；然而输入像素中靠近边缘的部分相比于中间部分对于输入像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。</li>\n<li>FULL：它进行了足够多的zero padding，使得每个像素在每个方向上恰好被访问了$k$次，最终输出图像的宽度为$m+k-1$。这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。</li>\n</ol>\n<h2 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h2><p>无论采用什么样的Pooling，当输入做少量变动时，Pooling能够 <strong>帮助输入的表示近似不变</strong>。Shift Invariant指得是当我们对输入进行少量平移时，经过Pooling后的大多数输出并不会发生改变。例如MaxPooling中，Pooling只对周围的最大值比较敏感，而不是对精确的位置。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.deeplearningbook.org/contents/convnets.html\" target=\"_blank\" rel=\"noopener\">Deep Learning–CNN</a></li>\n</ol>\n"},{"title":"[DL] Data Augmentation","date":"2018-11-10T12:08:23.000Z","mathjax":true,"catagories":["Machine Learning","Deep Learning","Data Augmentation"],"_content":"## Introduction\n众所周知，Deep Learning现如今的繁荣，和**大数据**、**GPU**和**深度学习算法**都是离不开关系的。Deep Learning模型参数众多，需要海量的数据进行拟合，否则很容易overfitting到training set上。而现实情况下，我们不一定能很容易地获取大量高质量标注样本，因此，Data Augmentation则起到了非常大的作用了。这便是本文所要讲述的主角。\n\n## Mixup\n> Paper: [mixup: Beyond Empirical Risk Minimization](https://openreview.net/pdf?id=r1Ddp1-Rb)\n\nMixup的核心idea如下：\n$$\n\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j\n$$\n\n$$\n\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j\n$$\n其中，$x_i, x_j$为raw input vectors，$y_i, y_j$为one-hot encodings。\n\n> Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.\n\nMixup的PyTorch代码如下，是不是非常简洁？\n```python\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam * x1 + (1. - lam) * x2)\n    y = Variable(lam * y1 + (1. - lam) * y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\n\n### What is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam's razor, since it is one of the simplest possible behaviors.\n\nmixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.\n\n\n## Reference\n1. Zhang, Hongyi, et al. [\"mixup: Beyond empirical risk minimization.\"](https://openreview.net/pdf?id=r1Ddp1-Rb) International Conference on Learning Representations (2018).","source":"_posts/dl-data-augmentation.md","raw":"---\ntitle: \"[DL] Data Augmentation\"\ndate: 2018-11-10 20:08:23\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Data Augmentation\ncatagories:\n- Machine Learning\n- Deep Learning\n- Data Augmentation\n---\n## Introduction\n众所周知，Deep Learning现如今的繁荣，和**大数据**、**GPU**和**深度学习算法**都是离不开关系的。Deep Learning模型参数众多，需要海量的数据进行拟合，否则很容易overfitting到training set上。而现实情况下，我们不一定能很容易地获取大量高质量标注样本，因此，Data Augmentation则起到了非常大的作用了。这便是本文所要讲述的主角。\n\n## Mixup\n> Paper: [mixup: Beyond Empirical Risk Minimization](https://openreview.net/pdf?id=r1Ddp1-Rb)\n\nMixup的核心idea如下：\n$$\n\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j\n$$\n\n$$\n\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j\n$$\n其中，$x_i, x_j$为raw input vectors，$y_i, y_j$为one-hot encodings。\n\n> Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.\n\nMixup的PyTorch代码如下，是不是非常简洁？\n```python\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam * x1 + (1. - lam) * x2)\n    y = Variable(lam * y1 + (1. - lam) * y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\n\n### What is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam's razor, since it is one of the simplest possible behaviors.\n\nmixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.\n\n\n## Reference\n1. Zhang, Hongyi, et al. [\"mixup: Beyond empirical risk minimization.\"](https://openreview.net/pdf?id=r1Ddp1-Rb) International Conference on Learning Representations (2018).","slug":"dl-data-augmentation","published":1,"updated":"2018-11-10T15:03:11.194Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c7000m608wiwmlwpvc","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>众所周知，Deep Learning现如今的繁荣，和<strong>大数据</strong>、<strong>GPU</strong>和<strong>深度学习算法</strong>都是离不开关系的。Deep Learning模型参数众多，需要海量的数据进行拟合，否则很容易overfitting到training set上。而现实情况下，我们不一定能很容易地获取大量高质量标注样本，因此，Data Augmentation则起到了非常大的作用了。这便是本文所要讲述的主角。</p>\n<h2 id=\"Mixup\"><a href=\"#Mixup\" class=\"headerlink\" title=\"Mixup\"></a>Mixup</h2><blockquote>\n<p>Paper: <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond Empirical Risk Minimization</a></p>\n</blockquote>\n<p>Mixup的核心idea如下：<br>$$<br>\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j<br>$$</p>\n<p>$$<br>\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j<br>$$<br>其中，$x_i, x_j$为raw input vectors，$y_i, y_j$为one-hot encodings。</p>\n<blockquote>\n<p>Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.</p>\n</blockquote>\n<p>Mixup的PyTorch代码如下，是不是非常简洁？<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y1, y2 should be one-hot vectors</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (x1, y1), (x2, y2) <span class=\"keyword\">in</span> zip(loader1, loader2):</span><br><span class=\"line\">    lam = numpy.random.beta(alpha, alpha)</span><br><span class=\"line\">    x = Variable(lam * x1 + (<span class=\"number\">1.</span> - lam) * x2)</span><br><span class=\"line\">    y = Variable(lam * y1 + (<span class=\"number\">1.</span> - lam) * y2)</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss(net(x), y).backward()</span><br><span class=\"line\">    optimizer.step()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"What-is-mixup-doing\"><a href=\"#What-is-mixup-doing\" class=\"headerlink\" title=\"What is mixup doing?\"></a>What is mixup doing?</h3><p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor, since it is one of the simplest possible behaviors.</p>\n<p>mixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Zhang, Hongyi, et al. <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">“mixup: Beyond empirical risk minimization.”</a> International Conference on Learning Representations (2018).</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>众所周知，Deep Learning现如今的繁荣，和<strong>大数据</strong>、<strong>GPU</strong>和<strong>深度学习算法</strong>都是离不开关系的。Deep Learning模型参数众多，需要海量的数据进行拟合，否则很容易overfitting到training set上。而现实情况下，我们不一定能很容易地获取大量高质量标注样本，因此，Data Augmentation则起到了非常大的作用了。这便是本文所要讲述的主角。</p>\n<h2 id=\"Mixup\"><a href=\"#Mixup\" class=\"headerlink\" title=\"Mixup\"></a>Mixup</h2><blockquote>\n<p>Paper: <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">mixup: Beyond Empirical Risk Minimization</a></p>\n</blockquote>\n<p>Mixup的核心idea如下：<br>$$<br>\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j<br>$$</p>\n<p>$$<br>\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j<br>$$<br>其中，$x_i, x_j$为raw input vectors，$y_i, y_j$为one-hot encodings。</p>\n<blockquote>\n<p>Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.</p>\n</blockquote>\n<p>Mixup的PyTorch代码如下，是不是非常简洁？<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># y1, y2 should be one-hot vectors</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (x1, y1), (x2, y2) <span class=\"keyword\">in</span> zip(loader1, loader2):</span><br><span class=\"line\">    lam = numpy.random.beta(alpha, alpha)</span><br><span class=\"line\">    x = Variable(lam * x1 + (<span class=\"number\">1.</span> - lam) * x2)</span><br><span class=\"line\">    y = Variable(lam * y1 + (<span class=\"number\">1.</span> - lam) * y2)</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    loss(net(x), y).backward()</span><br><span class=\"line\">    optimizer.step()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"What-is-mixup-doing\"><a href=\"#What-is-mixup-doing\" class=\"headerlink\" title=\"What is mixup doing?\"></a>What is mixup doing?</h3><p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model $f$ to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor, since it is one of the simplest possible behaviors.</p>\n<p>mixup is a data augmentation method that consists of only two parts: random convex combination of raw inputs, and correspondingly, convex combination of one-hot label encodings.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li>Zhang, Hongyi, et al. <a href=\"https://openreview.net/pdf?id=r1Ddp1-Rb\" target=\"_blank\" rel=\"noopener\">“mixup: Beyond empirical risk minimization.”</a> International Conference on Learning Representations (2018).</li>\n</ol>\n"},{"title":"[DL] Optimization Algorithm in Deep Learning","catalog":false,"date":"2018-07-20T03:46:37.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization"],"_content":"## 介绍\n与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，然后SGD/Adam Optimizer一波带走，但笔者还是觉得有必要把这些优化算法来一个自己的整理与总结的。\n\n> 注：本文大多数内容来自花书《[Deep Learning](https://www.deeplearningbook.org/)》，详情请阅读原著！\n\n## Background\n* Gradient Descent旨在朝\"下坡\"移动，而非明确寻求临界点。而牛顿法的目标是寻求梯度为0的点。\n* Gradient Clipping基本思想来源于梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统Gradient Descent算法提议更新很大一步时，启发式Gradient Clipping会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。\n* 假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么$t$步之后，相当于乘以$W^t$，假设有特征值分解$W=V diag(\\lambda)V^{-1}$，在这种情况下，很容易看出：\n$$\nW^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}\n$$\n因此，当特征值$\\lambda_i$ 不在$1$附近时，若在量级上大于1则会出现Gradient Exploding；若小于$1$时，则会出现Gradient Vanishing。\n\n## Basic Algorithm\n### SGD\nSGD是如今深度学习领域应用非常广泛的一种优化算法，它按照数据生产分布抽取$m$ 个mini-batch (独立同分布)样本，通过计算这些mini-batch的梯度均值，我们可以得到梯度的无偏估计。\n\n![SGD](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg)\n\n### Momentum\n为了加速训练，Momentum积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。Momentum主要目的为了解决Hessian矩阵的病态条件和随机梯度的方差。\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\\n\\theta\\leftarrow \\theta + v\n$$\n\n![Momentum](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg)\nSGD中步长只是梯度范数乘以学习率，现在步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果Momentum总是观测到梯度 $g$,那么它会在方向$-g$ 上不停加速，直到达到最终速度，其中步长大小为：\n$$\n\\frac{\\epsilon||g||}{1-\\alpha}\n$$\n因此将Momentum超参数视为$\\frac{1}{1-\\alpha}$有助于理解。例如$\\alpha=0.9$ 对应着最大速度10倍于Gradient Descent。\n\n### Nesterov\n更新规则如下：\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\\n\\theta\\leftarrow \\theta+v\n$$\nNesterov和标准Momentum之间的区别在于梯度计算上，Nesterov中，梯度计算在施加Momentum之后。\n\n![Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg)\n\n### AdaGrad\nLearning rate是一个非常难以调整的超参数之一，如果我们相信方向敏感度在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动使用这些学习率是合理的。\n\nAdaGrad是自适应学习率算法的一种。它独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。\n\n![AdaGrad](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg)\n\n### RMSProp\nRMSProp修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均，AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。__RMSProp使用指数衰减平均以丢弃遥远过去的历史__，使其能够在找到凸碗结构后快速收敛。\n\n* Standard RMSProp\n![RMSProp](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg)\n\n* RMSProp with Nesterov\n![RMSProp with Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg)\n\n### Adam\n在Adam中，动量直接并入了梯度一阶矩(指数加权)的估计。将动量加入RMSProp最直接的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次，Adam包括偏置修正，修正从原点初始化的一阶矩(动量项)，和非中心的二阶矩的估计。\n\n![Adam](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg)\n\n## 二阶近似方法\n### 牛顿法\n牛顿法是基于二阶泰勒级数展开在某点$\\theta_0$附近来近似$J(\\theta)$的优化方法，其忽略了高阶导数：\n$$\nJ(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)\n$$\n\n更新规则：\n$$\n\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n因此，__对于局部的二次函数(具有正定的$H$)，用$H^{-1}$重新调整梯度，牛顿法会直接跳到极小值__。\n\n![Newton's Method](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg)\n\nDeep Learning中，Loss Function表明通常是非凸的(有很多特征)，如鞍点。因此使用Newton's Method是有问题的，若Hessian Matrix的特征值并不都是正的，Newton's Method实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian Matrix来避免。常用的正则化策略包括在Hessian Matrix对角线上增加常数$\\alpha$。正则化更新变为：\n$$\n\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n\n### 共轭梯度\n通过迭代下降的共轭方向以有效避免Hessian Matrix求逆计算的方法。\n\n### BFGS\nBFGS是使用矩阵$M_t$近似逆，迭代地低秩更新精度以更好地近似$H^{-1}$。当Hessian逆近似$M_t$更新时，下降方向$\\rho_t$为$\\rho_t=M_tg_t$。该方向上的线性搜索用于决定该方向上的步长$\\epsilon^{\\star}$。参数的最后更新为：\n$$\n\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t\n$$\n相比于共轭梯度，BFGS的优点在于其花费较少的时间改进每个线搜索。另一方面，BFGS算法必须存储必须存储Hessian 逆矩阵$M$，需要$O(n^2)$的存储空间，使BFGS不适用于大多数参数巨大的Deep Model。\n\n## 优化策略和元算法\n### Batch Normalization\n设$H$是需要标准化的某层mini batch激活函数，每个样本的激活出现在矩阵的每一行中。为了标准化$H$，我们将其替换为\n$$\nH^{'}=\\frac{H-\\mu}{\\sigma}\n$$\n\n$$\n\\mu=\\frac{1}{m}\\sum_i H_{i,:}\n$$\n\n$$\n\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}\n$$\n$\\delta$是个很小的正值，以避免遇到$\\sqrt{z}$的梯度在$z=0$处未定义的问题。\n\n至关重要的是，我们反向传播这些操作，来计算$\\mu$和$\\sigma$，并应用它们于标准化$H$。这意味着，梯度不会再简单地增加$h_i$的标准差或均值；BatchNorm会消除这一操作的影响，归零其在梯度中的元素。\n\n在测试阶段，$\\mu$和$\\sigma$可以被替换为训练阶段收集的运行均值。这使得模型可以对单一样本评估，而无需使用定义于整个mini-batch的$\\mu$和$\\sigma$。","source":"_posts/dl-optimization.md","raw":"---\ntitle: \"[DL] Optimization Algorithm in Deep Learning\"\ncatalog: false\ndate: 2018-07-20 11:46:37\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n---\n## 介绍\n与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，然后SGD/Adam Optimizer一波带走，但笔者还是觉得有必要把这些优化算法来一个自己的整理与总结的。\n\n> 注：本文大多数内容来自花书《[Deep Learning](https://www.deeplearningbook.org/)》，详情请阅读原著！\n\n## Background\n* Gradient Descent旨在朝\"下坡\"移动，而非明确寻求临界点。而牛顿法的目标是寻求梯度为0的点。\n* Gradient Clipping基本思想来源于梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统Gradient Descent算法提议更新很大一步时，启发式Gradient Clipping会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。\n* 假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么$t$步之后，相当于乘以$W^t$，假设有特征值分解$W=V diag(\\lambda)V^{-1}$，在这种情况下，很容易看出：\n$$\nW^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}\n$$\n因此，当特征值$\\lambda_i$ 不在$1$附近时，若在量级上大于1则会出现Gradient Exploding；若小于$1$时，则会出现Gradient Vanishing。\n\n## Basic Algorithm\n### SGD\nSGD是如今深度学习领域应用非常广泛的一种优化算法，它按照数据生产分布抽取$m$ 个mini-batch (独立同分布)样本，通过计算这些mini-batch的梯度均值，我们可以得到梯度的无偏估计。\n\n![SGD](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg)\n\n### Momentum\n为了加速训练，Momentum积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。Momentum主要目的为了解决Hessian矩阵的病态条件和随机梯度的方差。\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\\n\\theta\\leftarrow \\theta + v\n$$\n\n![Momentum](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg)\nSGD中步长只是梯度范数乘以学习率，现在步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果Momentum总是观测到梯度 $g$,那么它会在方向$-g$ 上不停加速，直到达到最终速度，其中步长大小为：\n$$\n\\frac{\\epsilon||g||}{1-\\alpha}\n$$\n因此将Momentum超参数视为$\\frac{1}{1-\\alpha}$有助于理解。例如$\\alpha=0.9$ 对应着最大速度10倍于Gradient Descent。\n\n### Nesterov\n更新规则如下：\n$$\nv\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\\n\\theta\\leftarrow \\theta+v\n$$\nNesterov和标准Momentum之间的区别在于梯度计算上，Nesterov中，梯度计算在施加Momentum之后。\n\n![Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg)\n\n### AdaGrad\nLearning rate是一个非常难以调整的超参数之一，如果我们相信方向敏感度在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动使用这些学习率是合理的。\n\nAdaGrad是自适应学习率算法的一种。它独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。\n\n![AdaGrad](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg)\n\n### RMSProp\nRMSProp修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均，AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。__RMSProp使用指数衰减平均以丢弃遥远过去的历史__，使其能够在找到凸碗结构后快速收敛。\n\n* Standard RMSProp\n![RMSProp](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg)\n\n* RMSProp with Nesterov\n![RMSProp with Nesterov](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg)\n\n### Adam\n在Adam中，动量直接并入了梯度一阶矩(指数加权)的估计。将动量加入RMSProp最直接的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次，Adam包括偏置修正，修正从原点初始化的一阶矩(动量项)，和非中心的二阶矩的估计。\n\n![Adam](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg)\n\n## 二阶近似方法\n### 牛顿法\n牛顿法是基于二阶泰勒级数展开在某点$\\theta_0$附近来近似$J(\\theta)$的优化方法，其忽略了高阶导数：\n$$\nJ(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)\n$$\n\n更新规则：\n$$\n\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n因此，__对于局部的二次函数(具有正定的$H$)，用$H^{-1}$重新调整梯度，牛顿法会直接跳到极小值__。\n\n![Newton's Method](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg)\n\nDeep Learning中，Loss Function表明通常是非凸的(有很多特征)，如鞍点。因此使用Newton's Method是有问题的，若Hessian Matrix的特征值并不都是正的，Newton's Method实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian Matrix来避免。常用的正则化策略包括在Hessian Matrix对角线上增加常数$\\alpha$。正则化更新变为：\n$$\n\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)\n$$\n\n### 共轭梯度\n通过迭代下降的共轭方向以有效避免Hessian Matrix求逆计算的方法。\n\n### BFGS\nBFGS是使用矩阵$M_t$近似逆，迭代地低秩更新精度以更好地近似$H^{-1}$。当Hessian逆近似$M_t$更新时，下降方向$\\rho_t$为$\\rho_t=M_tg_t$。该方向上的线性搜索用于决定该方向上的步长$\\epsilon^{\\star}$。参数的最后更新为：\n$$\n\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t\n$$\n相比于共轭梯度，BFGS的优点在于其花费较少的时间改进每个线搜索。另一方面，BFGS算法必须存储必须存储Hessian 逆矩阵$M$，需要$O(n^2)$的存储空间，使BFGS不适用于大多数参数巨大的Deep Model。\n\n## 优化策略和元算法\n### Batch Normalization\n设$H$是需要标准化的某层mini batch激活函数，每个样本的激活出现在矩阵的每一行中。为了标准化$H$，我们将其替换为\n$$\nH^{'}=\\frac{H-\\mu}{\\sigma}\n$$\n\n$$\n\\mu=\\frac{1}{m}\\sum_i H_{i,:}\n$$\n\n$$\n\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}\n$$\n$\\delta$是个很小的正值，以避免遇到$\\sqrt{z}$的梯度在$z=0$处未定义的问题。\n\n至关重要的是，我们反向传播这些操作，来计算$\\mu$和$\\sigma$，并应用它们于标准化$H$。这意味着，梯度不会再简单地增加$h_i$的标准差或均值；BatchNorm会消除这一操作的影响，归零其在梯度中的元素。\n\n在测试阶段，$\\mu$和$\\sigma$可以被替换为训练阶段收集的运行均值。这使得模型可以对单一样本评估，而无需使用定义于整个mini-batch的$\\mu$和$\\sigma$。","slug":"dl-optimization","published":1,"updated":"2018-10-01T04:40:08.837Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03c9000n608w4okd7r5h","content":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，然后SGD/Adam Optimizer一波带走，但笔者还是觉得有必要把这些优化算法来一个自己的整理与总结的。</p>\n<blockquote>\n<p>注：本文大多数内容来自花书《<a href=\"https://www.deeplearningbook.org/\" target=\"_blank\" rel=\"noopener\">Deep Learning</a>》，详情请阅读原著！</p>\n</blockquote>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>Gradient Descent旨在朝”下坡”移动，而非明确寻求临界点。而牛顿法的目标是寻求梯度为0的点。</li>\n<li>Gradient Clipping基本思想来源于梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统Gradient Descent算法提议更新很大一步时，启发式Gradient Clipping会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。</li>\n<li>假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么$t$步之后，相当于乘以$W^t$，假设有特征值分解$W=V diag(\\lambda)V^{-1}$，在这种情况下，很容易看出：<br>$$<br>W^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}<br>$$<br>因此，当特征值$\\lambda_i$ 不在$1$附近时，若在量级上大于1则会出现Gradient Exploding；若小于$1$时，则会出现Gradient Vanishing。</li>\n</ul>\n<h2 id=\"Basic-Algorithm\"><a href=\"#Basic-Algorithm\" class=\"headerlink\" title=\"Basic Algorithm\"></a>Basic Algorithm</h2><h3 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h3><p>SGD是如今深度学习领域应用非常广泛的一种优化算法，它按照数据生产分布抽取$m$ 个mini-batch (独立同分布)样本，通过计算这些mini-batch的梯度均值，我们可以得到梯度的无偏估计。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg\" alt=\"SGD\"></p>\n<h3 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h3><p>为了加速训练，Momentum积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。Momentum主要目的为了解决Hessian矩阵的病态条件和随机梯度的方差。<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta + v<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg\" alt=\"Momentum\"><br>SGD中步长只是梯度范数乘以学习率，现在步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果Momentum总是观测到梯度 $g$,那么它会在方向$-g$ 上不停加速，直到达到最终速度，其中步长大小为：<br>$$<br>\\frac{\\epsilon||g||}{1-\\alpha}<br>$$<br>因此将Momentum超参数视为$\\frac{1}{1-\\alpha}$有助于理解。例如$\\alpha=0.9$ 对应着最大速度10倍于Gradient Descent。</p>\n<h3 id=\"Nesterov\"><a href=\"#Nesterov\" class=\"headerlink\" title=\"Nesterov\"></a>Nesterov</h3><p>更新规则如下：<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta+v<br>$$<br>Nesterov和标准Momentum之间的区别在于梯度计算上，Nesterov中，梯度计算在施加Momentum之后。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg\" alt=\"Nesterov\"></p>\n<h3 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h3><p>Learning rate是一个非常难以调整的超参数之一，如果我们相信方向敏感度在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动使用这些学习率是合理的。</p>\n<p>AdaGrad是自适应学习率算法的一种。它独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg\" alt=\"AdaGrad\"></p>\n<h3 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h3><p>RMSProp修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均，AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。<strong>RMSProp使用指数衰减平均以丢弃遥远过去的历史</strong>，使其能够在找到凸碗结构后快速收敛。</p>\n<ul>\n<li><p>Standard RMSProp<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg\" alt=\"RMSProp\"></p>\n</li>\n<li><p>RMSProp with Nesterov<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg\" alt=\"RMSProp with Nesterov\"></p>\n</li>\n</ul>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>在Adam中，动量直接并入了梯度一阶矩(指数加权)的估计。将动量加入RMSProp最直接的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次，Adam包括偏置修正，修正从原点初始化的一阶矩(动量项)，和非中心的二阶矩的估计。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg\" alt=\"Adam\"></p>\n<h2 id=\"二阶近似方法\"><a href=\"#二阶近似方法\" class=\"headerlink\" title=\"二阶近似方法\"></a>二阶近似方法</h2><h3 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h3><p>牛顿法是基于二阶泰勒级数展开在某点$\\theta_0$附近来近似$J(\\theta)$的优化方法，其忽略了高阶导数：<br>$$<br>J(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)<br>$$</p>\n<p>更新规则：<br>$$<br>\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$<br>因此，<strong>对于局部的二次函数(具有正定的$H$)，用$H^{-1}$重新调整梯度，牛顿法会直接跳到极小值</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg\" alt=\"Newton&#39;s Method\"></p>\n<p>Deep Learning中，Loss Function表明通常是非凸的(有很多特征)，如鞍点。因此使用Newton’s Method是有问题的，若Hessian Matrix的特征值并不都是正的，Newton’s Method实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian Matrix来避免。常用的正则化策略包括在Hessian Matrix对角线上增加常数$\\alpha$。正则化更新变为：<br>$$<br>\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$</p>\n<h3 id=\"共轭梯度\"><a href=\"#共轭梯度\" class=\"headerlink\" title=\"共轭梯度\"></a>共轭梯度</h3><p>通过迭代下降的共轭方向以有效避免Hessian Matrix求逆计算的方法。</p>\n<h3 id=\"BFGS\"><a href=\"#BFGS\" class=\"headerlink\" title=\"BFGS\"></a>BFGS</h3><p>BFGS是使用矩阵$M_t$近似逆，迭代地低秩更新精度以更好地近似$H^{-1}$。当Hessian逆近似$M_t$更新时，下降方向$\\rho_t$为$\\rho_t=M_tg_t$。该方向上的线性搜索用于决定该方向上的步长$\\epsilon^{\\star}$。参数的最后更新为：<br>$$<br>\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t<br>$$<br>相比于共轭梯度，BFGS的优点在于其花费较少的时间改进每个线搜索。另一方面，BFGS算法必须存储必须存储Hessian 逆矩阵$M$，需要$O(n^2)$的存储空间，使BFGS不适用于大多数参数巨大的Deep Model。</p>\n<h2 id=\"优化策略和元算法\"><a href=\"#优化策略和元算法\" class=\"headerlink\" title=\"优化策略和元算法\"></a>优化策略和元算法</h2><h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><p>设$H$是需要标准化的某层mini batch激活函数，每个样本的激活出现在矩阵的每一行中。为了标准化$H$，我们将其替换为<br>$$<br>H^{‘}=\\frac{H-\\mu}{\\sigma}<br>$$</p>\n<p>$$<br>\\mu=\\frac{1}{m}\\sum_i H_{i,:}<br>$$</p>\n<p>$$<br>\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}<br>$$<br>$\\delta$是个很小的正值，以避免遇到$\\sqrt{z}$的梯度在$z=0$处未定义的问题。</p>\n<p>至关重要的是，我们反向传播这些操作，来计算$\\mu$和$\\sigma$，并应用它们于标准化$H$。这意味着，梯度不会再简单地增加$h_i$的标准差或均值；BatchNorm会消除这一操作的影响，归零其在梯度中的元素。</p>\n<p>在测试阶段，$\\mu$和$\\sigma$可以被替换为训练阶段收集的运行均值。这使得模型可以对单一样本评估，而无需使用定义于整个mini-batch的$\\mu$和$\\sigma$。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>与很多传统机器学习算法相比，由于深度神经网络的本身特性(例如非凸、高度非线性等)，使得整个目标函数的优化极为困难。因此优化算法是深度学习领域一个非常非常重要的组成部分。私以为，Deep Learning主要有3大组件：1) 网络结构，2) Loss Function, 3) 优化算法。虽然目前Paper中大多数都是设计Networks Architecture + Loss Function，然后SGD/Adam Optimizer一波带走，但笔者还是觉得有必要把这些优化算法来一个自己的整理与总结的。</p>\n<blockquote>\n<p>注：本文大多数内容来自花书《<a href=\"https://www.deeplearningbook.org/\" target=\"_blank\" rel=\"noopener\">Deep Learning</a>》，详情请阅读原著！</p>\n</blockquote>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>Gradient Descent旨在朝”下坡”移动，而非明确寻求临界点。而牛顿法的目标是寻求梯度为0的点。</li>\n<li>Gradient Clipping基本思想来源于梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当传统Gradient Descent算法提议更新很大一步时，启发式Gradient Clipping会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。</li>\n<li>假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么$t$步之后，相当于乘以$W^t$，假设有特征值分解$W=V diag(\\lambda)V^{-1}$，在这种情况下，很容易看出：<br>$$<br>W^t=(V diag(\\lambda)V^{-1})^{t}=V diag(\\lambda)^tV^{-1}<br>$$<br>因此，当特征值$\\lambda_i$ 不在$1$附近时，若在量级上大于1则会出现Gradient Exploding；若小于$1$时，则会出现Gradient Vanishing。</li>\n</ul>\n<h2 id=\"Basic-Algorithm\"><a href=\"#Basic-Algorithm\" class=\"headerlink\" title=\"Basic Algorithm\"></a>Basic Algorithm</h2><h3 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h3><p>SGD是如今深度学习领域应用非常广泛的一种优化算法，它按照数据生产分布抽取$m$ 个mini-batch (独立同分布)样本，通过计算这些mini-batch的梯度均值，我们可以得到梯度的无偏估计。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/sgd.jpg\" alt=\"SGD\"></p>\n<h3 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h3><p>为了加速训练，Momentum积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。Momentum主要目的为了解决Hessian矩阵的病态条件和随机梯度的方差。<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta + v<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/momentum.jpg\" alt=\"Momentum\"><br>SGD中步长只是梯度范数乘以学习率，现在步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果Momentum总是观测到梯度 $g$,那么它会在方向$-g$ 上不停加速，直到达到最终速度，其中步长大小为：<br>$$<br>\\frac{\\epsilon||g||}{1-\\alpha}<br>$$<br>因此将Momentum超参数视为$\\frac{1}{1-\\alpha}$有助于理解。例如$\\alpha=0.9$ 对应着最大速度10倍于Gradient Descent。</p>\n<h3 id=\"Nesterov\"><a href=\"#Nesterov\" class=\"headerlink\" title=\"Nesterov\"></a>Nesterov</h3><p>更新规则如下：<br>$$<br>v\\leftarrow \\alpha v-\\epsilon \\bigtriangledown_{\\theta}(\\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta+\\alpha v),y^{(i)})\\\\<br>\\theta\\leftarrow \\theta+v<br>$$<br>Nesterov和标准Momentum之间的区别在于梯度计算上，Nesterov中，梯度计算在施加Momentum之后。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/nesterov.jpg\" alt=\"Nesterov\"></p>\n<h3 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h3><p>Learning rate是一个非常难以调整的超参数之一，如果我们相信方向敏感度在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动使用这些学习率是合理的。</p>\n<p>AdaGrad是自适应学习率算法的一种。它独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adagrad.jpg\" alt=\"AdaGrad\"></p>\n<h3 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h3><p>RMSProp修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均，AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。<strong>RMSProp使用指数衰减平均以丢弃遥远过去的历史</strong>，使其能够在找到凸碗结构后快速收敛。</p>\n<ul>\n<li><p>Standard RMSProp<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop.jpg\" alt=\"RMSProp\"></p>\n</li>\n<li><p>RMSProp with Nesterov<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/rmsprop_with_nesterov.jpg\" alt=\"RMSProp with Nesterov\"></p>\n</li>\n</ul>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>在Adam中，动量直接并入了梯度一阶矩(指数加权)的估计。将动量加入RMSProp最直接的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次，Adam包括偏置修正，修正从原点初始化的一阶矩(动量项)，和非中心的二阶矩的估计。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/adam.jpg\" alt=\"Adam\"></p>\n<h2 id=\"二阶近似方法\"><a href=\"#二阶近似方法\" class=\"headerlink\" title=\"二阶近似方法\"></a>二阶近似方法</h2><h3 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h3><p>牛顿法是基于二阶泰勒级数展开在某点$\\theta_0$附近来近似$J(\\theta)$的优化方法，其忽略了高阶导数：<br>$$<br>J(\\theta)\\approx J(\\theta_0)+(\\theta-\\theta_0)^T\\bigtriangledown_{\\theta} J(\\theta_0) + \\frac{1}{2}(\\theta-\\theta_0)^T H(\\theta-\\theta_0)<br>$$</p>\n<p>更新规则：<br>$$<br>\\theta^{\\star}=\\theta_0-H^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$<br>因此，<strong>对于局部的二次函数(具有正定的$H$)，用$H^{-1}$重新调整梯度，牛顿法会直接跳到极小值</strong>。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-optimization/newtons_method.jpg\" alt=\"Newton&#39;s Method\"></p>\n<p>Deep Learning中，Loss Function表明通常是非凸的(有很多特征)，如鞍点。因此使用Newton’s Method是有问题的，若Hessian Matrix的特征值并不都是正的，Newton’s Method实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian Matrix来避免。常用的正则化策略包括在Hessian Matrix对角线上增加常数$\\alpha$。正则化更新变为：<br>$$<br>\\theta^{\\star}=\\theta_0-[H(f(\\theta_0))+\\alpha I]^{-1}\\bigtriangledown _{\\theta}J(\\theta_0)<br>$$</p>\n<h3 id=\"共轭梯度\"><a href=\"#共轭梯度\" class=\"headerlink\" title=\"共轭梯度\"></a>共轭梯度</h3><p>通过迭代下降的共轭方向以有效避免Hessian Matrix求逆计算的方法。</p>\n<h3 id=\"BFGS\"><a href=\"#BFGS\" class=\"headerlink\" title=\"BFGS\"></a>BFGS</h3><p>BFGS是使用矩阵$M_t$近似逆，迭代地低秩更新精度以更好地近似$H^{-1}$。当Hessian逆近似$M_t$更新时，下降方向$\\rho_t$为$\\rho_t=M_tg_t$。该方向上的线性搜索用于决定该方向上的步长$\\epsilon^{\\star}$。参数的最后更新为：<br>$$<br>\\theta_{t+1}=\\theta_t + \\epsilon^{\\star}\\rho_t<br>$$<br>相比于共轭梯度，BFGS的优点在于其花费较少的时间改进每个线搜索。另一方面，BFGS算法必须存储必须存储Hessian 逆矩阵$M$，需要$O(n^2)$的存储空间，使BFGS不适用于大多数参数巨大的Deep Model。</p>\n<h2 id=\"优化策略和元算法\"><a href=\"#优化策略和元算法\" class=\"headerlink\" title=\"优化策略和元算法\"></a>优化策略和元算法</h2><h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><p>设$H$是需要标准化的某层mini batch激活函数，每个样本的激活出现在矩阵的每一行中。为了标准化$H$，我们将其替换为<br>$$<br>H^{‘}=\\frac{H-\\mu}{\\sigma}<br>$$</p>\n<p>$$<br>\\mu=\\frac{1}{m}\\sum_i H_{i,:}<br>$$</p>\n<p>$$<br>\\sigma = \\sqrt{\\delta+\\frac{1}{m}\\sum_i (H-\\mu)_i^2}<br>$$<br>$\\delta$是个很小的正值，以避免遇到$\\sqrt{z}$的梯度在$z=0$处未定义的问题。</p>\n<p>至关重要的是，我们反向传播这些操作，来计算$\\mu$和$\\sigma$，并应用它们于标准化$H$。这意味着，梯度不会再简单地增加$h_i$的标准差或均值；BatchNorm会消除这一操作的影响，归零其在梯度中的元素。</p>\n<p>在测试阶段，$\\mu$和$\\sigma$可以被替换为训练阶段收集的运行均值。这使得模型可以对单一样本评估，而无需使用定义于整个mini-batch的$\\mu$和$\\sigma$。</p>\n"},{"title":"[DL] Regularization","date":"2018-08-06T09:13:07.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","Optimization","Regularization"],"_content":"## Introduction\nRegularization是Machine Learning中一个非常重要的概念，是对抗Overfitting最有用的利器。DNN由于参数众多，很容易overfitting，若直接选用small model，则会导致model特征学习、分类能力不足。因此现实中往往是 __使用较大的模型 + 正则化__ 来解决相应问题。因此本文简要介绍一下Deep Learning中常用的正则化方法。\n\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)\n$$\n\n## 参数范数惩罚\n我们通常只对weight做惩罚，而不对bias做正则惩罚。精确拟合bias所需的数据通常比拟合weight少得多，每个weight会指定两个变量如何相互作用。而每个bias仅控制一个单变量。这意味着我们不对其进行正则化也不会导致太大的方差。\n\n在DNN中，有时会希望对Network的每一层使用单独的惩罚，并分配不同的$\\alpha$系数，而寻找合适的多个超参代价很大。因此为了减少搜索空间，我们会在所有层使用相同的weight decay。\n\n### $L_2$ Regularization\n为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：\n$$\n\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)\n$$\n与之对应的梯度为：\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)\n$$\n使用SGD更新权重：\n$$\nw\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))\n$$\n换种写法就是：\n$$\nw\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)\n$$\n可以看到，加入weight decay后会引起学习规则的修改，即在每一步执行通常的SGD之前会 __先收缩权重向量(将权重向量乘以一个常数因子)__。\n\n以Linear Regression为例，其Cost Function是MSE:\n$$\n(Xw-y)^T(Xw-y)\n$$\n我们添加$L_2$ Regularization之后，Cost Function变为:\n$$\n(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw\n$$\n这将正规方程的解由 $w=(X^TX)^{-1}X^Ty$ 变为 $w=(X^TX+\\alpha I)^{-1}X^Ty$。其中，矩阵$X^TX$与协方差矩阵$\\frac{1}{m}X^TX$成正比。$L_2$ Regularization将这个矩阵替换为上式中的$(X^TX+\\alpha I)^{-1}$，这个新矩阵与原来的是一样的，不同的仅仅是在对角线加了$\\alpha$。这个矩阵的对角项对应每个输入特征的方差。我们可以看到，$L_2$ Regularization能让学习算法“感知到”具有较高方差的输入$x$，因此与输出目标的协方差较小(相对增加方差)的特征的权重将会收缩。\n\n### $L_1$ Regularization\n为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：\n$$\n\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)\n$$\n对应的梯度:\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)\n$$\n可与看到，__此时正则化对梯度的影响不再是线性地缩放每个$w_i$，而是添加了一项与$sign(w_i)$同号的常数，使用这种形式的梯度之后，我们不一定能得到$J(X,y;w)$二次近似的直接算数解($L_2$正则化时可以)__。\n\n我们可以将$L_1$ Regularization Cost Function的二次近似分解成关于参数的求和：\n$$\n\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]\n$$\n如下形式的解析解(对每一维$i$)可以最小化这个近似Cost Function：\n$$\nw_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}\n$$\n此时： \n1) 若$w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$，正则化后目标中的$w_i$最优值是0。这是因为在方向$i$上$J(w;X,y)$对$\\hat{J}(w;X,y)$的贡献被抵消，$L_1$ Regularization将$w_i$推至0。\n2) 若$w_i^{\\star}> \\frac{\\alpha}{H_{i,i}}$，正则化不会将$w_i$的最优值推至0，而仅仅在那个方向上移动$\\frac{\\alpha}{H_{i,i}}$的距离。\n\n相比$L_2$ Regularization，$L_1$ Regularization会产生更稀疏的解(最优值中的一些参数为0)。\n\n## 作为约束的范数惩罚\n$$\n\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)\n$$\n如果$\\Omega$是$L_2$范数，那么权重就是被约束在一个$L_2$球中；如果$\\Omega$是$L_1$范数，那么权重就是被约束在一个$L_1$范数限制的区域中。\n\n## Data Augmentation\n在NN的输入层注入噪声也可以看作Data Augmentation的一种方式。然而，NN对噪声不是很robust。改善NN robustness的方法之一是简单地将随机噪声添加到输入再训练。输入噪声注入是一些Unsupervised Learning Algorithm的一部分（例如Denoise Auto Encoder）。向hidden layer施加噪声也是可行的，这可以被看作在多个抽象层上进行的Data Augmentation。\n\n## Robustness of Noise\n对某些模型而言，__向输入添加方差极小的噪声等价于对权重施加范数惩罚__。一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到hidden units时会更加强大。\n\n## Multi-Task Learning\nMTL是通过合并几个任务中的样例(__可以视为对参数施加的软约束__)来提高泛化的一种方式。__当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值，通常会带来更好的泛化能力__。\n\n## Early Stopping\n在训练中只返回使validation set error最低的参数设置，就可以获得使validation set更低的模型(并且因此有希望获得更好的test set error)。在每次validation set有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当validation set error在事先指定的循环次数内没有进一步改善时，算法就会终止。这种策略称为Early Stopping。\n\n对于weight decay，必须小心不能使用太多的weight decay，__以防止网络陷入不良局部极小点__。\n\nEarly Stopping需要validation set，这意味着某些training samples不能被输入到模型。为了更好地利用这一额外数据，我们可以在完成Early Stopping的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的training data都会被包括在内。\n* 一种策略是再次初始化模型，然后使用所有数据再次训练。在第二轮训练过程中，我们使用第一轮Early Stopping确定的 __最佳Epoch__。\n* 另一种策略是保持从第一轮训练获得的参数，__然后使用全部数据继续训练__。在这个阶段，已经没有validation set指导我们需要训练多少步停止。我们可以监控validation set的平均loss，并继续训练，直到它低于Early Stopping终止时的目标值。\n\n![Early Stopping](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg)\n\n### 为什么Early Stopping具有Regularization效果？\nBishop __认为Early Stopping可以将优化过程的参数空间限制在初始参数值$\\theta_0$的小领域内__。事实上，在二次误差的简单Linear Model和Gradient Descend情况下，我们可以展示Early Stopping相当于$L_2$ Regularization。\n\n![Early Stopping As Regularization](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg)\n\n## 参数绑定和参数共享\n假设有两个model执行两个分类任务，但输入分布稍有不同。这两个模型将输入映射到两个不同但相关的输出：$\\hat{y}^{(A)}=f(w^{(A)},x)$和$\\hat{y}^{(B)}=f(w^{(B)},x)$。\n\n我们可以想象，这些任务会足够相似，因此我们认为模型参数应彼此接近：$\\forall_i,w_i^{(A)}$应该与$w_i^{(B)}$接近，我们可通过正则化利用此信息，即：$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$。这里使用$L_2$ Regularization，也可以使用其他Regularization。\n\n正则化一个模型(监督模式下训练的分类器)的参数，使其接近另一个无监督模式下训练的模型参数，构造的这种架构使得分类模型中的许多参数能与无监督模型中对应的参数匹配。\n\nCNN通过在多个位置共享参数来考虑 __平移不变性__。相同的特征(具有相同权重的hidden units)在输入的不同位置上计算获得，这意味着无论人脸在图像中的第$i$列或是$i+1$列，我们都可以使用相同的feature detector找到人脸。\n\n## Sparse Representation\nWeight decay是直接惩罚模型参数，另一种策略是惩罚NN中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。\n\n表示的稀疏惩罚正则化是通过向Loss Function $J$ 添加对表示的范数惩罚来实现的，记作$\\Omega(h)$：\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)\n$$\n对表示元素的$L_1$惩罚诱导稀疏的表示：$\\Omega(h)=||h||_1=\\sum_i|h_i|$。\n\n还有一些其他方法通过激活值的硬性约束来获得表示稀疏，例如正交匹配跟踪通过解决以下约束优化问题将输入值$x$编码成表示$h$:\n$$\n\\mathop{argmin} \\limits_{h,||h||_0< k} ||x-Wh||^2\n$$\n其中$||h||_0$是$h$中非零项的个数。当$W$被约束为正交时，我们可以高效地解决这个问题。\n\n## Bagging和其他集成方法\n### Why Model Averaging Works?\n假设我们有$k$个regression model，每个model在每个例子上的误差是$\\epsilon_i$，这个误差服从零均值方差为$\\mathbb{E}[\\epsilon_i^2]=v$且协方差为$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$的多维正态分布。通过所有集成模型的平均预测所得误差是$\\frac{1}{k}\\sum_i\\epsilon_i$。集成模型的MSE期望是：\n$$\n\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c\n$$\n在误差完全相关即$c=v$的情况下，MSE减少到$v$，所以模型平均没有任何帮助。在错误完全不相关即$c=0$的情况下，该集成模型MSE仅为$\\frac{v}{k}$。这意味着集成MSE的期望会随着集成规模增大而线性减小。换言之，ensemble model至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，ensemble将显著地比其他成员表现得更好。\n\nNN能找到足够多的不同解，意味着它们可以从Model Averaging中受益(即使所有模型都在同一个数据集上训练))。NN中随机初始化的差异、不同输出的非确定性往往足以使得ensemble中的不同成员具有部分独立的误差。\n\n## Dropout\nDropout可以被认为是集成大量DNN的实用Bagging。Dropout训练的ensemble包括所有从base NN除去非输出单元后形成的子网络。\n\nDropout训练与Bagging训练不太一样，Bagging中所有模型都是独立的，在Dropout中所有模型共享参数。其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。\n\n若使用0.5的keep_prob，权重比例规则一般相当于在训练结束后将权重除以2，然后像平常一样使用模型。实现相同结果的另一种方法是在训练期间将单元的状态乘以2。\n\nDropout是一个Regularization技术，它减少了模型的有效容量，为了抵消这种影响，我们必须增大模型规模。当Dropout用于Linear Regression时，相当于每个输入特征具有不同weight decay系数的$L_2$ weight decay。每个特征的weight decay系数的大小是由其方差来确定的。其他Linear Model也有类似的结果。对于Deep Model而言，Dropout与weight decay是不等同的。\n\n__DropConnect__ 是Dropout的一个特殊情况，其中一个标量权重和单个hidden unit状态之间的每个乘积被认为是可以丢弃的一个单元。\n\n__Batch Normalization__ 在训练时向hidden unit引入加性和乘性噪声重新参数化模型。BatchNorm主要目的是改善优化，但噪声具有正则化效果，有时没必要再使用Dropout。\n\n## Adverserial Training\nDNN对对抗样本非常不robust的主要原因之一是 __过度线性__。DNN主要是基于线性块构建的，因此在一些实验中，它们实现的整体函数被证明是高度线性的。这些线性函数很容易优化，不幸的是，如果一个线性函数具有许多输入，那么它的值可以非常迅速地改变。如果我们用$\\epsilon$改变每个输入，那么权重为$w$的线性函数可以改变$\\epsilon||w||_1$之多，如果$w$是高维的这会是一个非常大的数。Adverserial training通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以看作一种明确地向监督NN引入局部恒定先验的方法。\n\n对抗样本也提供了一种实现semi-supervised learning的方法，在与数据集中的label不相关联的点$x$处，模型本身为其分配一些label $\\hat{y}$。模型的label $\\hat{y}$ 未必是真正的label，但如果模型是高品质的，那么$\\hat{y}$提供正确标签的可能性很大。我们可以搜索一个对抗样本$x^{'}$，导致分类器输出一个标签$y^{'}$且$y^{'}\\neq y$。不使用真正的label，而是由训练好的model提供label产生的adverserial samples被称为“虚拟对抗样本”。我们可以训练分类器为$x$和$x^{'}$分配相同的标签。__这鼓励classifier学习一个沿着未标注数据所在流形上任意微小变化都很robust的函数__。驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。","source":"_posts/dl-regularization.md","raw":"---\ntitle: \"[DL] Regularization\"\ndate: 2018-08-06 17:13:07\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- Optimization\n- Regularization\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- Optimization\n- Regularization\n---\n## Introduction\nRegularization是Machine Learning中一个非常重要的概念，是对抗Overfitting最有用的利器。DNN由于参数众多，很容易overfitting，若直接选用small model，则会导致model特征学习、分类能力不足。因此现实中往往是 __使用较大的模型 + 正则化__ 来解决相应问题。因此本文简要介绍一下Deep Learning中常用的正则化方法。\n\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)\n$$\n\n## 参数范数惩罚\n我们通常只对weight做惩罚，而不对bias做正则惩罚。精确拟合bias所需的数据通常比拟合weight少得多，每个weight会指定两个变量如何相互作用。而每个bias仅控制一个单变量。这意味着我们不对其进行正则化也不会导致太大的方差。\n\n在DNN中，有时会希望对Network的每一层使用单独的惩罚，并分配不同的$\\alpha$系数，而寻找合适的多个超参代价很大。因此为了减少搜索空间，我们会在所有层使用相同的weight decay。\n\n### $L_2$ Regularization\n为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：\n$$\n\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)\n$$\n与之对应的梯度为：\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)\n$$\n使用SGD更新权重：\n$$\nw\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))\n$$\n换种写法就是：\n$$\nw\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)\n$$\n可以看到，加入weight decay后会引起学习规则的修改，即在每一步执行通常的SGD之前会 __先收缩权重向量(将权重向量乘以一个常数因子)__。\n\n以Linear Regression为例，其Cost Function是MSE:\n$$\n(Xw-y)^T(Xw-y)\n$$\n我们添加$L_2$ Regularization之后，Cost Function变为:\n$$\n(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw\n$$\n这将正规方程的解由 $w=(X^TX)^{-1}X^Ty$ 变为 $w=(X^TX+\\alpha I)^{-1}X^Ty$。其中，矩阵$X^TX$与协方差矩阵$\\frac{1}{m}X^TX$成正比。$L_2$ Regularization将这个矩阵替换为上式中的$(X^TX+\\alpha I)^{-1}$，这个新矩阵与原来的是一样的，不同的仅仅是在对角线加了$\\alpha$。这个矩阵的对角项对应每个输入特征的方差。我们可以看到，$L_2$ Regularization能让学习算法“感知到”具有较高方差的输入$x$，因此与输出目标的协方差较小(相对增加方差)的特征的权重将会收缩。\n\n### $L_1$ Regularization\n为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：\n$$\n\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)\n$$\n对应的梯度:\n$$\n\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)\n$$\n可与看到，__此时正则化对梯度的影响不再是线性地缩放每个$w_i$，而是添加了一项与$sign(w_i)$同号的常数，使用这种形式的梯度之后，我们不一定能得到$J(X,y;w)$二次近似的直接算数解($L_2$正则化时可以)__。\n\n我们可以将$L_1$ Regularization Cost Function的二次近似分解成关于参数的求和：\n$$\n\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]\n$$\n如下形式的解析解(对每一维$i$)可以最小化这个近似Cost Function：\n$$\nw_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}\n$$\n此时： \n1) 若$w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$，正则化后目标中的$w_i$最优值是0。这是因为在方向$i$上$J(w;X,y)$对$\\hat{J}(w;X,y)$的贡献被抵消，$L_1$ Regularization将$w_i$推至0。\n2) 若$w_i^{\\star}> \\frac{\\alpha}{H_{i,i}}$，正则化不会将$w_i$的最优值推至0，而仅仅在那个方向上移动$\\frac{\\alpha}{H_{i,i}}$的距离。\n\n相比$L_2$ Regularization，$L_1$ Regularization会产生更稀疏的解(最优值中的一些参数为0)。\n\n## 作为约束的范数惩罚\n$$\n\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)\n$$\n如果$\\Omega$是$L_2$范数，那么权重就是被约束在一个$L_2$球中；如果$\\Omega$是$L_1$范数，那么权重就是被约束在一个$L_1$范数限制的区域中。\n\n## Data Augmentation\n在NN的输入层注入噪声也可以看作Data Augmentation的一种方式。然而，NN对噪声不是很robust。改善NN robustness的方法之一是简单地将随机噪声添加到输入再训练。输入噪声注入是一些Unsupervised Learning Algorithm的一部分（例如Denoise Auto Encoder）。向hidden layer施加噪声也是可行的，这可以被看作在多个抽象层上进行的Data Augmentation。\n\n## Robustness of Noise\n对某些模型而言，__向输入添加方差极小的噪声等价于对权重施加范数惩罚__。一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到hidden units时会更加强大。\n\n## Multi-Task Learning\nMTL是通过合并几个任务中的样例(__可以视为对参数施加的软约束__)来提高泛化的一种方式。__当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值，通常会带来更好的泛化能力__。\n\n## Early Stopping\n在训练中只返回使validation set error最低的参数设置，就可以获得使validation set更低的模型(并且因此有希望获得更好的test set error)。在每次validation set有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当validation set error在事先指定的循环次数内没有进一步改善时，算法就会终止。这种策略称为Early Stopping。\n\n对于weight decay，必须小心不能使用太多的weight decay，__以防止网络陷入不良局部极小点__。\n\nEarly Stopping需要validation set，这意味着某些training samples不能被输入到模型。为了更好地利用这一额外数据，我们可以在完成Early Stopping的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的training data都会被包括在内。\n* 一种策略是再次初始化模型，然后使用所有数据再次训练。在第二轮训练过程中，我们使用第一轮Early Stopping确定的 __最佳Epoch__。\n* 另一种策略是保持从第一轮训练获得的参数，__然后使用全部数据继续训练__。在这个阶段，已经没有validation set指导我们需要训练多少步停止。我们可以监控validation set的平均loss，并继续训练，直到它低于Early Stopping终止时的目标值。\n\n![Early Stopping](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg)\n\n### 为什么Early Stopping具有Regularization效果？\nBishop __认为Early Stopping可以将优化过程的参数空间限制在初始参数值$\\theta_0$的小领域内__。事实上，在二次误差的简单Linear Model和Gradient Descend情况下，我们可以展示Early Stopping相当于$L_2$ Regularization。\n\n![Early Stopping As Regularization](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg)\n\n## 参数绑定和参数共享\n假设有两个model执行两个分类任务，但输入分布稍有不同。这两个模型将输入映射到两个不同但相关的输出：$\\hat{y}^{(A)}=f(w^{(A)},x)$和$\\hat{y}^{(B)}=f(w^{(B)},x)$。\n\n我们可以想象，这些任务会足够相似，因此我们认为模型参数应彼此接近：$\\forall_i,w_i^{(A)}$应该与$w_i^{(B)}$接近，我们可通过正则化利用此信息，即：$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$。这里使用$L_2$ Regularization，也可以使用其他Regularization。\n\n正则化一个模型(监督模式下训练的分类器)的参数，使其接近另一个无监督模式下训练的模型参数，构造的这种架构使得分类模型中的许多参数能与无监督模型中对应的参数匹配。\n\nCNN通过在多个位置共享参数来考虑 __平移不变性__。相同的特征(具有相同权重的hidden units)在输入的不同位置上计算获得，这意味着无论人脸在图像中的第$i$列或是$i+1$列，我们都可以使用相同的feature detector找到人脸。\n\n## Sparse Representation\nWeight decay是直接惩罚模型参数，另一种策略是惩罚NN中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。\n\n表示的稀疏惩罚正则化是通过向Loss Function $J$ 添加对表示的范数惩罚来实现的，记作$\\Omega(h)$：\n$$\n\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)\n$$\n对表示元素的$L_1$惩罚诱导稀疏的表示：$\\Omega(h)=||h||_1=\\sum_i|h_i|$。\n\n还有一些其他方法通过激活值的硬性约束来获得表示稀疏，例如正交匹配跟踪通过解决以下约束优化问题将输入值$x$编码成表示$h$:\n$$\n\\mathop{argmin} \\limits_{h,||h||_0< k} ||x-Wh||^2\n$$\n其中$||h||_0$是$h$中非零项的个数。当$W$被约束为正交时，我们可以高效地解决这个问题。\n\n## Bagging和其他集成方法\n### Why Model Averaging Works?\n假设我们有$k$个regression model，每个model在每个例子上的误差是$\\epsilon_i$，这个误差服从零均值方差为$\\mathbb{E}[\\epsilon_i^2]=v$且协方差为$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$的多维正态分布。通过所有集成模型的平均预测所得误差是$\\frac{1}{k}\\sum_i\\epsilon_i$。集成模型的MSE期望是：\n$$\n\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c\n$$\n在误差完全相关即$c=v$的情况下，MSE减少到$v$，所以模型平均没有任何帮助。在错误完全不相关即$c=0$的情况下，该集成模型MSE仅为$\\frac{v}{k}$。这意味着集成MSE的期望会随着集成规模增大而线性减小。换言之，ensemble model至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，ensemble将显著地比其他成员表现得更好。\n\nNN能找到足够多的不同解，意味着它们可以从Model Averaging中受益(即使所有模型都在同一个数据集上训练))。NN中随机初始化的差异、不同输出的非确定性往往足以使得ensemble中的不同成员具有部分独立的误差。\n\n## Dropout\nDropout可以被认为是集成大量DNN的实用Bagging。Dropout训练的ensemble包括所有从base NN除去非输出单元后形成的子网络。\n\nDropout训练与Bagging训练不太一样，Bagging中所有模型都是独立的，在Dropout中所有模型共享参数。其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。\n\n若使用0.5的keep_prob，权重比例规则一般相当于在训练结束后将权重除以2，然后像平常一样使用模型。实现相同结果的另一种方法是在训练期间将单元的状态乘以2。\n\nDropout是一个Regularization技术，它减少了模型的有效容量，为了抵消这种影响，我们必须增大模型规模。当Dropout用于Linear Regression时，相当于每个输入特征具有不同weight decay系数的$L_2$ weight decay。每个特征的weight decay系数的大小是由其方差来确定的。其他Linear Model也有类似的结果。对于Deep Model而言，Dropout与weight decay是不等同的。\n\n__DropConnect__ 是Dropout的一个特殊情况，其中一个标量权重和单个hidden unit状态之间的每个乘积被认为是可以丢弃的一个单元。\n\n__Batch Normalization__ 在训练时向hidden unit引入加性和乘性噪声重新参数化模型。BatchNorm主要目的是改善优化，但噪声具有正则化效果，有时没必要再使用Dropout。\n\n## Adverserial Training\nDNN对对抗样本非常不robust的主要原因之一是 __过度线性__。DNN主要是基于线性块构建的，因此在一些实验中，它们实现的整体函数被证明是高度线性的。这些线性函数很容易优化，不幸的是，如果一个线性函数具有许多输入，那么它的值可以非常迅速地改变。如果我们用$\\epsilon$改变每个输入，那么权重为$w$的线性函数可以改变$\\epsilon||w||_1$之多，如果$w$是高维的这会是一个非常大的数。Adverserial training通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以看作一种明确地向监督NN引入局部恒定先验的方法。\n\n对抗样本也提供了一种实现semi-supervised learning的方法，在与数据集中的label不相关联的点$x$处，模型本身为其分配一些label $\\hat{y}$。模型的label $\\hat{y}$ 未必是真正的label，但如果模型是高品质的，那么$\\hat{y}$提供正确标签的可能性很大。我们可以搜索一个对抗样本$x^{'}$，导致分类器输出一个标签$y^{'}$且$y^{'}\\neq y$。不使用真正的label，而是由训练好的model提供label产生的adverserial samples被称为“虚拟对抗样本”。我们可以训练分类器为$x$和$x^{'}$分配相同的标签。__这鼓励classifier学习一个沿着未标注数据所在流形上任意微小变化都很robust的函数__。驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。","slug":"dl-regularization","published":1,"updated":"2018-10-01T04:40:08.901Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cb000q608wl2b759r4","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Regularization是Machine Learning中一个非常重要的概念，是对抗Overfitting最有用的利器。DNN由于参数众多，很容易overfitting，若直接选用small model，则会导致model特征学习、分类能力不足。因此现实中往往是 <strong>使用较大的模型 + 正则化</strong> 来解决相应问题。因此本文简要介绍一下Deep Learning中常用的正则化方法。</p>\n<p>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)<br>$$</p>\n<h2 id=\"参数范数惩罚\"><a href=\"#参数范数惩罚\" class=\"headerlink\" title=\"参数范数惩罚\"></a>参数范数惩罚</h2><p>我们通常只对weight做惩罚，而不对bias做正则惩罚。精确拟合bias所需的数据通常比拟合weight少得多，每个weight会指定两个变量如何相互作用。而每个bias仅控制一个单变量。这意味着我们不对其进行正则化也不会导致太大的方差。</p>\n<p>在DNN中，有时会希望对Network的每一层使用单独的惩罚，并分配不同的$\\alpha$系数，而寻找合适的多个超参代价很大。因此为了减少搜索空间，我们会在所有层使用相同的weight decay。</p>\n<h3 id=\"L-2-Regularization\"><a href=\"#L-2-Regularization\" class=\"headerlink\" title=\"$L_2$ Regularization\"></a>$L_2$ Regularization</h3><p>为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：<br>$$<br>\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)<br>$$<br>与之对应的梯度为：<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)<br>$$<br>使用SGD更新权重：<br>$$<br>w\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))<br>$$<br>换种写法就是：<br>$$<br>w\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)<br>$$<br>可以看到，加入weight decay后会引起学习规则的修改，即在每一步执行通常的SGD之前会 <strong>先收缩权重向量(将权重向量乘以一个常数因子)</strong>。</p>\n<p>以Linear Regression为例，其Cost Function是MSE:<br>$$<br>(Xw-y)^T(Xw-y)<br>$$<br>我们添加$L_2$ Regularization之后，Cost Function变为:<br>$$<br>(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw<br>$$<br>这将正规方程的解由 $w=(X^TX)^{-1}X^Ty$ 变为 $w=(X^TX+\\alpha I)^{-1}X^Ty$。其中，矩阵$X^TX$与协方差矩阵$\\frac{1}{m}X^TX$成正比。$L_2$ Regularization将这个矩阵替换为上式中的$(X^TX+\\alpha I)^{-1}$，这个新矩阵与原来的是一样的，不同的仅仅是在对角线加了$\\alpha$。这个矩阵的对角项对应每个输入特征的方差。我们可以看到，$L_2$ Regularization能让学习算法“感知到”具有较高方差的输入$x$，因此与输出目标的协方差较小(相对增加方差)的特征的权重将会收缩。</p>\n<h3 id=\"L-1-Regularization\"><a href=\"#L-1-Regularization\" class=\"headerlink\" title=\"$L_1$ Regularization\"></a>$L_1$ Regularization</h3><p>为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：<br>$$<br>\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)<br>$$<br>对应的梯度:<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)<br>$$<br>可与看到，<strong>此时正则化对梯度的影响不再是线性地缩放每个$w_i$，而是添加了一项与$sign(w_i)$同号的常数，使用这种形式的梯度之后，我们不一定能得到$J(X,y;w)$二次近似的直接算数解($L_2$正则化时可以)</strong>。</p>\n<p>我们可以将$L_1$ Regularization Cost Function的二次近似分解成关于参数的求和：<br>$$<br>\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]<br>$$<br>如下形式的解析解(对每一维$i$)可以最小化这个近似Cost Function：<br>$$<br>w_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}<br>$$<br>此时：<br>1) 若$w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$，正则化后目标中的$w_i$最优值是0。这是因为在方向$i$上$J(w;X,y)$对$\\hat{J}(w;X,y)$的贡献被抵消，$L_1$ Regularization将$w_i$推至0。<br>2) 若$w_i^{\\star}&gt; \\frac{\\alpha}{H_{i,i}}$，正则化不会将$w_i$的最优值推至0，而仅仅在那个方向上移动$\\frac{\\alpha}{H_{i,i}}$的距离。</p>\n<p>相比$L_2$ Regularization，$L_1$ Regularization会产生更稀疏的解(最优值中的一些参数为0)。</p>\n<h2 id=\"作为约束的范数惩罚\"><a href=\"#作为约束的范数惩罚\" class=\"headerlink\" title=\"作为约束的范数惩罚\"></a>作为约束的范数惩罚</h2><p>$$<br>\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)<br>$$<br>如果$\\Omega$是$L_2$范数，那么权重就是被约束在一个$L_2$球中；如果$\\Omega$是$L_1$范数，那么权重就是被约束在一个$L_1$范数限制的区域中。</p>\n<h2 id=\"Data-Augmentation\"><a href=\"#Data-Augmentation\" class=\"headerlink\" title=\"Data Augmentation\"></a>Data Augmentation</h2><p>在NN的输入层注入噪声也可以看作Data Augmentation的一种方式。然而，NN对噪声不是很robust。改善NN robustness的方法之一是简单地将随机噪声添加到输入再训练。输入噪声注入是一些Unsupervised Learning Algorithm的一部分（例如Denoise Auto Encoder）。向hidden layer施加噪声也是可行的，这可以被看作在多个抽象层上进行的Data Augmentation。</p>\n<h2 id=\"Robustness-of-Noise\"><a href=\"#Robustness-of-Noise\" class=\"headerlink\" title=\"Robustness of Noise\"></a>Robustness of Noise</h2><p>对某些模型而言，<strong>向输入添加方差极小的噪声等价于对权重施加范数惩罚</strong>。一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到hidden units时会更加强大。</p>\n<h2 id=\"Multi-Task-Learning\"><a href=\"#Multi-Task-Learning\" class=\"headerlink\" title=\"Multi-Task Learning\"></a>Multi-Task Learning</h2><p>MTL是通过合并几个任务中的样例(<strong>可以视为对参数施加的软约束</strong>)来提高泛化的一种方式。<strong>当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值，通常会带来更好的泛化能力</strong>。</p>\n<h2 id=\"Early-Stopping\"><a href=\"#Early-Stopping\" class=\"headerlink\" title=\"Early Stopping\"></a>Early Stopping</h2><p>在训练中只返回使validation set error最低的参数设置，就可以获得使validation set更低的模型(并且因此有希望获得更好的test set error)。在每次validation set有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当validation set error在事先指定的循环次数内没有进一步改善时，算法就会终止。这种策略称为Early Stopping。</p>\n<p>对于weight decay，必须小心不能使用太多的weight decay，<strong>以防止网络陷入不良局部极小点</strong>。</p>\n<p>Early Stopping需要validation set，这意味着某些training samples不能被输入到模型。为了更好地利用这一额外数据，我们可以在完成Early Stopping的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的training data都会被包括在内。</p>\n<ul>\n<li>一种策略是再次初始化模型，然后使用所有数据再次训练。在第二轮训练过程中，我们使用第一轮Early Stopping确定的 <strong>最佳Epoch</strong>。</li>\n<li>另一种策略是保持从第一轮训练获得的参数，<strong>然后使用全部数据继续训练</strong>。在这个阶段，已经没有validation set指导我们需要训练多少步停止。我们可以监控validation set的平均loss，并继续训练，直到它低于Early Stopping终止时的目标值。</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg\" alt=\"Early Stopping\"></p>\n<h3 id=\"为什么Early-Stopping具有Regularization效果？\"><a href=\"#为什么Early-Stopping具有Regularization效果？\" class=\"headerlink\" title=\"为什么Early Stopping具有Regularization效果？\"></a>为什么Early Stopping具有Regularization效果？</h3><p>Bishop <strong>认为Early Stopping可以将优化过程的参数空间限制在初始参数值$\\theta_0$的小领域内</strong>。事实上，在二次误差的简单Linear Model和Gradient Descend情况下，我们可以展示Early Stopping相当于$L_2$ Regularization。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg\" alt=\"Early Stopping As Regularization\"></p>\n<h2 id=\"参数绑定和参数共享\"><a href=\"#参数绑定和参数共享\" class=\"headerlink\" title=\"参数绑定和参数共享\"></a>参数绑定和参数共享</h2><p>假设有两个model执行两个分类任务，但输入分布稍有不同。这两个模型将输入映射到两个不同但相关的输出：$\\hat{y}^{(A)}=f(w^{(A)},x)$和$\\hat{y}^{(B)}=f(w^{(B)},x)$。</p>\n<p>我们可以想象，这些任务会足够相似，因此我们认为模型参数应彼此接近：$\\forall_i,w_i^{(A)}$应该与$w_i^{(B)}$接近，我们可通过正则化利用此信息，即：$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$。这里使用$L_2$ Regularization，也可以使用其他Regularization。</p>\n<p>正则化一个模型(监督模式下训练的分类器)的参数，使其接近另一个无监督模式下训练的模型参数，构造的这种架构使得分类模型中的许多参数能与无监督模型中对应的参数匹配。</p>\n<p>CNN通过在多个位置共享参数来考虑 <strong>平移不变性</strong>。相同的特征(具有相同权重的hidden units)在输入的不同位置上计算获得，这意味着无论人脸在图像中的第$i$列或是$i+1$列，我们都可以使用相同的feature detector找到人脸。</p>\n<h2 id=\"Sparse-Representation\"><a href=\"#Sparse-Representation\" class=\"headerlink\" title=\"Sparse Representation\"></a>Sparse Representation</h2><p>Weight decay是直接惩罚模型参数，另一种策略是惩罚NN中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。</p>\n<p>表示的稀疏惩罚正则化是通过向Loss Function $J$ 添加对表示的范数惩罚来实现的，记作$\\Omega(h)$：<br>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)<br>$$<br>对表示元素的$L_1$惩罚诱导稀疏的表示：$\\Omega(h)=||h||_1=\\sum_i|h_i|$。</p>\n<p>还有一些其他方法通过激活值的硬性约束来获得表示稀疏，例如正交匹配跟踪通过解决以下约束优化问题将输入值$x$编码成表示$h$:<br>$$<br>\\mathop{argmin} \\limits_{h,||h||_0&lt; k} ||x-Wh||^2<br>$$<br>其中$||h||_0$是$h$中非零项的个数。当$W$被约束为正交时，我们可以高效地解决这个问题。</p>\n<h2 id=\"Bagging和其他集成方法\"><a href=\"#Bagging和其他集成方法\" class=\"headerlink\" title=\"Bagging和其他集成方法\"></a>Bagging和其他集成方法</h2><h3 id=\"Why-Model-Averaging-Works\"><a href=\"#Why-Model-Averaging-Works\" class=\"headerlink\" title=\"Why Model Averaging Works?\"></a>Why Model Averaging Works?</h3><p>假设我们有$k$个regression model，每个model在每个例子上的误差是$\\epsilon_i$，这个误差服从零均值方差为$\\mathbb{E}[\\epsilon_i^2]=v$且协方差为$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$的多维正态分布。通过所有集成模型的平均预测所得误差是$\\frac{1}{k}\\sum_i\\epsilon_i$。集成模型的MSE期望是：<br>$$<br>\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c<br>$$<br>在误差完全相关即$c=v$的情况下，MSE减少到$v$，所以模型平均没有任何帮助。在错误完全不相关即$c=0$的情况下，该集成模型MSE仅为$\\frac{v}{k}$。这意味着集成MSE的期望会随着集成规模增大而线性减小。换言之，ensemble model至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，ensemble将显著地比其他成员表现得更好。</p>\n<p>NN能找到足够多的不同解，意味着它们可以从Model Averaging中受益(即使所有模型都在同一个数据集上训练))。NN中随机初始化的差异、不同输出的非确定性往往足以使得ensemble中的不同成员具有部分独立的误差。</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>Dropout可以被认为是集成大量DNN的实用Bagging。Dropout训练的ensemble包括所有从base NN除去非输出单元后形成的子网络。</p>\n<p>Dropout训练与Bagging训练不太一样，Bagging中所有模型都是独立的，在Dropout中所有模型共享参数。其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。</p>\n<p>若使用0.5的keep_prob，权重比例规则一般相当于在训练结束后将权重除以2，然后像平常一样使用模型。实现相同结果的另一种方法是在训练期间将单元的状态乘以2。</p>\n<p>Dropout是一个Regularization技术，它减少了模型的有效容量，为了抵消这种影响，我们必须增大模型规模。当Dropout用于Linear Regression时，相当于每个输入特征具有不同weight decay系数的$L_2$ weight decay。每个特征的weight decay系数的大小是由其方差来确定的。其他Linear Model也有类似的结果。对于Deep Model而言，Dropout与weight decay是不等同的。</p>\n<p><strong>DropConnect</strong> 是Dropout的一个特殊情况，其中一个标量权重和单个hidden unit状态之间的每个乘积被认为是可以丢弃的一个单元。</p>\n<p><strong>Batch Normalization</strong> 在训练时向hidden unit引入加性和乘性噪声重新参数化模型。BatchNorm主要目的是改善优化，但噪声具有正则化效果，有时没必要再使用Dropout。</p>\n<h2 id=\"Adverserial-Training\"><a href=\"#Adverserial-Training\" class=\"headerlink\" title=\"Adverserial Training\"></a>Adverserial Training</h2><p>DNN对对抗样本非常不robust的主要原因之一是 <strong>过度线性</strong>。DNN主要是基于线性块构建的，因此在一些实验中，它们实现的整体函数被证明是高度线性的。这些线性函数很容易优化，不幸的是，如果一个线性函数具有许多输入，那么它的值可以非常迅速地改变。如果我们用$\\epsilon$改变每个输入，那么权重为$w$的线性函数可以改变$\\epsilon||w||_1$之多，如果$w$是高维的这会是一个非常大的数。Adverserial training通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以看作一种明确地向监督NN引入局部恒定先验的方法。</p>\n<p>对抗样本也提供了一种实现semi-supervised learning的方法，在与数据集中的label不相关联的点$x$处，模型本身为其分配一些label $\\hat{y}$。模型的label $\\hat{y}$ 未必是真正的label，但如果模型是高品质的，那么$\\hat{y}$提供正确标签的可能性很大。我们可以搜索一个对抗样本$x^{‘}$，导致分类器输出一个标签$y^{‘}$且$y^{‘}\\neq y$。不使用真正的label，而是由训练好的model提供label产生的adverserial samples被称为“虚拟对抗样本”。我们可以训练分类器为$x$和$x^{‘}$分配相同的标签。<strong>这鼓励classifier学习一个沿着未标注数据所在流形上任意微小变化都很robust的函数</strong>。驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Regularization是Machine Learning中一个非常重要的概念，是对抗Overfitting最有用的利器。DNN由于参数众多，很容易overfitting，若直接选用small model，则会导致model特征学习、分类能力不足。因此现实中往往是 <strong>使用较大的模型 + 正则化</strong> 来解决相应问题。因此本文简要介绍一下Deep Learning中常用的正则化方法。</p>\n<p>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)<br>$$</p>\n<h2 id=\"参数范数惩罚\"><a href=\"#参数范数惩罚\" class=\"headerlink\" title=\"参数范数惩罚\"></a>参数范数惩罚</h2><p>我们通常只对weight做惩罚，而不对bias做正则惩罚。精确拟合bias所需的数据通常比拟合weight少得多，每个weight会指定两个变量如何相互作用。而每个bias仅控制一个单变量。这意味着我们不对其进行正则化也不会导致太大的方差。</p>\n<p>在DNN中，有时会希望对Network的每一层使用单独的惩罚，并分配不同的$\\alpha$系数，而寻找合适的多个超参代价很大。因此为了减少搜索空间，我们会在所有层使用相同的weight decay。</p>\n<h3 id=\"L-2-Regularization\"><a href=\"#L-2-Regularization\" class=\"headerlink\" title=\"$L_2$ Regularization\"></a>$L_2$ Regularization</h3><p>为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：<br>$$<br>\\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y)<br>$$<br>与之对应的梯度为：<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha w+\\bigtriangledown_w J(w;X,y)<br>$$<br>使用SGD更新权重：<br>$$<br>w\\leftarrow w-\\epsilon(\\alpha w+\\bigtriangledown_w J(w;X,y))<br>$$<br>换种写法就是：<br>$$<br>w\\leftarrow (1-\\epsilon\\alpha)w-\\epsilon\\bigtriangledown_w J(w;X,y)<br>$$<br>可以看到，加入weight decay后会引起学习规则的修改，即在每一步执行通常的SGD之前会 <strong>先收缩权重向量(将权重向量乘以一个常数因子)</strong>。</p>\n<p>以Linear Regression为例，其Cost Function是MSE:<br>$$<br>(Xw-y)^T(Xw-y)<br>$$<br>我们添加$L_2$ Regularization之后，Cost Function变为:<br>$$<br>(Xw-y)^T(Xw-y)+\\frac{1}{2}\\alpha w^Tw<br>$$<br>这将正规方程的解由 $w=(X^TX)^{-1}X^Ty$ 变为 $w=(X^TX+\\alpha I)^{-1}X^Ty$。其中，矩阵$X^TX$与协方差矩阵$\\frac{1}{m}X^TX$成正比。$L_2$ Regularization将这个矩阵替换为上式中的$(X^TX+\\alpha I)^{-1}$，这个新矩阵与原来的是一样的，不同的仅仅是在对角线加了$\\alpha$。这个矩阵的对角项对应每个输入特征的方差。我们可以看到，$L_2$ Regularization能让学习算法“感知到”具有较高方差的输入$x$，因此与输出目标的协方差较小(相对增加方差)的特征的权重将会收缩。</p>\n<h3 id=\"L-1-Regularization\"><a href=\"#L-1-Regularization\" class=\"headerlink\" title=\"$L_1$ Regularization\"></a>$L_1$ Regularization</h3><p>为简单起见，假定DNN中没有bias，因此$\\theta$就是$w$。模型Loss Function如下：<br>$$<br>\\tilde{J}(w;X,y)=\\alpha||w||_1+J(w;X,y)<br>$$<br>对应的梯度:<br>$$<br>\\bigtriangledown_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\bigtriangledown_w J(w;X,y)<br>$$<br>可与看到，<strong>此时正则化对梯度的影响不再是线性地缩放每个$w_i$，而是添加了一项与$sign(w_i)$同号的常数，使用这种形式的梯度之后，我们不一定能得到$J(X,y;w)$二次近似的直接算数解($L_2$正则化时可以)</strong>。</p>\n<p>我们可以将$L_1$ Regularization Cost Function的二次近似分解成关于参数的求和：<br>$$<br>\\hat{J}(w;X,y)=J(w^{\\star};X,y)+\\sum_i [\\frac{1}{2}H_{i,i}(w_i-w_i^{\\star})^2+\\alpha |w_i|]<br>$$<br>如下形式的解析解(对每一维$i$)可以最小化这个近似Cost Function：<br>$$<br>w_i=sign(w_i^{\\star})max\\{|w_i^{\\star}|-\\frac{\\alpha}{H_{i,i}},0\\}<br>$$<br>此时：<br>1) 若$w_i^{\\star}\\leq \\frac{\\alpha}{H_{i,i}}$，正则化后目标中的$w_i$最优值是0。这是因为在方向$i$上$J(w;X,y)$对$\\hat{J}(w;X,y)$的贡献被抵消，$L_1$ Regularization将$w_i$推至0。<br>2) 若$w_i^{\\star}&gt; \\frac{\\alpha}{H_{i,i}}$，正则化不会将$w_i$的最优值推至0，而仅仅在那个方向上移动$\\frac{\\alpha}{H_{i,i}}$的距离。</p>\n<p>相比$L_2$ Regularization，$L_1$ Regularization会产生更稀疏的解(最优值中的一些参数为0)。</p>\n<h2 id=\"作为约束的范数惩罚\"><a href=\"#作为约束的范数惩罚\" class=\"headerlink\" title=\"作为约束的范数惩罚\"></a>作为约束的范数惩罚</h2><p>$$<br>\\theta^{\\star}=\\mathop{argmin} \\limits_{\\theta} \\mathcal{L}(\\theta,\\alpha^{\\star})=\\mathop{argmin} \\limits_{\\theta} J(\\theta;X,y)+\\alpha^{\\star}\\Omega(\\theta)<br>$$<br>如果$\\Omega$是$L_2$范数，那么权重就是被约束在一个$L_2$球中；如果$\\Omega$是$L_1$范数，那么权重就是被约束在一个$L_1$范数限制的区域中。</p>\n<h2 id=\"Data-Augmentation\"><a href=\"#Data-Augmentation\" class=\"headerlink\" title=\"Data Augmentation\"></a>Data Augmentation</h2><p>在NN的输入层注入噪声也可以看作Data Augmentation的一种方式。然而，NN对噪声不是很robust。改善NN robustness的方法之一是简单地将随机噪声添加到输入再训练。输入噪声注入是一些Unsupervised Learning Algorithm的一部分（例如Denoise Auto Encoder）。向hidden layer施加噪声也是可行的，这可以被看作在多个抽象层上进行的Data Augmentation。</p>\n<h2 id=\"Robustness-of-Noise\"><a href=\"#Robustness-of-Noise\" class=\"headerlink\" title=\"Robustness of Noise\"></a>Robustness of Noise</h2><p>对某些模型而言，<strong>向输入添加方差极小的噪声等价于对权重施加范数惩罚</strong>。一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到hidden units时会更加强大。</p>\n<h2 id=\"Multi-Task-Learning\"><a href=\"#Multi-Task-Learning\" class=\"headerlink\" title=\"Multi-Task Learning\"></a>Multi-Task Learning</h2><p>MTL是通过合并几个任务中的样例(<strong>可以视为对参数施加的软约束</strong>)来提高泛化的一种方式。<strong>当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值，通常会带来更好的泛化能力</strong>。</p>\n<h2 id=\"Early-Stopping\"><a href=\"#Early-Stopping\" class=\"headerlink\" title=\"Early Stopping\"></a>Early Stopping</h2><p>在训练中只返回使validation set error最低的参数设置，就可以获得使validation set更低的模型(并且因此有希望获得更好的test set error)。在每次validation set有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当validation set error在事先指定的循环次数内没有进一步改善时，算法就会终止。这种策略称为Early Stopping。</p>\n<p>对于weight decay，必须小心不能使用太多的weight decay，<strong>以防止网络陷入不良局部极小点</strong>。</p>\n<p>Early Stopping需要validation set，这意味着某些training samples不能被输入到模型。为了更好地利用这一额外数据，我们可以在完成Early Stopping的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的training data都会被包括在内。</p>\n<ul>\n<li>一种策略是再次初始化模型，然后使用所有数据再次训练。在第二轮训练过程中，我们使用第一轮Early Stopping确定的 <strong>最佳Epoch</strong>。</li>\n<li>另一种策略是保持从第一轮训练获得的参数，<strong>然后使用全部数据继续训练</strong>。在这个阶段，已经没有validation set指导我们需要训练多少步停止。我们可以监控validation set的平均loss，并继续训练，直到它低于Early Stopping终止时的目标值。</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/early_stopping.jpg\" alt=\"Early Stopping\"></p>\n<h3 id=\"为什么Early-Stopping具有Regularization效果？\"><a href=\"#为什么Early-Stopping具有Regularization效果？\" class=\"headerlink\" title=\"为什么Early Stopping具有Regularization效果？\"></a>为什么Early Stopping具有Regularization效果？</h3><p>Bishop <strong>认为Early Stopping可以将优化过程的参数空间限制在初始参数值$\\theta_0$的小领域内</strong>。事实上，在二次误差的简单Linear Model和Gradient Descend情况下，我们可以展示Early Stopping相当于$L_2$ Regularization。</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-regularization/es.jpg\" alt=\"Early Stopping As Regularization\"></p>\n<h2 id=\"参数绑定和参数共享\"><a href=\"#参数绑定和参数共享\" class=\"headerlink\" title=\"参数绑定和参数共享\"></a>参数绑定和参数共享</h2><p>假设有两个model执行两个分类任务，但输入分布稍有不同。这两个模型将输入映射到两个不同但相关的输出：$\\hat{y}^{(A)}=f(w^{(A)},x)$和$\\hat{y}^{(B)}=f(w^{(B)},x)$。</p>\n<p>我们可以想象，这些任务会足够相似，因此我们认为模型参数应彼此接近：$\\forall_i,w_i^{(A)}$应该与$w_i^{(B)}$接近，我们可通过正则化利用此信息，即：$\\Omega(w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$。这里使用$L_2$ Regularization，也可以使用其他Regularization。</p>\n<p>正则化一个模型(监督模式下训练的分类器)的参数，使其接近另一个无监督模式下训练的模型参数，构造的这种架构使得分类模型中的许多参数能与无监督模型中对应的参数匹配。</p>\n<p>CNN通过在多个位置共享参数来考虑 <strong>平移不变性</strong>。相同的特征(具有相同权重的hidden units)在输入的不同位置上计算获得，这意味着无论人脸在图像中的第$i$列或是$i+1$列，我们都可以使用相同的feature detector找到人脸。</p>\n<h2 id=\"Sparse-Representation\"><a href=\"#Sparse-Representation\" class=\"headerlink\" title=\"Sparse Representation\"></a>Sparse Representation</h2><p>Weight decay是直接惩罚模型参数，另一种策略是惩罚NN中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。</p>\n<p>表示的稀疏惩罚正则化是通过向Loss Function $J$ 添加对表示的范数惩罚来实现的，记作$\\Omega(h)$：<br>$$<br>\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(h)<br>$$<br>对表示元素的$L_1$惩罚诱导稀疏的表示：$\\Omega(h)=||h||_1=\\sum_i|h_i|$。</p>\n<p>还有一些其他方法通过激活值的硬性约束来获得表示稀疏，例如正交匹配跟踪通过解决以下约束优化问题将输入值$x$编码成表示$h$:<br>$$<br>\\mathop{argmin} \\limits_{h,||h||_0&lt; k} ||x-Wh||^2<br>$$<br>其中$||h||_0$是$h$中非零项的个数。当$W$被约束为正交时，我们可以高效地解决这个问题。</p>\n<h2 id=\"Bagging和其他集成方法\"><a href=\"#Bagging和其他集成方法\" class=\"headerlink\" title=\"Bagging和其他集成方法\"></a>Bagging和其他集成方法</h2><h3 id=\"Why-Model-Averaging-Works\"><a href=\"#Why-Model-Averaging-Works\" class=\"headerlink\" title=\"Why Model Averaging Works?\"></a>Why Model Averaging Works?</h3><p>假设我们有$k$个regression model，每个model在每个例子上的误差是$\\epsilon_i$，这个误差服从零均值方差为$\\mathbb{E}[\\epsilon_i^2]=v$且协方差为$\\mathbb{E}[\\epsilon_i\\epsilon_j]=c$的多维正态分布。通过所有集成模型的平均预测所得误差是$\\frac{1}{k}\\sum_i\\epsilon_i$。集成模型的MSE期望是：<br>$$<br>\\mathbb{E}[(\\frac{1}{k}\\sum_i \\epsilon_i)^2]=\\frac{1}{k^2}\\mathbb{E}[\\sum_i (\\epsilon_i^2+\\sum_{j\\neq i}\\epsilon_i \\epsilon_j)]=\\frac{v}{k}+\\frac{k-1}{k}c<br>$$<br>在误差完全相关即$c=v$的情况下，MSE减少到$v$，所以模型平均没有任何帮助。在错误完全不相关即$c=0$的情况下，该集成模型MSE仅为$\\frac{v}{k}$。这意味着集成MSE的期望会随着集成规模增大而线性减小。换言之，ensemble model至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，ensemble将显著地比其他成员表现得更好。</p>\n<p>NN能找到足够多的不同解，意味着它们可以从Model Averaging中受益(即使所有模型都在同一个数据集上训练))。NN中随机初始化的差异、不同输出的非确定性往往足以使得ensemble中的不同成员具有部分独立的误差。</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>Dropout可以被认为是集成大量DNN的实用Bagging。Dropout训练的ensemble包括所有从base NN除去非输出单元后形成的子网络。</p>\n<p>Dropout训练与Bagging训练不太一样，Bagging中所有模型都是独立的，在Dropout中所有模型共享参数。其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。</p>\n<p>若使用0.5的keep_prob，权重比例规则一般相当于在训练结束后将权重除以2，然后像平常一样使用模型。实现相同结果的另一种方法是在训练期间将单元的状态乘以2。</p>\n<p>Dropout是一个Regularization技术，它减少了模型的有效容量，为了抵消这种影响，我们必须增大模型规模。当Dropout用于Linear Regression时，相当于每个输入特征具有不同weight decay系数的$L_2$ weight decay。每个特征的weight decay系数的大小是由其方差来确定的。其他Linear Model也有类似的结果。对于Deep Model而言，Dropout与weight decay是不等同的。</p>\n<p><strong>DropConnect</strong> 是Dropout的一个特殊情况，其中一个标量权重和单个hidden unit状态之间的每个乘积被认为是可以丢弃的一个单元。</p>\n<p><strong>Batch Normalization</strong> 在训练时向hidden unit引入加性和乘性噪声重新参数化模型。BatchNorm主要目的是改善优化，但噪声具有正则化效果，有时没必要再使用Dropout。</p>\n<h2 id=\"Adverserial-Training\"><a href=\"#Adverserial-Training\" class=\"headerlink\" title=\"Adverserial Training\"></a>Adverserial Training</h2><p>DNN对对抗样本非常不robust的主要原因之一是 <strong>过度线性</strong>。DNN主要是基于线性块构建的，因此在一些实验中，它们实现的整体函数被证明是高度线性的。这些线性函数很容易优化，不幸的是，如果一个线性函数具有许多输入，那么它的值可以非常迅速地改变。如果我们用$\\epsilon$改变每个输入，那么权重为$w$的线性函数可以改变$\\epsilon||w||_1$之多，如果$w$是高维的这会是一个非常大的数。Adverserial training通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以看作一种明确地向监督NN引入局部恒定先验的方法。</p>\n<p>对抗样本也提供了一种实现semi-supervised learning的方法，在与数据集中的label不相关联的点$x$处，模型本身为其分配一些label $\\hat{y}$。模型的label $\\hat{y}$ 未必是真正的label，但如果模型是高品质的，那么$\\hat{y}$提供正确标签的可能性很大。我们可以搜索一个对抗样本$x^{‘}$，导致分类器输出一个标签$y^{‘}$且$y^{‘}\\neq y$。不使用真正的label，而是由训练好的model提供label产生的adverserial samples被称为“虚拟对抗样本”。我们可以训练分类器为$x$和$x^{‘}$分配相同的标签。<strong>这鼓励classifier学习一个沿着未标注数据所在流形上任意微小变化都很robust的函数</strong>。驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。</p>\n"},{"title":"[DL] RNN","date":"2018-08-10T06:18:19.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning","Deep Learning","RNN","NLP"],"_content":"## Introduction\nRNN(Recurrent Neural Network)是一类专门处理序列数据的网络。RNN主要在NLP领域有着非常广泛的应用，也是当今火热的Deep Learning的其中模型之一。RNN在模型的不同部分共享参数，从而使得模型能够扩展到不同形式的样本并进行泛化。\n\n## 展开Computational Graph\n![Computational Graph Unfold](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg)\n\n我们可以用一个函数$g^{(t)}$代表经过 $t$ 步展开后的循环：\n$$\nh^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)\n$$\n1. 无论序列的长度，学成的model始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。\n2. 我们可以在每个时间步使用相同参数的转移函数 $f$。\n\n学习单一的共享模型允许泛化到没有见过的序列长度(not appear in training data)，并且估计模型所需的training samples远远少于不带参数共享的模型。\n\n## Recurrent Neural Network\nRNN的一些重要设计模式包括以下几种：\n1. 每个时间步都有输出，并且hidden units有循环连接的循环网络：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg)\n\n2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的hidden units之间有循环单元：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg)\n\n3. Hidden units之间存在循环连接，但读取整个sequence之后产生单个输出：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg)\n\n表示离散变量的常规方式是把输出 $o$ 作为每个离散变量可能值的非标准化对数概率，然后我们可以应用Softmax处理，得到标准化后的概率输出向量$\\hat{y}$。RNN从特定的初始状态$h^{(0)}$开始前向传播。从$t=1$到$t=\\tau$的每个时间步，我们应用以下更新方程：\n$$\na^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\\nh^{(t)}=tanh(a^{(t)}) \\\\\no^{(t)}=Vh^{(t)}+c \\\\\n\\hat{y}^{(t)}=softmax(o^{(t)})\n$$\n$U, V, W$分别对应 input layers to hidden layers，hidden layers to output layers，hidden to next hidden layers的连接。该RNN将一个input sequence映射到相同长度的output sequence。__与 $x$ 序列配对的 $y$ 的总Loss就是所有时间步的Loss之和__。\n\n### Teacher Forcing and Networks with Output Recurrence\n由输出反馈到模型而产生循环连接的model可用teacher forcing进行训练。训练模型时，teacher forcing不再使用最大似然准则，而在时刻 $t+1$ 接收真实值 $y^{(t)}$ 作为输入。条件最大似然准则是：\n$$\nlog p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})\n$$\n\n![Teacher Forcing](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg)\n\n\n### Computing the Gradient in RNN\n以最终的Loss开始，计算：\n$\\frac{\\partial L}{\\partial L^{(t)}}=1$\n\n对所有 $i,t$ 关于时间步 $t$ 输出的梯度 $\\triangledown_{o^{(t)}}L$ 如下：  \n$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$\n\n从序列的末尾开始，反向进行计算，在最后的时间步 $\\tau, h^{(\\tau)}$ 只有 $o^{(\\tau)}$ 作为后续结点，因此这个梯度计算很简单：  \n$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$\n\n然后，我们从时刻$t=\\tau -1$到$t=1$反向迭代，通过时间反向传播梯度，注意 $h^{(t)} (t<\\tau)$同时具有 $o^{(t)}$ 和 $h^{(t+1)}$两个后续结点。因此，它的梯度如下计算：  \n$$\n\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)\n$$\n\n剩下的参数梯度可以由下式给出：\n\n![RNN-BP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg)\n\n### Recurrent Networks as Directed Graphical Models\n","source":"_posts/dl-rnn.md","raw":"---\ntitle: \"[DL] RNN\"\ndate: 2018-08-10 14:18:19\nmathjax: true\ntags:\n- Machine Learning\n- Deep Learning\n- RNN\n- Data Science\n- NLP\ncatagories:\n- Algorithm\n- Machine Learning\n- Deep Learning\n- RNN\n- NLP\n---\n## Introduction\nRNN(Recurrent Neural Network)是一类专门处理序列数据的网络。RNN主要在NLP领域有着非常广泛的应用，也是当今火热的Deep Learning的其中模型之一。RNN在模型的不同部分共享参数，从而使得模型能够扩展到不同形式的样本并进行泛化。\n\n## 展开Computational Graph\n![Computational Graph Unfold](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg)\n\n我们可以用一个函数$g^{(t)}$代表经过 $t$ 步展开后的循环：\n$$\nh^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)\n$$\n1. 无论序列的长度，学成的model始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。\n2. 我们可以在每个时间步使用相同参数的转移函数 $f$。\n\n学习单一的共享模型允许泛化到没有见过的序列长度(not appear in training data)，并且估计模型所需的training samples远远少于不带参数共享的模型。\n\n## Recurrent Neural Network\nRNN的一些重要设计模式包括以下几种：\n1. 每个时间步都有输出，并且hidden units有循环连接的循环网络：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg)\n\n2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的hidden units之间有循环单元：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg)\n\n3. Hidden units之间存在循环连接，但读取整个sequence之后产生单个输出：  \n![RNN](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg)\n\n表示离散变量的常规方式是把输出 $o$ 作为每个离散变量可能值的非标准化对数概率，然后我们可以应用Softmax处理，得到标准化后的概率输出向量$\\hat{y}$。RNN从特定的初始状态$h^{(0)}$开始前向传播。从$t=1$到$t=\\tau$的每个时间步，我们应用以下更新方程：\n$$\na^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\\nh^{(t)}=tanh(a^{(t)}) \\\\\no^{(t)}=Vh^{(t)}+c \\\\\n\\hat{y}^{(t)}=softmax(o^{(t)})\n$$\n$U, V, W$分别对应 input layers to hidden layers，hidden layers to output layers，hidden to next hidden layers的连接。该RNN将一个input sequence映射到相同长度的output sequence。__与 $x$ 序列配对的 $y$ 的总Loss就是所有时间步的Loss之和__。\n\n### Teacher Forcing and Networks with Output Recurrence\n由输出反馈到模型而产生循环连接的model可用teacher forcing进行训练。训练模型时，teacher forcing不再使用最大似然准则，而在时刻 $t+1$ 接收真实值 $y^{(t)}$ 作为输入。条件最大似然准则是：\n$$\nlog p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})\n$$\n\n![Teacher Forcing](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg)\n\n\n### Computing the Gradient in RNN\n以最终的Loss开始，计算：\n$\\frac{\\partial L}{\\partial L^{(t)}}=1$\n\n对所有 $i,t$ 关于时间步 $t$ 输出的梯度 $\\triangledown_{o^{(t)}}L$ 如下：  \n$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$\n\n从序列的末尾开始，反向进行计算，在最后的时间步 $\\tau, h^{(\\tau)}$ 只有 $o^{(\\tau)}$ 作为后续结点，因此这个梯度计算很简单：  \n$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$\n\n然后，我们从时刻$t=\\tau -1$到$t=1$反向迭代，通过时间反向传播梯度，注意 $h^{(t)} (t<\\tau)$同时具有 $o^{(t)}$ 和 $h^{(t+1)}$两个后续结点。因此，它的梯度如下计算：  \n$$\n\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)\n$$\n\n剩下的参数梯度可以由下式给出：\n\n![RNN-BP](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg)\n\n### Recurrent Networks as Directed Graphical Models\n","slug":"dl-rnn","published":1,"updated":"2018-10-01T04:40:08.921Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cd000s608wvyyxpd8f","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RNN(Recurrent Neural Network)是一类专门处理序列数据的网络。RNN主要在NLP领域有着非常广泛的应用，也是当今火热的Deep Learning的其中模型之一。RNN在模型的不同部分共享参数，从而使得模型能够扩展到不同形式的样本并进行泛化。</p>\n<h2 id=\"展开Computational-Graph\"><a href=\"#展开Computational-Graph\" class=\"headerlink\" title=\"展开Computational Graph\"></a>展开Computational Graph</h2><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg\" alt=\"Computational Graph Unfold\"></p>\n<p>我们可以用一个函数$g^{(t)}$代表经过 $t$ 步展开后的循环：<br>$$<br>h^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)<br>$$</p>\n<ol>\n<li>无论序列的长度，学成的model始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。</li>\n<li>我们可以在每个时间步使用相同参数的转移函数 $f$。</li>\n</ol>\n<p>学习单一的共享模型允许泛化到没有见过的序列长度(not appear in training data)，并且估计模型所需的training samples远远少于不带参数共享的模型。</p>\n<h2 id=\"Recurrent-Neural-Network\"><a href=\"#Recurrent-Neural-Network\" class=\"headerlink\" title=\"Recurrent Neural Network\"></a>Recurrent Neural Network</h2><p>RNN的一些重要设计模式包括以下几种：</p>\n<ol>\n<li><p>每个时间步都有输出，并且hidden units有循环连接的循环网络：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的hidden units之间有循环单元：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>Hidden units之间存在循环连接，但读取整个sequence之后产生单个输出：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg\" alt=\"RNN\"></p>\n</li>\n</ol>\n<p>表示离散变量的常规方式是把输出 $o$ 作为每个离散变量可能值的非标准化对数概率，然后我们可以应用Softmax处理，得到标准化后的概率输出向量$\\hat{y}$。RNN从特定的初始状态$h^{(0)}$开始前向传播。从$t=1$到$t=\\tau$的每个时间步，我们应用以下更新方程：<br>$$<br>a^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\<br>h^{(t)}=tanh(a^{(t)}) \\\\<br>o^{(t)}=Vh^{(t)}+c \\\\<br>\\hat{y}^{(t)}=softmax(o^{(t)})<br>$$<br>$U, V, W$分别对应 input layers to hidden layers，hidden layers to output layers，hidden to next hidden layers的连接。该RNN将一个input sequence映射到相同长度的output sequence。<strong>与 $x$ 序列配对的 $y$ 的总Loss就是所有时间步的Loss之和</strong>。</p>\n<h3 id=\"Teacher-Forcing-and-Networks-with-Output-Recurrence\"><a href=\"#Teacher-Forcing-and-Networks-with-Output-Recurrence\" class=\"headerlink\" title=\"Teacher Forcing and Networks with Output Recurrence\"></a>Teacher Forcing and Networks with Output Recurrence</h3><p>由输出反馈到模型而产生循环连接的model可用teacher forcing进行训练。训练模型时，teacher forcing不再使用最大似然准则，而在时刻 $t+1$ 接收真实值 $y^{(t)}$ 作为输入。条件最大似然准则是：<br>$$<br>log p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg\" alt=\"Teacher Forcing\"></p>\n<h3 id=\"Computing-the-Gradient-in-RNN\"><a href=\"#Computing-the-Gradient-in-RNN\" class=\"headerlink\" title=\"Computing the Gradient in RNN\"></a>Computing the Gradient in RNN</h3><p>以最终的Loss开始，计算：<br>$\\frac{\\partial L}{\\partial L^{(t)}}=1$</p>\n<p>对所有 $i,t$ 关于时间步 $t$ 输出的梯度 $\\triangledown_{o^{(t)}}L$ 如下：<br>$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$</p>\n<p>从序列的末尾开始，反向进行计算，在最后的时间步 $\\tau, h^{(\\tau)}$ 只有 $o^{(\\tau)}$ 作为后续结点，因此这个梯度计算很简单：<br>$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$</p>\n<p>然后，我们从时刻$t=\\tau -1$到$t=1$反向迭代，通过时间反向传播梯度，注意 $h^{(t)} (t&lt;\\tau)$同时具有 $o^{(t)}$ 和 $h^{(t+1)}$两个后续结点。因此，它的梯度如下计算：<br>$$<br>\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)<br>$$</p>\n<p>剩下的参数梯度可以由下式给出：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg\" alt=\"RNN-BP\"></p>\n<h3 id=\"Recurrent-Networks-as-Directed-Graphical-Models\"><a href=\"#Recurrent-Networks-as-Directed-Graphical-Models\" class=\"headerlink\" title=\"Recurrent Networks as Directed Graphical Models\"></a>Recurrent Networks as Directed Graphical Models</h3>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>RNN(Recurrent Neural Network)是一类专门处理序列数据的网络。RNN主要在NLP领域有着非常广泛的应用，也是当今火热的Deep Learning的其中模型之一。RNN在模型的不同部分共享参数，从而使得模型能够扩展到不同形式的样本并进行泛化。</p>\n<h2 id=\"展开Computational-Graph\"><a href=\"#展开Computational-Graph\" class=\"headerlink\" title=\"展开Computational Graph\"></a>展开Computational Graph</h2><p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/cg-unfold.jpg\" alt=\"Computational Graph Unfold\"></p>\n<p>我们可以用一个函数$g^{(t)}$代表经过 $t$ 步展开后的循环：<br>$$<br>h^{(t)}=g^{(t)}(x^{(t)}, x^{(t-1)}, x^{(t-2)}, \\cdots, x^{(1)})=f(h^{(t-1)}, x^{(t)}; \\theta)<br>$$</p>\n<ol>\n<li>无论序列的长度，学成的model始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。</li>\n<li>我们可以在每个时间步使用相同参数的转移函数 $f$。</li>\n</ol>\n<p>学习单一的共享模型允许泛化到没有见过的序列长度(not appear in training data)，并且估计模型所需的training samples远远少于不带参数共享的模型。</p>\n<h2 id=\"Recurrent-Neural-Network\"><a href=\"#Recurrent-Neural-Network\" class=\"headerlink\" title=\"Recurrent Neural Network\"></a>Recurrent Neural Network</h2><p>RNN的一些重要设计模式包括以下几种：</p>\n<ol>\n<li><p>每个时间步都有输出，并且hidden units有循环连接的循环网络：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn1.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的hidden units之间有循环单元：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn2.jpg\" alt=\"RNN\"></p>\n</li>\n<li><p>Hidden units之间存在循环连接，但读取整个sequence之后产生单个输出：<br><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn3.jpg\" alt=\"RNN\"></p>\n</li>\n</ol>\n<p>表示离散变量的常规方式是把输出 $o$ 作为每个离散变量可能值的非标准化对数概率，然后我们可以应用Softmax处理，得到标准化后的概率输出向量$\\hat{y}$。RNN从特定的初始状态$h^{(0)}$开始前向传播。从$t=1$到$t=\\tau$的每个时间步，我们应用以下更新方程：<br>$$<br>a^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b \\\\<br>h^{(t)}=tanh(a^{(t)}) \\\\<br>o^{(t)}=Vh^{(t)}+c \\\\<br>\\hat{y}^{(t)}=softmax(o^{(t)})<br>$$<br>$U, V, W$分别对应 input layers to hidden layers，hidden layers to output layers，hidden to next hidden layers的连接。该RNN将一个input sequence映射到相同长度的output sequence。<strong>与 $x$ 序列配对的 $y$ 的总Loss就是所有时间步的Loss之和</strong>。</p>\n<h3 id=\"Teacher-Forcing-and-Networks-with-Output-Recurrence\"><a href=\"#Teacher-Forcing-and-Networks-with-Output-Recurrence\" class=\"headerlink\" title=\"Teacher Forcing and Networks with Output Recurrence\"></a>Teacher Forcing and Networks with Output Recurrence</h3><p>由输出反馈到模型而产生循环连接的model可用teacher forcing进行训练。训练模型时，teacher forcing不再使用最大似然准则，而在时刻 $t+1$ 接收真实值 $y^{(t)}$ 作为输入。条件最大似然准则是：<br>$$<br>log p(y^{(1)}, y^{(2)}|x^{(1)},y^{(2)})=log p(y^{(2)}|y^{(1)},x^{(1)},y^{(2)})+log p(y^{(1)}|x^{(1)},y^{(2)})<br>$$</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/teacher-forcing.jpg\" alt=\"Teacher Forcing\"></p>\n<h3 id=\"Computing-the-Gradient-in-RNN\"><a href=\"#Computing-the-Gradient-in-RNN\" class=\"headerlink\" title=\"Computing the Gradient in RNN\"></a>Computing the Gradient in RNN</h3><p>以最终的Loss开始，计算：<br>$\\frac{\\partial L}{\\partial L^{(t)}}=1$</p>\n<p>对所有 $i,t$ 关于时间步 $t$ 输出的梯度 $\\triangledown_{o^{(t)}}L$ 如下：<br>$(\\triangledown_{o^{(t)}}L)_i=\\frac{\\partial L}{\\partial o_i^{(t)}}=\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_i^{(t)}}=\\hat{y}_i^{(t)}-\\textbf{1}_{i,y^{(t)}}$</p>\n<p>从序列的末尾开始，反向进行计算，在最后的时间步 $\\tau, h^{(\\tau)}$ 只有 $o^{(\\tau)}$ 作为后续结点，因此这个梯度计算很简单：<br>$\\triangledown_{h^{(\\tau)}}L=V^T\\triangledown_{o^{(\\tau)}}L$</p>\n<p>然后，我们从时刻$t=\\tau -1$到$t=1$反向迭代，通过时间反向传播梯度，注意 $h^{(t)} (t&lt;\\tau)$同时具有 $o^{(t)}$ 和 $h^{(t+1)}$两个后续结点。因此，它的梯度如下计算：<br>$$<br>\\triangledown_{h^{(t)}}L=(\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T(\\triangledown_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T(\\triangledown_{o^{(t)}}L)=W^T(\\triangledown_{h^{(t+1)}}L)diag(1-(h^{(t+1)})^2)+V^T(\\triangledown_{o^{(t)}}L)<br>$$</p>\n<p>剩下的参数梯度可以由下式给出：</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/dl-rnn/rnn-bp.jpg\" alt=\"RNN-BP\"></p>\n<h3 id=\"Recurrent-Networks-as-Directed-Graphical-Models\"><a href=\"#Recurrent-Networks-as-Directed-Graphical-Models\" class=\"headerlink\" title=\"Recurrent Networks as Directed Graphical Models\"></a>Recurrent Networks as Directed Graphical Models</h3>"},{"title":"[ML] Clustering","date":"2018-07-30T14:01:45.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Performance Metric\n聚类的性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考查聚类结果而不利用任何参考模型，称为“内部指标”。\n\n对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的簇划分为$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$，相应的，另$\\lambda$与$\\lambda^{\\star}$分别表示$\\mathcal{C}$与$\\mathcal{C}^{\\star}$对应的簇标记向量，我们将样本两两配对考虑，定义：\n$$\na=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nb=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\} \\\\\nc=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nd=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\}\n$$\n其中集合$SS$包含了在$\\mathcal{C}$中隶属于相同簇且在$\\mathcal{C}^{\\star}$中也隶属于相同簇的样本对，集合$SD$包含了在$\\mathcal{C}$中隶属于相同簇但在$\\mathcal{C}^{\\star}$中隶属于不同簇的样本对。易得：\n$$\na+b+c+d=\\frac{m(m-1)}{2}\n$$\n\n基于上式可得以下外部指标：\n* Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$\n* FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$\n* Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$\n\n考虑聚类结果的簇划分$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，定义：\n* $AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i<j \\leq |C|}} dist(x_i,x_j)$\n* $diam(C)=\\mathop{max} \\limits_{1\\leq i < j \\leq |C|} dist(x_i,x_j)$\n* $d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$\n* $d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$\n\n基于上式可以推导聚类的内部指标：\n* DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$\n* Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$\n\n## Distance Metric\n对于无序属性可采用VDM(Value Difference Metric)，令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示第$i$个样本在属性$u$上取值为$a$的样本数，$k$为样本数，则属性$u$上两个离散值$a$和$b$之间的VDM为：\n$$\nVDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p\n$$\n\n于是，将$L_P$ Distance和VDM结合可以处理混合属性。假定有$n_c$个有序属性、$n-n_c$个无序属性，则：\n$$\nMinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}\n$$\n当样本空间中不同属性的重要性不同时，可使用“加权距离”：\n$$\ndist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}\n$$\n\n## Prototype-based Clustering\n### KMeans\nKMeans通过最小MSE Loss来进行学习，\n$$\nE=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2\n$$\n其中$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$是簇$C_i$的均值向量。\n\n### Learning Vector Quantization\nLVQ假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来辅助聚类。在学得一组原型向量$\\{p_1,p_2,\\cdots,p_q\\}$后，即可实现对样本$\\chi$的簇划分，对任意样本$x$，它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量$p_i$定义了与之相关的一个区域$R_i$，该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i^{'}}(i^{'}\\neq i)$的距离，即：\n$$\nR_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{'}}||_2,i\\neq i^{'}\\}\n$$\n\n### Mixture of Gaussian\n与KMeans、LVQ采用原型向量来刻画聚类结构不同，Mixture-of-Gaussian采用概率模型来表达聚类原型。\n\n对$n$维样本空间$\\chi$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为：\n$$\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n$$\n$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵。由上式可知，高斯分布完全由均值向量$\\mu$和协方差矩阵$\\Sigma$确定。我们将概率密度函数记为$p(x|\\mu,\\Sigma)$。\n\n我们可定义高斯混合分布：\n$$\np_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)\n$$\n该分布共由$k$个混合分布组成，每个混合成分对应一个高斯分布。其中$\\mu_i$与$\\Sigma_i$是第$i$个高斯混合成分的参数，而$\\alpha_i>0$为相应的混合系数，$\\sum_{i=1}^k \\alpha_i=1$。\n\n假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$定义的先验分布选择高斯混合成分，其中$\\alpha_i$为选择的第$i$个混合成分的概率；然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。\n\n若training set $D=\\{x_1,x_2,\\cdots,x_m\\}$由上述过程生成，另随机变量$z_j\\in \\{1,2,\\cdots,k\\}$表示生成样本$x_j$的高斯混合成分。根据Bayesian Theorem，$z_j$的后验分布对应于：\n$$\np_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}\n$$\n$p_{\\mathcal{M}}(z_j=i|x_j)$给出了样本$x_j$由第$i$个高斯混合成分生成的后验概率，我们将其记作$\\gamma_{ji}$。\n\n当高斯混合分布已知，高斯混合聚类将把样本$D$划分为$k$个簇$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，每个样本$x_j$的簇标记如下确定：\n$$\n\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}\n$$\n\n## Density-based Clustering\n基于密度的聚类假设聚类结构能通过样本分布的紧密程度确定。DBSCAN是一种著名的密度聚类算法，它基于一组“领域”参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。给定数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，定义下面几个概念：\n* $\\epsilon-$邻域：对$x_j\\in D$，其$\\epsilon-$邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$；\n* 核心对象：若$x_j$的$\\epsilon-$邻域至少包含$MinPts$个样本，即$|N_{\\epsilon}(x_j)|\\geq MinPts$，则$x_j$是一个核心对象；\n* 密度直达：若$x_j$位于$x_i$的$\\epsilon-$邻域内，且$x_i$是核心对象，则称$x_j$由$x_i$密度直达；\n* 密度可达：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,\\cdots,p_n$，其中$p_1=x_i,p_2=x_j$，且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；\n* 密度相连：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连。\n\n基于以上概念，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。形式化的说，给定邻域参数$(\\epsilon,MinPts)$，簇$C\\subseteq D$是满足以下性质的非空样本子集：\n* 连接性：$x_i\\in C,x_j \\in C, \\implies x_i$与$x_j$密度相连\n* 最大性：$x_i\\in C, x_j$由$x_i$密度可达$\\implies$ $x_j \\in C$\n\n## Hierarchical Clustering\n它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数。\n\n## Tips\n聚类簇数$k$通常需由用户提供，可运行不同的$k$值后选取最佳的结果。","source":"_posts/ml-clustering.md","raw":"---\ntitle: \"[ML] Clustering\"\ndate: 2018-07-30 22:01:45\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Performance Metric\n聚类的性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考查聚类结果而不利用任何参考模型，称为“内部指标”。\n\n对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的簇划分为$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$，相应的，另$\\lambda$与$\\lambda^{\\star}$分别表示$\\mathcal{C}$与$\\mathcal{C}^{\\star}$对应的簇标记向量，我们将样本两两配对考虑，定义：\n$$\na=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nb=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\} \\\\\nc=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i<j\\} \\\\\nd=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i<j\\}\n$$\n其中集合$SS$包含了在$\\mathcal{C}$中隶属于相同簇且在$\\mathcal{C}^{\\star}$中也隶属于相同簇的样本对，集合$SD$包含了在$\\mathcal{C}$中隶属于相同簇但在$\\mathcal{C}^{\\star}$中隶属于不同簇的样本对。易得：\n$$\na+b+c+d=\\frac{m(m-1)}{2}\n$$\n\n基于上式可得以下外部指标：\n* Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$\n* FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$\n* Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$\n\n考虑聚类结果的簇划分$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，定义：\n* $AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i<j \\leq |C|}} dist(x_i,x_j)$\n* $diam(C)=\\mathop{max} \\limits_{1\\leq i < j \\leq |C|} dist(x_i,x_j)$\n* $d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$\n* $d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$\n\n基于上式可以推导聚类的内部指标：\n* DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$\n* Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$\n\n## Distance Metric\n对于无序属性可采用VDM(Value Difference Metric)，令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示第$i$个样本在属性$u$上取值为$a$的样本数，$k$为样本数，则属性$u$上两个离散值$a$和$b$之间的VDM为：\n$$\nVDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p\n$$\n\n于是，将$L_P$ Distance和VDM结合可以处理混合属性。假定有$n_c$个有序属性、$n-n_c$个无序属性，则：\n$$\nMinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}\n$$\n当样本空间中不同属性的重要性不同时，可使用“加权距离”：\n$$\ndist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}\n$$\n\n## Prototype-based Clustering\n### KMeans\nKMeans通过最小MSE Loss来进行学习，\n$$\nE=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2\n$$\n其中$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$是簇$C_i$的均值向量。\n\n### Learning Vector Quantization\nLVQ假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来辅助聚类。在学得一组原型向量$\\{p_1,p_2,\\cdots,p_q\\}$后，即可实现对样本$\\chi$的簇划分，对任意样本$x$，它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量$p_i$定义了与之相关的一个区域$R_i$，该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i^{'}}(i^{'}\\neq i)$的距离，即：\n$$\nR_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{'}}||_2,i\\neq i^{'}\\}\n$$\n\n### Mixture of Gaussian\n与KMeans、LVQ采用原型向量来刻画聚类结构不同，Mixture-of-Gaussian采用概率模型来表达聚类原型。\n\n对$n$维样本空间$\\chi$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为：\n$$\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n$$\n$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵。由上式可知，高斯分布完全由均值向量$\\mu$和协方差矩阵$\\Sigma$确定。我们将概率密度函数记为$p(x|\\mu,\\Sigma)$。\n\n我们可定义高斯混合分布：\n$$\np_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)\n$$\n该分布共由$k$个混合分布组成，每个混合成分对应一个高斯分布。其中$\\mu_i$与$\\Sigma_i$是第$i$个高斯混合成分的参数，而$\\alpha_i>0$为相应的混合系数，$\\sum_{i=1}^k \\alpha_i=1$。\n\n假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$定义的先验分布选择高斯混合成分，其中$\\alpha_i$为选择的第$i$个混合成分的概率；然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。\n\n若training set $D=\\{x_1,x_2,\\cdots,x_m\\}$由上述过程生成，另随机变量$z_j\\in \\{1,2,\\cdots,k\\}$表示生成样本$x_j$的高斯混合成分。根据Bayesian Theorem，$z_j$的后验分布对应于：\n$$\np_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}\n$$\n$p_{\\mathcal{M}}(z_j=i|x_j)$给出了样本$x_j$由第$i$个高斯混合成分生成的后验概率，我们将其记作$\\gamma_{ji}$。\n\n当高斯混合分布已知，高斯混合聚类将把样本$D$划分为$k$个簇$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，每个样本$x_j$的簇标记如下确定：\n$$\n\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}\n$$\n\n## Density-based Clustering\n基于密度的聚类假设聚类结构能通过样本分布的紧密程度确定。DBSCAN是一种著名的密度聚类算法，它基于一组“领域”参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。给定数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，定义下面几个概念：\n* $\\epsilon-$邻域：对$x_j\\in D$，其$\\epsilon-$邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$；\n* 核心对象：若$x_j$的$\\epsilon-$邻域至少包含$MinPts$个样本，即$|N_{\\epsilon}(x_j)|\\geq MinPts$，则$x_j$是一个核心对象；\n* 密度直达：若$x_j$位于$x_i$的$\\epsilon-$邻域内，且$x_i$是核心对象，则称$x_j$由$x_i$密度直达；\n* 密度可达：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,\\cdots,p_n$，其中$p_1=x_i,p_2=x_j$，且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；\n* 密度相连：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连。\n\n基于以上概念，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。形式化的说，给定邻域参数$(\\epsilon,MinPts)$，簇$C\\subseteq D$是满足以下性质的非空样本子集：\n* 连接性：$x_i\\in C,x_j \\in C, \\implies x_i$与$x_j$密度相连\n* 最大性：$x_i\\in C, x_j$由$x_i$密度可达$\\implies$ $x_j \\in C$\n\n## Hierarchical Clustering\n它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数。\n\n## Tips\n聚类簇数$k$通常需由用户提供，可运行不同的$k$值后选取最佳的结果。","slug":"ml-clustering","published":1,"updated":"2018-10-01T04:40:08.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cf000u608w5h4oygdy","content":"<h2 id=\"Performance-Metric\"><a href=\"#Performance-Metric\" class=\"headerlink\" title=\"Performance Metric\"></a>Performance Metric</h2><p>聚类的性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考查聚类结果而不利用任何参考模型，称为“内部指标”。</p>\n<p>对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的簇划分为$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$，相应的，另$\\lambda$与$\\lambda^{\\star}$分别表示$\\mathcal{C}$与$\\mathcal{C}^{\\star}$对应的簇标记向量，我们将样本两两配对考虑，定义：<br>$$<br>a=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>b=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\} \\\\<br>c=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>d=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\}<br>$$<br>其中集合$SS$包含了在$\\mathcal{C}$中隶属于相同簇且在$\\mathcal{C}^{\\star}$中也隶属于相同簇的样本对，集合$SD$包含了在$\\mathcal{C}$中隶属于相同簇但在$\\mathcal{C}^{\\star}$中隶属于不同簇的样本对。易得：<br>$$<br>a+b+c+d=\\frac{m(m-1)}{2}<br>$$</p>\n<p>基于上式可得以下外部指标：</p>\n<ul>\n<li>Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$</li>\n<li>FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$</li>\n<li>Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$</li>\n</ul>\n<p>考虑聚类结果的簇划分$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，定义：</p>\n<ul>\n<li>$AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i&lt;j \\leq |C|}} dist(x_i,x_j)$</li>\n<li>$diam(C)=\\mathop{max} \\limits_{1\\leq i &lt; j \\leq |C|} dist(x_i,x_j)$</li>\n<li>$d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$</li>\n<li>$d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$</li>\n</ul>\n<p>基于上式可以推导聚类的内部指标：</p>\n<ul>\n<li>DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$</li>\n<li>Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$</li>\n</ul>\n<h2 id=\"Distance-Metric\"><a href=\"#Distance-Metric\" class=\"headerlink\" title=\"Distance Metric\"></a>Distance Metric</h2><p>对于无序属性可采用VDM(Value Difference Metric)，令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示第$i$个样本在属性$u$上取值为$a$的样本数，$k$为样本数，则属性$u$上两个离散值$a$和$b$之间的VDM为：<br>$$<br>VDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p<br>$$</p>\n<p>于是，将$L_P$ Distance和VDM结合可以处理混合属性。假定有$n_c$个有序属性、$n-n_c$个无序属性，则：<br>$$<br>MinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}<br>$$<br>当样本空间中不同属性的重要性不同时，可使用“加权距离”：<br>$$<br>dist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}<br>$$</p>\n<h2 id=\"Prototype-based-Clustering\"><a href=\"#Prototype-based-Clustering\" class=\"headerlink\" title=\"Prototype-based Clustering\"></a>Prototype-based Clustering</h2><h3 id=\"KMeans\"><a href=\"#KMeans\" class=\"headerlink\" title=\"KMeans\"></a>KMeans</h3><p>KMeans通过最小MSE Loss来进行学习，<br>$$<br>E=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2<br>$$<br>其中$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$是簇$C_i$的均值向量。</p>\n<h3 id=\"Learning-Vector-Quantization\"><a href=\"#Learning-Vector-Quantization\" class=\"headerlink\" title=\"Learning Vector Quantization\"></a>Learning Vector Quantization</h3><p>LVQ假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来辅助聚类。在学得一组原型向量$\\{p_1,p_2,\\cdots,p_q\\}$后，即可实现对样本$\\chi$的簇划分，对任意样本$x$，它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量$p_i$定义了与之相关的一个区域$R_i$，该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i^{‘}}(i^{‘}\\neq i)$的距离，即：<br>$$<br>R_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{‘}}||_2,i\\neq i^{‘}\\}<br>$$</p>\n<h3 id=\"Mixture-of-Gaussian\"><a href=\"#Mixture-of-Gaussian\" class=\"headerlink\" title=\"Mixture of Gaussian\"></a>Mixture of Gaussian</h3><p>与KMeans、LVQ采用原型向量来刻画聚类结构不同，Mixture-of-Gaussian采用概率模型来表达聚类原型。</p>\n<p>对$n$维样本空间$\\chi$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为：<br>$$<br>p(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}<br>$$<br>$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵。由上式可知，高斯分布完全由均值向量$\\mu$和协方差矩阵$\\Sigma$确定。我们将概率密度函数记为$p(x|\\mu,\\Sigma)$。</p>\n<p>我们可定义高斯混合分布：<br>$$<br>p_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)<br>$$<br>该分布共由$k$个混合分布组成，每个混合成分对应一个高斯分布。其中$\\mu_i$与$\\Sigma_i$是第$i$个高斯混合成分的参数，而$\\alpha_i&gt;0$为相应的混合系数，$\\sum_{i=1}^k \\alpha_i=1$。</p>\n<p>假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$定义的先验分布选择高斯混合成分，其中$\\alpha_i$为选择的第$i$个混合成分的概率；然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p>\n<p>若training set $D=\\{x_1,x_2,\\cdots,x_m\\}$由上述过程生成，另随机变量$z_j\\in \\{1,2,\\cdots,k\\}$表示生成样本$x_j$的高斯混合成分。根据Bayesian Theorem，$z_j$的后验分布对应于：<br>$$<br>p_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}<br>$$<br>$p_{\\mathcal{M}}(z_j=i|x_j)$给出了样本$x_j$由第$i$个高斯混合成分生成的后验概率，我们将其记作$\\gamma_{ji}$。</p>\n<p>当高斯混合分布已知，高斯混合聚类将把样本$D$划分为$k$个簇$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，每个样本$x_j$的簇标记如下确定：<br>$$<br>\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}<br>$$</p>\n<h2 id=\"Density-based-Clustering\"><a href=\"#Density-based-Clustering\" class=\"headerlink\" title=\"Density-based Clustering\"></a>Density-based Clustering</h2><p>基于密度的聚类假设聚类结构能通过样本分布的紧密程度确定。DBSCAN是一种著名的密度聚类算法，它基于一组“领域”参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。给定数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，定义下面几个概念：</p>\n<ul>\n<li>$\\epsilon-$邻域：对$x_j\\in D$，其$\\epsilon-$邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$；</li>\n<li>核心对象：若$x_j$的$\\epsilon-$邻域至少包含$MinPts$个样本，即$|N_{\\epsilon}(x_j)|\\geq MinPts$，则$x_j$是一个核心对象；</li>\n<li>密度直达：若$x_j$位于$x_i$的$\\epsilon-$邻域内，且$x_i$是核心对象，则称$x_j$由$x_i$密度直达；</li>\n<li>密度可达：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,\\cdots,p_n$，其中$p_1=x_i,p_2=x_j$，且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；</li>\n<li>密度相连：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连。</li>\n</ul>\n<p>基于以上概念，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。形式化的说，给定邻域参数$(\\epsilon,MinPts)$，簇$C\\subseteq D$是满足以下性质的非空样本子集：</p>\n<ul>\n<li>连接性：$x_i\\in C,x_j \\in C, \\implies x_i$与$x_j$密度相连</li>\n<li>最大性：$x_i\\in C, x_j$由$x_i$密度可达$\\implies$ $x_j \\in C$</li>\n</ul>\n<h2 id=\"Hierarchical-Clustering\"><a href=\"#Hierarchical-Clustering\" class=\"headerlink\" title=\"Hierarchical Clustering\"></a>Hierarchical Clustering</h2><p>它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数。</p>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><p>聚类簇数$k$通常需由用户提供，可运行不同的$k$值后选取最佳的结果。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Performance-Metric\"><a href=\"#Performance-Metric\" class=\"headerlink\" title=\"Performance Metric\"></a>Performance Metric</h2><p>聚类的性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考查聚类结果而不利用任何参考模型，称为“内部指标”。</p>\n<p>对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的簇划分为$\\mathcal{C}^{\\star}=\\{C^{\\star}_1,C^{\\star}_2,\\cdots,C^{\\star}_s\\}$，相应的，另$\\lambda$与$\\lambda^{\\star}$分别表示$\\mathcal{C}$与$\\mathcal{C}^{\\star}$对应的簇标记向量，我们将样本两两配对考虑，定义：<br>$$<br>a=|SS|, SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>b=|SD|, SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\} \\\\<br>c=|DS|, DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}=\\lambda_j^{\\star},i&lt;j\\} \\\\<br>d=|DD|, DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{\\star}\\neq\\lambda_j^{\\star},i&lt;j\\}<br>$$<br>其中集合$SS$包含了在$\\mathcal{C}$中隶属于相同簇且在$\\mathcal{C}^{\\star}$中也隶属于相同簇的样本对，集合$SD$包含了在$\\mathcal{C}$中隶属于相同簇但在$\\mathcal{C}^{\\star}$中隶属于不同簇的样本对。易得：<br>$$<br>a+b+c+d=\\frac{m(m-1)}{2}<br>$$</p>\n<p>基于上式可得以下外部指标：</p>\n<ul>\n<li>Jaccard Coefficient: $JC=\\frac{a}{a+b+c}$</li>\n<li>FM Index: $FM=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$</li>\n<li>Rand Index: $RI=\\frac{2(a+d)}{m(m-1)}$</li>\n</ul>\n<p>考虑聚类结果的簇划分$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，定义：</p>\n<ul>\n<li>$AVG(C)=\\frac{2}{|C|(|C|-1)\\sum_{1\\leq i&lt;j \\leq |C|}} dist(x_i,x_j)$</li>\n<li>$diam(C)=\\mathop{max} \\limits_{1\\leq i &lt; j \\leq |C|} dist(x_i,x_j)$</li>\n<li>$d_{min}(C_i,C_j)=\\mathop{min} \\limits_{x_i\\in C_i,x_j\\in C_j} dist(x_i,x_j)$</li>\n<li>$d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$</li>\n</ul>\n<p>基于上式可以推导聚类的内部指标：</p>\n<ul>\n<li>DB Index: $DBI=\\frac{1}{k}\\sum_{i=1}^k \\mathop{max} \\limits_{j\\neq i} \\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}$</li>\n<li>Dunn Index: $DI=\\mathop{min} \\limits_{1\\leq i \\leq k} \\{\\mathop{min} \\limits_{j\\neq i} (\\frac{d_{min}(C_i,C_j)}{\\mathop{max} \\limits_{1\\leq l \\leq k}diam(C_l)})\\}$</li>\n</ul>\n<h2 id=\"Distance-Metric\"><a href=\"#Distance-Metric\" class=\"headerlink\" title=\"Distance Metric\"></a>Distance Metric</h2><p>对于无序属性可采用VDM(Value Difference Metric)，令$m_{u,a}$表示在属性$u$上取值为$a$的样本数，$m_{u,a,i}$表示第$i$个样本在属性$u$上取值为$a$的样本数，$k$为样本数，则属性$u$上两个离散值$a$和$b$之间的VDM为：<br>$$<br>VDM_p(a,b)=\\sum_{i=1}^k |\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p<br>$$</p>\n<p>于是，将$L_P$ Distance和VDM结合可以处理混合属性。假定有$n_c$个有序属性、$n-n_c$个无序属性，则：<br>$$<br>MinkovDM_p(x_i,x_j)=\\big(\\sum_{u=1}^{u_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju})\\big)^{\\frac{1}{p}}<br>$$<br>当样本空间中不同属性的重要性不同时，可使用“加权距离”：<br>$$<br>dist_{wmk}(x_i,x_j)=\\big(w_1\\cdot|x_{i1}-x_{j1}|^p + \\cdots + w_n\\cdot|x_{in}-x_{jn}|^p \\big)^{\\frac{1}{p}}<br>$$</p>\n<h2 id=\"Prototype-based-Clustering\"><a href=\"#Prototype-based-Clustering\" class=\"headerlink\" title=\"Prototype-based Clustering\"></a>Prototype-based Clustering</h2><h3 id=\"KMeans\"><a href=\"#KMeans\" class=\"headerlink\" title=\"KMeans\"></a>KMeans</h3><p>KMeans通过最小MSE Loss来进行学习，<br>$$<br>E=\\sum_{i=1}^k \\sum_{x\\in C_i}||x-\\mu_i||_2^2<br>$$<br>其中$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$是簇$C_i$的均值向量。</p>\n<h3 id=\"Learning-Vector-Quantization\"><a href=\"#Learning-Vector-Quantization\" class=\"headerlink\" title=\"Learning Vector Quantization\"></a>Learning Vector Quantization</h3><p>LVQ假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来辅助聚类。在学得一组原型向量$\\{p_1,p_2,\\cdots,p_q\\}$后，即可实现对样本$\\chi$的簇划分，对任意样本$x$，它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量$p_i$定义了与之相关的一个区域$R_i$，该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i^{‘}}(i^{‘}\\neq i)$的距离，即：<br>$$<br>R_i=\\{x\\in \\chi| ||x-p_i||_2\\leq ||x-p_{i^{‘}}||_2,i\\neq i^{‘}\\}<br>$$</p>\n<h3 id=\"Mixture-of-Gaussian\"><a href=\"#Mixture-of-Gaussian\" class=\"headerlink\" title=\"Mixture of Gaussian\"></a>Mixture of Gaussian</h3><p>与KMeans、LVQ采用原型向量来刻画聚类结构不同，Mixture-of-Gaussian采用概率模型来表达聚类原型。</p>\n<p>对$n$维样本空间$\\chi$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为：<br>$$<br>p(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}<br>$$<br>$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵。由上式可知，高斯分布完全由均值向量$\\mu$和协方差矩阵$\\Sigma$确定。我们将概率密度函数记为$p(x|\\mu,\\Sigma)$。</p>\n<p>我们可定义高斯混合分布：<br>$$<br>p_{\\mathcal{M}}(x)=\\sum_{i=1}^k \\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)<br>$$<br>该分布共由$k$个混合分布组成，每个混合成分对应一个高斯分布。其中$\\mu_i$与$\\Sigma_i$是第$i$个高斯混合成分的参数，而$\\alpha_i&gt;0$为相应的混合系数，$\\sum_{i=1}^k \\alpha_i=1$。</p>\n<p>假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,\\cdots,\\alpha_k$定义的先验分布选择高斯混合成分，其中$\\alpha_i$为选择的第$i$个混合成分的概率；然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p>\n<p>若training set $D=\\{x_1,x_2,\\cdots,x_m\\}$由上述过程生成，另随机变量$z_j\\in \\{1,2,\\cdots,k\\}$表示生成样本$x_j$的高斯混合成分。根据Bayesian Theorem，$z_j$的后验分布对应于：<br>$$<br>p_{\\mathcal{M}}(z_j=i|x_j)=\\frac{P(z_j=i)\\cdot p_{\\mathcal{M}}(x_j|z_j=i)}{p_{\\mathcal{M}}(x_j)}=\\frac{\\alpha_i \\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k \\alpha_l \\cdot p(x_j|\\mu_l,\\Sigma_l)}<br>$$<br>$p_{\\mathcal{M}}(z_j=i|x_j)$给出了样本$x_j$由第$i$个高斯混合成分生成的后验概率，我们将其记作$\\gamma_{ji}$。</p>\n<p>当高斯混合分布已知，高斯混合聚类将把样本$D$划分为$k$个簇$\\mathcal{C}=\\{C_1,C_2,\\cdots,C_k\\}$，每个样本$x_j$的簇标记如下确定：<br>$$<br>\\lambda_j=\\mathop{argmax} \\limits_{i\\in \\{1,2,\\cdots,k\\}} \\gamma_{ji}<br>$$</p>\n<h2 id=\"Density-based-Clustering\"><a href=\"#Density-based-Clustering\" class=\"headerlink\" title=\"Density-based Clustering\"></a>Density-based Clustering</h2><p>基于密度的聚类假设聚类结构能通过样本分布的紧密程度确定。DBSCAN是一种著名的密度聚类算法，它基于一组“领域”参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。给定数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，定义下面几个概念：</p>\n<ul>\n<li>$\\epsilon-$邻域：对$x_j\\in D$，其$\\epsilon-$邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$；</li>\n<li>核心对象：若$x_j$的$\\epsilon-$邻域至少包含$MinPts$个样本，即$|N_{\\epsilon}(x_j)|\\geq MinPts$，则$x_j$是一个核心对象；</li>\n<li>密度直达：若$x_j$位于$x_i$的$\\epsilon-$邻域内，且$x_i$是核心对象，则称$x_j$由$x_i$密度直达；</li>\n<li>密度可达：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,\\cdots,p_n$，其中$p_1=x_i,p_2=x_j$，且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；</li>\n<li>密度相连：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连。</li>\n</ul>\n<p>基于以上概念，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。形式化的说，给定邻域参数$(\\epsilon,MinPts)$，簇$C\\subseteq D$是满足以下性质的非空样本子集：</p>\n<ul>\n<li>连接性：$x_i\\in C,x_j \\in C, \\implies x_i$与$x_j$密度相连</li>\n<li>最大性：$x_i\\in C, x_j$由$x_i$密度可达$\\implies$ $x_j \\in C$</li>\n</ul>\n<h2 id=\"Hierarchical-Clustering\"><a href=\"#Hierarchical-Clustering\" class=\"headerlink\" title=\"Hierarchical Clustering\"></a>Hierarchical Clustering</h2><p>它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数。</p>\n<h2 id=\"Tips\"><a href=\"#Tips\" class=\"headerlink\" title=\"Tips\"></a>Tips</h2><p>聚类簇数$k$通常需由用户提供，可运行不同的$k$值后选取最佳的结果。</p>\n"},{"title":"[ML] Dimension Reduction and Metric Learning","date":"2018-08-20T14:02:55.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Low-Dimension Embedding\n在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有ML算法共同面临的严重障碍，被称为curse of dimensionality。\n\n**Why dimension reduction works?** \n在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅仅是某个低维分布，即高维空间中的一个低维Embedding。\n\n### Multiple Dimensional Scaling\nMDS算法的目标是获得样本在$d^{'}$维空间的表示$Z\\in \\mathbb{R}^{d^{'}\\times m},d^{'}\\leq d$，且任意两个样本在$d^{'}$维空间中的欧氏距离等于原始空间中的距离，即$||z_i - z_j||=dist_{ij}$。\n\n令$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有\n$$\ndist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j\n$$\n\n令降维后的样本$Z$被中心化，即$\\sum_{i=1}^mz_i=0$。显然，矩阵$B$的行和列之和均为0，易知：\n$$\n\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\\n\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\\n\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)\n$$\n\n其中$tr(B)=\\sum_{i=1}^m||z_i||^2$，令：\n$$\ndist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\\n$$\n\n可得：\n$$\nb_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)\n$$\n\n由此即可通过降维前后保持不变的距离$D$求取内积矩阵$B$。\n\n对矩阵$B$做特征值分解，$B=V\\bigwedge V^T$，其中$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$为特征值构成的对角矩阵，$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$，$V$为特征向量矩阵。假定其中有$d^{\\star}$个非零特征值，它们构成对角矩阵$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$，另$V_{\\star}$表示相应的特征向量矩阵，则$Z$可表达为：\n$$\nZ=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}\n$$\n\n在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离 **尽可能相近**，而不必严格相等。此时可取$d^{'}\\ll d$个最大特征值构成对角矩阵$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{'}})$，另$\\tilde{V}$表示相应的特征向量矩阵，则$Z$可表达为：\n$$\nZ=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{'}\\times m}\n$$\n\n## PCA\n假定数据进行了中心化，即$\\sum_i x_i=0$；再假定投影变换后得到的新坐标系为$\\{w_1,w_2,\\cdots,w_d\\}$，其中$w_i$是标准正交基向量，$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$。若丢弃新坐标系中的部分坐标，即将维度降低到$d^{'}< d$，则样本点$x_i$在低维坐标系中的投影是$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{'}})$，其中$z_{ij}=w_j^Tx_i$是$x_i$在低维坐标系下第$j$维的坐标。若基于$z_i$来重构$x_i$，则会得到$\\hat{x}_i=\\sum_{j=1}^{d^{'}z_{ij}w_j}$。\n\n$$\n\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n样本点$x_i$在新空间中超平面上的投影是$W^Tx_i$，若所有样本点的投影能尽可能分开，则应使 **投影后样本点的方差最大**。\n\n即：\n$$\n\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n对上式使用拉格朗日乘子法可得：\n$$\nXX^TW=\\lambda W\n$$\n\n于是，只需对协方差矩阵$XX^T$进行特征值分解，将求得的特征值排序：$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$，再取前$d^{'}$个特征值对应的特征向量构成$W=(w_1,w_2,\\cdots,w_{d^{'}})$。这就是PCA的解。\n\n降维会导致$d-d^{'}$个特征值的特征向量被舍弃了，这样带来的好处是：一方面，舍弃这部分信息之后能使样本采样密度增大，这正是降维的重要动机；另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的作用。\n\n## Kernel Linear Dimension Reduction\n线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。\n\n非线性降维的一种常见方法是基于Kernel Tricks(读者若对SVM熟悉，此处自然不会陌生了，SVM在对线性不可分的情况下也是利用Kernel Tricks将原始空间映射到高维超平面再基于软间隔最大化去分类)对线性降维方法进行“核化”。\n\n以Kernel PCA为例，假定我们将在高维空间中把数据投影到由$W$确定的超平面上，即PCA欲求解：\n$$\n(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W\n$$\n\n其中$z_i$是样本点$x_i$在高维特征空间中的像，易知：\n$$\nW=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i\n$$\n其中$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$，假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i=\\phi(x_i),i=1,2,\\cdots,m$。若$\\phi$能被显式表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。有：\n$$\n(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W\n$$\n\n$$\nW=\\sum_{i=1}^m \\phi(x_i) \\alpha_i\n$$\n\n一般情况下，我们不清楚$\\phi$的具体形式，于是引入核函数：\n$$\n\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)\n$$\n\n化简可得：\n$$\nKA=\\lambda A\n$$\n其中$K$为$\\mathcal{k}$对应的核矩阵，$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$。显然，上式是特征值分解问题，取$K$最大的$d^{'}$个特征值对应的特征向量即可。\n\n对新样本$x$，其投影后的第$j (j=1,2,\\cdots,d^{'})$维坐标为：\n$$\nz_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)\n$$\n其中$\\alpha_i$已经过规范化，$\\alpha_i^j$是$\\alpha_i$的第$j$个分量。\n\n## Manifold Learning\nManifold是在局部与欧式空间同胚的空间，换言之，<font color=\"red\">它在局部具有欧式空间的性质，能用欧氏距离来进行距离计算</font>。这给降维带来了很大的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在<font color=\"red\">局部上仍具有欧式空间的性质</font>。因此可以容易地在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。\n\n### Isometric Mapping\nIsomap认为低维流形嵌入到高维空间后，直接在高维空间计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的。\n\n我们可利用<font color=\"red\">流行在局部上与欧式空间同胚</font>这个性质，对每一个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。于是，计算两点之间测地线距离问题就转换为计算<font color=\"red\">近邻连接图上两点之间的最短路径</font>问题。\n\n对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离最近的$k$个点为近邻点，这样得到的近邻图称为$k$近邻图；另一种是指定距离阈值$\\epsilon$，距离小于$\\epsilon$的点被认为是近邻点，这样得到的近邻图称为$\\epsilon$近邻图。\n\n### Locally Linear Embedding (LLE)\n与Isomap试图保持近邻样本之间的距离不同，<font color=\"red\">LLE试图保持邻域内样本之间的线性关系</font>。假定样本点$x_i$的坐标能通过它的近邻样本$x_j,x_k,x_l$的坐标通过线性组合而重构出来，即：\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE先为每个样本$x_i$找到其近邻下标集合$Q_i$，然后计算出基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$:\n$$\n\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1\n$$\n其中$x_i$和$x_j$均为已知，令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$w_{ij}$有close-form solution：\n$$\nw_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}\n$$\nLLE在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解:\n$$\n\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2\n$$\n\n令$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{'}\\times m}, (W)_{ij}=w_{ij}$，\n$$\nM=(I-W)^T(I-W)\n$$\n则有：\n$$\n\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I\n$$\n可通过特征值分解求解：$M$最小的$d^{'}$个特征值对应的特征向量组成的矩阵即为$Z^T$。\n\n## Metric Learning\nMachine Learning中，对高维数据进行降维的目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实际上就是字寻找一个合适的距离度量。那么为何不直接尝试学习出一个合适的Distance Metric呢？这就是Metric Learning的idea。\n\n对两个$d$维样本$x_i$和$x_j$，它们之间的$L_2$ Distance可表示为：\n$$\ndist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2\n$$\n其中$dist_{ij,k}^2$表示$x_i$与$x_j$在第$k$维上的距离。若假定不同属性的重要性不同，则可引入权重$w$，得到：\n$$\ndist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)\n$$\n其中$w_i\\geq 0, W=diag(w)$是一个对角矩阵，$(W)_{ii}=w_i$。\n\n我们不仅可以将Error Rate这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。例如，若已知某些样本相似、另一些样本不相似，则可定义must-link约束集合$\\mathcal{M}$和cannot-link约束集合$\\mathcal{C}$。$(x_i,x_j)\\in \\mathcal{M}$表示$x_i$与$x_j$相似，$(x_i,x_k)\\in \\mathcal{C}$表示$x_i$与$x_k$不相似。显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大。于是可通过求解下面的凸优化问题获得适当的度量矩阵：\n$$\n\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0\n$$\n$M\\succeq 0$表明M必须是半正定的。上式要求在不相似样本间的距离不小于1的前提下，使相似样本间的距离尽可能小。\n\n<font color=\"red\">若$M$是一个低秩矩阵，则通过对$M$进行特征分解，总能找到一组正交基，其正交基数目为矩阵$M$的秩$rank(M)$，小于原属性数$d$。于是，Metric Learning学得的结果可衍生出一个降维矩阵$P\\in \\mathbb{R}^{d\\times rank(M)}$，能用于降维的目的</font>。","source":"_posts/ml-dimen-red-metric-learning.md","raw":"---\ntitle: \"[ML] Dimension Reduction and Metric Learning\"\ndate: 2018-08-20 22:02:55\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Low-Dimension Embedding\n在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有ML算法共同面临的严重障碍，被称为curse of dimensionality。\n\n**Why dimension reduction works?** \n在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅仅是某个低维分布，即高维空间中的一个低维Embedding。\n\n### Multiple Dimensional Scaling\nMDS算法的目标是获得样本在$d^{'}$维空间的表示$Z\\in \\mathbb{R}^{d^{'}\\times m},d^{'}\\leq d$，且任意两个样本在$d^{'}$维空间中的欧氏距离等于原始空间中的距离，即$||z_i - z_j||=dist_{ij}$。\n\n令$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有\n$$\ndist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j\n$$\n\n令降维后的样本$Z$被中心化，即$\\sum_{i=1}^mz_i=0$。显然，矩阵$B$的行和列之和均为0，易知：\n$$\n\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\\n\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\\n\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)\n$$\n\n其中$tr(B)=\\sum_{i=1}^m||z_i||^2$，令：\n$$\ndist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\\ndist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\\n$$\n\n可得：\n$$\nb_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)\n$$\n\n由此即可通过降维前后保持不变的距离$D$求取内积矩阵$B$。\n\n对矩阵$B$做特征值分解，$B=V\\bigwedge V^T$，其中$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$为特征值构成的对角矩阵，$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$，$V$为特征向量矩阵。假定其中有$d^{\\star}$个非零特征值，它们构成对角矩阵$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$，另$V_{\\star}$表示相应的特征向量矩阵，则$Z$可表达为：\n$$\nZ=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}\n$$\n\n在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离 **尽可能相近**，而不必严格相等。此时可取$d^{'}\\ll d$个最大特征值构成对角矩阵$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{'}})$，另$\\tilde{V}$表示相应的特征向量矩阵，则$Z$可表达为：\n$$\nZ=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{'}\\times m}\n$$\n\n## PCA\n假定数据进行了中心化，即$\\sum_i x_i=0$；再假定投影变换后得到的新坐标系为$\\{w_1,w_2,\\cdots,w_d\\}$，其中$w_i$是标准正交基向量，$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$。若丢弃新坐标系中的部分坐标，即将维度降低到$d^{'}< d$，则样本点$x_i$在低维坐标系中的投影是$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{'}})$，其中$z_{ij}=w_j^Tx_i$是$x_i$在低维坐标系下第$j$维的坐标。若基于$z_i$来重构$x_i$，则会得到$\\hat{x}_i=\\sum_{j=1}^{d^{'}z_{ij}w_j}$。\n\n$$\n\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n样本点$x_i$在新空间中超平面上的投影是$W^Tx_i$，若所有样本点的投影能尽可能分开，则应使 **投影后样本点的方差最大**。\n\n即：\n$$\n\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I\n$$\n\n对上式使用拉格朗日乘子法可得：\n$$\nXX^TW=\\lambda W\n$$\n\n于是，只需对协方差矩阵$XX^T$进行特征值分解，将求得的特征值排序：$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$，再取前$d^{'}$个特征值对应的特征向量构成$W=(w_1,w_2,\\cdots,w_{d^{'}})$。这就是PCA的解。\n\n降维会导致$d-d^{'}$个特征值的特征向量被舍弃了，这样带来的好处是：一方面，舍弃这部分信息之后能使样本采样密度增大，这正是降维的重要动机；另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的作用。\n\n## Kernel Linear Dimension Reduction\n线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。\n\n非线性降维的一种常见方法是基于Kernel Tricks(读者若对SVM熟悉，此处自然不会陌生了，SVM在对线性不可分的情况下也是利用Kernel Tricks将原始空间映射到高维超平面再基于软间隔最大化去分类)对线性降维方法进行“核化”。\n\n以Kernel PCA为例，假定我们将在高维空间中把数据投影到由$W$确定的超平面上，即PCA欲求解：\n$$\n(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W\n$$\n\n其中$z_i$是样本点$x_i$在高维特征空间中的像，易知：\n$$\nW=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i\n$$\n其中$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$，假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i=\\phi(x_i),i=1,2,\\cdots,m$。若$\\phi$能被显式表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。有：\n$$\n(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W\n$$\n\n$$\nW=\\sum_{i=1}^m \\phi(x_i) \\alpha_i\n$$\n\n一般情况下，我们不清楚$\\phi$的具体形式，于是引入核函数：\n$$\n\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)\n$$\n\n化简可得：\n$$\nKA=\\lambda A\n$$\n其中$K$为$\\mathcal{k}$对应的核矩阵，$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$。显然，上式是特征值分解问题，取$K$最大的$d^{'}$个特征值对应的特征向量即可。\n\n对新样本$x$，其投影后的第$j (j=1,2,\\cdots,d^{'})$维坐标为：\n$$\nz_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)\n$$\n其中$\\alpha_i$已经过规范化，$\\alpha_i^j$是$\\alpha_i$的第$j$个分量。\n\n## Manifold Learning\nManifold是在局部与欧式空间同胚的空间，换言之，<font color=\"red\">它在局部具有欧式空间的性质，能用欧氏距离来进行距离计算</font>。这给降维带来了很大的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在<font color=\"red\">局部上仍具有欧式空间的性质</font>。因此可以容易地在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。\n\n### Isometric Mapping\nIsomap认为低维流形嵌入到高维空间后，直接在高维空间计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的。\n\n我们可利用<font color=\"red\">流行在局部上与欧式空间同胚</font>这个性质，对每一个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。于是，计算两点之间测地线距离问题就转换为计算<font color=\"red\">近邻连接图上两点之间的最短路径</font>问题。\n\n对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离最近的$k$个点为近邻点，这样得到的近邻图称为$k$近邻图；另一种是指定距离阈值$\\epsilon$，距离小于$\\epsilon$的点被认为是近邻点，这样得到的近邻图称为$\\epsilon$近邻图。\n\n### Locally Linear Embedding (LLE)\n与Isomap试图保持近邻样本之间的距离不同，<font color=\"red\">LLE试图保持邻域内样本之间的线性关系</font>。假定样本点$x_i$的坐标能通过它的近邻样本$x_j,x_k,x_l$的坐标通过线性组合而重构出来，即：\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE先为每个样本$x_i$找到其近邻下标集合$Q_i$，然后计算出基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$:\n$$\n\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1\n$$\n其中$x_i$和$x_j$均为已知，令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$w_{ij}$有close-form solution：\n$$\nw_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}\n$$\nLLE在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解:\n$$\n\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2\n$$\n\n令$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{'}\\times m}, (W)_{ij}=w_{ij}$，\n$$\nM=(I-W)^T(I-W)\n$$\n则有：\n$$\n\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I\n$$\n可通过特征值分解求解：$M$最小的$d^{'}$个特征值对应的特征向量组成的矩阵即为$Z^T$。\n\n## Metric Learning\nMachine Learning中，对高维数据进行降维的目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实际上就是字寻找一个合适的距离度量。那么为何不直接尝试学习出一个合适的Distance Metric呢？这就是Metric Learning的idea。\n\n对两个$d$维样本$x_i$和$x_j$，它们之间的$L_2$ Distance可表示为：\n$$\ndist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2\n$$\n其中$dist_{ij,k}^2$表示$x_i$与$x_j$在第$k$维上的距离。若假定不同属性的重要性不同，则可引入权重$w$，得到：\n$$\ndist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)\n$$\n其中$w_i\\geq 0, W=diag(w)$是一个对角矩阵，$(W)_{ii}=w_i$。\n\n我们不仅可以将Error Rate这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。例如，若已知某些样本相似、另一些样本不相似，则可定义must-link约束集合$\\mathcal{M}$和cannot-link约束集合$\\mathcal{C}$。$(x_i,x_j)\\in \\mathcal{M}$表示$x_i$与$x_j$相似，$(x_i,x_k)\\in \\mathcal{C}$表示$x_i$与$x_k$不相似。显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大。于是可通过求解下面的凸优化问题获得适当的度量矩阵：\n$$\n\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0\n$$\n$M\\succeq 0$表明M必须是半正定的。上式要求在不相似样本间的距离不小于1的前提下，使相似样本间的距离尽可能小。\n\n<font color=\"red\">若$M$是一个低秩矩阵，则通过对$M$进行特征分解，总能找到一组正交基，其正交基数目为矩阵$M$的秩$rank(M)$，小于原属性数$d$。于是，Metric Learning学得的结果可衍生出一个降维矩阵$P\\in \\mathbb{R}^{d\\times rank(M)}$，能用于降维的目的</font>。","slug":"ml-dimen-red-metric-learning","published":1,"updated":"2018-10-01T04:40:08.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cf000v608wanrj1kpm","content":"<h2 id=\"Low-Dimension-Embedding\"><a href=\"#Low-Dimension-Embedding\" class=\"headerlink\" title=\"Low-Dimension Embedding\"></a>Low-Dimension Embedding</h2><p>在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有ML算法共同面临的严重障碍，被称为curse of dimensionality。</p>\n<p><strong>Why dimension reduction works?</strong><br>在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅仅是某个低维分布，即高维空间中的一个低维Embedding。</p>\n<h3 id=\"Multiple-Dimensional-Scaling\"><a href=\"#Multiple-Dimensional-Scaling\" class=\"headerlink\" title=\"Multiple Dimensional Scaling\"></a>Multiple Dimensional Scaling</h3><p>MDS算法的目标是获得样本在$d^{‘}$维空间的表示$Z\\in \\mathbb{R}^{d^{‘}\\times m},d^{‘}\\leq d$，且任意两个样本在$d^{‘}$维空间中的欧氏距离等于原始空间中的距离，即$||z_i - z_j||=dist_{ij}$。</p>\n<p>令$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有<br>$$<br>dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j<br>$$</p>\n<p>令降维后的样本$Z$被中心化，即$\\sum_{i=1}^mz_i=0$。显然，矩阵$B$的行和列之和均为0，易知：<br>$$<br>\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\<br>\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\<br>\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)<br>$$</p>\n<p>其中$tr(B)=\\sum_{i=1}^m||z_i||^2$，令：<br>$$<br>dist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\<br>$$</p>\n<p>可得：<br>$$<br>b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)<br>$$</p>\n<p>由此即可通过降维前后保持不变的距离$D$求取内积矩阵$B$。</p>\n<p>对矩阵$B$做特征值分解，$B=V\\bigwedge V^T$，其中$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$为特征值构成的对角矩阵，$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$，$V$为特征向量矩阵。假定其中有$d^{\\star}$个非零特征值，它们构成对角矩阵$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$，另$V_{\\star}$表示相应的特征向量矩阵，则$Z$可表达为：<br>$$<br>Z=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}<br>$$</p>\n<p>在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离 <strong>尽可能相近</strong>，而不必严格相等。此时可取$d^{‘}\\ll d$个最大特征值构成对角矩阵$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{‘}})$，另$\\tilde{V}$表示相应的特征向量矩阵，则$Z$可表达为：<br>$$<br>Z=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{‘}\\times m}<br>$$</p>\n<h2 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h2><p>假定数据进行了中心化，即$\\sum_i x_i=0$；再假定投影变换后得到的新坐标系为$\\{w_1,w_2,\\cdots,w_d\\}$，其中$w_i$是标准正交基向量，$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$。若丢弃新坐标系中的部分坐标，即将维度降低到$d^{‘}&lt; d$，则样本点$x_i$在低维坐标系中的投影是$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{‘}})$，其中$z_{ij}=w_j^Tx_i$是$x_i$在低维坐标系下第$j$维的坐标。若基于$z_i$来重构$x_i$，则会得到$\\hat{x}_i=\\sum_{j=1}^{d^{‘}z_{ij}w_j}$。</p>\n<p>$$<br>\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>样本点$x_i$在新空间中超平面上的投影是$W^Tx_i$，若所有样本点的投影能尽可能分开，则应使 <strong>投影后样本点的方差最大</strong>。</p>\n<p>即：<br>$$<br>\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>对上式使用拉格朗日乘子法可得：<br>$$<br>XX^TW=\\lambda W<br>$$</p>\n<p>于是，只需对协方差矩阵$XX^T$进行特征值分解，将求得的特征值排序：$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$，再取前$d^{‘}$个特征值对应的特征向量构成$W=(w_1,w_2,\\cdots,w_{d^{‘}})$。这就是PCA的解。</p>\n<p>降维会导致$d-d^{‘}$个特征值的特征向量被舍弃了，这样带来的好处是：一方面，舍弃这部分信息之后能使样本采样密度增大，这正是降维的重要动机；另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的作用。</p>\n<h2 id=\"Kernel-Linear-Dimension-Reduction\"><a href=\"#Kernel-Linear-Dimension-Reduction\" class=\"headerlink\" title=\"Kernel Linear Dimension Reduction\"></a>Kernel Linear Dimension Reduction</h2><p>线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。</p>\n<p>非线性降维的一种常见方法是基于Kernel Tricks(读者若对SVM熟悉，此处自然不会陌生了，SVM在对线性不可分的情况下也是利用Kernel Tricks将原始空间映射到高维超平面再基于软间隔最大化去分类)对线性降维方法进行“核化”。</p>\n<p>以Kernel PCA为例，假定我们将在高维空间中把数据投影到由$W$确定的超平面上，即PCA欲求解：<br>$$<br>(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W<br>$$</p>\n<p>其中$z_i$是样本点$x_i$在高维特征空间中的像，易知：<br>$$<br>W=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i<br>$$<br>其中$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$，假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i=\\phi(x_i),i=1,2,\\cdots,m$。若$\\phi$能被显式表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。有：<br>$$<br>(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W<br>$$</p>\n<p>$$<br>W=\\sum_{i=1}^m \\phi(x_i) \\alpha_i<br>$$</p>\n<p>一般情况下，我们不清楚$\\phi$的具体形式，于是引入核函数：<br>$$<br>\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)<br>$$</p>\n<p>化简可得：<br>$$<br>KA=\\lambda A<br>$$<br>其中$K$为$\\mathcal{k}$对应的核矩阵，$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$。显然，上式是特征值分解问题，取$K$最大的$d^{‘}$个特征值对应的特征向量即可。</p>\n<p>对新样本$x$，其投影后的第$j (j=1,2,\\cdots,d^{‘})$维坐标为：<br>$$<br>z_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)<br>$$<br>其中$\\alpha_i$已经过规范化，$\\alpha_i^j$是$\\alpha_i$的第$j$个分量。</p>\n<h2 id=\"Manifold-Learning\"><a href=\"#Manifold-Learning\" class=\"headerlink\" title=\"Manifold Learning\"></a>Manifold Learning</h2><p>Manifold是在局部与欧式空间同胚的空间，换言之，<font color=\"red\">它在局部具有欧式空间的性质，能用欧氏距离来进行距离计算</font>。这给降维带来了很大的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在<font color=\"red\">局部上仍具有欧式空间的性质</font>。因此可以容易地在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。</p>\n<h3 id=\"Isometric-Mapping\"><a href=\"#Isometric-Mapping\" class=\"headerlink\" title=\"Isometric Mapping\"></a>Isometric Mapping</h3><p>Isomap认为低维流形嵌入到高维空间后，直接在高维空间计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的。</p>\n<p>我们可利用<font color=\"red\">流行在局部上与欧式空间同胚</font>这个性质，对每一个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。于是，计算两点之间测地线距离问题就转换为计算<font color=\"red\">近邻连接图上两点之间的最短路径</font>问题。</p>\n<p>对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离最近的$k$个点为近邻点，这样得到的近邻图称为$k$近邻图；另一种是指定距离阈值$\\epsilon$，距离小于$\\epsilon$的点被认为是近邻点，这样得到的近邻图称为$\\epsilon$近邻图。</p>\n<h3 id=\"Locally-Linear-Embedding-LLE\"><a href=\"#Locally-Linear-Embedding-LLE\" class=\"headerlink\" title=\"Locally Linear Embedding (LLE)\"></a>Locally Linear Embedding (LLE)</h3><p>与Isomap试图保持近邻样本之间的距离不同，<font color=\"red\">LLE试图保持邻域内样本之间的线性关系</font>。假定样本点$x_i$的坐标能通过它的近邻样本$x_j,x_k,x_l$的坐标通过线性组合而重构出来，即：<br>$$<br>x_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l<br>$$<br>LLE先为每个样本$x_i$找到其近邻下标集合$Q_i$，然后计算出基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$:<br>$$<br>\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1<br>$$<br>其中$x_i$和$x_j$均为已知，令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$w_{ij}$有close-form solution：<br>$$<br>w_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}<br>$$<br>LLE在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解:<br>$$<br>\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2<br>$$</p>\n<p>令$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{‘}\\times m}, (W)_{ij}=w_{ij}$，<br>$$<br>M=(I-W)^T(I-W)<br>$$<br>则有：<br>$$<br>\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I<br>$$<br>可通过特征值分解求解：$M$最小的$d^{‘}$个特征值对应的特征向量组成的矩阵即为$Z^T$。</p>\n<h2 id=\"Metric-Learning\"><a href=\"#Metric-Learning\" class=\"headerlink\" title=\"Metric Learning\"></a>Metric Learning</h2><p>Machine Learning中，对高维数据进行降维的目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实际上就是字寻找一个合适的距离度量。那么为何不直接尝试学习出一个合适的Distance Metric呢？这就是Metric Learning的idea。</p>\n<p>对两个$d$维样本$x_i$和$x_j$，它们之间的$L_2$ Distance可表示为：<br>$$<br>dist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2<br>$$<br>其中$dist_{ij,k}^2$表示$x_i$与$x_j$在第$k$维上的距离。若假定不同属性的重要性不同，则可引入权重$w$，得到：<br>$$<br>dist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)<br>$$<br>其中$w_i\\geq 0, W=diag(w)$是一个对角矩阵，$(W)_{ii}=w_i$。</p>\n<p>我们不仅可以将Error Rate这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。例如，若已知某些样本相似、另一些样本不相似，则可定义must-link约束集合$\\mathcal{M}$和cannot-link约束集合$\\mathcal{C}$。$(x_i,x_j)\\in \\mathcal{M}$表示$x_i$与$x_j$相似，$(x_i,x_k)\\in \\mathcal{C}$表示$x_i$与$x_k$不相似。显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大。于是可通过求解下面的凸优化问题获得适当的度量矩阵：<br>$$<br>\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0<br>$$<br>$M\\succeq 0$表明M必须是半正定的。上式要求在不相似样本间的距离不小于1的前提下，使相似样本间的距离尽可能小。</p>\n<p><font color=\"red\">若$M$是一个低秩矩阵，则通过对$M$进行特征分解，总能找到一组正交基，其正交基数目为矩阵$M$的秩$rank(M)$，小于原属性数$d$。于是，Metric Learning学得的结果可衍生出一个降维矩阵$P\\in \\mathbb{R}^{d\\times rank(M)}$，能用于降维的目的</font>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Low-Dimension-Embedding\"><a href=\"#Low-Dimension-Embedding\" class=\"headerlink\" title=\"Low-Dimension Embedding\"></a>Low-Dimension Embedding</h2><p>在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有ML算法共同面临的严重障碍，被称为curse of dimensionality。</p>\n<p><strong>Why dimension reduction works?</strong><br>在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅仅是某个低维分布，即高维空间中的一个低维Embedding。</p>\n<h3 id=\"Multiple-Dimensional-Scaling\"><a href=\"#Multiple-Dimensional-Scaling\" class=\"headerlink\" title=\"Multiple Dimensional Scaling\"></a>Multiple Dimensional Scaling</h3><p>MDS算法的目标是获得样本在$d^{‘}$维空间的表示$Z\\in \\mathbb{R}^{d^{‘}\\times m},d^{‘}\\leq d$，且任意两个样本在$d^{‘}$维空间中的欧氏距离等于原始空间中的距离，即$||z_i - z_j||=dist_{ij}$。</p>\n<p>令$B=Z^TZ\\in \\mathbb{R}^{m\\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有<br>$$<br>dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_ib_j<br>$$</p>\n<p>令降维后的样本$Z$被中心化，即$\\sum_{i=1}^mz_i=0$。显然，矩阵$B$的行和列之和均为0，易知：<br>$$<br>\\sum_{i=1}^m dist_{ij}^2=tr(B)+mb_{jj} \\\\<br>\\sum_{j=1}^m dist_{ij}^2=tr(B)+mb_{ii} \\\\<br>\\sum_{i=1}^m\\sum_{j=1}^m dist_{ij}^2=2tr(B)<br>$$</p>\n<p>其中$tr(B)=\\sum_{i=1}^m||z_i||^2$，令：<br>$$<br>dist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot j}^2=\\frac{1}{m}\\sum_{i=1}^m dist_{ij}^2 \\\\<br>dist_{\\cdot \\cdot}^2=\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2 \\\\<br>$$</p>\n<p>可得：<br>$$<br>b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}^2-dist_{\\cdot j}^2+dist_{\\cdot \\cdot}^2)<br>$$</p>\n<p>由此即可通过降维前后保持不变的距离$D$求取内积矩阵$B$。</p>\n<p>对矩阵$B$做特征值分解，$B=V\\bigwedge V^T$，其中$\\bigwedge=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d)$为特征值构成的对角矩阵，$\\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$，$V$为特征向量矩阵。假定其中有$d^{\\star}$个非零特征值，它们构成对角矩阵$\\bigwedge_{\\star}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_{d^{\\star}})$，另$V_{\\star}$表示相应的特征向量矩阵，则$Z$可表达为：<br>$$<br>Z=\\bigwedge_{\\star}^{1/2}V_{\\star}^T\\in \\mathbb{R}^{d^{\\star}\\times m}<br>$$</p>\n<p>在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离 <strong>尽可能相近</strong>，而不必严格相等。此时可取$d^{‘}\\ll d$个最大特征值构成对角矩阵$\\tilde{\\bigwedge}=diag(\\lambda_1,\\cdots,\\lambda_{d^{‘}})$，另$\\tilde{V}$表示相应的特征向量矩阵，则$Z$可表达为：<br>$$<br>Z=\\tilde{\\bigwedge}^{1/2}\\tilde{V}^T\\in \\mathbb{R}^{d^{‘}\\times m}<br>$$</p>\n<h2 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h2><p>假定数据进行了中心化，即$\\sum_i x_i=0$；再假定投影变换后得到的新坐标系为$\\{w_1,w_2,\\cdots,w_d\\}$，其中$w_i$是标准正交基向量，$||w_i||_2=1,w_i^Tw_j=0 (i\\neq j)$。若丢弃新坐标系中的部分坐标，即将维度降低到$d^{‘}&lt; d$，则样本点$x_i$在低维坐标系中的投影是$z_i=(z_{i1},z_{i2},\\cdots,z_{id^{‘}})$，其中$z_{ij}=w_j^Tx_i$是$x_i$在低维坐标系下第$j$维的坐标。若基于$z_i$来重构$x_i$，则会得到$\\hat{x}_i=\\sum_{j=1}^{d^{‘}z_{ij}w_j}$。</p>\n<p>$$<br>\\min \\limits_{W} -tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>样本点$x_i$在新空间中超平面上的投影是$W^Tx_i$，若所有样本点的投影能尽可能分开，则应使 <strong>投影后样本点的方差最大</strong>。</p>\n<p>即：<br>$$<br>\\max \\limits_{W} tr(W^TXX^TW) \\quad s.t. W^TW=I<br>$$</p>\n<p>对上式使用拉格朗日乘子法可得：<br>$$<br>XX^TW=\\lambda W<br>$$</p>\n<p>于是，只需对协方差矩阵$XX^T$进行特征值分解，将求得的特征值排序：$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_d$，再取前$d^{‘}$个特征值对应的特征向量构成$W=(w_1,w_2,\\cdots,w_{d^{‘}})$。这就是PCA的解。</p>\n<p>降维会导致$d-d^{‘}$个特征值的特征向量被舍弃了，这样带来的好处是：一方面，舍弃这部分信息之后能使样本采样密度增大，这正是降维的重要动机；另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的作用。</p>\n<h2 id=\"Kernel-Linear-Dimension-Reduction\"><a href=\"#Kernel-Linear-Dimension-Reduction\" class=\"headerlink\" title=\"Kernel Linear Dimension Reduction\"></a>Kernel Linear Dimension Reduction</h2><p>线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。</p>\n<p>非线性降维的一种常见方法是基于Kernel Tricks(读者若对SVM熟悉，此处自然不会陌生了，SVM在对线性不可分的情况下也是利用Kernel Tricks将原始空间映射到高维超平面再基于软间隔最大化去分类)对线性降维方法进行“核化”。</p>\n<p>以Kernel PCA为例，假定我们将在高维空间中把数据投影到由$W$确定的超平面上，即PCA欲求解：<br>$$<br>(\\sum_{i=1}^m z_iz_i^T)W=\\lambda W<br>$$</p>\n<p>其中$z_i$是样本点$x_i$在高维特征空间中的像，易知：<br>$$<br>W=\\frac{1}{\\lambda}(\\sum_{i=1}^m z_iz_i^T)W=\\sum_{i=1}^m z_i \\frac{z_i^T W}{\\lambda}=\\sum_{i=1}^m z_i \\alpha_i<br>$$<br>其中$\\alpha_i=\\frac{1}{\\lambda}z_i^TW$，假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i=\\phi(x_i),i=1,2,\\cdots,m$。若$\\phi$能被显式表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。有：<br>$$<br>(\\sum_{i=1}^m \\phi(x_i)\\phi(x_i)^T)W=\\lambda W<br>$$</p>\n<p>$$<br>W=\\sum_{i=1}^m \\phi(x_i) \\alpha_i<br>$$</p>\n<p>一般情况下，我们不清楚$\\phi$的具体形式，于是引入核函数：<br>$$<br>\\mathcal{k}(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)<br>$$</p>\n<p>化简可得：<br>$$<br>KA=\\lambda A<br>$$<br>其中$K$为$\\mathcal{k}$对应的核矩阵，$(K)_{ij}=\\mathcal{k}(x_i,x_j), A=(\\alpha_1;\\alpha_2,\\cdots,\\alpha_m)$。显然，上式是特征值分解问题，取$K$最大的$d^{‘}$个特征值对应的特征向量即可。</p>\n<p>对新样本$x$，其投影后的第$j (j=1,2,\\cdots,d^{‘})$维坐标为：<br>$$<br>z_j=w_j^T\\phi(x)=\\sum_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum_{i=1}^m \\alpha_i^j \\mathcal{k}(x_i, x)<br>$$<br>其中$\\alpha_i$已经过规范化，$\\alpha_i^j$是$\\alpha_i$的第$j$个分量。</p>\n<h2 id=\"Manifold-Learning\"><a href=\"#Manifold-Learning\" class=\"headerlink\" title=\"Manifold Learning\"></a>Manifold Learning</h2><p>Manifold是在局部与欧式空间同胚的空间，换言之，<font color=\"red\">它在局部具有欧式空间的性质，能用欧氏距离来进行距离计算</font>。这给降维带来了很大的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在<font color=\"red\">局部上仍具有欧式空间的性质</font>。因此可以容易地在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。</p>\n<h3 id=\"Isometric-Mapping\"><a href=\"#Isometric-Mapping\" class=\"headerlink\" title=\"Isometric Mapping\"></a>Isometric Mapping</h3><p>Isomap认为低维流形嵌入到高维空间后，直接在高维空间计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的。</p>\n<p>我们可利用<font color=\"red\">流行在局部上与欧式空间同胚</font>这个性质，对每一个点基于欧式距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。于是，计算两点之间测地线距离问题就转换为计算<font color=\"red\">近邻连接图上两点之间的最短路径</font>问题。</p>\n<p>对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离最近的$k$个点为近邻点，这样得到的近邻图称为$k$近邻图；另一种是指定距离阈值$\\epsilon$，距离小于$\\epsilon$的点被认为是近邻点，这样得到的近邻图称为$\\epsilon$近邻图。</p>\n<h3 id=\"Locally-Linear-Embedding-LLE\"><a href=\"#Locally-Linear-Embedding-LLE\" class=\"headerlink\" title=\"Locally Linear Embedding (LLE)\"></a>Locally Linear Embedding (LLE)</h3><p>与Isomap试图保持近邻样本之间的距离不同，<font color=\"red\">LLE试图保持邻域内样本之间的线性关系</font>。假定样本点$x_i$的坐标能通过它的近邻样本$x_j,x_k,x_l$的坐标通过线性组合而重构出来，即：<br>$$<br>x_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l<br>$$<br>LLE先为每个样本$x_i$找到其近邻下标集合$Q_i$，然后计算出基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$:<br>$$<br>\\mathop{min} \\limits_{w_1,w_2,\\cdots,w_m} \\sum_{i=1}^m ||x_i - \\sum_{j\\in Q_i} w_{ij}x_j||^2_2 \\quad \\sum_{j\\in Q_i}w_{ij}=1<br>$$<br>其中$x_i$和$x_j$均为已知，令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$w_{ij}$有close-form solution：<br>$$<br>w_{ij}=\\frac{\\sum_{k\\in Q_i}C_{jk}^{-1}}{\\sum_{l,s\\in Q_i}C_{ls}^{-1}}<br>$$<br>LLE在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解:<br>$$<br>\\mathop{min} \\limits_{z_1,z_2,\\cdots,z_m} \\sum_{i=1}^m ||z_i-\\sum_{j\\in Q_i}w_{ij}z_j||_2^2<br>$$</p>\n<p>令$Z=(z_1,z_2,\\cdots,z_m)\\in \\mathbb{R}^{d^{‘}\\times m}, (W)_{ij}=w_{ij}$，<br>$$<br>M=(I-W)^T(I-W)<br>$$<br>则有：<br>$$<br>\\mathop{min} \\limits_{Z} tr(ZMZ^T) \\quad s.t. \\quad ZZ^T=I<br>$$<br>可通过特征值分解求解：$M$最小的$d^{‘}$个特征值对应的特征向量组成的矩阵即为$Z^T$。</p>\n<h2 id=\"Metric-Learning\"><a href=\"#Metric-Learning\" class=\"headerlink\" title=\"Metric Learning\"></a>Metric Learning</h2><p>Machine Learning中，对高维数据进行降维的目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实际上就是字寻找一个合适的距离度量。那么为何不直接尝试学习出一个合适的Distance Metric呢？这就是Metric Learning的idea。</p>\n<p>对两个$d$维样本$x_i$和$x_j$，它们之间的$L_2$ Distance可表示为：<br>$$<br>dist_{ed}^2(x_i,x_j)=||x_i - x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2<br>$$<br>其中$dist_{ij,k}^2$表示$x_i$与$x_j$在第$k$维上的距离。若假定不同属性的重要性不同，则可引入权重$w$，得到：<br>$$<br>dist_{wed}^2(x_i,x_j)=||x_i - x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)<br>$$<br>其中$w_i\\geq 0, W=diag(w)$是一个对角矩阵，$(W)_{ii}=w_i$。</p>\n<p>我们不仅可以将Error Rate这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。例如，若已知某些样本相似、另一些样本不相似，则可定义must-link约束集合$\\mathcal{M}$和cannot-link约束集合$\\mathcal{C}$。$(x_i,x_j)\\in \\mathcal{M}$表示$x_i$与$x_j$相似，$(x_i,x_k)\\in \\mathcal{C}$表示$x_i$与$x_k$不相似。显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大。于是可通过求解下面的凸优化问题获得适当的度量矩阵：<br>$$<br>\\mathop{min} \\limits_{M} \\quad \\sum_{(x_i,x_j)\\in \\mathcal{M}} ||x_i-x_j||_M^2 \\quad s.t. \\quad \\sum_{(x_i,x_k)\\in \\mathcal{C}}||x_i-x_k||_M^2\\geq 1,M\\succeq 0<br>$$<br>$M\\succeq 0$表明M必须是半正定的。上式要求在不相似样本间的距离不小于1的前提下，使相似样本间的距离尽可能小。</p>\n<p><font color=\"red\">若$M$是一个低秩矩阵，则通过对$M$进行特征分解，总能找到一组正交基，其正交基数目为矩阵$M$的秩$rank(M)$，小于原属性数$d$。于是，Metric Learning学得的结果可衍生出一个降维矩阵$P\\in \\mathbb{R}^{d\\times rank(M)}$，能用于降维的目的</font>。</p>\n"},{"title":"[ML] Decision Tree","date":"2018-07-24T02:02:19.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。\n\n## 决策树学习\n__决策树学习的Loss Function通常是正则化的极大似然函数__，决策树学习的策略是以Loss Function最小化为目标函数的最小化。\n\n决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。\n\n### 特征选择\n* Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：  \n  $P(X=x_i)=p_i$\n    \n  则随机变量$X$的Entropy定义为：  \n  $H(X)=-\\sum_{i=1}^n p_ilogp_i$\n\n  由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：  \n  $H(p)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy越大，随机变量的不确定性就越大，$0\\leq H(p)\\leq logn$\n\n  当随机变量只取两个值，即$X$的分布为：  \n  $P(X=1)=p, P(X=0)=1-p$  \n  Entropy为：  \n  $H(p)=-plog_2p-(1-p)log_2(1-p)$\n\n* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：  \n  $H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$\n\n* 信息增益表示 __得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度__：  \n  定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：  \n  $g(D,A)=H(D)-H(D|A)$  \n\n  一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，__决策树学习中的信息增益等价于dataset中类与特征的互信息__。\n\n决策树学习应用 __信息增益__ 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。\n\n根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。\n\n#### 信息增益算法\n设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\\{a_1,a_2,\\cdots,a_n\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：\n1. 计算数据集$D$的经验熵$H(D)$：  \n   $H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$\n\n2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：  \n   $H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$\n\n3. 计算信息增益：  \n   $g(D,A)=H(D)-H(D|A)$\n\n以信息增益作为划分训练数据集的特征，存在 __偏向于选择取值较多的特征的问题__。使用信息增益比可以对这一问题进行校正。\n\n* 信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：  \n  $g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$  \n  其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。\n\n## 决策树的生成\n### ID3算法\nID3算法的核心是在决策树各个结点上应用 __信息增益__ 准则进行特征选择，递归地构建决策树。具体做法是：从根节点开始对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。__ID3相当于用极大似然法进行概率模型的选择__。\n\n1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；\n2. 若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；\n3. 否则，计算$A$中各个特征对$D$的信息增益，选择 __信息增益__ 最大的特征$A_g$；\n4. 若$A_g$的信息增益小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；\n5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n6. 对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。\n\nID3算法只有树的生成，所以该算法生成的树容易过拟合。\n\n### C4.5\nC4.5算法在生成的过程中，用 __信息增益比__ 来选择特征。\n\n1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；\n2. 若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；\n3. 否则，计算$A$中各个特征对$D$的 __信息增益比__，选择 __信息增益比__ 最大的特征$A_g$；\n4. 若$A_g$的信息增益比小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；\n5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n6. 对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。\n\n## 决策树的剪枝\n决策树的剪枝往往通过极小化决策树整体的Loss Function来实现，设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\\alpha\\geq 0$为参数，则决策树学习的Loss Function可以定义为：  \n$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$  \n其中经验熵为：  \n$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$  \n在Loss Function中，将式子右端的第一项记作：  \n$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$  \n这时有：  \n$C_{\\alpha}(T)=C(T)+\\alpha|T|$\n\n$C(T)$表示模型对训练数据的预测误差，即模型对训练数据的拟合程度；$|T|$表示模型复杂度，参数$\\alpha\\geq 0$控制两者之间的影响。较大的$\\alpha$促使选择较简单的模型，较小的$\\alpha$促使选择较复杂的模型。\n\n可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化Loss Function还考虑了减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体模型。\n\nLoss Function的极小化等价于正则化的极大似然估计，所以，利用Loss Function最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。\n\n### 剪枝算法\n1. 计算每个节点的经验熵\n2. 递归地从树的叶节点向上回缩：  \n   设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的Loss Function值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果 $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$，则进行剪枝，即将父节点变为新的叶节点。\n3. 返回2，直到不能继续为止，得到Loss Function最小的子树$T_{\\alpha}$。\n\n## CART算法\nCART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定条件下输出的条件概率分布。\n\nCART算法由以下两步组成：  \n1. 决策树生成：基于training set生成决策树，生成的决策树要尽量大；\n2. 决策树剪枝：用validation set对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准\n\n### CART生成\n决策树的生成就是递归地构建二叉决策树的过程，__对回归树用MSE最小化准则，对分类树用Gini Index最小化__ 准则，进行特征选择，生成二叉树。\n\n#### 回归树的生成\n假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可以表示为：  \n$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$\n\n当输入空间的划分确定时，可以用MSE来表示回归树对于training set的预测误差，用MSE Loss最小化准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：  \n$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$\n\n可以采用启发式的方法对输入空间进行划分：选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：  \n$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ 和 $R_2(j,s)=\\{x|x^{(j)}> s\\}$  \n然后寻找最优切分变量$j$和切分点$s$，具体的，求解：  \n$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n\n对固定输入变量$j$可以找到最优切分点$s$：  \n$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$ 和 $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$\n\n遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$，依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树，成为最小二乘回归树。\n\n##### 最小二乘回归树生成算法\n1. 选择最优切分变量$j$和切分点$s$，求解：  \n   $\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n   \n   遍历$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。\n\n2. 用选定的对$(j,s)$划分区域并决定相应的输出值：  \n   $R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$， $R_2(j,s)=\\{x|x^{(j)}> s\\}$\n\n   $\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$\n\n3. 继续对两个子区域调用步骤1和2，直至满足停止条件。\n\n4. 将输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$，生成决策树：  \n   $f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$\n\n#### 分类树的生成\n分类树用Gini Index选择最优特征，同时决定该特征的最优二值切分点。\n\n* Gini Index: 分类问题中，假设有$K$个类，样本点属于第$k$类的概率是$p_k$，则概率分布的Gini Index定义为：  \n  $Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$\n\n  对于二分类问题，若样本点属于第1类的概率是$p$，则概率分布的Gini Index为：  \n  $Gini(p)=2p(1-p)$\n  \n  对于给定的样本集合$D$，其Gini Index为：  \n  $Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$\n\n  这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。\n\n  如果样本集合$D$根据$A$是否取某一可能值$a$被分成$D_1$和$D_2$两部分，即：  \n  $D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$\n\n  则在特征$A$的条件下，集合$D$的Gini Index定义为：  \n  $Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$\n\n  Gini Index表示集合$D$的不确定性，Gini Index $G(D,A)$表示经$A=a$分割后集合$D$的不确定性，Gini Index越大，样本集合的不确定性也就越大，这一点与Entropy相似。\n\n##### CART生成算法\n1. 设结点的training set为$D$，计算现有特征对该数据集的Gini Index，此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或者“否”将$D$分割成$D_1$和$D_2$两部分，并计算$A=a$时的Gini Index。\n2. 在所有可能的特征$A$及它们所有可能的切分点$a$中，选择Gini Index最小的特征及其对应的切分点作为最优特征与最优切分点。从该结点生成两个子结点，将training set依特征分配到两个子结点中去。\n3. 对两个子结点递归调用1和2，直至满足停止条件。\n4. 生成CART决策树。\n\n算法停止的条件是结点中的样本个数小于指定阈值，或样本集的Gini Index小于阈值(样本基本属于同一类)，或者没有更多特征。\n\n#### CART剪枝\nCART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,\\cdots,T_n\\}$然后通过交叉验证在validation set上进行测试，从中选择最优子树。\n\n1. 剪枝，形成一个子树序列：  \n   在剪枝过程中，计算子树的Loss Function：  \n   $C_{\\alpha}(T)=C(T)+\\alpha |T|$  \n   其中，$T$为任意子树，$C(T)$为对training set的预测误差(如Gini Index)，$|T|$为子树的叶结点个数，$\\alpha \\geq 0$为参数。\n\n   可以用递归的方法对树进行剪枝，将$\\alpha$从小增大，$0=\\alpha_0 < \\alpha_1 < \\alpha_2 < \\cdots < \\alpha_n < +\\infty$，产生一系列的区间$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$；剪枝得到的子树序列对应着区间$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$的最优子树序列$\\{T_0,T_1,\\cdots,T_n\\}$，序列中的子树是嵌套的。\n\n   具体的，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的Loss Function是：  \n   $C_{\\alpha}=C(t)+\\alpha$\n\n   以$t$为根结点的子树$T_t$的Loss Function是：  \n   $C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$\n\n   当$\\alpha=0$及$\\alpha$充分小时，有不等式：  \n   $C_{\\alpha}(T_t)<C_{\\alpha}(t)$\n\n   当$\\alpha$增大时，在某一$\\alpha$有：  \n   $C_{\\alpha}(T_t)=C_{\\alpha}(t)$\n\n   当$\\alpha$再增大时，不等式反向，只要$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的Loss Function值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。\n\n   为此，对$T_0$中每一内部结点$t$，计算：  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$  \n   它表示剪枝后整体Loss减少的程度，在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\\alpha_1$，$T_1$为区间$[\\alpha_1,\\alpha_2)$的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断增加$\\alpha$的值，产生新的区间。\n\n2. 在剪枝得到的子树序列$T_0,T_1,\\cdots,T_n$中通过交叉验证选取最优子树$T_{\\alpha}$\n   利用validation set测试子树序列$T_1,\\cdots,T_n$中各棵子树的MSE或Gini Index，值最小的为最优决策树。每棵子树$T_1,\\cdots,T_n$都对应于一个参数$\\alpha_1,\\cdots,\\alpha_n$。所以当最优子树$T_k$确定时，对应的$\\alpha_k$也确定了，即得到最优决策树了。\n\n##### CART剪枝算法\n1. 设$k=0,T=T_0$\n2. 设$\\alpha=+\\infty$\n3. 自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$  \n   $T_t$表示以$t$为根结点的子树，$C(T_t)$是对training set的预测误差，$|T_t|$是$T_t$的叶结点个数\n4. 自上而下地访问内部结点$t$，若有$g(t)=\\alpha$，进行剪枝，并对叶结点$t$以多数表决法决定类别，得到树$T$。\n5. 设$k=k+1,\\alpha_k=\\alpha,T_k=T$\n6. 若$T$不是由根结点单独构成的树，则返回到步骤4\n7. 采用cross validation在子树序列中选取最优子树\n\n\n\n\n\n","source":"_posts/ml-dt.md","raw":"---\ntitle: \"[ML] Decision Tree\"\ndate: 2018-07-24 10:02:19\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。\n\n## 决策树学习\n__决策树学习的Loss Function通常是正则化的极大似然函数__，决策树学习的策略是以Loss Function最小化为目标函数的最小化。\n\n决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。\n\n### 特征选择\n* Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：  \n  $P(X=x_i)=p_i$\n    \n  则随机变量$X$的Entropy定义为：  \n  $H(X)=-\\sum_{i=1}^n p_ilogp_i$\n\n  由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：  \n  $H(p)=-\\sum_{i=1}^n p_ilogp_i$\n\n  Entropy越大，随机变量的不确定性就越大，$0\\leq H(p)\\leq logn$\n\n  当随机变量只取两个值，即$X$的分布为：  \n  $P(X=1)=p, P(X=0)=1-p$  \n  Entropy为：  \n  $H(p)=-plog_2p-(1-p)log_2(1-p)$\n\n* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：  \n  $H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$\n\n* 信息增益表示 __得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度__：  \n  定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：  \n  $g(D,A)=H(D)-H(D|A)$  \n\n  一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，__决策树学习中的信息增益等价于dataset中类与特征的互信息__。\n\n决策树学习应用 __信息增益__ 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。\n\n根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。\n\n#### 信息增益算法\n设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\\{a_1,a_2,\\cdots,a_n\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：\n1. 计算数据集$D$的经验熵$H(D)$：  \n   $H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$\n\n2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：  \n   $H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$\n\n3. 计算信息增益：  \n   $g(D,A)=H(D)-H(D|A)$\n\n以信息增益作为划分训练数据集的特征，存在 __偏向于选择取值较多的特征的问题__。使用信息增益比可以对这一问题进行校正。\n\n* 信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：  \n  $g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$  \n  其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。\n\n## 决策树的生成\n### ID3算法\nID3算法的核心是在决策树各个结点上应用 __信息增益__ 准则进行特征选择，递归地构建决策树。具体做法是：从根节点开始对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。__ID3相当于用极大似然法进行概率模型的选择__。\n\n1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；\n2. 若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；\n3. 否则，计算$A$中各个特征对$D$的信息增益，选择 __信息增益__ 最大的特征$A_g$；\n4. 若$A_g$的信息增益小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；\n5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n6. 对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。\n\nID3算法只有树的生成，所以该算法生成的树容易过拟合。\n\n### C4.5\nC4.5算法在生成的过程中，用 __信息增益比__ 来选择特征。\n\n1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；\n2. 若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；\n3. 否则，计算$A$中各个特征对$D$的 __信息增益比__，选择 __信息增益比__ 最大的特征$A_g$；\n4. 若$A_g$的信息增益比小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；\n5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n6. 对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。\n\n## 决策树的剪枝\n决策树的剪枝往往通过极小化决策树整体的Loss Function来实现，设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\\alpha\\geq 0$为参数，则决策树学习的Loss Function可以定义为：  \n$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$  \n其中经验熵为：  \n$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$  \n在Loss Function中，将式子右端的第一项记作：  \n$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$  \n这时有：  \n$C_{\\alpha}(T)=C(T)+\\alpha|T|$\n\n$C(T)$表示模型对训练数据的预测误差，即模型对训练数据的拟合程度；$|T|$表示模型复杂度，参数$\\alpha\\geq 0$控制两者之间的影响。较大的$\\alpha$促使选择较简单的模型，较小的$\\alpha$促使选择较复杂的模型。\n\n可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化Loss Function还考虑了减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体模型。\n\nLoss Function的极小化等价于正则化的极大似然估计，所以，利用Loss Function最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。\n\n### 剪枝算法\n1. 计算每个节点的经验熵\n2. 递归地从树的叶节点向上回缩：  \n   设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的Loss Function值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果 $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$，则进行剪枝，即将父节点变为新的叶节点。\n3. 返回2，直到不能继续为止，得到Loss Function最小的子树$T_{\\alpha}$。\n\n## CART算法\nCART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定条件下输出的条件概率分布。\n\nCART算法由以下两步组成：  \n1. 决策树生成：基于training set生成决策树，生成的决策树要尽量大；\n2. 决策树剪枝：用validation set对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准\n\n### CART生成\n决策树的生成就是递归地构建二叉决策树的过程，__对回归树用MSE最小化准则，对分类树用Gini Index最小化__ 准则，进行特征选择，生成二叉树。\n\n#### 回归树的生成\n假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可以表示为：  \n$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$\n\n当输入空间的划分确定时，可以用MSE来表示回归树对于training set的预测误差，用MSE Loss最小化准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：  \n$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$\n\n可以采用启发式的方法对输入空间进行划分：选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：  \n$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ 和 $R_2(j,s)=\\{x|x^{(j)}> s\\}$  \n然后寻找最优切分变量$j$和切分点$s$，具体的，求解：  \n$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n\n对固定输入变量$j$可以找到最优切分点$s$：  \n$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$ 和 $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$\n\n遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$，依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树，成为最小二乘回归树。\n\n##### 最小二乘回归树生成算法\n1. 选择最优切分变量$j$和切分点$s$，求解：  \n   $\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n   \n   遍历$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。\n\n2. 用选定的对$(j,s)$划分区域并决定相应的输出值：  \n   $R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$， $R_2(j,s)=\\{x|x^{(j)}> s\\}$\n\n   $\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$\n\n3. 继续对两个子区域调用步骤1和2，直至满足停止条件。\n\n4. 将输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$，生成决策树：  \n   $f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$\n\n#### 分类树的生成\n分类树用Gini Index选择最优特征，同时决定该特征的最优二值切分点。\n\n* Gini Index: 分类问题中，假设有$K$个类，样本点属于第$k$类的概率是$p_k$，则概率分布的Gini Index定义为：  \n  $Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$\n\n  对于二分类问题，若样本点属于第1类的概率是$p$，则概率分布的Gini Index为：  \n  $Gini(p)=2p(1-p)$\n  \n  对于给定的样本集合$D$，其Gini Index为：  \n  $Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$\n\n  这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。\n\n  如果样本集合$D$根据$A$是否取某一可能值$a$被分成$D_1$和$D_2$两部分，即：  \n  $D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$\n\n  则在特征$A$的条件下，集合$D$的Gini Index定义为：  \n  $Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$\n\n  Gini Index表示集合$D$的不确定性，Gini Index $G(D,A)$表示经$A=a$分割后集合$D$的不确定性，Gini Index越大，样本集合的不确定性也就越大，这一点与Entropy相似。\n\n##### CART生成算法\n1. 设结点的training set为$D$，计算现有特征对该数据集的Gini Index，此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或者“否”将$D$分割成$D_1$和$D_2$两部分，并计算$A=a$时的Gini Index。\n2. 在所有可能的特征$A$及它们所有可能的切分点$a$中，选择Gini Index最小的特征及其对应的切分点作为最优特征与最优切分点。从该结点生成两个子结点，将training set依特征分配到两个子结点中去。\n3. 对两个子结点递归调用1和2，直至满足停止条件。\n4. 生成CART决策树。\n\n算法停止的条件是结点中的样本个数小于指定阈值，或样本集的Gini Index小于阈值(样本基本属于同一类)，或者没有更多特征。\n\n#### CART剪枝\nCART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,\\cdots,T_n\\}$然后通过交叉验证在validation set上进行测试，从中选择最优子树。\n\n1. 剪枝，形成一个子树序列：  \n   在剪枝过程中，计算子树的Loss Function：  \n   $C_{\\alpha}(T)=C(T)+\\alpha |T|$  \n   其中，$T$为任意子树，$C(T)$为对training set的预测误差(如Gini Index)，$|T|$为子树的叶结点个数，$\\alpha \\geq 0$为参数。\n\n   可以用递归的方法对树进行剪枝，将$\\alpha$从小增大，$0=\\alpha_0 < \\alpha_1 < \\alpha_2 < \\cdots < \\alpha_n < +\\infty$，产生一系列的区间$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$；剪枝得到的子树序列对应着区间$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$的最优子树序列$\\{T_0,T_1,\\cdots,T_n\\}$，序列中的子树是嵌套的。\n\n   具体的，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的Loss Function是：  \n   $C_{\\alpha}=C(t)+\\alpha$\n\n   以$t$为根结点的子树$T_t$的Loss Function是：  \n   $C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$\n\n   当$\\alpha=0$及$\\alpha$充分小时，有不等式：  \n   $C_{\\alpha}(T_t)<C_{\\alpha}(t)$\n\n   当$\\alpha$增大时，在某一$\\alpha$有：  \n   $C_{\\alpha}(T_t)=C_{\\alpha}(t)$\n\n   当$\\alpha$再增大时，不等式反向，只要$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的Loss Function值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。\n\n   为此，对$T_0$中每一内部结点$t$，计算：  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$  \n   它表示剪枝后整体Loss减少的程度，在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\\alpha_1$，$T_1$为区间$[\\alpha_1,\\alpha_2)$的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断增加$\\alpha$的值，产生新的区间。\n\n2. 在剪枝得到的子树序列$T_0,T_1,\\cdots,T_n$中通过交叉验证选取最优子树$T_{\\alpha}$\n   利用validation set测试子树序列$T_1,\\cdots,T_n$中各棵子树的MSE或Gini Index，值最小的为最优决策树。每棵子树$T_1,\\cdots,T_n$都对应于一个参数$\\alpha_1,\\cdots,\\alpha_n$。所以当最优子树$T_k$确定时，对应的$\\alpha_k$也确定了，即得到最优决策树了。\n\n##### CART剪枝算法\n1. 设$k=0,T=T_0$\n2. 设$\\alpha=+\\infty$\n3. 自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及  \n   $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$  \n   $T_t$表示以$t$为根结点的子树，$C(T_t)$是对training set的预测误差，$|T_t|$是$T_t$的叶结点个数\n4. 自上而下地访问内部结点$t$，若有$g(t)=\\alpha$，进行剪枝，并对叶结点$t$以多数表决法决定类别，得到树$T$。\n5. 设$k=k+1,\\alpha_k=\\alpha,T_k=T$\n6. 若$T$不是由根结点单独构成的树，则返回到步骤4\n7. 采用cross validation在子树序列中选取最优子树\n\n\n\n\n\n","slug":"ml-dt","published":1,"updated":"2018-10-01T04:40:08.993Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03ch000y608w2p2hng7v","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。</p>\n<h2 id=\"决策树学习\"><a href=\"#决策树学习\" class=\"headerlink\" title=\"决策树学习\"></a>决策树学习</h2><p><strong>决策树学习的Loss Function通常是正则化的极大似然函数</strong>，决策树学习的策略是以Loss Function最小化为目标函数的最小化。</p>\n<p>决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><ul>\n<li><p>Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：<br>$P(X=x_i)=p_i$</p>\n<p>则随机变量$X$的Entropy定义为：<br>$H(X)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：<br>$H(p)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy越大，随机变量的不确定性就越大，$0\\leq H(p)\\leq logn$</p>\n<p>当随机变量只取两个值，即$X$的分布为：<br>$P(X=1)=p, P(X=0)=1-p$<br>Entropy为：<br>$H(p)=-plog_2p-(1-p)log_2(1-p)$</p>\n</li>\n<li><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：<br>$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$</p>\n</li>\n<li><p>信息增益表示 <strong>得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度</strong>：<br>定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：<br>$g(D,A)=H(D)-H(D|A)$  </p>\n<p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，<strong>决策树学习中的信息增益等价于dataset中类与特征的互信息</strong>。</p>\n</li>\n</ul>\n<p>决策树学习应用 <strong>信息增益</strong> 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。</p>\n<p>根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>\n<h4 id=\"信息增益算法\"><a href=\"#信息增益算法\" class=\"headerlink\" title=\"信息增益算法\"></a>信息增益算法</h4><p>设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\\{a_1,a_2,\\cdots,a_n\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：</p>\n<ol>\n<li><p>计算数据集$D$的经验熵$H(D)$：<br>$H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$</p>\n</li>\n<li><p>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：<br>$H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$</p>\n</li>\n<li><p>计算信息增益：<br>$g(D,A)=H(D)-H(D|A)$</p>\n</li>\n</ol>\n<p>以信息增益作为划分训练数据集的特征，存在 <strong>偏向于选择取值较多的特征的问题</strong>。使用信息增益比可以对这一问题进行校正。</p>\n<ul>\n<li>信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：<br>$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$<br>其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。</li>\n</ul>\n<h2 id=\"决策树的生成\"><a href=\"#决策树的生成\" class=\"headerlink\" title=\"决策树的生成\"></a>决策树的生成</h2><h3 id=\"ID3算法\"><a href=\"#ID3算法\" class=\"headerlink\" title=\"ID3算法\"></a>ID3算法</h3><p>ID3算法的核心是在决策树各个结点上应用 <strong>信息增益</strong> 准则进行特征选择，递归地构建决策树。具体做法是：从根节点开始对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。<strong>ID3相当于用极大似然法进行概率模型的选择</strong>。</p>\n<ol>\n<li>若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；</li>\n<li>若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，计算$A$中各个特征对$D$的信息增益，选择 <strong>信息增益</strong> 最大的特征$A_g$；</li>\n<li>若$A_g$的信息增益小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；</li>\n<li>对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。</li>\n</ol>\n<p>ID3算法只有树的生成，所以该算法生成的树容易过拟合。</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a>C4.5</h3><p>C4.5算法在生成的过程中，用 <strong>信息增益比</strong> 来选择特征。</p>\n<ol>\n<li>若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；</li>\n<li>若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，计算$A$中各个特征对$D$的 <strong>信息增益比</strong>，选择 <strong>信息增益比</strong> 最大的特征$A_g$；</li>\n<li>若$A_g$的信息增益比小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；</li>\n<li>对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。</li>\n</ol>\n<h2 id=\"决策树的剪枝\"><a href=\"#决策树的剪枝\" class=\"headerlink\" title=\"决策树的剪枝\"></a>决策树的剪枝</h2><p>决策树的剪枝往往通过极小化决策树整体的Loss Function来实现，设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\\alpha\\geq 0$为参数，则决策树学习的Loss Function可以定义为：<br>$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$<br>其中经验熵为：<br>$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$<br>在Loss Function中，将式子右端的第一项记作：<br>$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$<br>这时有：<br>$C_{\\alpha}(T)=C(T)+\\alpha|T|$</p>\n<p>$C(T)$表示模型对训练数据的预测误差，即模型对训练数据的拟合程度；$|T|$表示模型复杂度，参数$\\alpha\\geq 0$控制两者之间的影响。较大的$\\alpha$促使选择较简单的模型，较小的$\\alpha$促使选择较复杂的模型。</p>\n<p>可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化Loss Function还考虑了减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体模型。</p>\n<p>Loss Function的极小化等价于正则化的极大似然估计，所以，利用Loss Function最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p>\n<h3 id=\"剪枝算法\"><a href=\"#剪枝算法\" class=\"headerlink\" title=\"剪枝算法\"></a>剪枝算法</h3><ol>\n<li>计算每个节点的经验熵</li>\n<li>递归地从树的叶节点向上回缩：<br>设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的Loss Function值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果 $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$，则进行剪枝，即将父节点变为新的叶节点。</li>\n<li>返回2，直到不能继续为止，得到Loss Function最小的子树$T_{\\alpha}$。</li>\n</ol>\n<h2 id=\"CART算法\"><a href=\"#CART算法\" class=\"headerlink\" title=\"CART算法\"></a>CART算法</h2><p>CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定条件下输出的条件概率分布。</p>\n<p>CART算法由以下两步组成：  </p>\n<ol>\n<li>决策树生成：基于training set生成决策树，生成的决策树要尽量大；</li>\n<li>决策树剪枝：用validation set对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准</li>\n</ol>\n<h3 id=\"CART生成\"><a href=\"#CART生成\" class=\"headerlink\" title=\"CART生成\"></a>CART生成</h3><p>决策树的生成就是递归地构建二叉决策树的过程，<strong>对回归树用MSE最小化准则，对分类树用Gini Index最小化</strong> 准则，进行特征选择，生成二叉树。</p>\n<h4 id=\"回归树的生成\"><a href=\"#回归树的生成\" class=\"headerlink\" title=\"回归树的生成\"></a>回归树的生成</h4><p>假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可以表示为：<br>$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$</p>\n<p>当输入空间的划分确定时，可以用MSE来表示回归树对于training set的预测误差，用MSE Loss最小化准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：<br>$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$</p>\n<p>可以采用启发式的方法对输入空间进行划分：选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ 和 $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$<br>然后寻找最优切分变量$j$和切分点$s$，具体的，求解：<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>对固定输入变量$j$可以找到最优切分点$s$：<br>$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$ 和 $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$</p>\n<p>遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$，依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树，成为最小二乘回归树。</p>\n<h5 id=\"最小二乘回归树生成算法\"><a href=\"#最小二乘回归树生成算法\" class=\"headerlink\" title=\"最小二乘回归树生成算法\"></a>最小二乘回归树生成算法</h5><ol>\n<li><p>选择最优切分变量$j$和切分点$s$，求解：<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>遍历$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。</p>\n</li>\n<li><p>用选定的对$(j,s)$划分区域并决定相应的输出值：<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$， $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$</p>\n<p>$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$</p>\n</li>\n<li><p>继续对两个子区域调用步骤1和2，直至满足停止条件。</p>\n</li>\n<li><p>将输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$，生成决策树：<br>$f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$</p>\n</li>\n</ol>\n<h4 id=\"分类树的生成\"><a href=\"#分类树的生成\" class=\"headerlink\" title=\"分类树的生成\"></a>分类树的生成</h4><p>分类树用Gini Index选择最优特征，同时决定该特征的最优二值切分点。</p>\n<ul>\n<li><p>Gini Index: 分类问题中，假设有$K$个类，样本点属于第$k$类的概率是$p_k$，则概率分布的Gini Index定义为：<br>$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$</p>\n<p>对于二分类问题，若样本点属于第1类的概率是$p$，则概率分布的Gini Index为：<br>$Gini(p)=2p(1-p)$</p>\n<p>对于给定的样本集合$D$，其Gini Index为：<br>$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$</p>\n<p>这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。</p>\n<p>如果样本集合$D$根据$A$是否取某一可能值$a$被分成$D_1$和$D_2$两部分，即：<br>$D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$</p>\n<p>则在特征$A$的条件下，集合$D$的Gini Index定义为：<br>$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$</p>\n<p>Gini Index表示集合$D$的不确定性，Gini Index $G(D,A)$表示经$A=a$分割后集合$D$的不确定性，Gini Index越大，样本集合的不确定性也就越大，这一点与Entropy相似。</p>\n</li>\n</ul>\n<h5 id=\"CART生成算法\"><a href=\"#CART生成算法\" class=\"headerlink\" title=\"CART生成算法\"></a>CART生成算法</h5><ol>\n<li>设结点的training set为$D$，计算现有特征对该数据集的Gini Index，此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或者“否”将$D$分割成$D_1$和$D_2$两部分，并计算$A=a$时的Gini Index。</li>\n<li>在所有可能的特征$A$及它们所有可能的切分点$a$中，选择Gini Index最小的特征及其对应的切分点作为最优特征与最优切分点。从该结点生成两个子结点，将training set依特征分配到两个子结点中去。</li>\n<li>对两个子结点递归调用1和2，直至满足停止条件。</li>\n<li>生成CART决策树。</li>\n</ol>\n<p>算法停止的条件是结点中的样本个数小于指定阈值，或样本集的Gini Index小于阈值(样本基本属于同一类)，或者没有更多特征。</p>\n<h4 id=\"CART剪枝\"><a href=\"#CART剪枝\" class=\"headerlink\" title=\"CART剪枝\"></a>CART剪枝</h4><p>CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,\\cdots,T_n\\}$然后通过交叉验证在validation set上进行测试，从中选择最优子树。</p>\n<ol>\n<li><p>剪枝，形成一个子树序列：<br>在剪枝过程中，计算子树的Loss Function：<br>$C_{\\alpha}(T)=C(T)+\\alpha |T|$<br>其中，$T$为任意子树，$C(T)$为对training set的预测误差(如Gini Index)，$|T|$为子树的叶结点个数，$\\alpha \\geq 0$为参数。</p>\n<p>可以用递归的方法对树进行剪枝，将$\\alpha$从小增大，$0=\\alpha_0 &lt; \\alpha_1 &lt; \\alpha_2 &lt; \\cdots &lt; \\alpha_n &lt; +\\infty$，产生一系列的区间$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$；剪枝得到的子树序列对应着区间$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$的最优子树序列$\\{T_0,T_1,\\cdots,T_n\\}$，序列中的子树是嵌套的。</p>\n<p>具体的，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的Loss Function是：<br>$C_{\\alpha}=C(t)+\\alpha$</p>\n<p>以$t$为根结点的子树$T_t$的Loss Function是：<br>$C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$</p>\n<p>当$\\alpha=0$及$\\alpha$充分小时，有不等式：<br>$C_{\\alpha}(T_t)&lt;C_{\\alpha}(t)$</p>\n<p>当$\\alpha$增大时，在某一$\\alpha$有：<br>$C_{\\alpha}(T_t)=C_{\\alpha}(t)$</p>\n<p>当$\\alpha$再增大时，不等式反向，只要$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的Loss Function值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。</p>\n<p>为此，对$T_0$中每一内部结点$t$，计算：<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$<br>它表示剪枝后整体Loss减少的程度，在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\\alpha_1$，$T_1$为区间$[\\alpha_1,\\alpha_2)$的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断增加$\\alpha$的值，产生新的区间。</p>\n</li>\n<li><p>在剪枝得到的子树序列$T_0,T_1,\\cdots,T_n$中通过交叉验证选取最优子树$T_{\\alpha}$<br>利用validation set测试子树序列$T_1,\\cdots,T_n$中各棵子树的MSE或Gini Index，值最小的为最优决策树。每棵子树$T_1,\\cdots,T_n$都对应于一个参数$\\alpha_1,\\cdots,\\alpha_n$。所以当最优子树$T_k$确定时，对应的$\\alpha_k$也确定了，即得到最优决策树了。</p>\n</li>\n</ol>\n<h5 id=\"CART剪枝算法\"><a href=\"#CART剪枝算法\" class=\"headerlink\" title=\"CART剪枝算法\"></a>CART剪枝算法</h5><ol>\n<li>设$k=0,T=T_0$</li>\n<li>设$\\alpha=+\\infty$</li>\n<li>自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$<br>$T_t$表示以$t$为根结点的子树，$C(T_t)$是对training set的预测误差，$|T_t|$是$T_t$的叶结点个数</li>\n<li>自上而下地访问内部结点$t$，若有$g(t)=\\alpha$，进行剪枝，并对叶结点$t$以多数表决法决定类别，得到树$T$。</li>\n<li>设$k=k+1,\\alpha_k=\\alpha,T_k=T$</li>\n<li>若$T$不是由根结点单独构成的树，则返回到步骤4</li>\n<li>采用cross validation在子树序列中选取最优子树</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。</p>\n<h2 id=\"决策树学习\"><a href=\"#决策树学习\" class=\"headerlink\" title=\"决策树学习\"></a>决策树学习</h2><p><strong>决策树学习的Loss Function通常是正则化的极大似然函数</strong>，决策树学习的策略是以Loss Function最小化为目标函数的最小化。</p>\n<p>决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><ul>\n<li><p>Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：<br>$P(X=x_i)=p_i$</p>\n<p>则随机变量$X$的Entropy定义为：<br>$H(X)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：<br>$H(p)=-\\sum_{i=1}^n p_ilogp_i$</p>\n<p>Entropy越大，随机变量的不确定性就越大，$0\\leq H(p)\\leq logn$</p>\n<p>当随机变量只取两个值，即$X$的分布为：<br>$P(X=1)=p, P(X=0)=1-p$<br>Entropy为：<br>$H(p)=-plog_2p-(1-p)log_2(1-p)$</p>\n</li>\n<li><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：<br>$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i), \\quad p_i=P(X=x_i)$</p>\n</li>\n<li><p>信息增益表示 <strong>得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度</strong>：<br>定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：<br>$g(D,A)=H(D)-H(D|A)$  </p>\n<p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，<strong>决策树学习中的信息增益等价于dataset中类与特征的互信息</strong>。</p>\n</li>\n</ul>\n<p>决策树学习应用 <strong>信息增益</strong> 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。</p>\n<p>根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>\n<h4 id=\"信息增益算法\"><a href=\"#信息增益算法\" class=\"headerlink\" title=\"信息增益算法\"></a>信息增益算法</h4><p>设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\\{a_1,a_2,\\cdots,a_n\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：</p>\n<ol>\n<li><p>计算数据集$D$的经验熵$H(D)$：<br>$H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$</p>\n</li>\n<li><p>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：<br>$H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n \\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$</p>\n</li>\n<li><p>计算信息增益：<br>$g(D,A)=H(D)-H(D|A)$</p>\n</li>\n</ol>\n<p>以信息增益作为划分训练数据集的特征，存在 <strong>偏向于选择取值较多的特征的问题</strong>。使用信息增益比可以对这一问题进行校正。</p>\n<ul>\n<li>信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：<br>$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$<br>其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。</li>\n</ul>\n<h2 id=\"决策树的生成\"><a href=\"#决策树的生成\" class=\"headerlink\" title=\"决策树的生成\"></a>决策树的生成</h2><h3 id=\"ID3算法\"><a href=\"#ID3算法\" class=\"headerlink\" title=\"ID3算法\"></a>ID3算法</h3><p>ID3算法的核心是在决策树各个结点上应用 <strong>信息增益</strong> 准则进行特征选择，递归地构建决策树。具体做法是：从根节点开始对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。<strong>ID3相当于用极大似然法进行概率模型的选择</strong>。</p>\n<ol>\n<li>若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；</li>\n<li>若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，计算$A$中各个特征对$D$的信息增益，选择 <strong>信息增益</strong> 最大的特征$A_g$；</li>\n<li>若$A_g$的信息增益小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；</li>\n<li>对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。</li>\n</ol>\n<p>ID3算法只有树的生成，所以该算法生成的树容易过拟合。</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a>C4.5</h3><p>C4.5算法在生成的过程中，用 <strong>信息增益比</strong> 来选择特征。</p>\n<ol>\n<li>若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；</li>\n<li>若特征集$A=\\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，计算$A$中各个特征对$D$的 <strong>信息增益比</strong>，选择 <strong>信息增益比</strong> 最大的特征$A_g$；</li>\n<li>若$A_g$的信息增益比小于阈值$\\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</li>\n<li>否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；</li>\n<li>对第$i$个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。</li>\n</ol>\n<h2 id=\"决策树的剪枝\"><a href=\"#决策树的剪枝\" class=\"headerlink\" title=\"决策树的剪枝\"></a>决策树的剪枝</h2><p>决策树的剪枝往往通过极小化决策树整体的Loss Function来实现，设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\\alpha\\geq 0$为参数，则决策树学习的Loss Function可以定义为：<br>$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_t H_t(T) + \\alpha |T|$<br>其中经验熵为：<br>$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$<br>在Loss Function中，将式子右端的第一项记作：<br>$C(T)=\\sum_{i=1}^{|T|}N_t H_t(T)=-\\sum_{i=1}^{|T|}\\sum_{k=1}^K N_{tk}log \\frac{N_{tk}}{N_t}$<br>这时有：<br>$C_{\\alpha}(T)=C(T)+\\alpha|T|$</p>\n<p>$C(T)$表示模型对训练数据的预测误差，即模型对训练数据的拟合程度；$|T|$表示模型复杂度，参数$\\alpha\\geq 0$控制两者之间的影响。较大的$\\alpha$促使选择较简单的模型，较小的$\\alpha$促使选择较复杂的模型。</p>\n<p>可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化Loss Function还考虑了减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体模型。</p>\n<p>Loss Function的极小化等价于正则化的极大似然估计，所以，利用Loss Function最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p>\n<h3 id=\"剪枝算法\"><a href=\"#剪枝算法\" class=\"headerlink\" title=\"剪枝算法\"></a>剪枝算法</h3><ol>\n<li>计算每个节点的经验熵</li>\n<li>递归地从树的叶节点向上回缩：<br>设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的Loss Function值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果 $C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$，则进行剪枝，即将父节点变为新的叶节点。</li>\n<li>返回2，直到不能继续为止，得到Loss Function最小的子树$T_{\\alpha}$。</li>\n</ol>\n<h2 id=\"CART算法\"><a href=\"#CART算法\" class=\"headerlink\" title=\"CART算法\"></a>CART算法</h2><p>CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定条件下输出的条件概率分布。</p>\n<p>CART算法由以下两步组成：  </p>\n<ol>\n<li>决策树生成：基于training set生成决策树，生成的决策树要尽量大；</li>\n<li>决策树剪枝：用validation set对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准</li>\n</ol>\n<h3 id=\"CART生成\"><a href=\"#CART生成\" class=\"headerlink\" title=\"CART生成\"></a>CART生成</h3><p>决策树的生成就是递归地构建二叉决策树的过程，<strong>对回归树用MSE最小化准则，对分类树用Gini Index最小化</strong> 准则，进行特征选择，生成二叉树。</p>\n<h4 id=\"回归树的生成\"><a href=\"#回归树的生成\" class=\"headerlink\" title=\"回归树的生成\"></a>回归树的生成</h4><p>假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可以表示为：<br>$f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)$</p>\n<p>当输入空间的划分确定时，可以用MSE来表示回归树对于training set的预测误差，用MSE Loss最小化准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：<br>$\\hat{c}_m=AVG(y_i|x_i\\in R_m)$</p>\n<p>可以采用启发式的方法对输入空间进行划分：选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$ 和 $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$<br>然后寻找最优切分变量$j$和切分点$s$，具体的，求解：<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>对固定输入变量$j$可以找到最优切分点$s$：<br>$\\hat{c}_1=AVG(y_i|x_i\\in R_1(j,s))$ 和 $\\hat{c}_2=AVG(y_i|x_i\\in R_2(j,s))$</p>\n<p>遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$，依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树，成为最小二乘回归树。</p>\n<h5 id=\"最小二乘回归树生成算法\"><a href=\"#最小二乘回归树生成算法\" class=\"headerlink\" title=\"最小二乘回归树生成算法\"></a>最小二乘回归树生成算法</h5><ol>\n<li><p>选择最优切分变量$j$和切分点$s$，求解：<br>$\\mathop{min} \\limits_{j,s}[\\mathop{min} \\limits_{c_1} \\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop{min} \\limits_{c_2} \\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$</p>\n<p>遍历$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。</p>\n</li>\n<li><p>用选定的对$(j,s)$划分区域并决定相应的输出值：<br>$R_1(j,s)=\\{x|x^{(j)}\\leq s\\}$， $R_2(j,s)=\\{x|x^{(j)}&gt; s\\}$</p>\n<p>$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\quad m=1,2$</p>\n</li>\n<li><p>继续对两个子区域调用步骤1和2，直至满足停止条件。</p>\n</li>\n<li><p>将输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$，生成决策树：<br>$f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)$</p>\n</li>\n</ol>\n<h4 id=\"分类树的生成\"><a href=\"#分类树的生成\" class=\"headerlink\" title=\"分类树的生成\"></a>分类树的生成</h4><p>分类树用Gini Index选择最优特征，同时决定该特征的最优二值切分点。</p>\n<ul>\n<li><p>Gini Index: 分类问题中，假设有$K$个类，样本点属于第$k$类的概率是$p_k$，则概率分布的Gini Index定义为：<br>$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$</p>\n<p>对于二分类问题，若样本点属于第1类的概率是$p$，则概率分布的Gini Index为：<br>$Gini(p)=2p(1-p)$</p>\n<p>对于给定的样本集合$D$，其Gini Index为：<br>$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$</p>\n<p>这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。</p>\n<p>如果样本集合$D$根据$A$是否取某一可能值$a$被分成$D_1$和$D_2$两部分，即：<br>$D_1=\\{(x,y)\\in D|A(x)=a\\}, \\quad D_2=D-D_1$</p>\n<p>则在特征$A$的条件下，集合$D$的Gini Index定义为：<br>$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$</p>\n<p>Gini Index表示集合$D$的不确定性，Gini Index $G(D,A)$表示经$A=a$分割后集合$D$的不确定性，Gini Index越大，样本集合的不确定性也就越大，这一点与Entropy相似。</p>\n</li>\n</ul>\n<h5 id=\"CART生成算法\"><a href=\"#CART生成算法\" class=\"headerlink\" title=\"CART生成算法\"></a>CART生成算法</h5><ol>\n<li>设结点的training set为$D$，计算现有特征对该数据集的Gini Index，此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或者“否”将$D$分割成$D_1$和$D_2$两部分，并计算$A=a$时的Gini Index。</li>\n<li>在所有可能的特征$A$及它们所有可能的切分点$a$中，选择Gini Index最小的特征及其对应的切分点作为最优特征与最优切分点。从该结点生成两个子结点，将training set依特征分配到两个子结点中去。</li>\n<li>对两个子结点递归调用1和2，直至满足停止条件。</li>\n<li>生成CART决策树。</li>\n</ol>\n<p>算法停止的条件是结点中的样本个数小于指定阈值，或样本集的Gini Index小于阈值(样本基本属于同一类)，或者没有更多特征。</p>\n<h4 id=\"CART剪枝\"><a href=\"#CART剪枝\" class=\"headerlink\" title=\"CART剪枝\"></a>CART剪枝</h4><p>CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,\\cdots,T_n\\}$然后通过交叉验证在validation set上进行测试，从中选择最优子树。</p>\n<ol>\n<li><p>剪枝，形成一个子树序列：<br>在剪枝过程中，计算子树的Loss Function：<br>$C_{\\alpha}(T)=C(T)+\\alpha |T|$<br>其中，$T$为任意子树，$C(T)$为对training set的预测误差(如Gini Index)，$|T|$为子树的叶结点个数，$\\alpha \\geq 0$为参数。</p>\n<p>可以用递归的方法对树进行剪枝，将$\\alpha$从小增大，$0=\\alpha_0 &lt; \\alpha_1 &lt; \\alpha_2 &lt; \\cdots &lt; \\alpha_n &lt; +\\infty$，产生一系列的区间$[\\alpha_i,\\alpha_{i+1}),\\quad i=0,1,\\cdots,n$；剪枝得到的子树序列对应着区间$\\alpha \\in [\\alpha_i, \\alpha_{i+1}), \\quad i=0,1,\\cdots,n$的最优子树序列$\\{T_0,T_1,\\cdots,T_n\\}$，序列中的子树是嵌套的。</p>\n<p>具体的，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的Loss Function是：<br>$C_{\\alpha}=C(t)+\\alpha$</p>\n<p>以$t$为根结点的子树$T_t$的Loss Function是：<br>$C_{\\alpha}(T_t)=C(T_t)+\\alpha |T_t|$</p>\n<p>当$\\alpha=0$及$\\alpha$充分小时，有不等式：<br>$C_{\\alpha}(T_t)&lt;C_{\\alpha}(t)$</p>\n<p>当$\\alpha$增大时，在某一$\\alpha$有：<br>$C_{\\alpha}(T_t)=C_{\\alpha}(t)$</p>\n<p>当$\\alpha$再增大时，不等式反向，只要$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的Loss Function值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。</p>\n<p>为此，对$T_0$中每一内部结点$t$，计算：<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$<br>它表示剪枝后整体Loss减少的程度，在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\\alpha_1$，$T_1$为区间$[\\alpha_1,\\alpha_2)$的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断增加$\\alpha$的值，产生新的区间。</p>\n</li>\n<li><p>在剪枝得到的子树序列$T_0,T_1,\\cdots,T_n$中通过交叉验证选取最优子树$T_{\\alpha}$<br>利用validation set测试子树序列$T_1,\\cdots,T_n$中各棵子树的MSE或Gini Index，值最小的为最优决策树。每棵子树$T_1,\\cdots,T_n$都对应于一个参数$\\alpha_1,\\cdots,\\alpha_n$。所以当最优子树$T_k$确定时，对应的$\\alpha_k$也确定了，即得到最优决策树了。</p>\n</li>\n</ol>\n<h5 id=\"CART剪枝算法\"><a href=\"#CART剪枝算法\" class=\"headerlink\" title=\"CART剪枝算法\"></a>CART剪枝算法</h5><ol>\n<li>设$k=0,T=T_0$</li>\n<li>设$\\alpha=+\\infty$</li>\n<li>自下而上地对各内部结点$t$计算$C(T_t)$，$|T_t|$以及<br>$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1} \\qquad \\alpha=min(\\alpha,g(t))$<br>$T_t$表示以$t$为根结点的子树，$C(T_t)$是对training set的预测误差，$|T_t|$是$T_t$的叶结点个数</li>\n<li>自上而下地访问内部结点$t$，若有$g(t)=\\alpha$，进行剪枝，并对叶结点$t$以多数表决法决定类别，得到树$T$。</li>\n<li>设$k=k+1,\\alpha_k=\\alpha,T_k=T$</li>\n<li>若$T$不是由根结点单独构成的树，则返回到步骤4</li>\n<li>采用cross validation在子树序列中选取最优子树</li>\n</ol>\n"},{"title":"[ML] Ensemble Learning","date":"2018-07-25T02:59:08.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nEnsemble Learning是ML中一个非常热门的领域，也是很多比赛Top方案的必选。本文对常见的Ensemble Learning做一个简要介绍。\n\n根据Base Learner的生成方式，目前的Ensemble Learning方法大致可以分为两大类：即base learner之间存在强依赖关系、必须串行生成的序列化方法，以及base learner间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和Random Forests。\n\nBoosting主要关注降低bias，因此Boosting能基于泛化性能相当弱的weak learner构建出很强的集成。而bagging主要降低variance，因此它在不剪枝决策树、NN等易受样本扰动的learner上效用更为明显。\n\n## Ensemble Learning Algorithm\n### Boosting\nBoosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。\n\nBoosting方法就是从weak learner出发，反复学习，得到一系列weak learner(base learner)，然后组合这些weak learner，构成一个强分类器。大多数的提升方法都是改变training set的概率分布，针对不同的training set分布调用weak learner algorithm学习一系列weak learner。\n\nAdaBoost的做法是，先从初始training set中训练一个base learner，再根据base learner的表现对训练样本分布进行调整，使得先前base learner做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个base learner；如此重复进行，直至base learner数目达到事先指定的值$T$，最终将这$T$个base learner进行加权结合。AdaBoost采用weighted majority voting的做法，具体的，加大分类误差率较小的weak learner的权重，使其在表决中起较大作用，减小分类误差大的weak learner的权重，使其在表决中起较小的作用。\n\n#### AdaBoost\n1. 初始化training set的权值分布\n$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$\n2. 对$m=1,2,\\cdots,M$\n    * 使用具有权值分布$D_m$的training set学习，得到base learner：  \n$G_m(x):\\chi \\to \\{-1,+1\\}$\n    * 计算$G_m(x)$在training set上的分类误差率：  \n$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$  \n$w_{mi}$是第$m$轮中第$i$个实例的权值。\n    * 计算$G_m(x)$的系数：  \n$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$\n    * 更新training set的权值分布：  \n$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$  \n$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$  \n    这里，$Z_m$是规范化因子：  \n$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$  \n它使得$D_{mi}$成为一个概率分布。\n    * 构建base learner的线性组合：  \n$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$  \n得到最终分类器：  \n$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$  \n注：$\\alpha_m$之和并不为1。\n\n#### AdaBoost算法的解释\nAdaBoost还可以认为是模型为加法模型、Loss Function为指数函数、学习算法为前向分步算法时的二分类学习方法。\n\n##### 前向分步算法\n考虑加法模型：  \n$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$  \n其中，$b(x;\\gamma_m)$为基函数的参数，$\\beta_m$为基函数的系数。\n\n在给定training set及Loss Function $L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即Loss Function极小化问题：  \n$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$\n\n前向分步算法求解复杂优化问题的思想是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每一步只需要优化以下Loss Function：  \n$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$\n\n1. 初始化$f_0(x)=0$\n2. 对$m=1,2,\\cdots,M$\n    * 极小化Loss：  \n      $(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$  \n      得到参数$\\beta_m, \\gamma_m$\n\n    * 更新  \n      $f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$\n3. 得到加法模型  \n   $f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$\n\n#### Boosting Tree\nBoosting方法实际采用加法模型(即基函数的线性组合)与前向分步算法，以决策树为基函数的Boosting方法称为Boosting Tree。Boosting Tree模型可以表示为决策树的加法模型：  \n$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$  \n其中$T(x;\\Theta_m)$表示决策树，$\\Theta_m$表示决策树参数，$T$为树的个数。\n\nBoosting Tree采用前向分步算法，首先确定初始提升树$f_0(x)=0$，第$m$步的模型是：  \n$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n\n其中，$f_{m-1}(x)$为当前模型，通过经验风险最小化确定下一棵决策树的参数$\\Theta$，  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$\n\n若将输入空间$\\chi$划分为$J$个互不相交的区域$R_1,R_2,\\cdots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可以表示为：  \n$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$  \n其中，参数$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$表示树的区域划分和各区域上的常数，$J$是回归树的复杂度即叶结点个数。\n\n回归问题Boosting Tree使用以下前向分步算法：  \n$$\nf_0(x)=0  \\\\\nf_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\\nf_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)\n$$\n\n在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解：  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$  \n得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。\n\n采用MSE Loss时，  \n$L(y,f(x))=(y-f(x))^2$\n\n其损失变为：  \n$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$\n\n这里，$r=y-f_{m-1}(x)$是模型拟合数据的残差。所以， __对回归问题的Boosting Tree来说，只需简单地拟合当前模型的残差__。\n\n##### Boosting Tree for Regression\n1. 初始化$f_0(x)=0$\n2. 对$m=1,2,\\cdots,M$\n   * 计算残差：$r_{mi}=y_i-f_{m-1}(x_i)$\n   * 拟合残差学习一个回归树，得到$T(x;\\Theta_m)$\n   * 更新$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n3. 得到回归问题Boosting Tree：  \n   $f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$\n\n#### Gradient Boosting (GBDT)\nBoosting Tree利用加法模型与前向分步算法实现学习的优化过程，当Loss Function是MSE和指数Loss时，每一步的优化是很简单的。但对于一般的Loss Function而言，往往每一步优化并不容易，这一问题可以利用Gradient Boosting解决。这是利用Gradient Descend的近似方法，其关键是利用Loss Function的负梯度在当前模型的值：  \n$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$\n作为回归问题提升树算法中的残差近似值，拟合一个回归树。\n\n1. 初始化$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$\n2. 对$m=1,2,\\cdots,M$\n   * 对$i=1,2,\\cdots,N$，计算：  \n     $r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$\n   * 对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},\\quad j=1,2, \\cdots,J$\n   * 对$j=1,2,\\cdots,J$计算：  \n     $c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$\n   * 更新$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$\n3. 得到回归树：  \n   $$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$\n\n### Bagging and Random Forests\n#### Bagging\nBagging是并行式集成学习方法最著名的代表。它基于bootstrap sampling，给定包含$m$个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过$m$次随机采样，我们得到含有$m$个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。\n\n这样，我们可采样出$T$个含有$m$个训练样本的采样集，然后基于每个采样集训练出一个base learner，再将这些base learner进行集成。在对预测输出进行结合时，Bagging通常对分类任务使用majority voting，对回归任务使用averaging。\n\n与AdaBoost只适用于binary classification不同，Bagging能不经修改地用于多分类、回归等任务。\nBootstrap sampling还给Bagging带来了另一个优点：由于每个base learner只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用于validation set来对泛化性能进行out-of-bag estimate。Bagging主要关注降低variance，因此它在不剪枝决策树、NN等易受样本扰动的学习器上效用更为明显。\n\n#### Random Forests\nRandom Forests是Bagging的一个变体，RF在以Decision Tree为base learner构建Bagging集成的基础上，进一步在Decision Tree的训练过程中引入了 __随机属性选择__。传统Decision Tree在选择划分属性时是在当前结点的属性集合（假定有$d$个属性）中选择一个最优属性；而在Random Forests中，对base decision tree的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性进行划分。这里参数$k$控制了随机性的引入程度；若$k=d$，则base learner的构建与传统decision tree相同；若$k=1$，则是随机选择一个属性用于划分。一般推荐$k=log_2d$。\n\n与Bagging中base learner的“多样性”仅通过样本扰动不同，__Random Forests中base learner不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过base learner之间差异度的增加而进一步提升__。\n\nRandom Forests的训练效率通常优于Bagging，因为在base decision tree的构建过程中，Bagging使用的是“确定型”decision tree，在选择划分属性时要对结点的所有属性进行考察，而Random Forests使用的“随机型”decision tree则只需考察一个属性子集。\n\n### 结合策略\nLearner的结合会从3方面带来好处：\n1. 从统计方面看，由于学习任务的假设空间往往很大，可能有多个假设在training set上达到同等性能，此时若使用单学习器可能因误选导致泛化性能不佳，结合多个learner则会减小这一风险。\n2. 从计算方面来看，learning algorithm往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入局部极小点的风险。\n3. 从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个learner，由于相应的假设空间有所扩大，有可能学得到更好的近似。\n\n* Averaging\n$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$\n\n* Weighted Averaging\n$H(x)=\\sum_{i=1}^Tw_i h_i(x)$\n其中$w_i$一般从training set中学习而得。在base learner性能相差较大时宜采用Weighted Averaging，而在base learner性能相差不大时宜采用Averaging。\n\n* Voting\n    * Majority Voting\n      $$\n      H(x)=\\begin{cases}\n      c_j,\\quad if \\sum_{i=1}^Th_i^j(x)>\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\\nreject, \\quad otherwise\n      \\end{cases}\n      $$\n\n    * Plurality Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$\n      即预测为得票数最多的标记，若同时有多个标记获最高票，则从中随机选取一个。\n      \n    * Weighted Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$\n\n* Stacking\nStacking先从初始training set中训练出初级 learner，然后“生成”一个新数据集用于训练次级learner。在这个新数据集中，初级learner的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。\n\n### 多样性\n#### 多样性增强\n* 数据样本扰动\n给定初始数据集，可从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，数据样本扰动通常是基于采样法。数据样本扰动法对“不稳定学习器”（例如NN、Decision Tree等）很有效。但是对“稳定学习器”（例如SVM、KNN、Naive Bayes）等，需要使用 __输入属性扰动__。\n\n* 输入属性扰动\n从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个base learner。对包含大量冗余属性的数据，子空间中训练base learner不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销。同时，由于冗余属性多，减少一些属性后训练出的base learner也不至于太差。\n\n* 输入表示扰动\n对输出表示进行操纵以增强多样性。可对training sample类标记稍作变动，如Flipping Out。\n\n* 算法参数扰动\n使用单一learner时通常需要cross validation来确定参数值，这事实上已经使用了不同参数训练出多个learner，只不过最终仅选择其中一个learner进行使用，而Ensemble Learning则相当于把这些learner都利用起来；由此可见，Ensemble Learning实际计算开销并不比使用单一learner大很多。","source":"_posts/ml-ensemble.md","raw":"---\ntitle: \"[ML] Ensemble Learning\"\ndate: 2018-07-25 10:59:08\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nEnsemble Learning是ML中一个非常热门的领域，也是很多比赛Top方案的必选。本文对常见的Ensemble Learning做一个简要介绍。\n\n根据Base Learner的生成方式，目前的Ensemble Learning方法大致可以分为两大类：即base learner之间存在强依赖关系、必须串行生成的序列化方法，以及base learner间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和Random Forests。\n\nBoosting主要关注降低bias，因此Boosting能基于泛化性能相当弱的weak learner构建出很强的集成。而bagging主要降低variance，因此它在不剪枝决策树、NN等易受样本扰动的learner上效用更为明显。\n\n## Ensemble Learning Algorithm\n### Boosting\nBoosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。\n\nBoosting方法就是从weak learner出发，反复学习，得到一系列weak learner(base learner)，然后组合这些weak learner，构成一个强分类器。大多数的提升方法都是改变training set的概率分布，针对不同的training set分布调用weak learner algorithm学习一系列weak learner。\n\nAdaBoost的做法是，先从初始training set中训练一个base learner，再根据base learner的表现对训练样本分布进行调整，使得先前base learner做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个base learner；如此重复进行，直至base learner数目达到事先指定的值$T$，最终将这$T$个base learner进行加权结合。AdaBoost采用weighted majority voting的做法，具体的，加大分类误差率较小的weak learner的权重，使其在表决中起较大作用，减小分类误差大的weak learner的权重，使其在表决中起较小的作用。\n\n#### AdaBoost\n1. 初始化training set的权值分布\n$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$\n2. 对$m=1,2,\\cdots,M$\n    * 使用具有权值分布$D_m$的training set学习，得到base learner：  \n$G_m(x):\\chi \\to \\{-1,+1\\}$\n    * 计算$G_m(x)$在training set上的分类误差率：  \n$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$  \n$w_{mi}$是第$m$轮中第$i$个实例的权值。\n    * 计算$G_m(x)$的系数：  \n$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$\n    * 更新training set的权值分布：  \n$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$  \n$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$  \n    这里，$Z_m$是规范化因子：  \n$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$  \n它使得$D_{mi}$成为一个概率分布。\n    * 构建base learner的线性组合：  \n$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$  \n得到最终分类器：  \n$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$  \n注：$\\alpha_m$之和并不为1。\n\n#### AdaBoost算法的解释\nAdaBoost还可以认为是模型为加法模型、Loss Function为指数函数、学习算法为前向分步算法时的二分类学习方法。\n\n##### 前向分步算法\n考虑加法模型：  \n$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$  \n其中，$b(x;\\gamma_m)$为基函数的参数，$\\beta_m$为基函数的系数。\n\n在给定training set及Loss Function $L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即Loss Function极小化问题：  \n$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$\n\n前向分步算法求解复杂优化问题的思想是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每一步只需要优化以下Loss Function：  \n$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$\n\n1. 初始化$f_0(x)=0$\n2. 对$m=1,2,\\cdots,M$\n    * 极小化Loss：  \n      $(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$  \n      得到参数$\\beta_m, \\gamma_m$\n\n    * 更新  \n      $f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$\n3. 得到加法模型  \n   $f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$\n\n#### Boosting Tree\nBoosting方法实际采用加法模型(即基函数的线性组合)与前向分步算法，以决策树为基函数的Boosting方法称为Boosting Tree。Boosting Tree模型可以表示为决策树的加法模型：  \n$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$  \n其中$T(x;\\Theta_m)$表示决策树，$\\Theta_m$表示决策树参数，$T$为树的个数。\n\nBoosting Tree采用前向分步算法，首先确定初始提升树$f_0(x)=0$，第$m$步的模型是：  \n$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n\n其中，$f_{m-1}(x)$为当前模型，通过经验风险最小化确定下一棵决策树的参数$\\Theta$，  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$\n\n若将输入空间$\\chi$划分为$J$个互不相交的区域$R_1,R_2,\\cdots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可以表示为：  \n$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$  \n其中，参数$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$表示树的区域划分和各区域上的常数，$J$是回归树的复杂度即叶结点个数。\n\n回归问题Boosting Tree使用以下前向分步算法：  \n$$\nf_0(x)=0  \\\\\nf_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\\nf_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)\n$$\n\n在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解：  \n$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$  \n得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。\n\n采用MSE Loss时，  \n$L(y,f(x))=(y-f(x))^2$\n\n其损失变为：  \n$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$\n\n这里，$r=y-f_{m-1}(x)$是模型拟合数据的残差。所以， __对回归问题的Boosting Tree来说，只需简单地拟合当前模型的残差__。\n\n##### Boosting Tree for Regression\n1. 初始化$f_0(x)=0$\n2. 对$m=1,2,\\cdots,M$\n   * 计算残差：$r_{mi}=y_i-f_{m-1}(x_i)$\n   * 拟合残差学习一个回归树，得到$T(x;\\Theta_m)$\n   * 更新$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$\n3. 得到回归问题Boosting Tree：  \n   $f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$\n\n#### Gradient Boosting (GBDT)\nBoosting Tree利用加法模型与前向分步算法实现学习的优化过程，当Loss Function是MSE和指数Loss时，每一步的优化是很简单的。但对于一般的Loss Function而言，往往每一步优化并不容易，这一问题可以利用Gradient Boosting解决。这是利用Gradient Descend的近似方法，其关键是利用Loss Function的负梯度在当前模型的值：  \n$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$\n作为回归问题提升树算法中的残差近似值，拟合一个回归树。\n\n1. 初始化$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$\n2. 对$m=1,2,\\cdots,M$\n   * 对$i=1,2,\\cdots,N$，计算：  \n     $r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$\n   * 对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},\\quad j=1,2, \\cdots,J$\n   * 对$j=1,2,\\cdots,J$计算：  \n     $c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$\n   * 更新$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$\n3. 得到回归树：  \n   $$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$\n\n### Bagging and Random Forests\n#### Bagging\nBagging是并行式集成学习方法最著名的代表。它基于bootstrap sampling，给定包含$m$个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过$m$次随机采样，我们得到含有$m$个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。\n\n这样，我们可采样出$T$个含有$m$个训练样本的采样集，然后基于每个采样集训练出一个base learner，再将这些base learner进行集成。在对预测输出进行结合时，Bagging通常对分类任务使用majority voting，对回归任务使用averaging。\n\n与AdaBoost只适用于binary classification不同，Bagging能不经修改地用于多分类、回归等任务。\nBootstrap sampling还给Bagging带来了另一个优点：由于每个base learner只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用于validation set来对泛化性能进行out-of-bag estimate。Bagging主要关注降低variance，因此它在不剪枝决策树、NN等易受样本扰动的学习器上效用更为明显。\n\n#### Random Forests\nRandom Forests是Bagging的一个变体，RF在以Decision Tree为base learner构建Bagging集成的基础上，进一步在Decision Tree的训练过程中引入了 __随机属性选择__。传统Decision Tree在选择划分属性时是在当前结点的属性集合（假定有$d$个属性）中选择一个最优属性；而在Random Forests中，对base decision tree的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性进行划分。这里参数$k$控制了随机性的引入程度；若$k=d$，则base learner的构建与传统decision tree相同；若$k=1$，则是随机选择一个属性用于划分。一般推荐$k=log_2d$。\n\n与Bagging中base learner的“多样性”仅通过样本扰动不同，__Random Forests中base learner不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过base learner之间差异度的增加而进一步提升__。\n\nRandom Forests的训练效率通常优于Bagging，因为在base decision tree的构建过程中，Bagging使用的是“确定型”decision tree，在选择划分属性时要对结点的所有属性进行考察，而Random Forests使用的“随机型”decision tree则只需考察一个属性子集。\n\n### 结合策略\nLearner的结合会从3方面带来好处：\n1. 从统计方面看，由于学习任务的假设空间往往很大，可能有多个假设在training set上达到同等性能，此时若使用单学习器可能因误选导致泛化性能不佳，结合多个learner则会减小这一风险。\n2. 从计算方面来看，learning algorithm往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入局部极小点的风险。\n3. 从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个learner，由于相应的假设空间有所扩大，有可能学得到更好的近似。\n\n* Averaging\n$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$\n\n* Weighted Averaging\n$H(x)=\\sum_{i=1}^Tw_i h_i(x)$\n其中$w_i$一般从training set中学习而得。在base learner性能相差较大时宜采用Weighted Averaging，而在base learner性能相差不大时宜采用Averaging。\n\n* Voting\n    * Majority Voting\n      $$\n      H(x)=\\begin{cases}\n      c_j,\\quad if \\sum_{i=1}^Th_i^j(x)>\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\\nreject, \\quad otherwise\n      \\end{cases}\n      $$\n\n    * Plurality Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$\n      即预测为得票数最多的标记，若同时有多个标记获最高票，则从中随机选取一个。\n      \n    * Weighted Voting\n      $H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$\n\n* Stacking\nStacking先从初始training set中训练出初级 learner，然后“生成”一个新数据集用于训练次级learner。在这个新数据集中，初级learner的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。\n\n### 多样性\n#### 多样性增强\n* 数据样本扰动\n给定初始数据集，可从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，数据样本扰动通常是基于采样法。数据样本扰动法对“不稳定学习器”（例如NN、Decision Tree等）很有效。但是对“稳定学习器”（例如SVM、KNN、Naive Bayes）等，需要使用 __输入属性扰动__。\n\n* 输入属性扰动\n从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个base learner。对包含大量冗余属性的数据，子空间中训练base learner不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销。同时，由于冗余属性多，减少一些属性后训练出的base learner也不至于太差。\n\n* 输入表示扰动\n对输出表示进行操纵以增强多样性。可对training sample类标记稍作变动，如Flipping Out。\n\n* 算法参数扰动\n使用单一learner时通常需要cross validation来确定参数值，这事实上已经使用了不同参数训练出多个learner，只不过最终仅选择其中一个learner进行使用，而Ensemble Learning则相当于把这些learner都利用起来；由此可见，Ensemble Learning实际计算开销并不比使用单一learner大很多。","slug":"ml-ensemble","published":1,"updated":"2018-10-01T04:40:08.997Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cj0010608wwmlxn6on","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Ensemble Learning是ML中一个非常热门的领域，也是很多比赛Top方案的必选。本文对常见的Ensemble Learning做一个简要介绍。</p>\n<p>根据Base Learner的生成方式，目前的Ensemble Learning方法大致可以分为两大类：即base learner之间存在强依赖关系、必须串行生成的序列化方法，以及base learner间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和Random Forests。</p>\n<p>Boosting主要关注降低bias，因此Boosting能基于泛化性能相当弱的weak learner构建出很强的集成。而bagging主要降低variance，因此它在不剪枝决策树、NN等易受样本扰动的learner上效用更为明显。</p>\n<h2 id=\"Ensemble-Learning-Algorithm\"><a href=\"#Ensemble-Learning-Algorithm\" class=\"headerlink\" title=\"Ensemble Learning Algorithm\"></a>Ensemble Learning Algorithm</h2><h3 id=\"Boosting\"><a href=\"#Boosting\" class=\"headerlink\" title=\"Boosting\"></a>Boosting</h3><p>Boosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。</p>\n<p>Boosting方法就是从weak learner出发，反复学习，得到一系列weak learner(base learner)，然后组合这些weak learner，构成一个强分类器。大多数的提升方法都是改变training set的概率分布，针对不同的training set分布调用weak learner algorithm学习一系列weak learner。</p>\n<p>AdaBoost的做法是，先从初始training set中训练一个base learner，再根据base learner的表现对训练样本分布进行调整，使得先前base learner做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个base learner；如此重复进行，直至base learner数目达到事先指定的值$T$，最终将这$T$个base learner进行加权结合。AdaBoost采用weighted majority voting的做法，具体的，加大分类误差率较小的weak learner的权重，使其在表决中起较大作用，减小分类误差大的weak learner的权重，使其在表决中起较小的作用。</p>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><ol>\n<li>初始化training set的权值分布<br>$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>使用具有权值分布$D_m$的training set学习，得到base learner：<br>$G_m(x):\\chi \\to \\{-1,+1\\}$</li>\n<li>计算$G_m(x)$在training set上的分类误差率：<br>$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$<br>$w_{mi}$是第$m$轮中第$i$个实例的权值。</li>\n<li>计算$G_m(x)$的系数：<br>$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$</li>\n<li>更新training set的权值分布：<br>$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$<br>$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$<br>这里，$Z_m$是规范化因子：<br>$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$<br>它使得$D_{mi}$成为一个概率分布。</li>\n<li>构建base learner的线性组合：<br>$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$<br>得到最终分类器：<br>$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$<br>注：$\\alpha_m$之和并不为1。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"AdaBoost算法的解释\"><a href=\"#AdaBoost算法的解释\" class=\"headerlink\" title=\"AdaBoost算法的解释\"></a>AdaBoost算法的解释</h4><p>AdaBoost还可以认为是模型为加法模型、Loss Function为指数函数、学习算法为前向分步算法时的二分类学习方法。</p>\n<h5 id=\"前向分步算法\"><a href=\"#前向分步算法\" class=\"headerlink\" title=\"前向分步算法\"></a>前向分步算法</h5><p>考虑加法模型：<br>$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$<br>其中，$b(x;\\gamma_m)$为基函数的参数，$\\beta_m$为基函数的系数。</p>\n<p>在给定training set及Loss Function $L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即Loss Function极小化问题：<br>$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$</p>\n<p>前向分步算法求解复杂优化问题的思想是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每一步只需要优化以下Loss Function：<br>$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$</p>\n<ol>\n<li>初始化$f_0(x)=0$</li>\n<li><p>对$m=1,2,\\cdots,M$</p>\n<ul>\n<li><p>极小化Loss：<br>$(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$<br>得到参数$\\beta_m, \\gamma_m$</p>\n</li>\n<li><p>更新<br>$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$</p>\n</li>\n</ul>\n</li>\n<li>得到加法模型<br>$f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$</li>\n</ol>\n<h4 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h4><p>Boosting方法实际采用加法模型(即基函数的线性组合)与前向分步算法，以决策树为基函数的Boosting方法称为Boosting Tree。Boosting Tree模型可以表示为决策树的加法模型：<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$<br>其中$T(x;\\Theta_m)$表示决策树，$\\Theta_m$表示决策树参数，$T$为树的个数。</p>\n<p>Boosting Tree采用前向分步算法，首先确定初始提升树$f_0(x)=0$，第$m$步的模型是：<br>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</p>\n<p>其中，$f_{m-1}(x)$为当前模型，通过经验风险最小化确定下一棵决策树的参数$\\Theta$，<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$</p>\n<p>若将输入空间$\\chi$划分为$J$个互不相交的区域$R_1,R_2,\\cdots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可以表示为：<br>$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$<br>其中，参数$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$表示树的区域划分和各区域上的常数，$J$是回归树的复杂度即叶结点个数。</p>\n<p>回归问题Boosting Tree使用以下前向分步算法：<br>$$<br>f_0(x)=0  \\\\<br>f_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\<br>f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)<br>$$</p>\n<p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解：<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$<br>得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。</p>\n<p>采用MSE Loss时，<br>$L(y,f(x))=(y-f(x))^2$</p>\n<p>其损失变为：<br>$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$</p>\n<p>这里，$r=y-f_{m-1}(x)$是模型拟合数据的残差。所以， <strong>对回归问题的Boosting Tree来说，只需简单地拟合当前模型的残差</strong>。</p>\n<h5 id=\"Boosting-Tree-for-Regression\"><a href=\"#Boosting-Tree-for-Regression\" class=\"headerlink\" title=\"Boosting Tree for Regression\"></a>Boosting Tree for Regression</h5><ol>\n<li>初始化$f_0(x)=0$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>计算残差：$r_{mi}=y_i-f_{m-1}(x_i)$</li>\n<li>拟合残差学习一个回归树，得到$T(x;\\Theta_m)$</li>\n<li>更新$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</li>\n</ul>\n</li>\n<li>得到回归问题Boosting Tree：<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$</li>\n</ol>\n<h4 id=\"Gradient-Boosting-GBDT\"><a href=\"#Gradient-Boosting-GBDT\" class=\"headerlink\" title=\"Gradient Boosting (GBDT)\"></a>Gradient Boosting (GBDT)</h4><p>Boosting Tree利用加法模型与前向分步算法实现学习的优化过程，当Loss Function是MSE和指数Loss时，每一步的优化是很简单的。但对于一般的Loss Function而言，往往每一步优化并不容易，这一问题可以利用Gradient Boosting解决。这是利用Gradient Descend的近似方法，其关键是利用Loss Function的负梯度在当前模型的值：<br>$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$<br>作为回归问题提升树算法中的残差近似值，拟合一个回归树。</p>\n<ol>\n<li>初始化$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>对$i=1,2,\\cdots,N$，计算：<br>$r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$</li>\n<li>对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},\\quad j=1,2, \\cdots,J$</li>\n<li>对$j=1,2,\\cdots,J$计算：<br>$c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$</li>\n<li>更新$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$</li>\n</ul>\n</li>\n<li>得到回归树：<br>$$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$</li>\n</ol>\n<h3 id=\"Bagging-and-Random-Forests\"><a href=\"#Bagging-and-Random-Forests\" class=\"headerlink\" title=\"Bagging and Random Forests\"></a>Bagging and Random Forests</h3><h4 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h4><p>Bagging是并行式集成学习方法最著名的代表。它基于bootstrap sampling，给定包含$m$个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过$m$次随机采样，我们得到含有$m$个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。</p>\n<p>这样，我们可采样出$T$个含有$m$个训练样本的采样集，然后基于每个采样集训练出一个base learner，再将这些base learner进行集成。在对预测输出进行结合时，Bagging通常对分类任务使用majority voting，对回归任务使用averaging。</p>\n<p>与AdaBoost只适用于binary classification不同，Bagging能不经修改地用于多分类、回归等任务。<br>Bootstrap sampling还给Bagging带来了另一个优点：由于每个base learner只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用于validation set来对泛化性能进行out-of-bag estimate。Bagging主要关注降低variance，因此它在不剪枝决策树、NN等易受样本扰动的学习器上效用更为明显。</p>\n<h4 id=\"Random-Forests\"><a href=\"#Random-Forests\" class=\"headerlink\" title=\"Random Forests\"></a>Random Forests</h4><p>Random Forests是Bagging的一个变体，RF在以Decision Tree为base learner构建Bagging集成的基础上，进一步在Decision Tree的训练过程中引入了 <strong>随机属性选择</strong>。传统Decision Tree在选择划分属性时是在当前结点的属性集合（假定有$d$个属性）中选择一个最优属性；而在Random Forests中，对base decision tree的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性进行划分。这里参数$k$控制了随机性的引入程度；若$k=d$，则base learner的构建与传统decision tree相同；若$k=1$，则是随机选择一个属性用于划分。一般推荐$k=log_2d$。</p>\n<p>与Bagging中base learner的“多样性”仅通过样本扰动不同，<strong>Random Forests中base learner不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过base learner之间差异度的增加而进一步提升</strong>。</p>\n<p>Random Forests的训练效率通常优于Bagging，因为在base decision tree的构建过程中，Bagging使用的是“确定型”decision tree，在选择划分属性时要对结点的所有属性进行考察，而Random Forests使用的“随机型”decision tree则只需考察一个属性子集。</p>\n<h3 id=\"结合策略\"><a href=\"#结合策略\" class=\"headerlink\" title=\"结合策略\"></a>结合策略</h3><p>Learner的结合会从3方面带来好处：</p>\n<ol>\n<li>从统计方面看，由于学习任务的假设空间往往很大，可能有多个假设在training set上达到同等性能，此时若使用单学习器可能因误选导致泛化性能不佳，结合多个learner则会减小这一风险。</li>\n<li>从计算方面来看，learning algorithm往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入局部极小点的风险。</li>\n<li>从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个learner，由于相应的假设空间有所扩大，有可能学得到更好的近似。</li>\n</ol>\n<ul>\n<li><p>Averaging<br>$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$</p>\n</li>\n<li><p>Weighted Averaging<br>$H(x)=\\sum_{i=1}^Tw_i h_i(x)$<br>其中$w_i$一般从training set中学习而得。在base learner性能相差较大时宜采用Weighted Averaging，而在base learner性能相差不大时宜采用Averaging。</p>\n</li>\n<li><p>Voting</p>\n<ul>\n<li><p>Majority Voting<br>$$<br>H(x)=\\begin{cases}<br>c_j,\\quad if \\sum_{i=1}^Th_i^j(x)&gt;\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\<br>reject, \\quad otherwise<br>\\end{cases}<br>$$</p>\n</li>\n<li><p>Plurality Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$<br>即预测为得票数最多的标记，若同时有多个标记获最高票，则从中随机选取一个。</p>\n</li>\n<li><p>Weighted Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$</p>\n</li>\n</ul>\n</li>\n<li><p>Stacking<br>Stacking先从初始training set中训练出初级 learner，然后“生成”一个新数据集用于训练次级learner。在这个新数据集中，初级learner的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。</p>\n</li>\n</ul>\n<h3 id=\"多样性\"><a href=\"#多样性\" class=\"headerlink\" title=\"多样性\"></a>多样性</h3><h4 id=\"多样性增强\"><a href=\"#多样性增强\" class=\"headerlink\" title=\"多样性增强\"></a>多样性增强</h4><ul>\n<li><p>数据样本扰动<br>给定初始数据集，可从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，数据样本扰动通常是基于采样法。数据样本扰动法对“不稳定学习器”（例如NN、Decision Tree等）很有效。但是对“稳定学习器”（例如SVM、KNN、Naive Bayes）等，需要使用 <strong>输入属性扰动</strong>。</p>\n</li>\n<li><p>输入属性扰动<br>从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个base learner。对包含大量冗余属性的数据，子空间中训练base learner不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销。同时，由于冗余属性多，减少一些属性后训练出的base learner也不至于太差。</p>\n</li>\n<li><p>输入表示扰动<br>对输出表示进行操纵以增强多样性。可对training sample类标记稍作变动，如Flipping Out。</p>\n</li>\n<li><p>算法参数扰动<br>使用单一learner时通常需要cross validation来确定参数值，这事实上已经使用了不同参数训练出多个learner，只不过最终仅选择其中一个learner进行使用，而Ensemble Learning则相当于把这些learner都利用起来；由此可见，Ensemble Learning实际计算开销并不比使用单一learner大很多。</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Ensemble Learning是ML中一个非常热门的领域，也是很多比赛Top方案的必选。本文对常见的Ensemble Learning做一个简要介绍。</p>\n<p>根据Base Learner的生成方式，目前的Ensemble Learning方法大致可以分为两大类：即base learner之间存在强依赖关系、必须串行生成的序列化方法，以及base learner间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和Random Forests。</p>\n<p>Boosting主要关注降低bias，因此Boosting能基于泛化性能相当弱的weak learner构建出很强的集成。而bagging主要降低variance，因此它在不剪枝决策树、NN等易受样本扰动的learner上效用更为明显。</p>\n<h2 id=\"Ensemble-Learning-Algorithm\"><a href=\"#Ensemble-Learning-Algorithm\" class=\"headerlink\" title=\"Ensemble Learning Algorithm\"></a>Ensemble Learning Algorithm</h2><h3 id=\"Boosting\"><a href=\"#Boosting\" class=\"headerlink\" title=\"Boosting\"></a>Boosting</h3><p>Boosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。</p>\n<p>Boosting方法就是从weak learner出发，反复学习，得到一系列weak learner(base learner)，然后组合这些weak learner，构成一个强分类器。大多数的提升方法都是改变training set的概率分布，针对不同的training set分布调用weak learner algorithm学习一系列weak learner。</p>\n<p>AdaBoost的做法是，先从初始training set中训练一个base learner，再根据base learner的表现对训练样本分布进行调整，使得先前base learner做错的样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个base learner；如此重复进行，直至base learner数目达到事先指定的值$T$，最终将这$T$个base learner进行加权结合。AdaBoost采用weighted majority voting的做法，具体的，加大分类误差率较小的weak learner的权重，使其在表决中起较大作用，减小分类误差大的weak learner的权重，使其在表决中起较小的作用。</p>\n<h4 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h4><ol>\n<li>初始化training set的权值分布<br>$D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N}$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>使用具有权值分布$D_m$的training set学习，得到base learner：<br>$G_m(x):\\chi \\to \\{-1,+1\\}$</li>\n<li>计算$G_m(x)$在training set上的分类误差率：<br>$e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$<br>$w_{mi}$是第$m$轮中第$i$个实例的权值。</li>\n<li>计算$G_m(x)$的系数：<br>$\\alpha_m=\\frac{1}{2}log \\frac{1-e_m}{e_m}$</li>\n<li>更新training set的权值分布：<br>$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$<br>$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_m y_i G_m(x_i))$<br>这里，$Z_m$是规范化因子：<br>$Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_m y_i G_m(x_i))$<br>它使得$D_{mi}$成为一个概率分布。</li>\n<li>构建base learner的线性组合：<br>$f(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$<br>得到最终分类器：<br>$f(x)=sign(f(x))=sign\\big(\\sum_{m=1}^M \\alpha_m G_m(x)\\big)$<br>注：$\\alpha_m$之和并不为1。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"AdaBoost算法的解释\"><a href=\"#AdaBoost算法的解释\" class=\"headerlink\" title=\"AdaBoost算法的解释\"></a>AdaBoost算法的解释</h4><p>AdaBoost还可以认为是模型为加法模型、Loss Function为指数函数、学习算法为前向分步算法时的二分类学习方法。</p>\n<h5 id=\"前向分步算法\"><a href=\"#前向分步算法\" class=\"headerlink\" title=\"前向分步算法\"></a>前向分步算法</h5><p>考虑加法模型：<br>$f(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$<br>其中，$b(x;\\gamma_m)$为基函数的参数，$\\beta_m$为基函数的系数。</p>\n<p>在给定training set及Loss Function $L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即Loss Function极小化问题：<br>$\\mathop{min} \\limits_{\\beta_m, \\gamma_m} \\sum_{i=1}^N L\\big(y_i,\\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)\\big)$</p>\n<p>前向分步算法求解复杂优化问题的思想是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每一步只需要优化以下Loss Function：<br>$\\mathop{min} \\limits_{\\beta, \\gamma}\\sum_{i=1}^N L(y_i,\\beta b(x_i;\\gamma))$</p>\n<ol>\n<li>初始化$f_0(x)=0$</li>\n<li><p>对$m=1,2,\\cdots,M$</p>\n<ul>\n<li><p>极小化Loss：<br>$(\\beta_m, \\gamma_m)=\\mathop{argmin} \\limits_{\\beta, \\gamma} \\sum_{i=1}^N L\\big(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma)\\big)$<br>得到参数$\\beta_m, \\gamma_m$</p>\n</li>\n<li><p>更新<br>$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$</p>\n</li>\n</ul>\n</li>\n<li>得到加法模型<br>$f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b(x;\\gamma_m)$</li>\n</ol>\n<h4 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h4><p>Boosting方法实际采用加法模型(即基函数的线性组合)与前向分步算法，以决策树为基函数的Boosting方法称为Boosting Tree。Boosting Tree模型可以表示为决策树的加法模型：<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$<br>其中$T(x;\\Theta_m)$表示决策树，$\\Theta_m$表示决策树参数，$T$为树的个数。</p>\n<p>Boosting Tree采用前向分步算法，首先确定初始提升树$f_0(x)=0$，第$m$步的模型是：<br>$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</p>\n<p>其中，$f_{m-1}(x)$为当前模型，通过经验风险最小化确定下一棵决策树的参数$\\Theta$，<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$</p>\n<p>若将输入空间$\\chi$划分为$J$个互不相交的区域$R_1,R_2,\\cdots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可以表示为：<br>$T(x;\\Theta)=\\sum_{j=1}^Jc_j I(x\\in R_j)$<br>其中，参数$\\Theta=\\{(R_1,c_1), (R_2,c_2), \\cdots, (R_J,c_J)\\}$表示树的区域划分和各区域上的常数，$J$是回归树的复杂度即叶结点个数。</p>\n<p>回归问题Boosting Tree使用以下前向分步算法：<br>$$<br>f_0(x)=0  \\\\<br>f_m(x)=f_{m-1}(x)+T(x;\\Theta_m) \\\\<br>f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)<br>$$</p>\n<p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解：<br>$\\hat{\\Theta}_m=\\mathop{argmin} \\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\\Theta_m))$<br>得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。</p>\n<p>采用MSE Loss时，<br>$L(y,f(x))=(y-f(x))^2$</p>\n<p>其损失变为：<br>$L(y,f_{m-1}(x) + T(x;\\Theta_m))=[y-f_{m-1}(x)-T(x;\\Theta_m)]^2=[r-T(x;\\Theta_m)]^2$</p>\n<p>这里，$r=y-f_{m-1}(x)$是模型拟合数据的残差。所以， <strong>对回归问题的Boosting Tree来说，只需简单地拟合当前模型的残差</strong>。</p>\n<h5 id=\"Boosting-Tree-for-Regression\"><a href=\"#Boosting-Tree-for-Regression\" class=\"headerlink\" title=\"Boosting Tree for Regression\"></a>Boosting Tree for Regression</h5><ol>\n<li>初始化$f_0(x)=0$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>计算残差：$r_{mi}=y_i-f_{m-1}(x_i)$</li>\n<li>拟合残差学习一个回归树，得到$T(x;\\Theta_m)$</li>\n<li>更新$f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)$</li>\n</ul>\n</li>\n<li>得到回归问题Boosting Tree：<br>$f_M(x)=\\sum_{m=1}^M T(x;\\Theta_m)$</li>\n</ol>\n<h4 id=\"Gradient-Boosting-GBDT\"><a href=\"#Gradient-Boosting-GBDT\" class=\"headerlink\" title=\"Gradient Boosting (GBDT)\"></a>Gradient Boosting (GBDT)</h4><p>Boosting Tree利用加法模型与前向分步算法实现学习的优化过程，当Loss Function是MSE和指数Loss时，每一步的优化是很简单的。但对于一般的Loss Function而言，往往每一步优化并不容易，这一问题可以利用Gradient Boosting解决。这是利用Gradient Descend的近似方法，其关键是利用Loss Function的负梯度在当前模型的值：<br>$$-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$$<br>作为回归问题提升树算法中的残差近似值，拟合一个回归树。</p>\n<ol>\n<li>初始化$f_0(x)=\\mathop{argmin} \\limits_{c} \\sum_{i=1}^N L(y_i,c)$</li>\n<li>对$m=1,2,\\cdots,M$<ul>\n<li>对$i=1,2,\\cdots,N$，计算：<br>$r_{mi}=-[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} ]_{f(x)=f_{m-1}(x)}$</li>\n<li>对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},\\quad j=1,2, \\cdots,J$</li>\n<li>对$j=1,2,\\cdots,J$计算：<br>$c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$</li>\n<li>更新$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$</li>\n</ul>\n</li>\n<li>得到回归树：<br>$$\\hat{f}(x)=f_M(x)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I(x\\in R_{mj})$$</li>\n</ol>\n<h3 id=\"Bagging-and-Random-Forests\"><a href=\"#Bagging-and-Random-Forests\" class=\"headerlink\" title=\"Bagging and Random Forests\"></a>Bagging and Random Forests</h3><h4 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h4><p>Bagging是并行式集成学习方法最著名的代表。它基于bootstrap sampling，给定包含$m$个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过$m$次随机采样，我们得到含有$m$个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。</p>\n<p>这样，我们可采样出$T$个含有$m$个训练样本的采样集，然后基于每个采样集训练出一个base learner，再将这些base learner进行集成。在对预测输出进行结合时，Bagging通常对分类任务使用majority voting，对回归任务使用averaging。</p>\n<p>与AdaBoost只适用于binary classification不同，Bagging能不经修改地用于多分类、回归等任务。<br>Bootstrap sampling还给Bagging带来了另一个优点：由于每个base learner只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用于validation set来对泛化性能进行out-of-bag estimate。Bagging主要关注降低variance，因此它在不剪枝决策树、NN等易受样本扰动的学习器上效用更为明显。</p>\n<h4 id=\"Random-Forests\"><a href=\"#Random-Forests\" class=\"headerlink\" title=\"Random Forests\"></a>Random Forests</h4><p>Random Forests是Bagging的一个变体，RF在以Decision Tree为base learner构建Bagging集成的基础上，进一步在Decision Tree的训练过程中引入了 <strong>随机属性选择</strong>。传统Decision Tree在选择划分属性时是在当前结点的属性集合（假定有$d$个属性）中选择一个最优属性；而在Random Forests中，对base decision tree的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性进行划分。这里参数$k$控制了随机性的引入程度；若$k=d$，则base learner的构建与传统decision tree相同；若$k=1$，则是随机选择一个属性用于划分。一般推荐$k=log_2d$。</p>\n<p>与Bagging中base learner的“多样性”仅通过样本扰动不同，<strong>Random Forests中base learner不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过base learner之间差异度的增加而进一步提升</strong>。</p>\n<p>Random Forests的训练效率通常优于Bagging，因为在base decision tree的构建过程中，Bagging使用的是“确定型”decision tree，在选择划分属性时要对结点的所有属性进行考察，而Random Forests使用的“随机型”decision tree则只需考察一个属性子集。</p>\n<h3 id=\"结合策略\"><a href=\"#结合策略\" class=\"headerlink\" title=\"结合策略\"></a>结合策略</h3><p>Learner的结合会从3方面带来好处：</p>\n<ol>\n<li>从统计方面看，由于学习任务的假设空间往往很大，可能有多个假设在training set上达到同等性能，此时若使用单学习器可能因误选导致泛化性能不佳，结合多个learner则会减小这一风险。</li>\n<li>从计算方面来看，learning algorithm往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入局部极小点的风险。</li>\n<li>从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个learner，由于相应的假设空间有所扩大，有可能学得到更好的近似。</li>\n</ol>\n<ul>\n<li><p>Averaging<br>$H(x)=\\frac{1}{T}\\sum_{i=1}^T h_i(x)$</p>\n</li>\n<li><p>Weighted Averaging<br>$H(x)=\\sum_{i=1}^Tw_i h_i(x)$<br>其中$w_i$一般从training set中学习而得。在base learner性能相差较大时宜采用Weighted Averaging，而在base learner性能相差不大时宜采用Averaging。</p>\n</li>\n<li><p>Voting</p>\n<ul>\n<li><p>Majority Voting<br>$$<br>H(x)=\\begin{cases}<br>c_j,\\quad if \\sum_{i=1}^Th_i^j(x)&gt;\\frac{1}{2}\\sum_{k=1}^N\\sum_{i=1}^T h_i^k(x), \\\\<br>reject, \\quad otherwise<br>\\end{cases}<br>$$</p>\n</li>\n<li><p>Plurality Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T h_i^j(x)}$<br>即预测为得票数最多的标记，若同时有多个标记获最高票，则从中随机选取一个。</p>\n</li>\n<li><p>Weighted Voting<br>$H(x)=c_{\\mathop{argmax} \\limits_{j} \\sum_{i=1}^T w_ih_i^j(x)}$</p>\n</li>\n</ul>\n</li>\n<li><p>Stacking<br>Stacking先从初始training set中训练出初级 learner，然后“生成”一个新数据集用于训练次级learner。在这个新数据集中，初级learner的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。</p>\n</li>\n</ul>\n<h3 id=\"多样性\"><a href=\"#多样性\" class=\"headerlink\" title=\"多样性\"></a>多样性</h3><h4 id=\"多样性增强\"><a href=\"#多样性增强\" class=\"headerlink\" title=\"多样性增强\"></a>多样性增强</h4><ul>\n<li><p>数据样本扰动<br>给定初始数据集，可从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，数据样本扰动通常是基于采样法。数据样本扰动法对“不稳定学习器”（例如NN、Decision Tree等）很有效。但是对“稳定学习器”（例如SVM、KNN、Naive Bayes）等，需要使用 <strong>输入属性扰动</strong>。</p>\n</li>\n<li><p>输入属性扰动<br>从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个base learner。对包含大量冗余属性的数据，子空间中训练base learner不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销。同时，由于冗余属性多，减少一些属性后训练出的base learner也不至于太差。</p>\n</li>\n<li><p>输入表示扰动<br>对输出表示进行操纵以增强多样性。可对training sample类标记稍作变动，如Flipping Out。</p>\n</li>\n<li><p>算法参数扰动<br>使用单一learner时通常需要cross validation来确定参数值，这事实上已经使用了不同参数训练出多个learner，只不过最终仅选择其中一个learner进行使用，而Ensemble Learning则相当于把这些learner都利用起来；由此可见，Ensemble Learning实际计算开销并不比使用单一learner大很多。</p>\n</li>\n</ul>\n"},{"title":"[ML] Feature Engineering in Machine Learning","date":"2018-08-20T11:19:59.000Z","catalog":true,"mathjax":true,"catagories":["Algorithm","Machine Learning","Feature Engineering"],"_content":"## Introduction\nFeature Engineering 是 Machine Learning 中一个非常非常重要的部分，尤其是工业界。很多时候，为了追求模型的*可解释性*，*效率*，我们会更加的倾向于选择*合适的特征* + *较为简单的模型*，而不会像research那样使用非常复杂的模型来刷分。因此本文主要对ML中常用的特征工程方法做一个简单的介绍。\n\n## Numerical Features\n### Scale\nExamples include **k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance**. For these models and modeling components, **it is often a good idea to normalize the features so that the output stays on an expected scale**.\n\nLogical functions, on the other hand, are not sensitive to input feature scale**. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). **Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort—eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.\n\n### Distribution\nIt's also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. **One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian**.\n\n### Quantization\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nIn order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: **fixed-width** or **adaptive**. We will give an example of each type.\n\n### Fixed-width binning\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. **To map from the count to the bin, we simply divide by the width of the bin and take the integer part**.\n\nIt’s also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.\n\n### Quantile binning\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.\n\n### Log Transformation\nThe log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.\n\n### Power Transforms: Generalization of the Log\nTransform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n$$\n\\tilde{x}=\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda} & if \\lambda \\neq 0,\\\\\nln(x) & if \\lambda = 0\n\\end{cases}\n$$\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.\n\n### Feature Scaling or Normalization\nSmooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.\n\n#### Min-Max Scaling\n$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$\n\n#### Standardization (Variance Scaling)\n$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$\n\nIt subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.\n\n> Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn't appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.\n\n#### $L^2$ Normalization\n$\\tilde{x}=\\frac{x}{||x||_2}$\n\n### Feature Selection\nFeature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\n#### Filtering\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\n#### Wrapper methods\nThese techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\n#### Embedded methods\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\n## Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n### Encoding Categorical Variables\n#### One-Hot Encoding\nA better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be \"on.\" This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.\n\n#### Dummy Coding\nThe problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding removes the extra degree of freedom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\n#### Effect Coding\nYet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1's.\n\n#### Pros and Cons of Categorical Variable Encodings \nOne-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\n### Dealing with Large Categorical Variables\nExisting solutions can be categorized as follows:\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n2. Compress the features. There are two choices: \n    * Feature hashing, popular with linear models \n    * Bin counting, popular with linear models as well as trees\n\n#### Feature Hashing\nThe idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.\n\n#### What about rare categories?\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.\n\n![Black-off Bin](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png)\n\nThere is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.\n\n#### Counts without bounds\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model \"knows\" the input data up to the observed scale.\n\nFor this reason, **it is often better to use normalized counts that are guaranteed to be bounded in a known interval**. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\n### Summary\n#### Plain one-hot encoding\n**Space requirement** $O(n)$ using the sparse vector format, where n is the number of data points.\n\n**Computation requirement** $O(nk)$ under a linear model, where k is the number of categories.\n\n**Pros**\n* Easiest to implement\n* Potentially most accurate\n* Feasible for online learning\n\n**Cons**\n* Computationally inefficient\n* Does not adapt to growing categories\n* Not feasible for anything other than linear models\n* Requires large-scale distributed optimization with truly\nlarge datasets\n\n#### Feature hashing\n**Space requirement** $O(n)$ using the sparse matrix format, where n is the number of data points.\n\n**Computation requirement** $O(nm)$ under a linear or kernel model, where m is the number of hash bins.\n\n**Pros** \n* Easy to implement\n* Makes model training cheaper\n* Easily adaptable to new categories\n* Easily handles rare categories\n* Feasible for online learning\n\n**Cons**\n* Only suitable for linear or kernelized models\n* Hashed features not interpretable\n* Mixed reports of accuracy\n\n#### Bin-counting\n**Space requirement** $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.\n\n**Computation requirement** $O(n)$ for linear models; also usable for nonlinear models such as trees.\n\n**Pros** \n* Smallest computational burden at training time\n* Enables tree-based models\n* Relatively easy to adapt to new categories\n* Handles rare categories with back-off or count-min sketch\n* Interpretable\n\n**Cons**\n* Requires historical data\n* Delayed updates required, not completely suitable for online\nlearning\n* Higher potential for leakage\n\n\n## Dimensionality Reduction: Squashing the Data Pancake with PCA","source":"_posts/ml-feml.md","raw":"---\ntitle: \"[ML] Feature Engineering in Machine Learning\"\ndate: 2018-08-20 19:19:59\ncatalog: true\nmathjax: true\ntags:\n- Feature Engineering\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n- Feature Engineering\n---\n## Introduction\nFeature Engineering 是 Machine Learning 中一个非常非常重要的部分，尤其是工业界。很多时候，为了追求模型的*可解释性*，*效率*，我们会更加的倾向于选择*合适的特征* + *较为简单的模型*，而不会像research那样使用非常复杂的模型来刷分。因此本文主要对ML中常用的特征工程方法做一个简单的介绍。\n\n## Numerical Features\n### Scale\nExamples include **k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance**. For these models and modeling components, **it is often a good idea to normalize the features so that the output stays on an expected scale**.\n\nLogical functions, on the other hand, are not sensitive to input feature scale**. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). **Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort—eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.\n\n### Distribution\nIt's also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. **One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian**.\n\n### Quantization\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nIn order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: **fixed-width** or **adaptive**. We will give an example of each type.\n\n### Fixed-width binning\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. **To map from the count to the bin, we simply divide by the width of the bin and take the integer part**.\n\nIt’s also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.\n\n### Quantile binning\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.\n\n### Log Transformation\nThe log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.\n\n### Power Transforms: Generalization of the Log\nTransform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n$$\n\\tilde{x}=\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda} & if \\lambda \\neq 0,\\\\\nln(x) & if \\lambda = 0\n\\end{cases}\n$$\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.\n\n### Feature Scaling or Normalization\nSmooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.\n\n#### Min-Max Scaling\n$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$\n\n#### Standardization (Variance Scaling)\n$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$\n\nIt subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.\n\n> Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn't appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.\n\n#### $L^2$ Normalization\n$\\tilde{x}=\\frac{x}{||x||_2}$\n\n### Feature Selection\nFeature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\n#### Filtering\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\n#### Wrapper methods\nThese techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\n#### Embedded methods\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\n## Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n### Encoding Categorical Variables\n#### One-Hot Encoding\nA better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be \"on.\" This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.\n\n#### Dummy Coding\nThe problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding removes the extra degree of freedom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\n#### Effect Coding\nYet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1's.\n\n#### Pros and Cons of Categorical Variable Encodings \nOne-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\n### Dealing with Large Categorical Variables\nExisting solutions can be categorized as follows:\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n2. Compress the features. There are two choices: \n    * Feature hashing, popular with linear models \n    * Bin counting, popular with linear models as well as trees\n\n#### Feature Hashing\nThe idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.\n\n#### What about rare categories?\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.\n\n![Black-off Bin](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png)\n\nThere is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.\n\n#### Counts without bounds\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model \"knows\" the input data up to the observed scale.\n\nFor this reason, **it is often better to use normalized counts that are guaranteed to be bounded in a known interval**. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\n### Summary\n#### Plain one-hot encoding\n**Space requirement** $O(n)$ using the sparse vector format, where n is the number of data points.\n\n**Computation requirement** $O(nk)$ under a linear model, where k is the number of categories.\n\n**Pros**\n* Easiest to implement\n* Potentially most accurate\n* Feasible for online learning\n\n**Cons**\n* Computationally inefficient\n* Does not adapt to growing categories\n* Not feasible for anything other than linear models\n* Requires large-scale distributed optimization with truly\nlarge datasets\n\n#### Feature hashing\n**Space requirement** $O(n)$ using the sparse matrix format, where n is the number of data points.\n\n**Computation requirement** $O(nm)$ under a linear or kernel model, where m is the number of hash bins.\n\n**Pros** \n* Easy to implement\n* Makes model training cheaper\n* Easily adaptable to new categories\n* Easily handles rare categories\n* Feasible for online learning\n\n**Cons**\n* Only suitable for linear or kernelized models\n* Hashed features not interpretable\n* Mixed reports of accuracy\n\n#### Bin-counting\n**Space requirement** $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.\n\n**Computation requirement** $O(n)$ for linear models; also usable for nonlinear models such as trees.\n\n**Pros** \n* Smallest computational burden at training time\n* Enables tree-based models\n* Relatively easy to adapt to new categories\n* Handles rare categories with back-off or count-min sketch\n* Interpretable\n\n**Cons**\n* Requires historical data\n* Delayed updates required, not completely suitable for online\nlearning\n* Higher potential for leakage\n\n\n## Dimensionality Reduction: Squashing the Data Pancake with PCA","slug":"ml-feml","published":1,"updated":"2018-10-01T04:40:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cl0013608wsc34nbfi","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Feature Engineering 是 Machine Learning 中一个非常非常重要的部分，尤其是工业界。很多时候，为了追求模型的<em>可解释性</em>，<em>效率</em>，我们会更加的倾向于选择<em>合适的特征</em> + <em>较为简单的模型</em>，而不会像research那样使用非常复杂的模型来刷分。因此本文主要对ML中常用的特征工程方法做一个简单的介绍。</p>\n<h2 id=\"Numerical-Features\"><a href=\"#Numerical-Features\" class=\"headerlink\" title=\"Numerical Features\"></a>Numerical Features</h2><h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>Examples include <strong>k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance</strong>. For these models and modeling components, <strong>it is often a good idea to normalize the features so that the output stays on an expected scale</strong>.</p>\n<p>Logical functions, on the other hand, are not sensitive to input feature scale<strong>. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). </strong>Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort—eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.</p>\n<h3 id=\"Distribution\"><a href=\"#Distribution\" class=\"headerlink\" title=\"Distribution\"></a>Distribution</h3><p>It’s also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. <strong>One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian</strong>.</p>\n<h3 id=\"Quantization\"><a href=\"#Quantization\" class=\"headerlink\" title=\"Quantization\"></a>Quantization</h3><p>Raw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.</p>\n<p>In order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: <strong>fixed-width</strong> or <strong>adaptive</strong>. We will give an example of each type.</p>\n<h3 id=\"Fixed-width-binning\"><a href=\"#Fixed-width-binning\" class=\"headerlink\" title=\"Fixed-width binning\"></a>Fixed-width binning</h3><p>With fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. <strong>To map from the count to the bin, we simply divide by the width of the bin and take the integer part</strong>.</p>\n<p>It’s also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.</p>\n<h3 id=\"Quantile-binning\"><a href=\"#Quantile-binning\" class=\"headerlink\" title=\"Quantile binning\"></a>Quantile binning</h3><p>Fixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.</p>\n<h3 id=\"Log-Transformation\"><a href=\"#Log-Transformation\" class=\"headerlink\" title=\"Log Transformation\"></a>Log Transformation</h3><p>The log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.</p>\n<h3 id=\"Power-Transforms-Generalization-of-the-Log\"><a href=\"#Power-Transforms-Generalization-of-the-Log\" class=\"headerlink\" title=\"Power Transforms: Generalization of the Log\"></a>Power Transforms: Generalization of the Log</h3><p>Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.</p>\n<p>A simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:<br>$$<br>\\tilde{x}=<br>\\begin{cases}<br>\\frac{x^{\\lambda}-1}{\\lambda} &amp; if \\lambda \\neq 0,\\\\<br>ln(x) &amp; if \\lambda = 0<br>\\end{cases}<br>$$</p>\n<p>The Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.</p>\n<h3 id=\"Feature-Scaling-or-Normalization\"><a href=\"#Feature-Scaling-or-Normalization\" class=\"headerlink\" title=\"Feature Scaling or Normalization\"></a>Feature Scaling or Normalization</h3><p>Smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.</p>\n<h4 id=\"Min-Max-Scaling\"><a href=\"#Min-Max-Scaling\" class=\"headerlink\" title=\"Min-Max Scaling\"></a>Min-Max Scaling</h4><p>$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$</p>\n<h4 id=\"Standardization-Variance-Scaling\"><a href=\"#Standardization-Variance-Scaling\" class=\"headerlink\" title=\"Standardization (Variance Scaling)\"></a>Standardization (Variance Scaling)</h4><p>$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$</p>\n<p>It subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.</p>\n<blockquote>\n<p>Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn’t appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.</p>\n</blockquote>\n<h4 id=\"L-2-Normalization\"><a href=\"#L-2-Normalization\" class=\"headerlink\" title=\"$L^2$ Normalization\"></a>$L^2$ Normalization</h4><p>$\\tilde{x}=\\frac{x}{||x||_2}$</p>\n<h3 id=\"Feature-Selection\"><a href=\"#Feature-Selection\" class=\"headerlink\" title=\"Feature Selection\"></a>Feature Selection</h3><p>Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.</p>\n<p>Roughly speaking, feature selection techniques fall into three classes:</p>\n<h4 id=\"Filtering\"><a href=\"#Filtering\" class=\"headerlink\" title=\"Filtering\"></a>Filtering</h4><p>Filtering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.</p>\n<h4 id=\"Wrapper-methods\"><a href=\"#Wrapper-methods\" class=\"headerlink\" title=\"Wrapper methods\"></a>Wrapper methods</h4><p>These techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.</p>\n<h4 id=\"Embedded-methods\"><a href=\"#Embedded-methods\" class=\"headerlink\" title=\"Embedded methods\"></a>Embedded methods</h4><p>These methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.</p>\n<h2 id=\"Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\"><a href=\"#Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\" class=\"headerlink\" title=\"Categorical Variables: Counting Eggs in the Age of Robotic Chickens\"></a>Categorical Variables: Counting Eggs in the Age of Robotic Chickens</h2><h3 id=\"Encoding-Categorical-Variables\"><a href=\"#Encoding-Categorical-Variables\" class=\"headerlink\" title=\"Encoding Categorical Variables\"></a>Encoding Categorical Variables</h3><h4 id=\"One-Hot-Encoding\"><a href=\"#One-Hot-Encoding\" class=\"headerlink\" title=\"One-Hot Encoding\"></a>One-Hot Encoding</h4><p>A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.</p>\n<h4 id=\"Dummy-Coding\"><a href=\"#Dummy-Coding\" class=\"headerlink\" title=\"Dummy Coding\"></a>Dummy Coding</h4><p>The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding removes the extra degree of freedom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.</p>\n<h4 id=\"Effect-Coding\"><a href=\"#Effect-Coding\" class=\"headerlink\" title=\"Effect Coding\"></a>Effect Coding</h4><p>Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1’s.</p>\n<h4 id=\"Pros-and-Cons-of-Categorical-Variable-Encodings\"><a href=\"#Pros-and-Cons-of-Categorical-Variable-Encodings\" class=\"headerlink\" title=\"Pros and Cons of Categorical Variable Encodings\"></a>Pros and Cons of Categorical Variable Encodings</h4><p>One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.</p>\n<h3 id=\"Dealing-with-Large-Categorical-Variables\"><a href=\"#Dealing-with-Large-Categorical-Variables\" class=\"headerlink\" title=\"Dealing with Large Categorical Variables\"></a>Dealing with Large Categorical Variables</h3><p>Existing solutions can be categorized as follows:</p>\n<ol>\n<li>Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.</li>\n<li>Compress the features. There are two choices: <ul>\n<li>Feature hashing, popular with linear models </li>\n<li>Bin counting, popular with linear models as well as trees</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"Feature-Hashing\"><a href=\"#Feature-Hashing\" class=\"headerlink\" title=\"Feature Hashing\"></a>Feature Hashing</h4><p>The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.</p>\n<h4 id=\"What-about-rare-categories\"><a href=\"#What-about-rare-categories\" class=\"headerlink\" title=\"What about rare categories?\"></a>What about rare categories?</h4><p>One way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png\" alt=\"Black-off Bin\"></p>\n<p>There is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.</p>\n<h4 id=\"Counts-without-bounds\"><a href=\"#Counts-without-bounds\" class=\"headerlink\" title=\"Counts without bounds\"></a>Counts without bounds</h4><p>If the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model “knows” the input data up to the observed scale.</p>\n<p>For this reason, <strong>it is often better to use normalized counts that are guaranteed to be bounded in a known interval</strong>. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.</p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><h4 id=\"Plain-one-hot-encoding\"><a href=\"#Plain-one-hot-encoding\" class=\"headerlink\" title=\"Plain one-hot encoding\"></a>Plain one-hot encoding</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse vector format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nk)$ under a linear model, where k is the number of categories.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easiest to implement</li>\n<li>Potentially most accurate</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Computationally inefficient</li>\n<li>Does not adapt to growing categories</li>\n<li>Not feasible for anything other than linear models</li>\n<li>Requires large-scale distributed optimization with truly<br>large datasets</li>\n</ul>\n<h4 id=\"Feature-hashing\"><a href=\"#Feature-hashing\" class=\"headerlink\" title=\"Feature hashing\"></a>Feature hashing</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse matrix format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nm)$ under a linear or kernel model, where m is the number of hash bins.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Easy to implement</li>\n<li>Makes model training cheaper</li>\n<li>Easily adaptable to new categories</li>\n<li>Easily handles rare categories</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Only suitable for linear or kernelized models</li>\n<li>Hashed features not interpretable</li>\n<li>Mixed reports of accuracy</li>\n</ul>\n<h4 id=\"Bin-counting\"><a href=\"#Bin-counting\" class=\"headerlink\" title=\"Bin-counting\"></a>Bin-counting</h4><p><strong>Space requirement</strong> $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.</p>\n<p><strong>Computation requirement</strong> $O(n)$ for linear models; also usable for nonlinear models such as trees.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Smallest computational burden at training time</li>\n<li>Enables tree-based models</li>\n<li>Relatively easy to adapt to new categories</li>\n<li>Handles rare categories with back-off or count-min sketch</li>\n<li>Interpretable</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Requires historical data</li>\n<li>Delayed updates required, not completely suitable for online<br>learning</li>\n<li>Higher potential for leakage</li>\n</ul>\n<h2 id=\"Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\"><a href=\"#Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\" class=\"headerlink\" title=\"Dimensionality Reduction: Squashing the Data Pancake with PCA\"></a>Dimensionality Reduction: Squashing the Data Pancake with PCA</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Feature Engineering 是 Machine Learning 中一个非常非常重要的部分，尤其是工业界。很多时候，为了追求模型的<em>可解释性</em>，<em>效率</em>，我们会更加的倾向于选择<em>合适的特征</em> + <em>较为简单的模型</em>，而不会像research那样使用非常复杂的模型来刷分。因此本文主要对ML中常用的特征工程方法做一个简单的介绍。</p>\n<h2 id=\"Numerical-Features\"><a href=\"#Numerical-Features\" class=\"headerlink\" title=\"Numerical Features\"></a>Numerical Features</h2><h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>Examples include <strong>k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance</strong>. For these models and modeling components, <strong>it is often a good idea to normalize the features so that the output stays on an expected scale</strong>.</p>\n<p>Logical functions, on the other hand, are not sensitive to input feature scale<strong>. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input $x$ greater than 5?). </strong>Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale**. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort—eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically.</p>\n<h3 id=\"Distribution\"><a href=\"#Distribution\" class=\"headerlink\" title=\"Distribution\"></a>Distribution</h3><p>It’s also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over several orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. <strong>One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not feature engineering.) Log transforms, which are a type of power transform, take the distribution of the variable closer to Gaussian</strong>.</p>\n<h3 id=\"Quantization\"><a href=\"#Quantization\" class=\"headerlink\" title=\"Quantization\"></a>Quantization</h3><p>Raw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. One solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.</p>\n<p>In order to quantize data, we have to decide how wide each bin should be. The solutions fall into two categories: <strong>fixed-width</strong> or <strong>adaptive</strong>. We will give an example of each type.</p>\n<h3 id=\"Fixed-width-binning\"><a href=\"#Fixed-width-binning\" class=\"headerlink\" title=\"Fixed-width binning\"></a>Fixed-width binning</h3><p>With fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. <strong>To map from the count to the bin, we simply divide by the width of the bin and take the integer part</strong>.</p>\n<p>It’s also common to see custom-designed age ranges that better correspond to stages of life. When the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count.</p>\n<h3 id=\"Quantile-binning\"><a href=\"#Quantile-binning\" class=\"headerlink\" title=\"Quantile binning\"></a>Quantile binning</h3><p>Fixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved byadaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution. Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.</p>\n<h3 id=\"Log-Transformation\"><a href=\"#Log-Transformation\" class=\"headerlink\" title=\"Log Transformation\"></a>Log Transformation</h3><p>The log transform is a powerful tool for dealing with positive numbers with a heavy-tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head.</p>\n<h3 id=\"Power-Transforms-Generalization-of-the-Log\"><a href=\"#Power-Transforms-Generalization-of-the-Log\" class=\"headerlink\" title=\"Power Transforms: Generalization of the Log\"></a>Power Transforms: Generalization of the Log</h3><p>Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power transforms change the distribution of the variable so that the variance is no longer dependent on the mean.</p>\n<p>A simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:<br>$$<br>\\tilde{x}=<br>\\begin{cases}<br>\\frac{x^{\\lambda}-1}{\\lambda} &amp; if \\lambda \\neq 0,\\\\<br>ln(x) &amp; if \\lambda = 0<br>\\end{cases}<br>$$</p>\n<p>The Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter $\\lambda$. This may be done via maximum likelihood (finding the $\\lambda$ that maximizes the Gaussian likelihood of the resulting transformed signal) or Bayesian methods.</p>\n<h3 id=\"Feature-Scaling-or-Normalization\"><a href=\"#Feature-Scaling-or-Normalization\" class=\"headerlink\" title=\"Feature Scaling or Normalization\"></a>Feature Scaling or Normalization</h3><p>Smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the other hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature.</p>\n<h4 id=\"Min-Max-Scaling\"><a href=\"#Min-Max-Scaling\" class=\"headerlink\" title=\"Min-Max Scaling\"></a>Min-Max Scaling</h4><p>$\\tilde{x}\\frac{x-min(x)}{max(x)-min(x)}$</p>\n<h4 id=\"Standardization-Variance-Scaling\"><a href=\"#Standardization-Variance-Scaling\" class=\"headerlink\" title=\"Standardization (Variance Scaling)\"></a>Standardization (Variance Scaling)</h4><p>$\\tilde{x}=\\frac{x-mean(x)}{sqrt{(var(x))}}$</p>\n<p>It subtracts off the mean of the feature (over all data points)and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too.</p>\n<blockquote>\n<p>Use caution when performing min-max scaling and standardization on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn’t appear in a document!). Bag-of-words is a sparse representation, and most classification libraries optimize for sparse inputs.</p>\n</blockquote>\n<h4 id=\"L-2-Normalization\"><a href=\"#L-2-Normalization\" class=\"headerlink\" title=\"$L^2$ Normalization\"></a>$L^2$ Normalization</h4><p>$\\tilde{x}=\\frac{x}{||x||_2}$</p>\n<h3 id=\"Feature-Selection\"><a href=\"#Feature-Selection\" class=\"headerlink\" title=\"Feature Selection\"></a>Feature Selection</h3><p>Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.</p>\n<p>Roughly speaking, feature selection techniques fall into three classes:</p>\n<h4 id=\"Filtering\"><a href=\"#Filtering\" class=\"headerlink\" title=\"Filtering\"></a>Filtering</h4><p>Filtering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techniques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.</p>\n<h4 id=\"Wrapper-methods\"><a href=\"#Wrapper-methods\" class=\"headerlink\" title=\"Wrapper methods\"></a>Wrapper methods</h4><p>These techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.</p>\n<h4 id=\"Embedded-methods\"><a href=\"#Embedded-methods\" class=\"headerlink\" title=\"Embedded methods\"></a>Embedded methods</h4><p>These methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another example is the regularizer, which can be added to the training objective of any linear model. The regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.</p>\n<h2 id=\"Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\"><a href=\"#Categorical-Variables-Counting-Eggs-in-the-Age-of-Robotic-Chickens\" class=\"headerlink\" title=\"Categorical Variables: Counting Eggs in the Age of Robotic Chickens\"></a>Categorical Variables: Counting Eggs in the Age of Robotic Chickens</h2><h3 id=\"Encoding-Categorical-Variables\"><a href=\"#Encoding-Categorical-Variables\" class=\"headerlink\" title=\"Encoding Categorical Variables\"></a>Encoding Categorical Variables</h3><h4 id=\"One-Hot-Encoding\"><a href=\"#One-Hot-Encoding\" class=\"headerlink\" title=\"One-Hot Encoding\"></a>One-Hot Encoding</h4><p>A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding, and it is implemented in scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k.</p>\n<h4 id=\"Dummy-Coding\"><a href=\"#Dummy-Coding\" class=\"headerlink\" title=\"Dummy Coding\"></a>Dummy Coding</h4><p>The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding removes the extra degree of freedom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.</p>\n<h4 id=\"Effect-Coding\"><a href=\"#Effect-Coding\" class=\"headerlink\" title=\"Effect Coding\"></a>Effect Coding</h4><p>Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1’s.</p>\n<h4 id=\"Pros-and-Cons-of-Categorical-Variable-Encodings\"><a href=\"#Pros-and-Cons-of-Categorical-Variable-Encodings\" class=\"headerlink\" title=\"Pros and Cons of Categorical Variable Encodings\"></a>Pros and Cons of Categorical Variable Encodings</h4><p>One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiplevalid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the allzeros vector, and the output should be the overall mean of the target variable. Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange. Effect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding. All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.</p>\n<h3 id=\"Dealing-with-Large-Categorical-Variables\"><a href=\"#Dealing-with-Large-Categorical-Variables\" class=\"headerlink\" title=\"Dealing with Large Categorical Variables\"></a>Dealing with Large Categorical Variables</h3><p>Existing solutions can be categorized as follows:</p>\n<ol>\n<li>Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.</li>\n<li>Compress the features. There are two choices: <ul>\n<li>Feature hashing, popular with linear models </li>\n<li>Bin counting, popular with linear models as well as trees</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"Feature-Hashing\"><a href=\"#Feature-Hashing\" class=\"headerlink\" title=\"Feature Hashing\"></a>Feature Hashing</h4><p>The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent.</p>\n<h4 id=\"What-about-rare-categories\"><a href=\"#What-about-rare-categories\" class=\"headerlink\" title=\"What about rare categories?\"></a>What about rare categories?</h4><p>One way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helps to also add a binary indicator for whether or not the statistics come from the back-off bin.</p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-feml/blackoff_bin.png\" alt=\"Black-off Bin\"></p>\n<p>There is another way to deal with this problem, called the count-min sketch (Cormode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low overall collision probability.</p>\n<h4 id=\"Counts-without-bounds\"><a href=\"#Counts-without-bounds\" class=\"headerlink\" title=\"Counts without bounds\"></a>Counts without bounds</h4><p>If the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model “knows” the input data up to the observed scale.</p>\n<p>For this reason, <strong>it is often better to use normalized counts that are guaranteed to be bounded in a known interval</strong>. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.</p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><h4 id=\"Plain-one-hot-encoding\"><a href=\"#Plain-one-hot-encoding\" class=\"headerlink\" title=\"Plain one-hot encoding\"></a>Plain one-hot encoding</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse vector format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nk)$ under a linear model, where k is the number of categories.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easiest to implement</li>\n<li>Potentially most accurate</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Computationally inefficient</li>\n<li>Does not adapt to growing categories</li>\n<li>Not feasible for anything other than linear models</li>\n<li>Requires large-scale distributed optimization with truly<br>large datasets</li>\n</ul>\n<h4 id=\"Feature-hashing\"><a href=\"#Feature-hashing\" class=\"headerlink\" title=\"Feature hashing\"></a>Feature hashing</h4><p><strong>Space requirement</strong> $O(n)$ using the sparse matrix format, where n is the number of data points.</p>\n<p><strong>Computation requirement</strong> $O(nm)$ under a linear or kernel model, where m is the number of hash bins.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Easy to implement</li>\n<li>Makes model training cheaper</li>\n<li>Easily adaptable to new categories</li>\n<li>Easily handles rare categories</li>\n<li>Feasible for online learning</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Only suitable for linear or kernelized models</li>\n<li>Hashed features not interpretable</li>\n<li>Mixed reports of accuracy</li>\n</ul>\n<h4 id=\"Bin-counting\"><a href=\"#Bin-counting\" class=\"headerlink\" title=\"Bin-counting\"></a>Bin-counting</h4><p><strong>Space requirement</strong> $O(n+k)$ for small, dense representation of each data point, plus the count statistics that must be kept for each category.</p>\n<p><strong>Computation requirement</strong> $O(n)$ for linear models; also usable for nonlinear models such as trees.</p>\n<p><strong>Pros</strong> </p>\n<ul>\n<li>Smallest computational burden at training time</li>\n<li>Enables tree-based models</li>\n<li>Relatively easy to adapt to new categories</li>\n<li>Handles rare categories with back-off or count-min sketch</li>\n<li>Interpretable</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Requires historical data</li>\n<li>Delayed updates required, not completely suitable for online<br>learning</li>\n<li>Higher potential for leakage</li>\n</ul>\n<h2 id=\"Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\"><a href=\"#Dimensionality-Reduction-Squashing-the-Data-Pancake-with-PCA\" class=\"headerlink\" title=\"Dimensionality Reduction: Squashing the Data Pancake with PCA\"></a>Dimensionality Reduction: Squashing the Data Pancake with PCA</h2>"},{"title":"[ML] Loss Function in ML","date":"2018-07-24T05:47:56.000Z","mathjax":true,"catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nLoss Function是ML/DL领域一个非常关键的因素，很多时候，我们如果能确定一个问题的Loss Function，那么这个问题就几乎已经被解决。常见的Loss可以归为以下几种：\n* Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss\n* Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss\n\n1. __MAE and MSE Loss__  \n   MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).\n   \n   For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be __median__ of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.\n\n   One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.\n\n2. __Huber Loss, Smooth Mean Absolute Error__   \n   Huber loss is less sensitive to outliers in data than the squared error loss. It’s also differentiable at 0. It's basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)\n\n   $$\n   L_{\\delta}(y,f(x))=\n   \\begin{cases}\n   \\frac{1}{2}(y-f(x))^2 & for |y-f(x)|\\leq \\delta \\\\\n   \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 & otherwise \n   \\end{cases}\n   $$\n\n   The choice of delta is critical because it determines what you're willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized \"appropriately\" with $L_2$.\n\n   __Why use Huber Loss?__  \n   One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n\n   Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n\n3. __Log-Cosh Loss__\n   Log-cosh is another function used in regression tasks that's smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.  \n   $L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ \n\n   ![Log-Cosh](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png)\n\n   __Advantage__: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it's twice differentiable everywhere,unlike Huber loss.\n\n   __Why do we need a 2nd derivative?__   \n   Many ML model implementations like XGBoost use Newton's method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.\n\n   But Log-cosh loss isn't perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.\n\n4. Quantile Loss\n   Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y—\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.\n\n   __Understanding the quantile loss function__  \n   The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median\n\n   $L_{\\gamma}(y,y^p)=\\sum_{i=y_i<y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$\n\n5. Exponential Loss  \n   $L(y,f(x))=exp(-yf(x))$","source":"_posts/ml-loss.md","raw":"---\ntitle: \"[ML] Loss Function in ML\"\ndate: 2018-07-24 13:47:56\nmathjax: true\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nLoss Function是ML/DL领域一个非常关键的因素，很多时候，我们如果能确定一个问题的Loss Function，那么这个问题就几乎已经被解决。常见的Loss可以归为以下几种：\n* Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss\n* Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss\n\n1. __MAE and MSE Loss__  \n   MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).\n   \n   For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be __median__ of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.\n\n   One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.\n\n2. __Huber Loss, Smooth Mean Absolute Error__   \n   Huber loss is less sensitive to outliers in data than the squared error loss. It’s also differentiable at 0. It's basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)\n\n   $$\n   L_{\\delta}(y,f(x))=\n   \\begin{cases}\n   \\frac{1}{2}(y-f(x))^2 & for |y-f(x)|\\leq \\delta \\\\\n   \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 & otherwise \n   \\end{cases}\n   $$\n\n   The choice of delta is critical because it determines what you're willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized \"appropriately\" with $L_2$.\n\n   __Why use Huber Loss?__  \n   One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n\n   Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n\n3. __Log-Cosh Loss__\n   Log-cosh is another function used in regression tasks that's smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.  \n   $L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ \n\n   ![Log-Cosh](https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png)\n\n   __Advantage__: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it's twice differentiable everywhere,unlike Huber loss.\n\n   __Why do we need a 2nd derivative?__   \n   Many ML model implementations like XGBoost use Newton's method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.\n\n   But Log-cosh loss isn't perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.\n\n4. Quantile Loss\n   Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y—\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.\n\n   __Understanding the quantile loss function__  \n   The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median\n\n   $L_{\\gamma}(y,y^p)=\\sum_{i=y_i<y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$\n\n5. Exponential Loss  \n   $L(y,f(x))=exp(-yf(x))$","slug":"ml-loss","published":1,"updated":"2018-10-01T04:40:09.048Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cm0015608wne980x56","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Loss Function是ML/DL领域一个非常关键的因素，很多时候，我们如果能确定一个问题的Loss Function，那么这个问题就几乎已经被解决。常见的Loss可以归为以下几种：</p>\n<ul>\n<li>Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss</li>\n<li>Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss</li>\n</ul>\n<ol>\n<li><p><strong>MAE and MSE Loss</strong><br>MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).</p>\n<p>For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be <strong>median</strong> of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.</p>\n<p>One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.</p>\n</li>\n<li><p><strong>Huber Loss, Smooth Mean Absolute Error</strong><br>Huber loss is less sensitive to outliers in data than the squared error loss. It’s also differentiable at 0. It’s basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)</p>\n<p>$$<br>L_{\\delta}(y,f(x))=<br>\\begin{cases}<br>\\frac{1}{2}(y-f(x))^2 &amp; for |y-f(x)|\\leq \\delta \\\\<br>\\delta|y-f(x)|-\\frac{1}{2}\\delta^2 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<p>The choice of delta is critical because it determines what you’re willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized “appropriately” with $L_2$.</p>\n<p><strong>Why use Huber Loss?</strong><br>One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.</p>\n<p>Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.</p>\n</li>\n<li><p><strong>Log-Cosh Loss</strong><br>Log-cosh is another function used in regression tasks that’s smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.<br>$L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png\" alt=\"Log-Cosh\"></p>\n<p><strong>Advantage</strong>: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that ‘logcosh’ works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it’s twice differentiable everywhere,unlike Huber loss.</p>\n<p><strong>Why do we need a 2nd derivative?</strong><br>Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.</p>\n<p>But Log-cosh loss isn’t perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.</p>\n</li>\n<li><p>Quantile Loss<br>Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y—\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.</p>\n<p><strong>Understanding the quantile loss function</strong><br>The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median</p>\n<p>$L_{\\gamma}(y,y^p)=\\sum_{i=y_i&lt;y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$</p>\n</li>\n<li><p>Exponential Loss<br>$L(y,f(x))=exp(-yf(x))$</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Loss Function是ML/DL领域一个非常关键的因素，很多时候，我们如果能确定一个问题的Loss Function，那么这个问题就几乎已经被解决。常见的Loss可以归为以下几种：</p>\n<ul>\n<li>Classification: Log Loss; Focal Loss; KL Divergence; Exponential Loss; Hinge Loss</li>\n<li>Regression: MSE; MAE; Huber Loss; Log Cosh Loss; Quantile Loss</li>\n</ul>\n<ol>\n<li><p><strong>MAE and MSE Loss</strong><br>MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously   receive unrealistically huge negative/positive values in our training environment, but not our testing environment). L1 loss is more robust to outliers, but its derivatives are not continuous,making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0).</p>\n<p>For MAE and MSE in regression, we can think about it like this: if we only had to give one prediction for all the observations that try to minimize MSE, then that prediction should be the mean of all target values. But if we try to minimize MAE, that prediction would be <strong>median</strong> of all observations. We know that median is more robust to outliers than MSE, which consequently makes MAE more robust to outliers than MSE.</p>\n<p>One big problem in using MAE Loss (for DNN especially) is that its gradient is the same throughout, which means that the gradient will be large even for small loss values. To fix this, we can use dynamic learning rate which decreases as we are more closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training.</p>\n</li>\n<li><p><strong>Huber Loss, Smooth Mean Absolute Error</strong><br>Huber loss is less sensitive to outliers in data than the squared error loss. It’s also differentiable at 0. It’s basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, $\\delta$ (delta), which can be tuned. Huber loss approaches MAE when $\\delta\\sim 0$ and MSE when $\\delta \\sim \\infty$ (large numbers.)</p>\n<p>$$<br>L_{\\delta}(y,f(x))=<br>\\begin{cases}<br>\\frac{1}{2}(y-f(x))^2 &amp; for |y-f(x)|\\leq \\delta \\\\<br>\\delta|y-f(x)|-\\frac{1}{2}\\delta^2 &amp; otherwise<br>\\end{cases}<br>$$</p>\n<p>The choice of delta is critical because it determines what you’re willing to consider as an outlier. Residuals larger than delta are minimized with $L_1$ (which is less sensitive to large outliers), while residuals smaller than delta are minimized “appropriately” with $L_2$.</p>\n<p><strong>Why use Huber Loss?</strong><br>One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.</p>\n<p>Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, it combines good properties from both MSE and MAE. However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.</p>\n</li>\n<li><p><strong>Log-Cosh Loss</strong><br>Log-cosh is another function used in regression tasks that’s smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.<br>$L(y,y^p)=\\sum_{i=1}^n log(cosh(y_i^p-y_i))$ </p>\n<p><img src=\"https://raw.githubusercontent.com/lucasxlu/blog/master/source/_posts/ml-loss/log-cosh.png\" alt=\"Log-Cosh\"></p>\n<p><strong>Advantage</strong>: $log(cosh(x))$ is approximately equal to $(x\\star \\star2)/2$ for small $x$ and to $abs(x)-log(2)$ for large $x$. This means that ‘logcosh’ works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it’s twice differentiable everywhere,unlike Huber loss.</p>\n<p><strong>Why do we need a 2nd derivative?</strong><br>Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, twice differentiable functions are more favorable.</p>\n<p>But Log-cosh loss isn’t perfect. It still suffers from the problem of gradient and hessian for very large off-target predictions being constant, therefore resulting in the absence of splits for XGBoost.</p>\n</li>\n<li><p>Quantile Loss<br>Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval from least square regression is based on an assumption that residuals ($y—\\hat{y}$) have constant variance across values of independent variables. We can not trust linear regression models which violate this assumption. We can not also just throw away the idea of fitting linear regression model as baseline by saying that such situations would always be better modeled using non-linear functions or tree based models. This is where quantile loss and quantile regression come to rescue as regression based on quantile loss provides sensible prediction intervals even for residuals with non-constant variance or non-normal distribution.</p>\n<p><strong>Understanding the quantile loss function</strong><br>The idea is to choose the quantile value based on whether we want to give more value to positive errors or negative errors. Loss function tries to give different penalties to overestimation and underestimation based on the value of chosen quantile ($\\gamma$). For example, a quantile loss function of $\\gamma=0.25$ gives more penalty to overestimation and tries to keep prediction values a little below median</p>\n<p>$L_{\\gamma}(y,y^p)=\\sum_{i=y_i&lt;y^p_i}(\\gamma-1)\\cdot |y_i-y_i^p|+\\sum_{i=y_i\\geq y_i^p}(\\gamma)\\cdot|y_i-y_i^p|$</p>\n</li>\n<li><p>Exponential Loss<br>$L(y,f(x))=exp(-yf(x))$</p>\n</li>\n</ol>\n"},{"title":"[ML] KNN","catalog":false,"mathjax":true,"date":"2018-07-19T04:37:17.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## 介绍\nKNN是机器学习中一种非常常见的分类/回归算法。在 __分类__ 时，根据majority voting来选择相应的预测Label；在 __回归__ 时，可取K个距离最近点的mean作为预测值。K值的选择、Distance Metric、分类决策规则是KNN算法的关键。\n\n### 常用的 Distance Metric\n$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$\n当$p=\\infty$时，它是各个坐标的极大值：$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$\n\n### K值的选择\n* 当K取较小值时，模型比较复杂，variance比较大，容易发生过拟合；当K取较大值时，模型比较简单，bias比较大，容易发生欠拟合。","source":"_posts/ml-knn.md","raw":"---\ntitle: \"[ML] KNN\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 12:37:17\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## 介绍\nKNN是机器学习中一种非常常见的分类/回归算法。在 __分类__ 时，根据majority voting来选择相应的预测Label；在 __回归__ 时，可取K个距离最近点的mean作为预测值。K值的选择、Distance Metric、分类决策规则是KNN算法的关键。\n\n### 常用的 Distance Metric\n$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$\n当$p=\\infty$时，它是各个坐标的极大值：$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$\n\n### K值的选择\n* 当K取较小值时，模型比较复杂，variance比较大，容易发生过拟合；当K取较大值时，模型比较简单，bias比较大，容易发生欠拟合。","slug":"ml-knn","published":1,"updated":"2018-10-01T04:40:09.017Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cp0017608wxlenocz5","content":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>KNN是机器学习中一种非常常见的分类/回归算法。在 <strong>分类</strong> 时，根据majority voting来选择相应的预测Label；在 <strong>回归</strong> 时，可取K个距离最近点的mean作为预测值。K值的选择、Distance Metric、分类决策规则是KNN算法的关键。</p>\n<h3 id=\"常用的-Distance-Metric\"><a href=\"#常用的-Distance-Metric\" class=\"headerlink\" title=\"常用的 Distance Metric\"></a>常用的 Distance Metric</h3><p>$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$<br>当$p=\\infty$时，它是各个坐标的极大值：$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$</p>\n<h3 id=\"K值的选择\"><a href=\"#K值的选择\" class=\"headerlink\" title=\"K值的选择\"></a>K值的选择</h3><ul>\n<li>当K取较小值时，模型比较复杂，variance比较大，容易发生过拟合；当K取较大值时，模型比较简单，bias比较大，容易发生欠拟合。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>KNN是机器学习中一种非常常见的分类/回归算法。在 <strong>分类</strong> 时，根据majority voting来选择相应的预测Label；在 <strong>回归</strong> 时，可取K个距离最近点的mean作为预测值。K值的选择、Distance Metric、分类决策规则是KNN算法的关键。</p>\n<h3 id=\"常用的-Distance-Metric\"><a href=\"#常用的-Distance-Metric\" class=\"headerlink\" title=\"常用的 Distance Metric\"></a>常用的 Distance Metric</h3><p>$L_p(x_i, x_j)=(\\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$<br>当$p=\\infty$时，它是各个坐标的极大值：$L_{\\infty}(x_i, x_j)=\\mathop{max}\\limits_{l}|x_i^{(l)}-x_j^{(l)}|$</p>\n<h3 id=\"K值的选择\"><a href=\"#K值的选择\" class=\"headerlink\" title=\"K值的选择\"></a>K值的选择</h3><ul>\n<li>当K取较小值时，模型比较复杂，variance比较大，容易发生过拟合；当K取较大值时，模型比较简单，bias比较大，容易发生欠拟合。</li>\n</ul>\n"},{"title":"[ML] Linear Model","catalog":false,"mathjax":true,"date":"2018-07-18T04:23:24.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Statistical Methods\n* 参数方法，例如Linear Model。参数方法容易拟合，且可解释性强，但也存在问题。以Linear Model为例，若真实存在的模型本身就不是Linear，那就会有问题。\n* 非参数方法：不需要显式指定$f$的形式，例如KNN。非参数方法的优点在于：因没有显示指定$f$，故Non-Parameter Methods可以拟合比Parameter Methods更广的函数。\n\n## Linear Model\n* Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:\n\n    $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$\n\n    That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.\n\n* Standard error也可用于假设检验：  \n  原假设$H_0$: $X$和$Y$之间没有关系，即$\\beta_1= 0$  \n  备择假设$H_{\\alpha}$: $X$和$Y$之间有关系，即$\\beta_1\\neq 0$  \n  \n  因此我们需要判断$\\hat{\\beta}_1$ (即$\\beta$的估计值)是否远离0。这取决于$SE(\\hat{\\beta}_1)$，若$SE(\\hat{\\beta}_1)$非常小，那么即便是相对较小的$\\hat{\\beta}_1$值也可以足够肯定$\\beta_1\\neq 0$。若$SE(\\hat{\\beta}_1)$非常大，那么$\\hat{\\beta}_1$的绝对值必须足够大才能拒绝原假设。\n\n* RSS measures the amount of variability that is left unexplained after performing the regression.\n\n* TSS-RSS measures the amount of variability in the response that is explained by performing the regression. \n\n* $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.\n\n* 可以使用 __residual plot__ 来分析Outliers，离得太远的可以被视为是outliers，但是如何量化\"多远\"这个度量呢？可以使用 __studentized residual__，计算方式为$e_i/SE(e_i)$。\n\n  __Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.__\n\n* A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\n\n  However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this __''multi-collinearity''__.\n\n  A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.\n\n  $$\n  VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}\n  $$\n  \n  VIF 最小为1，代表完全没有共线性，VIF大于5或10时代表有比较严重的共线性。\n\n* In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors ——that is called __curse of dimensionality__. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.\n\n* Generally, Parameter-Methods will tend to outperform non-parameter methods when there's a small number of observations per predictors.","source":"_posts/ml-lm.md","raw":"---\ntitle: \"[ML] Linear Model\"\ncatalog: false\nmathjax: true\ndate: 2018-07-18 12:23:24\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Statistical Methods\n* 参数方法，例如Linear Model。参数方法容易拟合，且可解释性强，但也存在问题。以Linear Model为例，若真实存在的模型本身就不是Linear，那就会有问题。\n* 非参数方法：不需要显式指定$f$的形式，例如KNN。非参数方法的优点在于：因没有显示指定$f$，故Non-Parameter Methods可以拟合比Parameter Methods更广的函数。\n\n## Linear Model\n* Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:\n\n    $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$\n\n    That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.\n\n* Standard error也可用于假设检验：  \n  原假设$H_0$: $X$和$Y$之间没有关系，即$\\beta_1= 0$  \n  备择假设$H_{\\alpha}$: $X$和$Y$之间有关系，即$\\beta_1\\neq 0$  \n  \n  因此我们需要判断$\\hat{\\beta}_1$ (即$\\beta$的估计值)是否远离0。这取决于$SE(\\hat{\\beta}_1)$，若$SE(\\hat{\\beta}_1)$非常小，那么即便是相对较小的$\\hat{\\beta}_1$值也可以足够肯定$\\beta_1\\neq 0$。若$SE(\\hat{\\beta}_1)$非常大，那么$\\hat{\\beta}_1$的绝对值必须足够大才能拒绝原假设。\n\n* RSS measures the amount of variability that is left unexplained after performing the regression.\n\n* TSS-RSS measures the amount of variability in the response that is explained by performing the regression. \n\n* $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.\n\n* 可以使用 __residual plot__ 来分析Outliers，离得太远的可以被视为是outliers，但是如何量化\"多远\"这个度量呢？可以使用 __studentized residual__，计算方式为$e_i/SE(e_i)$。\n\n  __Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.__\n\n* A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\n\n  However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this __''multi-collinearity''__.\n\n  A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.\n\n  $$\n  VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}\n  $$\n  \n  VIF 最小为1，代表完全没有共线性，VIF大于5或10时代表有比较严重的共线性。\n\n* In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors ——that is called __curse of dimensionality__. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.\n\n* Generally, Parameter-Methods will tend to outperform non-parameter methods when there's a small number of observations per predictors.","slug":"ml-lm","published":1,"updated":"2018-10-01T04:40:09.047Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cs001a608wk9gey017","content":"<h2 id=\"Statistical-Methods\"><a href=\"#Statistical-Methods\" class=\"headerlink\" title=\"Statistical Methods\"></a>Statistical Methods</h2><ul>\n<li>参数方法，例如Linear Model。参数方法容易拟合，且可解释性强，但也存在问题。以Linear Model为例，若真实存在的模型本身就不是Linear，那就会有问题。</li>\n<li>非参数方法：不需要显式指定$f$的形式，例如KNN。非参数方法的优点在于：因没有显示指定$f$，故Non-Parameter Methods可以拟合比Parameter Methods更广的函数。</li>\n</ul>\n<h2 id=\"Linear-Model\"><a href=\"#Linear-Model\" class=\"headerlink\" title=\"Linear Model\"></a>Linear Model</h2><ul>\n<li><p>Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:</p>\n<p>  $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$</p>\n<p>  That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.</p>\n</li>\n<li><p>Standard error也可用于假设检验：<br>原假设$H_0$: $X$和$Y$之间没有关系，即$\\beta_1= 0$<br>备择假设$H_{\\alpha}$: $X$和$Y$之间有关系，即$\\beta_1\\neq 0$  </p>\n<p>因此我们需要判断$\\hat{\\beta}_1$ (即$\\beta$的估计值)是否远离0。这取决于$SE(\\hat{\\beta}_1)$，若$SE(\\hat{\\beta}_1)$非常小，那么即便是相对较小的$\\hat{\\beta}_1$值也可以足够肯定$\\beta_1\\neq 0$。若$SE(\\hat{\\beta}_1)$非常大，那么$\\hat{\\beta}_1$的绝对值必须足够大才能拒绝原假设。</p>\n</li>\n<li><p>RSS measures the amount of variability that is left unexplained after performing the regression.</p>\n</li>\n<li><p>TSS-RSS measures the amount of variability in the response that is explained by performing the regression. </p>\n</li>\n<li><p>$R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.</p>\n</li>\n<li><p>可以使用 <strong>residual plot</strong> 来分析Outliers，离得太远的可以被视为是outliers，但是如何量化”多远”这个度量呢？可以使用 <strong>studentized residual</strong>，计算方式为$e_i/SE(e_i)$。</p>\n<p><strong>Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.</strong></p>\n</li>\n<li><p>A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>\n<p>However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this <strong>‘’multi-collinearity’’</strong>.</p>\n<p>A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.</p>\n<p>$$<br>VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}<br>$$</p>\n<p>VIF 最小为1，代表完全没有共线性，VIF大于5或10时代表有比较严重的共线性。</p>\n</li>\n<li><p>In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors ——that is called <strong>curse of dimensionality</strong>. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.</p>\n</li>\n<li><p>Generally, Parameter-Methods will tend to outperform non-parameter methods when there’s a small number of observations per predictors.</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Statistical-Methods\"><a href=\"#Statistical-Methods\" class=\"headerlink\" title=\"Statistical Methods\"></a>Statistical Methods</h2><ul>\n<li>参数方法，例如Linear Model。参数方法容易拟合，且可解释性强，但也存在问题。以Linear Model为例，若真实存在的模型本身就不是Linear，那就会有问题。</li>\n<li>非参数方法：不需要显式指定$f$的形式，例如KNN。非参数方法的优点在于：因没有显示指定$f$，故Non-Parameter Methods可以拟合比Parameter Methods更广的函数。</li>\n</ul>\n<h2 id=\"Linear-Model\"><a href=\"#Linear-Model\" class=\"headerlink\" title=\"Linear Model\"></a>Linear Model</h2><ul>\n<li><p>Standard error can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta1$ approximately takes the form:</p>\n<p>  $\\hat{\\beta}_1\\pm 2\\cdot SE(\\hat{\\beta}_1)$</p>\n<p>  That is, there is approximately a 95% chance that the interval $[\\hat{\\beta}_1- 2\\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+ 2\\cdot SE(\\hat{\\beta}_1)]$ will contain the true value of $\\beta_1$.</p>\n</li>\n<li><p>Standard error也可用于假设检验：<br>原假设$H_0$: $X$和$Y$之间没有关系，即$\\beta_1= 0$<br>备择假设$H_{\\alpha}$: $X$和$Y$之间有关系，即$\\beta_1\\neq 0$  </p>\n<p>因此我们需要判断$\\hat{\\beta}_1$ (即$\\beta$的估计值)是否远离0。这取决于$SE(\\hat{\\beta}_1)$，若$SE(\\hat{\\beta}_1)$非常小，那么即便是相对较小的$\\hat{\\beta}_1$值也可以足够肯定$\\beta_1\\neq 0$。若$SE(\\hat{\\beta}_1)$非常大，那么$\\hat{\\beta}_1$的绝对值必须足够大才能拒绝原假设。</p>\n</li>\n<li><p>RSS measures the amount of variability that is left unexplained after performing the regression.</p>\n</li>\n<li><p>TSS-RSS measures the amount of variability in the response that is explained by performing the regression. </p>\n</li>\n<li><p>$R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.</p>\n</li>\n<li><p>可以使用 <strong>residual plot</strong> 来分析Outliers，离得太远的可以被视为是outliers，但是如何量化”多远”这个度量呢？可以使用 <strong>studentized residual</strong>，计算方式为$e_i/SE(e_i)$。</p>\n<p><strong>Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.</strong></p>\n</li>\n<li><p>A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.</p>\n<p>However, it is possible for collinearity to exist between three or more variables even if no pair of variable has a particularly high correlation. We call this <strong>‘’multi-collinearity’’</strong>.</p>\n<p>A better way to detect multi-collinearity is to use VIF (variance inflation factor). VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fits on its own.</p>\n<p>$$<br>VIF(\\hat{\\beta_j})=\\frac{1}{1-R_{X_j|X_{-j}}^2}<br>$$</p>\n<p>VIF 最小为1，代表完全没有共线性，VIF大于5或10时代表有比较严重的共线性。</p>\n</li>\n<li><p>In high dimensions there is effectively a reduction in sample size. High dimensions result in a phenomenon in which a given observation has no nearby neighbors ——that is called <strong>curse of dimensionality</strong>. That is, the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit.</p>\n</li>\n<li><p>Generally, Parameter-Methods will tend to outperform non-parameter methods when there’s a small number of observations per predictors.</p>\n</li>\n</ul>\n"},{"title":"[ML] Logistic Regression and Maximum Entropy Model","mathjax":true,"date":"2018-07-23T09:28:30.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\nLogistic Regression是机器学习中一种非常经典且应用非常广泛的分类算法。Logistic Regression属于对数线性模型的一种。\n\n## Logistic Regression\n* Logistic Distribution: 设$X$是连续随机变量，$X$服从Logistic Distribution指的是$X$具有下列分布函数和密度函数:\n  $F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$\n\n  $f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$\n\n二项Logistic Regression是如下的条件概率分布：  \n$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$\n$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$  \n对于给定的输入实例$x$，可以求得$P(Y=1|X)$和$P(Y=0|X)$，Logistic Regression比较两个条件概率值的大小，将实例$x$划分到概率值大的那一类中。\n\nOdds: 一个事件发生的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\\frac{p}{1-p}$，该事件的对数几率(log odds)或logit函数是:   \n$logit(p)=log\\frac{p}{1-p}$  \n对Logistic Regression而言，可得：\n$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$\n\n这就是说，在Logistic Regression模型中，输出$Y=1$的对数几率是输入$x$的线性函数，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即Logistic Regression。\n\n### Logistic Regression参数估计\nLogistic Regression学习时，可以应用 __极大似然估计法估计模型参数__，从而得到Logistic Regression Model。\n设$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$，  \n似然函数为：\n$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$\n对数似然函数为  \n\n$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$\n\n$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  \n\n对$L(w)$求极大值，得到$w$的估计值。__Logistic Regression学习中通常采用梯度下降法和拟牛顿法__。\n\n#### 多项式Logistic Regression\n$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$\n\n$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$\n\n## Maximum Entropy Model\n","source":"_posts/ml-lr-me.md","raw":"---\ntitle: \"[ML] Logistic Regression and Maximum Entropy Model\"\nmathjax: true\ndate: 2018-07-23 17:28:30\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\nLogistic Regression是机器学习中一种非常经典且应用非常广泛的分类算法。Logistic Regression属于对数线性模型的一种。\n\n## Logistic Regression\n* Logistic Distribution: 设$X$是连续随机变量，$X$服从Logistic Distribution指的是$X$具有下列分布函数和密度函数:\n  $F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$\n\n  $f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$\n\n二项Logistic Regression是如下的条件概率分布：  \n$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$\n$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$  \n对于给定的输入实例$x$，可以求得$P(Y=1|X)$和$P(Y=0|X)$，Logistic Regression比较两个条件概率值的大小，将实例$x$划分到概率值大的那一类中。\n\nOdds: 一个事件发生的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\\frac{p}{1-p}$，该事件的对数几率(log odds)或logit函数是:   \n$logit(p)=log\\frac{p}{1-p}$  \n对Logistic Regression而言，可得：\n$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$\n\n这就是说，在Logistic Regression模型中，输出$Y=1$的对数几率是输入$x$的线性函数，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即Logistic Regression。\n\n### Logistic Regression参数估计\nLogistic Regression学习时，可以应用 __极大似然估计法估计模型参数__，从而得到Logistic Regression Model。\n设$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$，  \n似然函数为：\n$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$\n对数似然函数为  \n\n$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$\n\n$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  \n\n对$L(w)$求极大值，得到$w$的估计值。__Logistic Regression学习中通常采用梯度下降法和拟牛顿法__。\n\n#### 多项式Logistic Regression\n$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$\n\n$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$\n\n## Maximum Entropy Model\n","slug":"ml-lr-me","published":1,"updated":"2018-10-01T04:40:09.052Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cu001c608w09ol81m3","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Logistic Regression是机器学习中一种非常经典且应用非常广泛的分类算法。Logistic Regression属于对数线性模型的一种。</p>\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><ul>\n<li><p>Logistic Distribution: 设$X$是连续随机变量，$X$服从Logistic Distribution指的是$X$具有下列分布函数和密度函数:<br>$F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$</p>\n<p>$f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$</p>\n</li>\n</ul>\n<p>二项Logistic Regression是如下的条件概率分布：<br>$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$<br>$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$<br>对于给定的输入实例$x$，可以求得$P(Y=1|X)$和$P(Y=0|X)$，Logistic Regression比较两个条件概率值的大小，将实例$x$划分到概率值大的那一类中。</p>\n<p>Odds: 一个事件发生的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\\frac{p}{1-p}$，该事件的对数几率(log odds)或logit函数是:<br>$logit(p)=log\\frac{p}{1-p}$<br>对Logistic Regression而言，可得：<br>$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$</p>\n<p>这就是说，在Logistic Regression模型中，输出$Y=1$的对数几率是输入$x$的线性函数，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即Logistic Regression。</p>\n<h3 id=\"Logistic-Regression参数估计\"><a href=\"#Logistic-Regression参数估计\" class=\"headerlink\" title=\"Logistic Regression参数估计\"></a>Logistic Regression参数估计</h3><p>Logistic Regression学习时，可以应用 <strong>极大似然估计法估计模型参数</strong>，从而得到Logistic Regression Model。<br>设$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$，<br>似然函数为：<br>$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$<br>对数似然函数为  </p>\n<p>$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$</p>\n<p>$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  </p>\n<p>对$L(w)$求极大值，得到$w$的估计值。<strong>Logistic Regression学习中通常采用梯度下降法和拟牛顿法</strong>。</p>\n<h4 id=\"多项式Logistic-Regression\"><a href=\"#多项式Logistic-Regression\" class=\"headerlink\" title=\"多项式Logistic Regression\"></a>多项式Logistic Regression</h4><p>$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$</p>\n<p>$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$</p>\n<h2 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Logistic Regression是机器学习中一种非常经典且应用非常广泛的分类算法。Logistic Regression属于对数线性模型的一种。</p>\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><ul>\n<li><p>Logistic Distribution: 设$X$是连续随机变量，$X$服从Logistic Distribution指的是$X$具有下列分布函数和密度函数:<br>$F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$</p>\n<p>$f(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma (1+e^{-(x-\\mu)/\\gamma})^2}$</p>\n</li>\n</ul>\n<p>二项Logistic Regression是如下的条件概率分布：<br>$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$<br>$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$<br>对于给定的输入实例$x$，可以求得$P(Y=1|X)$和$P(Y=0|X)$，Logistic Regression比较两个条件概率值的大小，将实例$x$划分到概率值大的那一类中。</p>\n<p>Odds: 一个事件发生的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\\frac{p}{1-p}$，该事件的对数几率(log odds)或logit函数是:<br>$logit(p)=log\\frac{p}{1-p}$<br>对Logistic Regression而言，可得：<br>$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$</p>\n<p>这就是说，在Logistic Regression模型中，输出$Y=1$的对数几率是输入$x$的线性函数，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即Logistic Regression。</p>\n<h3 id=\"Logistic-Regression参数估计\"><a href=\"#Logistic-Regression参数估计\" class=\"headerlink\" title=\"Logistic Regression参数估计\"></a>Logistic Regression参数估计</h3><p>Logistic Regression学习时，可以应用 <strong>极大似然估计法估计模型参数</strong>，从而得到Logistic Regression Model。<br>设$P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$，<br>似然函数为：<br>$\\prod_{i=1}^N [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$<br>对数似然函数为  </p>\n<p>$L(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]=\\sum_{i=1}^N [y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i))]$</p>\n<p>$=\\sum_{i=1}^N[y_i(w\\cdot x_i)-log(1+exp(w\\cdot x_i))]$  </p>\n<p>对$L(w)$求极大值，得到$w$的估计值。<strong>Logistic Regression学习中通常采用梯度下降法和拟牛顿法</strong>。</p>\n<h4 id=\"多项式Logistic-Regression\"><a href=\"#多项式Logistic-Regression\" class=\"headerlink\" title=\"多项式Logistic Regression\"></a>多项式Logistic Regression</h4><p>$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}, \\quad k=1,2,\\cdots,K-1$</p>\n<p>$P(Y=K|x)=\\frac{1}{1+\\sum_{i=1}^{K-1} exp(w_k\\cdot x)}$</p>\n<h2 id=\"Maximum-Entropy-Model\"><a href=\"#Maximum-Entropy-Model\" class=\"headerlink\" title=\"Maximum Entropy Model\"></a>Maximum Entropy Model</h2>"},{"title":"[ML] Model Selection and Performance Metric","catalog":false,"mathjax":true,"date":"2018-07-19T03:02:28.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n* 当$p>n$时，无法使用Backward Selection，但Forward Selection可以。\n\n* $Precision=\\frac{TP}{TP+FP}$  \n  $Recall=\\frac{TP}{TP+FN}$  \n  $F_1=\\frac{2PR}{P+R}$  \n\n* 很多时候我们有多个二分类confusionmatrix，例如进行多次training/test，每次得到一个confusion matrix，或是在多个数据集上进行training/test，希望估计算法的全局性能。总之，我们希望在$n$ 个二分类confusion matrix上综合考查Precision和Recall：\n    * Macro-P/Macro-R/Macro-F1: 先在各个confusion matrix上分别计算 P/R/F1，再计算均值：\n    $Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$  \n    $Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$  \n    $Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$\n    \n    * Micro-P/Micro-R/Micro-F1: 先将各confusion matrix元素平均，得到TP, FP, TN, FN的均值，再计算：\n    $Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$  \n    $Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$  \n    $Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$\n\n* ROC曲线的纵轴是\"真正率\"(TPR)，横轴是\"假正率\"(FPR)：\n    $TPR=\\frac{TP}{TP+FN}$  \n    $FPR=\\frac{FP}{FP+TN}$\n现实任务中，通常是利用有限个test set samples来绘制ROC，此时仅能获得有限个(TPR, FPR)坐标对，只能绘制比较粗糙(带锯齿)的ROC曲线。\n\n* 泛化误差可分解为: $bias^2+variance+\\epsilon^2$\n  $bias$度量了learner本身的拟合能力；\n  $variance$度量了同样大小training set变动所导致的性能变化，即刻画了数据扰动所造成的影响；\n  $\\epsilon$表达了当前任务上任何learner所能达到的期望泛化误差下界，即刻画了学习问题本身的难度。","source":"_posts/ml-model-selection-metric.md","raw":"---\ntitle: \"[ML] Model Selection and Performance Metric\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 11:02:28\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n* 当$p>n$时，无法使用Backward Selection，但Forward Selection可以。\n\n* $Precision=\\frac{TP}{TP+FP}$  \n  $Recall=\\frac{TP}{TP+FN}$  \n  $F_1=\\frac{2PR}{P+R}$  \n\n* 很多时候我们有多个二分类confusionmatrix，例如进行多次training/test，每次得到一个confusion matrix，或是在多个数据集上进行training/test，希望估计算法的全局性能。总之，我们希望在$n$ 个二分类confusion matrix上综合考查Precision和Recall：\n    * Macro-P/Macro-R/Macro-F1: 先在各个confusion matrix上分别计算 P/R/F1，再计算均值：\n    $Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$  \n    $Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$  \n    $Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$\n    \n    * Micro-P/Micro-R/Micro-F1: 先将各confusion matrix元素平均，得到TP, FP, TN, FN的均值，再计算：\n    $Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$  \n    $Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$  \n    $Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$\n\n* ROC曲线的纵轴是\"真正率\"(TPR)，横轴是\"假正率\"(FPR)：\n    $TPR=\\frac{TP}{TP+FN}$  \n    $FPR=\\frac{FP}{FP+TN}$\n现实任务中，通常是利用有限个test set samples来绘制ROC，此时仅能获得有限个(TPR, FPR)坐标对，只能绘制比较粗糙(带锯齿)的ROC曲线。\n\n* 泛化误差可分解为: $bias^2+variance+\\epsilon^2$\n  $bias$度量了learner本身的拟合能力；\n  $variance$度量了同样大小training set变动所导致的性能变化，即刻画了数据扰动所造成的影响；\n  $\\epsilon$表达了当前任务上任何learner所能达到的期望泛化误差下界，即刻画了学习问题本身的难度。","slug":"ml-model-selection-metric","published":1,"updated":"2018-10-01T04:40:09.054Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cw001f608wuc5jmdy4","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ul>\n<li><p>当$p&gt;n$时，无法使用Backward Selection，但Forward Selection可以。</p>\n</li>\n<li><p>$Precision=\\frac{TP}{TP+FP}$<br>$Recall=\\frac{TP}{TP+FN}$<br>$F_1=\\frac{2PR}{P+R}$  </p>\n</li>\n<li><p>很多时候我们有多个二分类confusionmatrix，例如进行多次training/test，每次得到一个confusion matrix，或是在多个数据集上进行training/test，希望估计算法的全局性能。总之，我们希望在$n$ 个二分类confusion matrix上综合考查Precision和Recall：</p>\n<ul>\n<li><p>Macro-P/Macro-R/Macro-F1: 先在各个confusion matrix上分别计算 P/R/F1，再计算均值：<br>$Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$<br>$Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$<br>$Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$</p>\n</li>\n<li><p>Micro-P/Micro-R/Micro-F1: 先将各confusion matrix元素平均，得到TP, FP, TN, FN的均值，再计算：<br>$Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$<br>$Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$<br>$Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$</p>\n</li>\n</ul>\n</li>\n<li><p>ROC曲线的纵轴是”真正率”(TPR)，横轴是”假正率”(FPR)：<br>  $TPR=\\frac{TP}{TP+FN}$<br>  $FPR=\\frac{FP}{FP+TN}$<br>现实任务中，通常是利用有限个test set samples来绘制ROC，此时仅能获得有限个(TPR, FPR)坐标对，只能绘制比较粗糙(带锯齿)的ROC曲线。</p>\n</li>\n<li><p>泛化误差可分解为: $bias^2+variance+\\epsilon^2$<br>$bias$度量了learner本身的拟合能力；<br>$variance$度量了同样大小training set变动所导致的性能变化，即刻画了数据扰动所造成的影响；<br>$\\epsilon$表达了当前任务上任何learner所能达到的期望泛化误差下界，即刻画了学习问题本身的难度。</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ul>\n<li><p>当$p&gt;n$时，无法使用Backward Selection，但Forward Selection可以。</p>\n</li>\n<li><p>$Precision=\\frac{TP}{TP+FP}$<br>$Recall=\\frac{TP}{TP+FN}$<br>$F_1=\\frac{2PR}{P+R}$  </p>\n</li>\n<li><p>很多时候我们有多个二分类confusionmatrix，例如进行多次training/test，每次得到一个confusion matrix，或是在多个数据集上进行training/test，希望估计算法的全局性能。总之，我们希望在$n$ 个二分类confusion matrix上综合考查Precision和Recall：</p>\n<ul>\n<li><p>Macro-P/Macro-R/Macro-F1: 先在各个confusion matrix上分别计算 P/R/F1，再计算均值：<br>$Macro-P=\\frac{1}{n}\\sum_{i=1}^n P_i$<br>$Macro-R=\\frac{1}{n}\\sum_{i=1}^n R_i$<br>$Macro-F_1=\\frac{2\\times Macro-P \\times Macro-R}{Macro-P+Macro-R}$</p>\n</li>\n<li><p>Micro-P/Micro-R/Micro-F1: 先将各confusion matrix元素平均，得到TP, FP, TN, FN的均值，再计算：<br>$Micro-P=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FP}}$<br>$Micro-R=\\frac{\\bar{TP}}{\\bar{TP}+\\bar{FN}}$<br>$Micro-F_1=\\frac{2\\times Micro-P\\times Micro-R}{Micro-P+Micro-R}$</p>\n</li>\n</ul>\n</li>\n<li><p>ROC曲线的纵轴是”真正率”(TPR)，横轴是”假正率”(FPR)：<br>  $TPR=\\frac{TP}{TP+FN}$<br>  $FPR=\\frac{FP}{FP+TN}$<br>现实任务中，通常是利用有限个test set samples来绘制ROC，此时仅能获得有限个(TPR, FPR)坐标对，只能绘制比较粗糙(带锯齿)的ROC曲线。</p>\n</li>\n<li><p>泛化误差可分解为: $bias^2+variance+\\epsilon^2$<br>$bias$度量了learner本身的拟合能力；<br>$variance$度量了同样大小training set变动所导致的性能变化，即刻画了数据扰动所造成的影响；<br>$\\epsilon$表达了当前任务上任何learner所能达到的期望泛化误差下界，即刻画了学习问题本身的难度。</p>\n</li>\n</ul>\n"},{"title":"[ML] SVM","mathjax":true,"date":"2018-07-22T04:42:39.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## 介绍\nSVM是一种非常经典的分类算法，也是很多机器学习面试中必问的算法。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习策略就是间隔最大化，等价于 __正则化的Hinge Loss最小化问题__。\n\n当训练数据线性可分时，通过硬间隔最大化学习一个线性的分类器；  \n当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器；  \n当训练数据线性不可分时，通过Kernel Tricks和软间隔最大化学习一个非线性的分类器；\n\n当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示 __将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持的SVM，等价于隐式地在高维的特征空间中学习线性SVM__。\n\n一般的，当training set线性可分时，存在无穷个分离超平面可将两类数据正确分开。MLP利用 __误分类最小策略__，求得分离超平面，不过这时的解有无穷多个。线性可分SVM利用 __间隔最大化__求得分离超平面，这时解是唯一的。\n\n## 线性可分SVM与硬间隔最大化\n一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w\\cdot x+b=0$确定的情况下，$|w\\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\\cdot x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用量$y(w\\cdot x+b)$来表示分类的正确性及确信程度，此为 __\"函数间隔\"__。\n\n* 函数间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:  \n  $\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$\n\n  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  函数间隔可以表示分类预测的正确度及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面没有变，但是函数间隔却变为原来的2倍。这一事实启示我们，可以对分离超平面的法向量$w$加某些约束，如归一化$||w||=1$，使得间隔是确定的。这时函数间隔成为 __几何间隔__。\n\n* 几何间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：  \n  $\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$\n\n  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。\n\n  函数间隔和几何间隔有如下关系：  \n  $\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$\n\n  $\\gamma=\\frac{\\hat{\\gamma}}{||w||}$\n\n  __如果$||w||=1，那么函数间隔和几何间隔相等$__。如果超平面参数$w$和$b$成比例地改变(超平面未变)，则函数间隔也按此比例改变，但是几何间隔不变。\n\n\n最大间隔分离超平面  可以表示为下面的约束最优化问题：  \n$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$\n\n即我们希望最大化超平面$(w,b)$关于training set的几何间隔$\\gamma$，约束条件表示的是超平面$(w,b)$关于每个training sample的几何间隔至少是$\\gamma$。\n\n考虑几何间隔和函数间隔的关系，该问题等价于：\n$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$\n\n最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$是等价的，于是就得到下面的线性可分SVM的最优化问题：\n$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$\n\n* 最大间隔分离超平面的存在唯一性：若训练数据集T线性可分，则可将训练集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。\n\n在线性可分情况下，training set的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件等号成立的点，即：\n$y_i(w\\cdot x_i+b)-1=0$\n\n对$y_i=+1$的正例点，支持向量在超平面 $H_1:w\\cdot x+b=1$上，对$y_i=-1$的负例点，支持向量在超平面 $H_2:w\\cdot x+b=-1$上。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。由于支持向量在确定分离超平面中起着决定性的作用，所以将这种分类模型称为\"支持向量机\"。\n\n\n### 学习的对偶算法\n为了求解线性可分SVM的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVM的对偶算法。这样做一来对偶问题更容易求解，二来自然引入Kernel Function，可以扩展到非线性分类问题。\n\n引入拉格朗日乘子$\\alpha_i \\geq 0, i=1,2,\\cdots,N$，定义拉格朗日函数：\n$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$，其中，$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$为拉格朗日乘子向量。\n\n根据拉格朗日对偶性，原始问题的对偶问题是极大极小值问题：\n$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$，所以为了得到对偶问题的解，需要先求$L(w,b,\\alpha)$对$w,b$的极小，再求对$\\alpha$的极大。\n\n1. 求$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$：  \n   将拉格朗日函数$L(w,b,\\alpha)$分别对$w,b$求偏导，并令其等于0。  \n   $\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$\n\n   $\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$  \n  得:  \n  $w=\\sum_{i=1}^N\\alpha_i y_i x_i$  \n  $\\sum_{i=1}^N\\alpha_i y_i=0$  \n  可得:  \n  $\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$\n\n2. 求解$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题  \n  $$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n  可转换成下面等价的求极小值的对偶问题：\n  $$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n#### 线性可分SVM的学习算法\n1. 构造并求解约束最优化问题:\n  $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$  \n  得到最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。\n\n2. 计算 $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$ ，并选择 $\\alpha^{\\star}$ 的一个正分量 $\\alpha_j^{\\star}>0$，计算:  \n  $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$\n\n3. 求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，分类决策函数 $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$。\n\n## 线性SVM与软间隔最大化\n线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点$(x_i, y_i)$引入一个松弛变量$\\xi_i \\geq0$，使得函数间隔加上松弛变量大于等于1。这样，约束条件变为:  \n$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n同时，对每个松弛变量，支付一个代价$\\xi_i$，目标函数由原来的$\\frac{1}{2}||w||^2$变成 $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$。这里$C>0$称为惩罚参数，一般由问题决定。$C$值大时对误分类的惩罚加大，$C$值小时对误分类的惩罚变小。最小化Loss有两层含义：使$\\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使得误分类点的个数尽量小，$C$是两者的调和系数。\n\n线性不可分的SVM学习问题变成如下凸二次规划问题：  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$\n   \n### 学习的对偶算法\n原始问题的对偶问题是:\n$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$\n$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$\n\n#### 线性可分SVM的学习算法\n1. 选择惩罚参数$C>0$，构造并求解凸二次规划问题：  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$  \n   $s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$  \n   求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. 计算$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$  \n   选择$\\alpha^{\\star}$的一个分量$\\alpha_j^{\\star}$适合条件$0<\\alpha_j^{\\star}<C$ (通常取所有符合条件的样本点上的均值)，计算  \n   $b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$\n  \n3. 求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，\n   分类决策函数$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n#### 支持向量\n软间隔的支持向量$x_i$或者在间隔边界上，或者在降额边界与分离超平面之间，或者在分离超平面误分一侧。  \n\n若$\\alpha_i^{\\star}<C$，则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；  \n若$\\alpha_i^{\\star}=C, 0<\\xi_i<1$，则分类正确，$x_i$将在间隔边界与超平面之间；  \n若$\\alpha_i^{\\star}=C, \\xi_i=1$，则$x_i$在分离超平面上；  \n若$\\alpha_i^{\\star}=C, \\xi_i>1$，则$x_i$位于分离超平面误分一侧。\n\n#### Hinge Loss\nSVM还有另一种解释，即最小化以下Loss Function：  \n$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$\n\n$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$称为 Hinge Loss。这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔 $y_i(w\\cdot x_i+b)$大于1时，损失为0，否则损失是 $1-y_i(w\\cdot x_i+b)$。\n\n线性SVM原始最优化问题:  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$\n\n$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i\\geq 0$\n\n等价于最优化问题：\n\n$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$\n\n## 非线性SVM与Kernel Function\n设原空间为$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$，新空间为$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$，定义从原空间到新空间的变换(映射)：  \n$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$  \n经过变换$z=\\phi(x)$，原空间$\\chi \\subset R^2$变换为新空间$\\mathcal{Z}\\subset R^2$，原空间中的点变为新空间中的点。从而原空间线性不可分的情形变为新空间里的线性可分问题。\n\n### Kernel Function\n设$\\chi$是输入空间(欧式空间$R^n$或离散集合)，又设$\\mathcal{H}$为特征空间(希尔伯特空间)，若存在一个从$\\chi$到$\\mathcal{H}$的映射:  \n$\\phi(x):\\chi \\to \\mathcal{H}$\n使得对所有$x,z\\in \\chi$，函数$K(x,z)$都满足：\n$K(x,z)=\\phi(x)\\cdot \\phi(z)$  \n则称$K(x,z)$为核函数，$\\phi(x)$为映射函数，式子中$\\phi(x)\\cdot \\phi(z)$为$\\phi(x)$和$\\phi(z)$内积。\n\nKernel Tricks的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\\phi$，直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。\n\nKernel-based SVM等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\\cdot x_j$变换为特征空间中的内积$\\phi(x_i)\\cdot \\phi(x_j)$，在新的特征空间里从训练样本中学习线性SVM。当映射函数是非线性函数时，学习到的含有核函数的SVM是非线性分类模型。\n\nKernel Tricks：学习是隐式地在特征空间进行的，不需要显示地定义特征空间和映射函数，这样的技巧称为Kernel Tricks。\n\n### 常用Kernel Function\n1. 多项式核函数：  \n   $K(x,z)=(x\\cdot z+1)^p$  \n   对应的SVM是一个$p$次多项式分类器，在此情形下，分类决策函数成为:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$\n\n2. 高斯核函数：  \n   $K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$\n   对应的SVM是RBF分类器，在此情形下，分类决策函数成为:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$\n\n### 非线性SVM\n从非线性分类数据集，通过Kernel Tricks与软间隔最大化，或凸二次规划，学习到的分类决策函数：  \n$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$\n称为非线性支持向量，$K(x,z)$是正定核函数。\n\n#### 非线性SVM学习算法\n1. 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题：  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$\n\n   $s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$\n   求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。\n\n2. 选择$\\alpha^{\\star}$的一个正分量$0<\\alpha_i^{\\star}<C$，计算 $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$\n\n3. 构造决策函数：  \n   $f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$","source":"_posts/ml-svm.md","raw":"---\ntitle: \"[ML] SVM\"\nmathjax: true\ndate: 2018-07-22 12:42:39\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## 介绍\nSVM是一种非常经典的分类算法，也是很多机器学习面试中必问的算法。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习策略就是间隔最大化，等价于 __正则化的Hinge Loss最小化问题__。\n\n当训练数据线性可分时，通过硬间隔最大化学习一个线性的分类器；  \n当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器；  \n当训练数据线性不可分时，通过Kernel Tricks和软间隔最大化学习一个非线性的分类器；\n\n当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示 __将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持的SVM，等价于隐式地在高维的特征空间中学习线性SVM__。\n\n一般的，当training set线性可分时，存在无穷个分离超平面可将两类数据正确分开。MLP利用 __误分类最小策略__，求得分离超平面，不过这时的解有无穷多个。线性可分SVM利用 __间隔最大化__求得分离超平面，这时解是唯一的。\n\n## 线性可分SVM与硬间隔最大化\n一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w\\cdot x+b=0$确定的情况下，$|w\\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\\cdot x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用量$y(w\\cdot x+b)$来表示分类的正确性及确信程度，此为 __\"函数间隔\"__。\n\n* 函数间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:  \n  $\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$\n\n  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  函数间隔可以表示分类预测的正确度及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面没有变，但是函数间隔却变为原来的2倍。这一事实启示我们，可以对分离超平面的法向量$w$加某些约束，如归一化$||w||=1$，使得间隔是确定的。这时函数间隔成为 __几何间隔__。\n\n* 几何间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：  \n  $\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$\n\n  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  \n  $\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$\n\n  超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。\n\n  函数间隔和几何间隔有如下关系：  \n  $\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$\n\n  $\\gamma=\\frac{\\hat{\\gamma}}{||w||}$\n\n  __如果$||w||=1，那么函数间隔和几何间隔相等$__。如果超平面参数$w$和$b$成比例地改变(超平面未变)，则函数间隔也按此比例改变，但是几何间隔不变。\n\n\n最大间隔分离超平面  可以表示为下面的约束最优化问题：  \n$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$\n\n即我们希望最大化超平面$(w,b)$关于training set的几何间隔$\\gamma$，约束条件表示的是超平面$(w,b)$关于每个training sample的几何间隔至少是$\\gamma$。\n\n考虑几何间隔和函数间隔的关系，该问题等价于：\n$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$\n\n最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$是等价的，于是就得到下面的线性可分SVM的最优化问题：\n$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\ns.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$\n\n* 最大间隔分离超平面的存在唯一性：若训练数据集T线性可分，则可将训练集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。\n\n在线性可分情况下，training set的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件等号成立的点，即：\n$y_i(w\\cdot x_i+b)-1=0$\n\n对$y_i=+1$的正例点，支持向量在超平面 $H_1:w\\cdot x+b=1$上，对$y_i=-1$的负例点，支持向量在超平面 $H_2:w\\cdot x+b=-1$上。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。由于支持向量在确定分离超平面中起着决定性的作用，所以将这种分类模型称为\"支持向量机\"。\n\n\n### 学习的对偶算法\n为了求解线性可分SVM的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVM的对偶算法。这样做一来对偶问题更容易求解，二来自然引入Kernel Function，可以扩展到非线性分类问题。\n\n引入拉格朗日乘子$\\alpha_i \\geq 0, i=1,2,\\cdots,N$，定义拉格朗日函数：\n$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$，其中，$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$为拉格朗日乘子向量。\n\n根据拉格朗日对偶性，原始问题的对偶问题是极大极小值问题：\n$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$，所以为了得到对偶问题的解，需要先求$L(w,b,\\alpha)$对$w,b$的极小，再求对$\\alpha$的极大。\n\n1. 求$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$：  \n   将拉格朗日函数$L(w,b,\\alpha)$分别对$w,b$求偏导，并令其等于0。  \n   $\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$\n\n   $\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$  \n  得:  \n  $w=\\sum_{i=1}^N\\alpha_i y_i x_i$  \n  $\\sum_{i=1}^N\\alpha_i y_i=0$  \n  可得:  \n  $\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$\n\n2. 求解$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题  \n  $$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n  可转换成下面等价的求极小值的对偶问题：\n  $$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  \n\n#### 线性可分SVM的学习算法\n1. 构造并求解约束最优化问题:\n  $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$  \n  得到最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。\n\n2. 计算 $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$ ，并选择 $\\alpha^{\\star}$ 的一个正分量 $\\alpha_j^{\\star}>0$，计算:  \n  $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$\n\n3. 求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，分类决策函数 $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$。\n\n## 线性SVM与软间隔最大化\n线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点$(x_i, y_i)$引入一个松弛变量$\\xi_i \\geq0$，使得函数间隔加上松弛变量大于等于1。这样，约束条件变为:  \n$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n同时，对每个松弛变量，支付一个代价$\\xi_i$，目标函数由原来的$\\frac{1}{2}||w||^2$变成 $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$。这里$C>0$称为惩罚参数，一般由问题决定。$C$值大时对误分类的惩罚加大，$C$值小时对误分类的惩罚变小。最小化Loss有两层含义：使$\\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使得误分类点的个数尽量小，$C$是两者的调和系数。\n\n线性不可分的SVM学习问题变成如下凸二次规划问题：  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$\n   \n### 学习的对偶算法\n原始问题的对偶问题是:\n$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$\n$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$\n\n#### 线性可分SVM的学习算法\n1. 选择惩罚参数$C>0$，构造并求解凸二次规划问题：  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$  \n   $s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$  \n   求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$\n\n2. 计算$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$  \n   选择$\\alpha^{\\star}$的一个分量$\\alpha_j^{\\star}$适合条件$0<\\alpha_j^{\\star}<C$ (通常取所有符合条件的样本点上的均值)，计算  \n   $b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$\n  \n3. 求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，\n   分类决策函数$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$\n\n#### 支持向量\n软间隔的支持向量$x_i$或者在间隔边界上，或者在降额边界与分离超平面之间，或者在分离超平面误分一侧。  \n\n若$\\alpha_i^{\\star}<C$，则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；  \n若$\\alpha_i^{\\star}=C, 0<\\xi_i<1$，则分类正确，$x_i$将在间隔边界与超平面之间；  \n若$\\alpha_i^{\\star}=C, \\xi_i=1$，则$x_i$在分离超平面上；  \n若$\\alpha_i^{\\star}=C, \\xi_i>1$，则$x_i$位于分离超平面误分一侧。\n\n#### Hinge Loss\nSVM还有另一种解释，即最小化以下Loss Function：  \n$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$\n\n$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$称为 Hinge Loss。这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔 $y_i(w\\cdot x_i+b)$大于1时，损失为0，否则损失是 $1-y_i(w\\cdot x_i+b)$。\n\n线性SVM原始最优化问题:  \n$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$\n\n$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$\n\n$\\xi_i\\geq 0$\n\n等价于最优化问题：\n\n$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$\n\n## 非线性SVM与Kernel Function\n设原空间为$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$，新空间为$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$，定义从原空间到新空间的变换(映射)：  \n$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$  \n经过变换$z=\\phi(x)$，原空间$\\chi \\subset R^2$变换为新空间$\\mathcal{Z}\\subset R^2$，原空间中的点变为新空间中的点。从而原空间线性不可分的情形变为新空间里的线性可分问题。\n\n### Kernel Function\n设$\\chi$是输入空间(欧式空间$R^n$或离散集合)，又设$\\mathcal{H}$为特征空间(希尔伯特空间)，若存在一个从$\\chi$到$\\mathcal{H}$的映射:  \n$\\phi(x):\\chi \\to \\mathcal{H}$\n使得对所有$x,z\\in \\chi$，函数$K(x,z)$都满足：\n$K(x,z)=\\phi(x)\\cdot \\phi(z)$  \n则称$K(x,z)$为核函数，$\\phi(x)$为映射函数，式子中$\\phi(x)\\cdot \\phi(z)$为$\\phi(x)$和$\\phi(z)$内积。\n\nKernel Tricks的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\\phi$，直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。\n\nKernel-based SVM等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\\cdot x_j$变换为特征空间中的内积$\\phi(x_i)\\cdot \\phi(x_j)$，在新的特征空间里从训练样本中学习线性SVM。当映射函数是非线性函数时，学习到的含有核函数的SVM是非线性分类模型。\n\nKernel Tricks：学习是隐式地在特征空间进行的，不需要显示地定义特征空间和映射函数，这样的技巧称为Kernel Tricks。\n\n### 常用Kernel Function\n1. 多项式核函数：  \n   $K(x,z)=(x\\cdot z+1)^p$  \n   对应的SVM是一个$p$次多项式分类器，在此情形下，分类决策函数成为:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$\n\n2. 高斯核函数：  \n   $K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$\n   对应的SVM是RBF分类器，在此情形下，分类决策函数成为:  \n   $f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$\n\n### 非线性SVM\n从非线性分类数据集，通过Kernel Tricks与软间隔最大化，或凸二次规划，学习到的分类决策函数：  \n$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$\n称为非线性支持向量，$K(x,z)$是正定核函数。\n\n#### 非线性SVM学习算法\n1. 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题：  \n   $\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$\n\n   $s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$\n   求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。\n\n2. 选择$\\alpha^{\\star}$的一个正分量$0<\\alpha_i^{\\star}<C$，计算 $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$\n\n3. 构造决策函数：  \n   $f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$","slug":"ml-svm","published":1,"updated":"2018-10-01T04:40:09.601Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03cy001h608w6u6ihik2","content":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>SVM是一种非常经典的分类算法，也是很多机器学习面试中必问的算法。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习策略就是间隔最大化，等价于 <strong>正则化的Hinge Loss最小化问题</strong>。</p>\n<p>当训练数据线性可分时，通过硬间隔最大化学习一个线性的分类器；<br>当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器；<br>当训练数据线性不可分时，通过Kernel Tricks和软间隔最大化学习一个非线性的分类器；</p>\n<p>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示 <strong>将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持的SVM，等价于隐式地在高维的特征空间中学习线性SVM</strong>。</p>\n<p>一般的，当training set线性可分时，存在无穷个分离超平面可将两类数据正确分开。MLP利用 <strong>误分类最小策略</strong>，求得分离超平面，不过这时的解有无穷多个。线性可分SVM利用 <strong>间隔最大化</strong>求得分离超平面，这时解是唯一的。</p>\n<h2 id=\"线性可分SVM与硬间隔最大化\"><a href=\"#线性可分SVM与硬间隔最大化\" class=\"headerlink\" title=\"线性可分SVM与硬间隔最大化\"></a>线性可分SVM与硬间隔最大化</h2><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w\\cdot x+b=0$确定的情况下，$|w\\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\\cdot x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用量$y(w\\cdot x+b)$来表示分类的正确性及确信程度，此为 <strong>“函数间隔”</strong>。</p>\n<ul>\n<li><p>函数间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:<br>$\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$</p>\n<p>定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>函数间隔可以表示分类预测的正确度及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面没有变，但是函数间隔却变为原来的2倍。这一事实启示我们，可以对分离超平面的法向量$w$加某些约束，如归一化$||w||=1$，使得间隔是确定的。这时函数间隔成为 <strong>几何间隔</strong>。</p>\n</li>\n<li><p>几何间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：<br>$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$</p>\n<p>定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。</p>\n<p>函数间隔和几何间隔有如下关系：<br>$\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$</p>\n<p>$\\gamma=\\frac{\\hat{\\gamma}}{||w||}$</p>\n<p><strong>如果$||w||=1，那么函数间隔和几何间隔相等$</strong>。如果超平面参数$w$和$b$成比例地改变(超平面未变)，则函数间隔也按此比例改变，但是几何间隔不变。</p>\n</li>\n</ul>\n<p>最大间隔分离超平面  可以表示为下面的约束最优化问题：<br>$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$</p>\n<p>即我们希望最大化超平面$(w,b)$关于training set的几何间隔$\\gamma$，约束条件表示的是超平面$(w,b)$关于每个training sample的几何间隔至少是$\\gamma$。</p>\n<p>考虑几何间隔和函数间隔的关系，该问题等价于：<br>$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$</p>\n<p>最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$是等价的，于是就得到下面的线性可分SVM的最优化问题：<br>$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$</p>\n<ul>\n<li>最大间隔分离超平面的存在唯一性：若训练数据集T线性可分，则可将训练集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</li>\n</ul>\n<p>在线性可分情况下，training set的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件等号成立的点，即：<br>$y_i(w\\cdot x_i+b)-1=0$</p>\n<p>对$y_i=+1$的正例点，支持向量在超平面 $H_1:w\\cdot x+b=1$上，对$y_i=-1$的负例点，支持向量在超平面 $H_2:w\\cdot x+b=-1$上。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。由于支持向量在确定分离超平面中起着决定性的作用，所以将这种分类模型称为”支持向量机”。</p>\n<h3 id=\"学习的对偶算法\"><a href=\"#学习的对偶算法\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p>为了求解线性可分SVM的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVM的对偶算法。这样做一来对偶问题更容易求解，二来自然引入Kernel Function，可以扩展到非线性分类问题。</p>\n<p>引入拉格朗日乘子$\\alpha_i \\geq 0, i=1,2,\\cdots,N$，定义拉格朗日函数：<br>$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$，其中，$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$为拉格朗日乘子向量。</p>\n<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小值问题：<br>$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$，所以为了得到对偶问题的解，需要先求$L(w,b,\\alpha)$对$w,b$的极小，再求对$\\alpha$的极大。</p>\n<ol>\n<li><p>求$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$：<br>将拉格朗日函数$L(w,b,\\alpha)$分别对$w,b$求偏导，并令其等于0。<br>$\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$</p>\n<p>$\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$<br>得:<br>$w=\\sum_{i=1}^N\\alpha_i y_i x_i$<br>$\\sum_{i=1}^N\\alpha_i y_i=0$<br>可得:<br>$\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$</p>\n</li>\n<li><p>求解$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题<br>$$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n<p>可转换成下面等价的求极小值的对偶问题：<br>$$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n</li>\n</ol>\n<h4 id=\"线性可分SVM的学习算法\"><a href=\"#线性可分SVM的学习算法\" class=\"headerlink\" title=\"线性可分SVM的学习算法\"></a>线性可分SVM的学习算法</h4><ol>\n<li><p>构造并求解约束最优化问题:<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$<br>得到最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。</p>\n</li>\n<li><p>计算 $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$ ，并选择 $\\alpha^{\\star}$ 的一个正分量 $\\alpha_j^{\\star}&gt;0$，计算:<br>$b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$</p>\n</li>\n<li><p>求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，分类决策函数 $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$。</p>\n</li>\n</ol>\n<h2 id=\"线性SVM与软间隔最大化\"><a href=\"#线性SVM与软间隔最大化\" class=\"headerlink\" title=\"线性SVM与软间隔最大化\"></a>线性SVM与软间隔最大化</h2><p>线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点$(x_i, y_i)$引入一个松弛变量$\\xi_i \\geq0$，使得函数间隔加上松弛变量大于等于1。这样，约束条件变为:<br>$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>同时，对每个松弛变量，支付一个代价$\\xi_i$，目标函数由原来的$\\frac{1}{2}||w||^2$变成 $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$。这里$C&gt;0$称为惩罚参数，一般由问题决定。$C$值大时对误分类的惩罚加大，$C$值小时对误分类的惩罚变小。最小化Loss有两层含义：使$\\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使得误分类点的个数尽量小，$C$是两者的调和系数。</p>\n<p>线性不可分的SVM学习问题变成如下凸二次规划问题：<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$</p>\n<h3 id=\"学习的对偶算法-1\"><a href=\"#学习的对偶算法-1\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p>原始问题的对偶问题是:<br>$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$<br>$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$</p>\n<h4 id=\"线性可分SVM的学习算法-1\"><a href=\"#线性可分SVM的学习算法-1\" class=\"headerlink\" title=\"线性可分SVM的学习算法\"></a>线性可分SVM的学习算法</h4><ol>\n<li><p>选择惩罚参数$C&gt;0$，构造并求解凸二次规划问题：<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$<br>$s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$<br>求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>计算$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$<br>选择$\\alpha^{\\star}$的一个分量$\\alpha_j^{\\star}$适合条件$0&lt;\\alpha_j^{\\star}&lt;C$ (通常取所有符合条件的样本点上的均值)，计算<br>$b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$</p>\n</li>\n<li><p>求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，<br>分类决策函数$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h4 id=\"支持向量\"><a href=\"#支持向量\" class=\"headerlink\" title=\"支持向量\"></a>支持向量</h4><p>软间隔的支持向量$x_i$或者在间隔边界上，或者在降额边界与分离超平面之间，或者在分离超平面误分一侧。  </p>\n<p>若$\\alpha_i^{\\star}&lt;C$，则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；<br>若$\\alpha_i^{\\star}=C, 0&lt;\\xi_i&lt;1$，则分类正确，$x_i$将在间隔边界与超平面之间；<br>若$\\alpha_i^{\\star}=C, \\xi_i=1$，则$x_i$在分离超平面上；<br>若$\\alpha_i^{\\star}=C, \\xi_i&gt;1$，则$x_i$位于分离超平面误分一侧。</p>\n<h4 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h4><p>SVM还有另一种解释，即最小化以下Loss Function：<br>$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$</p>\n<p>$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$称为 Hinge Loss。这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔 $y_i(w\\cdot x_i+b)$大于1时，损失为0，否则损失是 $1-y_i(w\\cdot x_i+b)$。</p>\n<p>线性SVM原始最优化问题:<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$</p>\n<p>$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i\\geq 0$</p>\n<p>等价于最优化问题：</p>\n<p>$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$</p>\n<h2 id=\"非线性SVM与Kernel-Function\"><a href=\"#非线性SVM与Kernel-Function\" class=\"headerlink\" title=\"非线性SVM与Kernel Function\"></a>非线性SVM与Kernel Function</h2><p>设原空间为$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$，新空间为$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$，定义从原空间到新空间的变换(映射)：<br>$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$<br>经过变换$z=\\phi(x)$，原空间$\\chi \\subset R^2$变换为新空间$\\mathcal{Z}\\subset R^2$，原空间中的点变为新空间中的点。从而原空间线性不可分的情形变为新空间里的线性可分问题。</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><p>设$\\chi$是输入空间(欧式空间$R^n$或离散集合)，又设$\\mathcal{H}$为特征空间(希尔伯特空间)，若存在一个从$\\chi$到$\\mathcal{H}$的映射:<br>$\\phi(x):\\chi \\to \\mathcal{H}$<br>使得对所有$x,z\\in \\chi$，函数$K(x,z)$都满足：<br>$K(x,z)=\\phi(x)\\cdot \\phi(z)$<br>则称$K(x,z)$为核函数，$\\phi(x)$为映射函数，式子中$\\phi(x)\\cdot \\phi(z)$为$\\phi(x)$和$\\phi(z)$内积。</p>\n<p>Kernel Tricks的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\\phi$，直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。</p>\n<p>Kernel-based SVM等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\\cdot x_j$变换为特征空间中的内积$\\phi(x_i)\\cdot \\phi(x_j)$，在新的特征空间里从训练样本中学习线性SVM。当映射函数是非线性函数时，学习到的含有核函数的SVM是非线性分类模型。</p>\n<p>Kernel Tricks：学习是隐式地在特征空间进行的，不需要显示地定义特征空间和映射函数，这样的技巧称为Kernel Tricks。</p>\n<h3 id=\"常用Kernel-Function\"><a href=\"#常用Kernel-Function\" class=\"headerlink\" title=\"常用Kernel Function\"></a>常用Kernel Function</h3><ol>\n<li><p>多项式核函数：<br>$K(x,z)=(x\\cdot z+1)^p$<br>对应的SVM是一个$p$次多项式分类器，在此情形下，分类决策函数成为:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$</p>\n</li>\n<li><p>高斯核函数：<br>$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$<br>对应的SVM是RBF分类器，在此情形下，分类决策函数成为:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$</p>\n</li>\n</ol>\n<h3 id=\"非线性SVM\"><a href=\"#非线性SVM\" class=\"headerlink\" title=\"非线性SVM\"></a>非线性SVM</h3><p>从非线性分类数据集，通过Kernel Tricks与软间隔最大化，或凸二次规划，学习到的分类决策函数：<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$<br>称为非线性支持向量，$K(x,z)$是正定核函数。</p>\n<h4 id=\"非线性SVM学习算法\"><a href=\"#非线性SVM学习算法\" class=\"headerlink\" title=\"非线性SVM学习算法\"></a>非线性SVM学习算法</h4><ol>\n<li><p>选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题：<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$</p>\n<p>$s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$<br>求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。</p>\n</li>\n<li><p>选择$\\alpha^{\\star}$的一个正分量$0&lt;\\alpha_i^{\\star}&lt;C$，计算 $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$</p>\n</li>\n<li><p>构造决策函数：<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>SVM是一种非常经典的分类算法，也是很多机器学习面试中必问的算法。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习策略就是间隔最大化，等价于 <strong>正则化的Hinge Loss最小化问题</strong>。</p>\n<p>当训练数据线性可分时，通过硬间隔最大化学习一个线性的分类器；<br>当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器；<br>当训练数据线性不可分时，通过Kernel Tricks和软间隔最大化学习一个非线性的分类器；</p>\n<p>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示 <strong>将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持的SVM，等价于隐式地在高维的特征空间中学习线性SVM</strong>。</p>\n<p>一般的，当training set线性可分时，存在无穷个分离超平面可将两类数据正确分开。MLP利用 <strong>误分类最小策略</strong>，求得分离超平面，不过这时的解有无穷多个。线性可分SVM利用 <strong>间隔最大化</strong>求得分离超平面，这时解是唯一的。</p>\n<h2 id=\"线性可分SVM与硬间隔最大化\"><a href=\"#线性可分SVM与硬间隔最大化\" class=\"headerlink\" title=\"线性可分SVM与硬间隔最大化\"></a>线性可分SVM与硬间隔最大化</h2><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w\\cdot x+b=0$确定的情况下，$|w\\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\\cdot x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用量$y(w\\cdot x+b)$来表示分类的正确性及确信程度，此为 <strong>“函数间隔”</strong>。</p>\n<ul>\n<li><p>函数间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:<br>$\\hat{\\gamma}_i=y_i(w\\cdot x_i + b)$</p>\n<p>定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>函数间隔可以表示分类预测的正确度及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面没有变，但是函数间隔却变为原来的2倍。这一事实启示我们，可以对分离超平面的法向量$w$加某些约束，如归一化$||w||=1$，使得间隔是确定的。这时函数间隔成为 <strong>几何间隔</strong>。</p>\n</li>\n<li><p>几何间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：<br>$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$</p>\n<p>定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：<br>$\\hat{\\gamma}=\\mathop{min} \\limits_{i=1,\\cdots,N}\\hat{\\gamma}_i$</p>\n<p>超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。</p>\n<p>函数间隔和几何间隔有如下关系：<br>$\\gamma_i=\\frac{\\hat{\\gamma}_i}{||w||}$</p>\n<p>$\\gamma=\\frac{\\hat{\\gamma}}{||w||}$</p>\n<p><strong>如果$||w||=1，那么函数间隔和几何间隔相等$</strong>。如果超平面参数$w$和$b$成比例地改变(超平面未变)，则函数间隔也按此比例改变，但是几何间隔不变。</p>\n</li>\n</ul>\n<p>最大间隔分离超平面  可以表示为下面的约束最优化问题：<br>$$\\mathop{max} \\limits_{w,b} \\gamma s.t.\\quad y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})\\geq \\gamma,\\quad i=1,\\cdots,N$$</p>\n<p>即我们希望最大化超平面$(w,b)$关于training set的几何间隔$\\gamma$，约束条件表示的是超平面$(w,b)$关于每个training sample的几何间隔至少是$\\gamma$。</p>\n<p>考虑几何间隔和函数间隔的关系，该问题等价于：<br>$$\\mathop{max} \\limits_{w,b}\\frac{\\hat{\\gamma}}{||w||} \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)\\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N$$</p>\n<p>最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$是等价的，于是就得到下面的线性可分SVM的最优化问题：<br>$$\\mathop{min} \\limits_{w,b}\\frac{1}{2}||w||^2 \\\\<br>s.t.\\quad y_i(w\\cdot x_i+b)-1\\geq 0, \\quad i=1,\\cdots,N$$</p>\n<ul>\n<li>最大间隔分离超平面的存在唯一性：若训练数据集T线性可分，则可将训练集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</li>\n</ul>\n<p>在线性可分情况下，training set的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件等号成立的点，即：<br>$y_i(w\\cdot x_i+b)-1=0$</p>\n<p>对$y_i=+1$的正例点，支持向量在超平面 $H_1:w\\cdot x+b=1$上，对$y_i=-1$的负例点，支持向量在超平面 $H_2:w\\cdot x+b=-1$上。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。由于支持向量在确定分离超平面中起着决定性的作用，所以将这种分类模型称为”支持向量机”。</p>\n<h3 id=\"学习的对偶算法\"><a href=\"#学习的对偶算法\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p>为了求解线性可分SVM的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVM的对偶算法。这样做一来对偶问题更容易求解，二来自然引入Kernel Function，可以扩展到非线性分类问题。</p>\n<p>引入拉格朗日乘子$\\alpha_i \\geq 0, i=1,2,\\cdots,N$，定义拉格朗日函数：<br>$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^N \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N \\alpha_i$，其中，$\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$为拉格朗日乘子向量。</p>\n<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小值问题：<br>$\\mathop{max} \\limits_{\\alpha} \\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$，所以为了得到对偶问题的解，需要先求$L(w,b,\\alpha)$对$w,b$的极小，再求对$\\alpha$的极大。</p>\n<ol>\n<li><p>求$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$：<br>将拉格朗日函数$L(w,b,\\alpha)$分别对$w,b$求偏导，并令其等于0。<br>$\\bigtriangledown_wL(w,b,\\alpha)=w-\\sum_{i=1}^N \\alpha_i y_i x_i=0$</p>\n<p>$\\bigtriangledown_bL(w,b,\\alpha)=\\sum_{i=1}^N \\alpha_i y_i=0$<br>得:<br>$w=\\sum_{i=1}^N\\alpha_i y_i x_i$<br>$\\sum_{i=1}^N\\alpha_i y_i=0$<br>可得:<br>$\\mathop{min} \\limits_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i$</p>\n</li>\n<li><p>求解$\\mathop{min} \\limits_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题<br>$$\\mathop{max} \\limits_{\\alpha}-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) + \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n<p>可转换成下面等价的求极小值的对偶问题：<br>$$\\mathop{min} \\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_{i=1}^N \\alpha_i,\\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\quad \\alpha_i \\geq 0, i=1,2,\\cdots,N $$  </p>\n</li>\n</ol>\n<h4 id=\"线性可分SVM的学习算法\"><a href=\"#线性可分SVM的学习算法\" class=\"headerlink\" title=\"线性可分SVM的学习算法\"></a>线性可分SVM的学习算法</h4><ol>\n<li><p>构造并求解约束最优化问题:<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i \\alpha_j y_i y_j (x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i, \\quad s.t. \\sum_{i=1}^N \\alpha_i y_i=0, \\alpha_i\\geq 0$<br>得到最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。</p>\n</li>\n<li><p>计算 $w^{\\star}=\\sum_{i=1}^N\\alpha_i^{\\star} y_ix_i$ ，并选择 $\\alpha^{\\star}$ 的一个正分量 $\\alpha_j^{\\star}&gt;0$，计算:<br>$b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star} y_i(x_i\\cdot x_j)$</p>\n</li>\n<li><p>求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，分类决策函数 $f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$。</p>\n</li>\n</ol>\n<h2 id=\"线性SVM与软间隔最大化\"><a href=\"#线性SVM与软间隔最大化\" class=\"headerlink\" title=\"线性SVM与软间隔最大化\"></a>线性SVM与软间隔最大化</h2><p>线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点$(x_i, y_i)$引入一个松弛变量$\\xi_i \\geq0$，使得函数间隔加上松弛变量大于等于1。这样，约束条件变为:<br>$y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>同时，对每个松弛变量，支付一个代价$\\xi_i$，目标函数由原来的$\\frac{1}{2}||w||^2$变成 $\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i$。这里$C&gt;0$称为惩罚参数，一般由问题决定。$C$值大时对误分类的惩罚加大，$C$值小时对误分类的惩罚变小。最小化Loss有两层含义：使$\\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使得误分类点的个数尽量小，$C$是两者的调和系数。</p>\n<p>线性不可分的SVM学习问题变成如下凸二次规划问题：<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N \\xi_i \\quad s.t. \\quad y_i(w\\cdot x_i + b)\\geq 1-\\xi_i, i=1,2,\\cdots,N \\quad \\xi_i \\geq0$</p>\n<h3 id=\"学习的对偶算法-1\"><a href=\"#学习的对偶算法-1\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p>原始问题的对偶问题是:<br>$$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N \\alpha_i$$<br>$$s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq\\alpha_i \\leq C$$</p>\n<h4 id=\"线性可分SVM的学习算法-1\"><a href=\"#线性可分SVM的学习算法-1\" class=\"headerlink\" title=\"线性可分SVM的学习算法\"></a>线性可分SVM的学习算法</h4><ol>\n<li><p>选择惩罚参数$C&gt;0$，构造并求解凸二次规划问题：<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$<br>$s.t. \\sum_{i=1}^N\\alpha_i y_i=0, \\quad 0\\leq \\alpha_i \\leq C$<br>求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$</p>\n</li>\n<li><p>计算$w^{\\star}=\\sum_{i=1}^N\\alpha_1^{\\star} y_ix_i$<br>选择$\\alpha^{\\star}$的一个分量$\\alpha_j^{\\star}$适合条件$0&lt;\\alpha_j^{\\star}&lt;C$ (通常取所有符合条件的样本点上的均值)，计算<br>$b^{\\star}=y_j-\\sum_{i=1}^Ny_i \\alpha_i^{\\star}(x_i\\cdot x_j)$</p>\n</li>\n<li><p>求得分离超平面 $w^{\\star}\\cdot x+b^{\\star}=0$，<br>分类决策函数$f(x)=sign(w^{\\star}\\cdot x+b^{\\star})$</p>\n</li>\n</ol>\n<h4 id=\"支持向量\"><a href=\"#支持向量\" class=\"headerlink\" title=\"支持向量\"></a>支持向量</h4><p>软间隔的支持向量$x_i$或者在间隔边界上，或者在降额边界与分离超平面之间，或者在分离超平面误分一侧。  </p>\n<p>若$\\alpha_i^{\\star}&lt;C$，则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上；<br>若$\\alpha_i^{\\star}=C, 0&lt;\\xi_i&lt;1$，则分类正确，$x_i$将在间隔边界与超平面之间；<br>若$\\alpha_i^{\\star}=C, \\xi_i=1$，则$x_i$在分离超平面上；<br>若$\\alpha_i^{\\star}=C, \\xi_i&gt;1$，则$x_i$位于分离超平面误分一侧。</p>\n<h4 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h4><p>SVM还有另一种解释，即最小化以下Loss Function：<br>$\\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda ||w||^2$</p>\n<p>$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_{+}$称为 Hinge Loss。这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔 $y_i(w\\cdot x_i+b)$大于1时，损失为0，否则损失是 $1-y_i(w\\cdot x_i+b)$。</p>\n<p>线性SVM原始最优化问题:<br>$\\mathop{min} \\limits_{w,b,\\xi} \\frac{1}{2}||w||^2+C\\sum_{i=1}^N \\xi_i$</p>\n<p>$s.t.\\quad y_i(w\\cdot x_i+b)\\geq 1-\\xi_i$</p>\n<p>$\\xi_i\\geq 0$</p>\n<p>等价于最优化问题：</p>\n<p>$\\mathop{min} \\limits_{w,b} \\sum_{i=1}^N [1-y_i(w\\cdot x_i+b)]_{+} + \\lambda||w||^2$</p>\n<h2 id=\"非线性SVM与Kernel-Function\"><a href=\"#非线性SVM与Kernel-Function\" class=\"headerlink\" title=\"非线性SVM与Kernel Function\"></a>非线性SVM与Kernel Function</h2><p>设原空间为$\\chi \\subset R^2, x=(x^{(1)},x^{(2)})^T\\in \\chi$，新空间为$\\mathcal{Z} \\subset R^2, z=(z^{(1)},z^{(2)})^T\\in \\mathcal{Z}$，定义从原空间到新空间的变换(映射)：<br>$z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T$<br>经过变换$z=\\phi(x)$，原空间$\\chi \\subset R^2$变换为新空间$\\mathcal{Z}\\subset R^2$，原空间中的点变为新空间中的点。从而原空间线性不可分的情形变为新空间里的线性可分问题。</p>\n<h3 id=\"Kernel-Function\"><a href=\"#Kernel-Function\" class=\"headerlink\" title=\"Kernel Function\"></a>Kernel Function</h3><p>设$\\chi$是输入空间(欧式空间$R^n$或离散集合)，又设$\\mathcal{H}$为特征空间(希尔伯特空间)，若存在一个从$\\chi$到$\\mathcal{H}$的映射:<br>$\\phi(x):\\chi \\to \\mathcal{H}$<br>使得对所有$x,z\\in \\chi$，函数$K(x,z)$都满足：<br>$K(x,z)=\\phi(x)\\cdot \\phi(z)$<br>则称$K(x,z)$为核函数，$\\phi(x)$为映射函数，式子中$\\phi(x)\\cdot \\phi(z)$为$\\phi(x)$和$\\phi(z)$内积。</p>\n<p>Kernel Tricks的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\\phi$，直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。</p>\n<p>Kernel-based SVM等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\\cdot x_j$变换为特征空间中的内积$\\phi(x_i)\\cdot \\phi(x_j)$，在新的特征空间里从训练样本中学习线性SVM。当映射函数是非线性函数时，学习到的含有核函数的SVM是非线性分类模型。</p>\n<p>Kernel Tricks：学习是隐式地在特征空间进行的，不需要显示地定义特征空间和映射函数，这样的技巧称为Kernel Tricks。</p>\n<h3 id=\"常用Kernel-Function\"><a href=\"#常用Kernel-Function\" class=\"headerlink\" title=\"常用Kernel Function\"></a>常用Kernel Function</h3><ol>\n<li><p>多项式核函数：<br>$K(x,z)=(x\\cdot z+1)^p$<br>对应的SVM是一个$p$次多项式分类器，在此情形下，分类决策函数成为:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i(x_i\\cdot x+1)^p+b^{\\star})$</p>\n</li>\n<li><p>高斯核函数：<br>$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$<br>对应的SVM是RBF分类器，在此情形下，分类决策函数成为:<br>$f(x)=sign(\\sum_{i=1}^{N_s}a_i^{\\star}y_i exp(-\\frac{||x-z||^2}{2\\sigma^2})+b^{\\star})$</p>\n</li>\n</ol>\n<h3 id=\"非线性SVM\"><a href=\"#非线性SVM\" class=\"headerlink\" title=\"非线性SVM\"></a>非线性SVM</h3><p>从非线性分类数据集，通过Kernel Tricks与软间隔最大化，或凸二次规划，学习到的分类决策函数：<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_i K(x,x_i)+b^{\\star})$<br>称为非线性支持向量，$K(x,z)$是正定核函数。</p>\n<h4 id=\"非线性SVM学习算法\"><a href=\"#非线性SVM学习算法\" class=\"headerlink\" title=\"非线性SVM学习算法\"></a>非线性SVM学习算法</h4><ol>\n<li><p>选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题：<br>$\\mathop{min} \\limits_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)-\\sum_{i=1}^N \\alpha_i$</p>\n<p>$s.t. \\sum_{i=1}^N \\alpha_i y_i=0 \\qquad 0\\leq \\alpha_i \\leq C$<br>求得最优解$\\alpha^{\\star}=(\\alpha_1^{\\star},\\alpha_2^{\\star},\\cdots,\\alpha_N^{\\star})^T$。</p>\n</li>\n<li><p>选择$\\alpha^{\\star}$的一个正分量$0&lt;\\alpha_i^{\\star}&lt;C$，计算 $b^{\\star}=y_j-\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x_i,x_j)$</p>\n</li>\n<li><p>构造决策函数：<br>$f(x)=sign(\\sum_{i=1}^N\\alpha_i^{\\star}y_iK(x\\cdot x_i)+b^{\\star})$</p>\n</li>\n</ol>\n"},{"title":"[ML] Naive Bayes","catalog":false,"mathjax":true,"date":"2018-07-19T08:17:20.000Z","catagories":["Algorithm","Machine Learning"],"_content":"## Introduction\n1. Naive Bayes 是基于Bayes Theorem与 __特征条件独立__ 假设的分类算法。对于给定的数据集，首先基于特征条件独立假设 __学习输入/输出的联合概率分布__；然后基于此模型，对给定的输入$x$，利用Bayes Theorem求出后验概率最大的输出$y$。\n\n2. Naive Bayes通过训练数据集学习联合概率分布$P(X,Y)$。Naive Bayes对条件概率分布做了条件独立性假设：\n$$\nP(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)\n$$\n\nNaive Bayes实际上学习到生成数据的机制，所以属于 __生成模型__。条件独立性假设等于是说 __用于分类的特征在类确定的情况下都是条件独立的__。\n\n## 公式推导\n$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\\n\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$\n\n因上式中分母对所有$c_k$都是相同的，所以：\n$$\ny=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)\n$$\n\n采用0-1损失函数：\n$$\nL(Y,f(X))=\n\\begin{cases}\n    1, & Y\\neq f(X)\\\\\n    0, & otherwise\n\\end{cases}\n$$\n条件期望为：\n$$\nR_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)\n$$\n因此：\n$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\\n=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\\n=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$\n\n这样一来，根据期望风险最小化就得到了后验概率最大化准则：\n$$\nf(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)\n$$\n\n先验概率$P(Y=c_k)$的极大似然估计是：\n$$\nP(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K\n$$\n设第$j$个特征$x^{(j)}$可能取值的集合为$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：\n$$\nP(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}\n$$\n","source":"_posts/ml-nb.md","raw":"---\ntitle: \"[ML] Naive Bayes\"\ncatalog: false\nmathjax: true\ndate: 2018-07-19 16:17:20\ntags:\n- Machine Learning\n- Data Science\ncatagories:\n- Algorithm\n- Machine Learning\n---\n## Introduction\n1. Naive Bayes 是基于Bayes Theorem与 __特征条件独立__ 假设的分类算法。对于给定的数据集，首先基于特征条件独立假设 __学习输入/输出的联合概率分布__；然后基于此模型，对给定的输入$x$，利用Bayes Theorem求出后验概率最大的输出$y$。\n\n2. Naive Bayes通过训练数据集学习联合概率分布$P(X,Y)$。Naive Bayes对条件概率分布做了条件独立性假设：\n$$\nP(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)\n$$\n\nNaive Bayes实际上学习到生成数据的机制，所以属于 __生成模型__。条件独立性假设等于是说 __用于分类的特征在类确定的情况下都是条件独立的__。\n\n## 公式推导\n$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\\n\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$\n\n因上式中分母对所有$c_k$都是相同的，所以：\n$$\ny=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)\n$$\n\n采用0-1损失函数：\n$$\nL(Y,f(X))=\n\\begin{cases}\n    1, & Y\\neq f(X)\\\\\n    0, & otherwise\n\\end{cases}\n$$\n条件期望为：\n$$\nR_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)\n$$\n因此：\n$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\\n=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\\n=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$\n\n这样一来，根据期望风险最小化就得到了后验概率最大化准则：\n$$\nf(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)\n$$\n\n先验概率$P(Y=c_k)$的极大似然估计是：\n$$\nP(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K\n$$\n设第$j$个特征$x^{(j)}$可能取值的集合为$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：\n$$\nP(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}\n$$\n","slug":"ml-nb","published":1,"updated":"2018-10-01T04:40:09.058Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjopy03d0001k608wdj1ah9ts","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ol>\n<li><p>Naive Bayes 是基于Bayes Theorem与 <strong>特征条件独立</strong> 假设的分类算法。对于给定的数据集，首先基于特征条件独立假设 <strong>学习输入/输出的联合概率分布</strong>；然后基于此模型，对给定的输入$x$，利用Bayes Theorem求出后验概率最大的输出$y$。</p>\n</li>\n<li><p>Naive Bayes通过训练数据集学习联合概率分布$P(X,Y)$。Naive Bayes对条件概率分布做了条件独立性假设：<br>$$<br>P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)<br>$$</p>\n</li>\n</ol>\n<p>Naive Bayes实际上学习到生成数据的机制，所以属于 <strong>生成模型</strong>。条件独立性假设等于是说 <strong>用于分类的特征在类确定的情况下都是条件独立的</strong>。</p>\n<h2 id=\"公式推导\"><a href=\"#公式推导\" class=\"headerlink\" title=\"公式推导\"></a>公式推导</h2><p>$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\<br>\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$</p>\n<p>因上式中分母对所有$c_k$都是相同的，所以：<br>$$<br>y=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$</p>\n<p>采用0-1损失函数：<br>$$<br>L(Y,f(X))=<br>\\begin{cases}<br>    1, &amp; Y\\neq f(X)\\\\<br>    0, &amp; otherwise<br>\\end{cases}<br>$$<br>条件期望为：<br>$$<br>R_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)<br>$$<br>因此：<br>$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\<br>=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\<br>=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$</p>\n<p>这样一来，根据期望风险最小化就得到了后验概率最大化准则：<br>$$<br>f(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)<br>$$</p>\n<p>先验概率$P(Y=c_k)$的极大似然估计是：<br>$$<br>P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K<br>$$<br>设第$j$个特征$x^{(j)}$可能取值的集合为$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：<br>$$<br>P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}<br>$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><ol>\n<li><p>Naive Bayes 是基于Bayes Theorem与 <strong>特征条件独立</strong> 假设的分类算法。对于给定的数据集，首先基于特征条件独立假设 <strong>学习输入/输出的联合概率分布</strong>；然后基于此模型，对给定的输入$x$，利用Bayes Theorem求出后验概率最大的输出$y$。</p>\n</li>\n<li><p>Naive Bayes通过训练数据集学习联合概率分布$P(X,Y)$。Naive Bayes对条件概率分布做了条件独立性假设：<br>$$<br>P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)<br>$$</p>\n</li>\n</ol>\n<p>Naive Bayes实际上学习到生成数据的机制，所以属于 <strong>生成模型</strong>。条件独立性假设等于是说 <strong>用于分类的特征在类确定的情况下都是条件独立的</strong>。</p>\n<h2 id=\"公式推导\"><a href=\"#公式推导\" class=\"headerlink\" title=\"公式推导\"></a>公式推导</h2><p>$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}=\\\\<br>\\frac{P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$$</p>\n<p>因上式中分母对所有$c_k$都是相同的，所以：<br>$$<br>y=\\mathop{argmax}\\limits_{c_k}P(Y=c_k)\\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)<br>$$</p>\n<p>采用0-1损失函数：<br>$$<br>L(Y,f(X))=<br>\\begin{cases}<br>    1, &amp; Y\\neq f(X)\\\\<br>    0, &amp; otherwise<br>\\end{cases}<br>$$<br>条件期望为：<br>$$<br>R_{exp}(f)=E_x \\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)<br>$$<br>因此：<br>$$f(x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\\\<br>=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} \\sum_{k=1}^K P(y\\neq c_k|X=x)=\\mathop{argmin}\\limits_{y\\in \\mathcal{Y}} (1-P(y=c_k|X=x)) \\\\<br>=\\mathop{argmax}\\limits_{y\\in \\mathcal{Y}} P(y=c_k|X=x)$$</p>\n<p>这样一来，根据期望风险最小化就得到了后验概率最大化准则：<br>$$<br>f(x)=\\mathop{argmax}\\limits_{c_k}P(c_k|X=x)<br>$$</p>\n<p>先验概率$P(Y=c_k)$的极大似然估计是：<br>$$<br>P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\\cdots,K<br>$$<br>设第$j$个特征$x^{(j)}$可能取值的集合为$\\{a_{j1},a_{j2},\\cdots,a_{jS_j}\\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：<br>$$<br>P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^N I(y_i=c_k)}<br>$$</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bm0004608w17rmwwmo","_id":"cjopy03c3000g608wf7lx94fn"},{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bs0008608wwz5s7ov2","_id":"cjopy03c4000i608wu5lef2l1"},{"post_id":"cjopy03bc0000608wv8lsgxk0","tag_id":"cjopy03bw000b608w44ac37bt","_id":"cjopy03c7000l608wkkf0hm63"},{"post_id":"cjopy03bi0002608wgrt7lxch","tag_id":"cjopy03c1000e608wfz56akxl","_id":"cjopy03cb000p608ws5igym68"},{"post_id":"cjopy03bi0002608wgrt7lxch","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cd000r608wm525nrix"},{"post_id":"cjopy03cf000u608w5h4oygdy","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ch000x608ws3mnmrav"},{"post_id":"cjopy03cf000u608w5h4oygdy","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ci000z608w23t1bdzw"},{"post_id":"cjopy03cf000v608wanrj1kpm","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ck0011608wgcy1nhcq"},{"post_id":"cjopy03cf000v608wanrj1kpm","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cm0014608wi0rp9jc6"},{"post_id":"cjopy03ch000y608w2p2hng7v","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03co0016608wiab3uiac"},{"post_id":"cjopy03ch000y608w2p2hng7v","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cs0019608wmr0zk1qs"},{"post_id":"cjopy03cj0010608wwmlxn6on","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ct001b608w8ongyqzh"},{"post_id":"cjopy03cj0010608wwmlxn6on","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03cw001e608whlvgv9s0"},{"post_id":"cjopy03cm0015608wne980x56","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03cy001g608wkrnhbn3l"},{"post_id":"cjopy03cm0015608wne980x56","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d0001j608wk2rk2ddk"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d1001l608wnau16r6p"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03d2001n608w9joe0g42"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03d3001o608wi90wi7gl"},{"post_id":"cjopy03bo0005608wvuyyafyt","tag_id":"cjopy03ck0012608ws1vrr4sc","_id":"cjopy03d3001q608wnpcqumkk"},{"post_id":"cjopy03cp0017608wxlenocz5","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d4001r608w1nj135al"},{"post_id":"cjopy03cp0017608wxlenocz5","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d5001t608wxuys423q"},{"post_id":"cjopy03cs001a608wk9gey017","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d5001u608wc7fn2d1t"},{"post_id":"cjopy03cs001a608wk9gey017","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d6001w608wpow8d2v9"},{"post_id":"cjopy03cu001c608w09ol81m3","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d6001x608w9ua0r3vv"},{"post_id":"cjopy03cu001c608w09ol81m3","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d7001z608w6ux9inhi"},{"post_id":"cjopy03cw001f608wuc5jmdy4","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d70020608w3pstwl1w"},{"post_id":"cjopy03cw001f608wuc5jmdy4","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03d80022608wgof63nz0"},{"post_id":"cjopy03cy001h608w6u6ihik2","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03d90023608w0yj6x1ig"},{"post_id":"cjopy03cy001h608w6u6ihik2","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03db0025608wp2a2rypb"},{"post_id":"cjopy03d0001k608wdj1ah9ts","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dc0026608w2ob5z9ye"},{"post_id":"cjopy03d0001k608wdj1ah9ts","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03de0028608w3suobw94"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03de0029608w12gqsj3d"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03df002b608webh36oev"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dg002c608wgq7qttv4"},{"post_id":"cjopy03bp0006608wtrrk0xw5","tag_id":"cjopy03d2001m608w5bzwzhgj","_id":"cjopy03dg002d608wx8p2uo96"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dh002f608wfm82h97b"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03di002g608w9zn5d7n5"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dj002i608wkbj88e2u"},{"post_id":"cjopy03br0007608w4uz8pp8x","tag_id":"cjopy03d7001y608w5w80cwkc","_id":"cjopy03dj002j608wmxtsw8z3"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dk002l608wi2wc9g1z"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dk002m608w5eubtti6"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dl002o608w93ytwz4v"},{"post_id":"cjopy03bt0009608whetyugdt","tag_id":"cjopy03df002a608wnjm0x89p","_id":"cjopy03dm002p608w3nrpi7rz"},{"post_id":"cjopy03bv000a608w1fey66vl","tag_id":"cjopy03df002a608wnjm0x89p","_id":"cjopy03dn002r608wgunlukew"},{"post_id":"cjopy03bv000a608w1fey66vl","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03dn002s608w36ul8kyt"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03do002v608wmi2fm9vk"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dp002w608wxfg0jx0k"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03dq002y608wvfn4dfxl"},{"post_id":"cjopy03bx000c608w18wlry9o","tag_id":"cjopy03dn002t608wjl1u8sz4","_id":"cjopy03dq002z608w5s0rhc25"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dt0034608w88o3j3fe"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dt0035608w2cpxdwxm"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03du0037608wayu0mxbb"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03dr0031608wc9rz76c5","_id":"cjopy03du0038608wzwiht4m8"},{"post_id":"cjopy03bz000d608wtw5onf9s","tag_id":"cjopy03ds0032608wmfiofaxk","_id":"cjopy03dv003a608w8lvjzr0k"},{"post_id":"cjopy03c1000f608wrmkhhxc2","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dw003b608wcspxxvsn"},{"post_id":"cjopy03c1000f608wrmkhhxc2","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dx003d608wy0vhhu0y"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03dz003h608wcpxsj9z7"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03dz003i608wy0y12wa9"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03e1003k608wqvkpg3up"},{"post_id":"cjopy03c3000h608wvxqe83el","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03e1003l608wcixagd7i"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03e4003q608wnppln6n5"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03e5003r608wnm6xn1pv"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03e1003m608wniu2y5nv","_id":"cjopy03e6003t608whr0wj1pz"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03e6003u608w2aq4s9od"},{"post_id":"cjopy03c5000k608wl1cri8at","tag_id":"cjopy03ch000w608wemv3tk1g","_id":"cjopy03e7003w608w5tpdajo3"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03e8003y608wb5dbafwi"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03e8003z608wdlxml48c"},{"post_id":"cjopy03c7000m608wiwmlwpvc","tag_id":"cjopy03e6003v608w0qm0v0nv","_id":"cjopy03e90041608we3pexlfn"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03eb0044608wy4t4qrsq"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03eb0045608wfx69q3yu"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03ec0047608wkw40jckf"},{"post_id":"cjopy03c9000n608w4okd7r5h","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ec0048608wfr6w4wbd"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ee004b608wk7sjgg3l"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03ef004c608wn4wl7cs1"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03dx003e608wfnw34zqd","_id":"cjopy03ef004e608wlbwxfryj"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03ed0049608w3gbqmkyk","_id":"cjopy03eg004f608wfxmbo0el"},{"post_id":"cjopy03cb000q608wl2b759r4","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03eh004h608w2cshefss"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ei004j608wa9oehtiq"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ce000t608w91zh6ojj","_id":"cjopy03ej004k608wnxbenx7p"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03ef004d608w6zuoqhz8","_id":"cjopy03ej004l608wbg43n8n0"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03ej004m608wto6npl4a"},{"post_id":"cjopy03cd000s608wvyyxpd8f","tag_id":"cjopy03eg004g608w9t4zewzw","_id":"cjopy03ek004n608wuzc9bybv"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03eh004i608wpmg4jq72","_id":"cjopy03ek004o608wb8jaht27"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03ca000o608wz3qytqs9","_id":"cjopy03ek004p608wavr7vn3t"},{"post_id":"cjopy03cl0013608wsc34nbfi","tag_id":"cjopy03c5000j608wsgypu3v9","_id":"cjopy03el004q608wcdghwkzz"}],"Tag":[{"name":"Algorithm","_id":"cjopy03bm0004608w17rmwwmo"},{"name":"Data Structure","_id":"cjopy03bs0008608wwz5s7ov2"},{"name":"Graph","_id":"cjopy03bw000b608w44ac37bt"},{"name":"Data Visualization","_id":"cjopy03c1000e608wfz56akxl"},{"name":"Data Science","_id":"cjopy03c5000j608wsgypu3v9"},{"name":"Machine Learning","_id":"cjopy03ca000o608wz3qytqs9"},{"name":"Deep Learning","_id":"cjopy03ce000t608w91zh6ojj"},{"name":"Computer Vision","_id":"cjopy03ch000w608wemv3tk1g"},{"name":"Face Anti-Spoofing","_id":"cjopy03ck0012608ws1vrr4sc"},{"name":"Object Detection","_id":"cjopy03d2001m608w5bzwzhgj"},{"name":"Face Recognition","_id":"cjopy03d7001y608w5w80cwkc"},{"name":"Digital Image Processing","_id":"cjopy03df002a608wnjm0x89p"},{"name":"Auto Encoder","_id":"cjopy03dn002t608wjl1u8sz4"},{"name":"Image Classification","_id":"cjopy03dr0031608wc9rz76c5"},{"name":"Network Architecture","_id":"cjopy03ds0032608wmfiofaxk"},{"name":"Optimization","_id":"cjopy03dx003e608wfnw34zqd"},{"name":"CNN","_id":"cjopy03e1003m608wniu2y5nv"},{"name":"Data Augmentation","_id":"cjopy03e6003v608w0qm0v0nv"},{"name":"Regularization","_id":"cjopy03ed0049608w3gbqmkyk"},{"name":"RNN","_id":"cjopy03ef004d608w6zuoqhz8"},{"name":"NLP","_id":"cjopy03eg004g608w9t4zewzw"},{"name":"Feature Engineering","_id":"cjopy03eh004i608wpmg4jq72"}]}}