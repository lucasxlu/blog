<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>[dl] knowledge distillation | LucasX</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="Machine LearningDeep LearningData ScienceModel CompressionKnowledge Distillation" />
  
  
  
  
  <meta name="description" content="Introduction自 AlexNet 以来，Deep Learning 在许多领域取得了突破性的成果。理论上来说，模型的容量（即参数量）越大，性能越好，更能拟合海量数据的分布。与学术界一味追求在某个 benchmark 上刷分突破 SOTA 不同，AI 落地则非常关注能耗与移动端推理。轻量级模型设计与模型压缩也成为了近些年学术界与工业级关注的重心。常见的模型压缩方法有：  Quantizat">
<meta name="keywords" content="Machine Learning,Deep Learning,Data Science,Model Compression,Knowledge Distillation">
<meta property="og:type" content="article">
<meta property="og:title" content="[DL] Knowledge Distillation">
<meta property="og:url" content="https://lucasxlu.github.io/blog/2020/11/14/dl-kd/index.html">
<meta property="og:site_name" content="LucasX">
<meta property="og:description" content="Introduction自 AlexNet 以来，Deep Learning 在许多领域取得了突破性的成果。理论上来说，模型的容量（即参数量）越大，性能越好，更能拟合海量数据的分布。与学术界一味追求在某个 benchmark 上刷分突破 SOTA 不同，AI 落地则非常关注能耗与移动端推理。轻量级模型设计与模型压缩也成为了近些年学术界与工业级关注的重心。常见的模型压缩方法有：  Quantizat">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2020-11-14T08:17:01.637Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[DL] Knowledge Distillation">
<meta name="twitter:description" content="Introduction自 AlexNet 以来，Deep Learning 在许多领域取得了突破性的成果。理论上来说，模型的容量（即参数量）越大，性能越好，更能拟合海量数据的分布。与学术界一味追求在某个 benchmark 上刷分突破 SOTA 不同，AI 落地则非常关注能耗与移动端推理。轻量级模型设计与模型压缩也成为了近些年学术界与工业级关注的重心。常见的模型压缩方法有：  Quantizat">
  
    <link rel="alternate" href="/atom.xml" title="LucasX" type="application/atom+xml">
  

  

  <link rel="icon" href="/blog/css/images/mylogo.jpg">
  <link rel="apple-touch-icon" href="/blog/css/images/mylogo.jpg">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
  <link rel="stylesheet" href="/blog/css/style.css">

  <script src="/blog/js/jquery-3.1.1.min.js"></script>
  <script src="/blog/js/bootstrap.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/blog/css/bootstrap.css" >

  
    <link rel="stylesheet" href="/blog/css/dialog.css">
  

  

  
    <link rel="stylesheet" href="/blog/css/header-post.css" >
  

  
  
  
    <link rel="stylesheet" href="/blog/css/vdonate.css" ><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      
        <header>

    <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="navbar-inner">
          
          <div class="container"> 
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

            
              <a class="brand" style="
                 margin-top: 0px;"  
                href="#" data-toggle="modal" data-target="#myModal" >
                  <img width="124px" height="124px" alt="Hike News" src="/blog/css/images/mylogo.jpg">
              </a>
            
            
            <div class="navbar-collapse collapse">
              <ul class="hnav navbar-nav">
                
                  <li> <a class="main-nav-link" href="/blog/">Home</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/about">About</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/papers">Papers</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/projects">Projects</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/tags">Tags</a> </li>
                
                  <li> <a class="main-nav-link" href="/blog/archives">Archives</a> </li>
                
                  <li><div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/blog/',
        CONTENT_URL: '/blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/blog/js/insight.js"></script>

</div></li>
            </div>
          </div>
                
      </div>
    </div>

</header>



      
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-dl-kd" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      [DL] Knowledge Distillation
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/blog/2020/11/14/dl-kd/" class="article-date">
	  <time datetime="2020-11-14T08:15:34.000Z" itemprop="datePublished">2020-11-14</time>
	</a>

      
      
	<a class="article-views">
	<span id="busuanzi_container_page_pv">
		PV:<span id="busuanzi_value_page_pv"></span>
	</span>
	</a>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>自 AlexNet 以来，Deep Learning 在许多领域取得了突破性的成果。理论上来说，模型的容量（即参数量）越大，性能越好，更能拟合海量数据的分布。与学术界一味追求在某个 benchmark 上刷分突破 SOTA 不同，AI 落地则非常关注能耗与移动端推理。轻量级模型设计与模型压缩也成为了近些年学术界与工业级关注的重心。常见的模型压缩方法有：</p>
<ul>
<li>Quantization: 即模型量化，训练时通常采用 <code>FP32</code> 精度，但部署时可降低精度至 <code>FP16</code> 甚至 <code>Int8</code>，以此来实现加速的效果</li>
<li>Network Pruning: 即模型剪枝；因神经网络参数众多，因此模型存在非常大的冗余，可对权重值较小的分支直接删除，然后 <code>finetune</code>；比较有代表性的例子是砍掉 <code>VGG</code> 网络的 <code>fc layers</code>，并附加 <code>GAP layer</code> 训练之，参数量能降低一大半，而精度并不会有太大的损失</li>
<li>Compact Network Design: 即直接设计轻量级模型，代表作是 <code>MobileNet</code> 和 <code>ShuffleNet</code>，以及近些年比较火热的基于 <code>Neural Architecture Search</code> 方法直接搜一个最合适的轻量级模型</li>
<li>Knowledge Distillation: 即本文的主题——知识蒸馏，核心思想是先训练一个性能比较强的大模型，称之为 <code>teacher model</code>，然后来 teach 一个小模型，称之为 <code>student model</code>。通过迫使 <code>student model mimic teacher model&#39;s behavior</code>，从而最终让 <code>student model</code> 接近甚至超过 <code>teacher model</code> 的精度</li>
</ul>
<p>关于 <code>Quantization/Pruning</code>，我后面会专门再写两个相关的专题进行介绍，此处不再赘述。关于 <code>Compact Network Design</code>，可参考我之前的文章：<a href="https://lucasxlu.github.io/blog/2019/10/20/dl-architecture/">Architecture</a>。<code>Quantization/Pruning</code> 通常会带来一定程度的精度损失，即牺牲一定的精度换取推理速度，典型案例是 TikTok 在移动端的部署，人脸生成特效算法 GAN 网络与当前的 SOTA 相比还是相去甚远，但是要让模型在千元机上也能流畅运行起来，从商业角度来说则更为重要。而 <code>Knowledge Distillation</code> 虽然也有一定程度的精度损失（毕竟小模型的学习能力不如大模型强），但通过算法的改进，有时候小模型甚至能超过大模型的效果。</p>
<h2 id="What-is-KD"><a href="#What-is-KD" class="headerlink" title="What is KD?"></a>What is KD?</h2><p>提到 KD 就不得不介绍 Hinto 的经典 paper，<a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="noopener">Distilling the knowledge in a neural network</a>，算得上是 KD 算法的开山之作，核心 idea 也非常简单。以分类问题为例，$z_i$代表 logits，$q_i$ 代表 softmax layer 输出的概率：<br>$$<br>q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}<br>$$<br>其中 <code>T</code> 代表 <code>temperature</code>，若$T=1$，则与原始的 softmax 函数完全一样； <code>T</code> 值越大 probability distribution 越平滑。最终的 loss function 为 teacher model soft target 与 student model soft target 的 cross entropy loss，以及 student model hard target 与 gt label 的 cross entropy loss 的加权。</p>
<blockquote>
<p>附加一份 Hinto 的 Slide <a href="https://www.ttic.edu/dl/dark14.pdf" target="_blank" rel="noopener">Dark Knowledge</a>。</p>
</blockquote>
<h2 id="Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization"><a href="#Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization" class="headerlink" title="Revisiting Knowledge Distillation via Label Smoothing Regularization"></a>Revisiting Knowledge Distillation via Label Smoothing Regularization</h2><blockquote>
<p>Paper: <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Revisiting Knowledge Distillation via Label Smoothing Regularization</a></p>
</blockquote>
<p>这篇 paper 主要探讨了 KD 与 Label Smoothing 的关系，并且通过实验表明了 teacher model 并非一定要比 student model 强，一个性能比较差的 teacher model 也能够带来 performance gain，甚至 student model 来 teach teacher model 也能给 teacher model 带来 performance gain。说明 teacher model 不仅仅能起到提供 similarity information 的作用，还能起到 <strong>regularization</strong> 的作用（即 learnable label smoothing regularization）。基于此，作者提出了一个 teacher-free 的 KD 框架，即 student model 以自己作为 teacher model (aka, self-training)，或者从任意一个 manually-designed regularization distribution 中进行学习，能取得比传统 KD 更佳的学习效果。作者也在 paper 中 challenge 了一把 <strong>a strong teacher model 的必要性</strong>。笔者认为本文还是具备比较高价值的参考意义，下面进行细致讲解。</p>
<h3 id="LSR"><a href="#LSR" class="headerlink" title="LSR"></a>LSR</h3><p>先回顾一下 <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Label Smoothing</a>:<br>一个分类网络在过 softmax layer 之后：<br>$$<br>p(k|x)=\frac{exp(z_k)}{\sum_{i=1}^K exp(z_i)}<br>$$<br>这里 $z_i$ 为 logit，网络预测 $x$ 为 label $k$ 的概率为 $p(k|x)$，groundtruth label over the distribution 为 $q(k|x)$。对于分类问题，我们通常采用 cross entropy loss 来进行训练: $H(q, p)=-\sum_{k=1}^K q(k|x)log(p(k|x))$，对于 groundtruth label $y$，$q(y|x)=1$；当 $k\neq y$ 时 $q(k|x)=0$。</p>
<p>Label Smoothing 算法修改了 $q(k|x)$ 为 $q^{‘}(k|x)$：<br>$$<br>q^{‘}(k|x)=(1-\alpha)q(k|x) + \alpha u(k)<br>$$<br>即 $q^{‘}(k|x)$ 成了 $q(k|x)$ 与一个 fixed distribution $\alpha u(k)$ 的加权和。通常地，我们设定 $u(k)$ 为 uniform distribution，即 $u(k)=\frac{1}{K}$。因此，Label Smoothing Cross Entropy 定义为：<br>$$<br>H(q^{‘},p)=-\sum_{k=1}^K q^{‘}(k)log p(k)=(1-\alpha)H(q,p)+\alpha H(u,p)=(1-\alpha)H(q,p)+\alpha (D_{KL}(u,p) + H(u))<br>$$<br>$D_{KL}$ 代表 KL-divergence，$H(u)$ 代表 $u$ 的 entropy，是一个固定常数。因此，Label Smoothing 又可以写成：<br>$$<br>\mathcal{L}_{LS}=(1-\alpha)H(q,p)+\alpha D_{KL}(u,p)<br>$$</p>
<p>同样地，在 KD 中，$p_{\tau}^t=softmax(z_k^t)=\frac{exp(z_k^t/\tau)}{\sum_{i=1}^K exp(z_i^t/\tau)}$，$z^t$ 是 teacher model 输出的 logit。KD 的目的就在于 让 student model 通过优化 cross entropy loss 以及 KL divergence 来学习 teacher model 的信息：<br>$$<br>\mathcal{L}_{KD}=(1-\alpha)H(q,p)+\alpha D_{KL}(p_{\tau}^t,p_{\tau})<br>$$</p>
<p>重点来了，看看 $\mathcal{L}_{LS}$ 与 $\mathcal{L}_{KD}$，发现两者的唯一区别在于 $D_{KL}(p_{\tau}^t,p_{\tau})$ 中的 $p_{\tau}^t(k)$ 是来自 teacher model 的 distribution；而 $D_{KL}(u,p)$ 中的 $u(k)$ 是一个 predefined uniform distribution。因此，作者认为 <strong>LSR 是 KD 的一种special case</strong>。并且作者通过实验发现，temperature $\tau$ 越大，$p^t(k)$ 与 label smoothing 中的 uniform distribution $u(k)$ 越相似。</p>
<h3 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h3><p>此外，若 teacher model is unavailable 时该如何做 KD 呢？作者还提出了一个 self-training 方案：对于一个模型 $S$，首先按常规方式训练一个pretrained model $S^p$，然后在第二阶段的 KD 过程中用于优化 cross entropy 与 KL divergence:<br>$$<br>\mathcal{L}_{self}=(1-\alpha)H(q,p)+\alpha D_{KL}(p_{\tau}^t,p_{\tau})<br>$$<br>其中 $p_{\tau}^t,p_{\tau}$ 分别代表模型 $S$ 和 $S^p$ 的 output probability。</p>
<p>第二个 teacher-free KD 方案为手动设定一个100%准确的 virtual teacher model：<br>$$<br>p^d(k)=\left\{<br>\begin{aligned}<br>a,if &amp;&amp; k=c \\<br>(1-a)/(K-1),if &amp;&amp; k\neq c<br>\end{aligned}<br>\right.<br>$$<br>其中 $K$ 代表类别数量，$c$ 代表正确标签。此时 loss function 定义为：<br>$$<br>L_{reg}=(1-\alpha)H(q,p)+\alpha D_{KL}(p_{\tau}^d,p_{\tau})<br>$$</p>
<blockquote>
<p>这个算法我自己在项目里也试过，在一些分类任务上确实能带来比较明显的提升，具体实验数据在此不赘述了，请阅读原文。</p>
</blockquote>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li>Hinton G, Vinyals O, Dean J. <a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="noopener">Distilling the knowledge in a neural network</a>[J]. arXiv preprint arXiv:1503.02531, 2015.</li>
<li>Yuan L, Tay F E H, Li G, et al. <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Revisiting Knowledge Distillation via Label Smoothing Regularization</a>[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3903-3911.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <div id="donation_div"></div>

<script src="/blog/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: 'Donate', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/WeChatPay.png',
  alipayImage: 'https://raw.githubusercontent.com/lucasxlu/blog/master/source/images/Alipay.jpg'
});
</script>
      
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>Post author:  </strong>LucasX</a>
          </li>
          <li class="post-copyright-link">
          <strong>Post link:  </strong>
          <a href="/blog/2020/11/14/dl-kd/" target="_blank" title="[DL] Knowledge Distillation">https://lucasxlu.github.io/blog/2020/11/14/dl-kd/</a>
          </li>
          <li class="post-copyright-license">
            <strong>Copyright Notice:   </strong>
            All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            unless stating additionally.
          </li>
         
        </ul>
<div>

      
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
      

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/blog/2020/09/25/db-mongodb/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[DB] MongoDB</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="toc-sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-KD"><span class="nav-number">2.</span> <span class="nav-text">What is KD?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization"><span class="nav-number">3.</span> <span class="nav-text">Revisiting Knowledge Distillation via Label Smoothing Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSR"><span class="nav-number">3.1.</span> <span class="nav-text">LSR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-training"><span class="nav-number">3.2.</span> <span class="nav-text">Self-training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol>
    
    </div>
  </aside>

</section>
        
      </div>
      
      <footer id="footer">
  

  <div class="container">
      	<div class="row">
	      <p> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/iTimeTraveler/hexo-theme-hiker" target="_blank">Hexo-theme-hiker</a> </p>
	      <p id="copyRightEn">Copyright &copy; 2018 - 2022 LucasX All Rights Reserved.</p>
	      
	      
    		<p class="busuanzi_uv">
				UV : <span id="busuanzi_value_site_uv"></span> |  
				PV : <span id="busuanzi_value_site_pv"></span>
		    </p>
  		   
		</div>

		
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");
    var allheader = document.getElementById("allheader");

    wrapdiv.style.minHeight = document.body.offsetHeight + "px";
    if (allheader != null) {
      contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    } else {
      contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
    }
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/about" class="mobile-nav-link">About</a>
  
    <a href="/blog/papers" class="mobile-nav-link">Papers</a>
  
    <a href="/blog/projects" class="mobile-nav-link">Projects</a>
  
    <a href="/blog/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/scripts.js"></script>




  <script src="/blog/js/dialog.js"></script>








	<div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h2 class="modal-title" id="myModalLabel">设置</h2>
      </div>
      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">


      <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
          <div class="panel-body">
            您已调整页面字体大小
          </div>
        </div>
      


          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
        </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
          <div class="panel-body">
            夜间模式已经开启，再次单击按钮即可关闭 
          </div>
        </div>

        <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
         <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
          <div class="panel-body">
            LucasX
          </div>
          <div class="panel-body">
            Copyright © 2022 LucasX All Rights Reserved.
          </div>
        </div>
      </div>


      <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
      <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
      <div class="modal-footer">
        <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
      </div>
    </div>
  </div>
</div>
  
  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
  
    <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</body>
</html>