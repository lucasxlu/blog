---
title: "[ML] SVM"
mathjax: true
date: 2018-07-22 12:42:39
tags:
- Machine Learning
- Data Science
catagories:
- Algorithm
- Machine Learning
---
## 介绍
SVM是一种非常经典的分类算法，也是很多机器学习面试中必问的算法。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习策略就是间隔最大化，等价于 __正则化的Hinge Loss最小化问题__。

当训练数据线性可分时，通过硬间隔最大化学习一个线性的分类器；  
当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器；  
当训练数据线性不可分时，通过Kernel Tricks和软间隔最大化学习一个非线性的分类器；

当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示 __将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过使用核函数可以学习非线性支持的SVM，等价于隐式地在高维的特征空间中学习线性SVM__。

## 线性可分SVM与硬间隔最大化
一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w\cdot x+b=0$确定的情况下，$|w\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\cdot x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用量$y(w\cdot x+b)$来表示分类的正确性及确信程度，此为 __"函数间隔"__。

* 函数间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:  
  $\hat{\gamma}_i=y_i(w\cdot x_i + b)$

  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  
  $\hat{\gamma}=\mathop{min} \limits_{i=1,\cdots,N}\hat{\gamma}_i$

  函数间隔可以表示分类预测的正确度及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面没有变，但是函数间隔却变为原来的2倍。这一事实启示我们，可以对分离超平面的法向量$w$加某些约束，如归一化$||w||=1$，使得间隔是确定的。这时函数间隔成为 __几何间隔__。

* 几何间隔：对于给定的训练集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：  
  $\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})$

  定义超平面$(w,b)$关于训练集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔的最小值，即：  
  $\hat{\gamma}=\mathop{min} \limits_{i=1,\cdots,N}\hat{\gamma}_i$

  超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。

  函数间隔和几何间隔有如下关系：  
  $\gamma_i=\frac{\hat{\gamma}_i}{||w||}$

  $\gamma=\frac{\hat{\gamma}}{||w||}$

  __如果$||w||=1，那么函数间隔和几何间隔相等$__。如果超平面参数$w$和$b$成比例地改变(超平面未变)，则函数间隔也按此比例改变，但是几何间隔不变。

  