---
title: "[ML] Decision Tree"
date: 2018-07-24 10:02:19
mathjax: true
tags:
- Machine Learning
- Data Science
catagories:
- Algorithm
- Machine Learning
---
## Introduction
决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。

## 决策树学习
__决策树学习的Loss Function通常是正则化的极大似然函数__，决策树学习的策略是以Loss Function最小化为目标函数的最小化。

决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。

### 特征选择
* Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：  
  $P(X=x_i)=p_i$
    
  则随机变量$X$的Entropy定义为：  
  $H(X)=-\sum_{i=1}^n p_ilogp_i$

  由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：  
  $H(p)=-\sum_{i=1}^n p_ilogp_i$

  Entropy越大，随机变量的不确定性就越大，$0\leq H(p)\leq logn$

  当随机变量只取两个值，即$X$的分布为：  
  $P(X=1)=p, P(X=0)=1-p$  
  Entropy为：  
  $H(p)=-plog_2p-(1-p)log_2(1-p)$

* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：  
  $H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i), \quad p_i=P(X=x_i)$

* 信息增益表示 __得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度__：  
  定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：  
  $g(D,A)=H(D)-H(D|A)$  

  一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，__决策树学习中的信息增益等价于dataset中类与特征的互信息__。

决策树学习应用 __信息增益__ 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

#### 信息增益算法
设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：
1. 计算数据集$D$的经验熵$H(D)$：  
   $H(D)=\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}$

2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：  
   $H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2 \frac{|D_{ik}|}{|D_i|}$

3. 计算信息增益：  
   $g(D,A)=H(D)-H(D|A)$

以信息增益作为划分训练数据集的特征，存在 __偏向于选择取值较多的特征的问题__。使用信息增益比可以对这一问题进行校正。

* 信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：  
  $g_R(D,A)=\frac{g(D,A)}{H_A(D)}$  
  其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。

## 决策树的生成

