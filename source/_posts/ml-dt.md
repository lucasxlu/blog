---
title: "[ML] Decision Tree"
date: 2018-07-24 10:02:19
mathjax: true
tags:
- Machine Learning
- Data Science
catagories:
- Algorithm
- Machine Learning
---
## Introduction
决策树也是机器学习中一种非常经典的分类和回归算法，在工业界有着非常广泛的应用，并且有着非常好的可解释性。

## 决策树学习
__决策树学习的Loss Function通常是正则化的极大似然函数__，决策树学习的策略是以Loss Function最小化为目标函数的最小化。

决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。

### 特征选择
* Entropy是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：  
  $P(X=x_i)=p_i$
    
  则随机变量$X$的Entropy定义为：  
  $H(X)=-\sum_{i=1}^n p_ilogp_i$

  由定义可知Entropy只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的Entropy记作$H(p)$：  
  $H(p)=-\sum_{i=1}^n p_ilogp_i$

  Entropy越大，随机变量的不确定性就越大，$0\leq H(p)\leq logn$

  当随机变量只取两个值，即$X$的分布为：  
  $P(X=1)=p, P(X=0)=1-p$  
  Entropy为：  
  $H(p)=-plog_2p-(1-p)log_2(1-p)$

* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：  
  $H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i), \quad p_i=P(X=x_i)$

* 信息增益表示 __得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度__：  
  定义：特征$A$对dataset $D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：  
  $g(D,A)=H(D)-H(D|A)$  

  一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为“互信息”，__决策树学习中的信息增益等价于dataset中类与特征的互信息__。

决策树学习应用 __信息增益__ 准则选择特征，给定dataset $D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么他们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法是：对training set $D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

#### 信息增益算法
设training set为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$。设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。于是信息增益算法如下：
1. 计算数据集$D$的经验熵$H(D)$：  
   $H(D)=\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}$

2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：  
   $H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2 \frac{|D_{ik}|}{|D_i|}$

3. 计算信息增益：  
   $g(D,A)=H(D)-H(D|A)$

以信息增益作为划分训练数据集的特征，存在 __偏向于选择取值较多的特征的问题__。使用信息增益比可以对这一问题进行校正。

* 信息增益比：特征$A$对training set $D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：  
  $g_R(D,A)=\frac{g(D,A)}{H_A(D)}$  
  其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数。

## 决策树的生成
### ID3算法
ID3算法的核心是在决策树各个结点上应用 __信息增益__ 准则进行特征选择，递归地构建决策树。具体做法是：从根节点开始对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。__ID3相当于用极大似然法进行概率模型的选择__。

1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；
2. 若特征集$A=\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；
3. 否则，计算$A$中各个特征对$D$的信息增益，选择 __信息增益__ 最大的特征$A_g$；
4. 若$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；
5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；
6. 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。

ID3算法只有树的生成，所以该算法生成的树容易过拟合。

### C4.5
C4.5算法在生成的过程中，用 __信息增益比__ 来选择特征。

1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将$C_k$作为该结点的类标记，返回决策树$T$；
2. 若特征集$A=\varnothing$，则$T$为单结点树，并将$D$中实例数最大的的类$C_k$作为该结点的类标记，返回$T$；
3. 否则，计算$A$中各个特征对$D$的 __信息增益比__，选择 __信息增益比__ 最大的特征$A_g$；
4. 若$A_g$的信息增益比小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；
5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；
6. 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用 1~5 步，得到子树$T_i$，返回$T_i$。

## 决策树的剪枝
决策树的剪枝往往通过极小化决策树整体的Loss Function来实现，设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_t(T)$为叶节点$t$上的经验熵，$\alpha\geq 0$为参数，则决策树学习的Loss Function可以定义为：  
$C_{\alpha}(T)=\sum_{i=1}^{|T|}N_t H_t(T) + \alpha |T|$  
其中经验熵为：  
$H_t(T)=-\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$  
在Loss Function中，将式子右端的第一项记作：  
$C(T)=\sum_{i=1}^{|T|}N_t H_t(T)=-\sum_{i=1}^{|T|}\sum_{k=1}^K N_{tk}log \frac{N_{tk}}{N_t}$  
这时有：  
$C_{\alpha}(T)=C(T)+\alpha|T|$

$C(T)$表示模型对训练数据的预测误差，即模型对训练数据的拟合程度；$|T|$表示模型复杂度，参数$\alpha\geq 0$控制两者之间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型。

可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，而决策树剪枝通过优化Loss Function还考虑了减小模型复杂度。决策树生成学习局部模型，决策树剪枝学习整体模型。

Loss Function的极小化等价于正则化的极大似然估计，所以，利用Loss Function最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。

### 剪枝算法
1. 计算每个节点的经验熵
2. 递归地从树的叶节点向上回缩：  
   设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的Loss Function值分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果 $C_{\alpha}(T_A)\leq C_{\alpha}(T_B)$，则进行剪枝，即将父节点变为新的叶节点。
3. 返回2，直到不能继续为止，得到Loss Function最小的子树$T_{\alpha}$。

## CART算法

